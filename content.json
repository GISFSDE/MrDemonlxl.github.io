{"pages":[{"title":"","text":"【个人简介】 分享不知出处的一段话： &quot;你所有的烦恼都在于你想得太多，而做得太少。&quot; 无论多么精美浩瀚的绝世计划，都应该点点滴滴赋予行动。 在这里你可以得到什么? 😎 效率的种种利器！ 😎 Java全栈技术学习！ 😎 代码之外的丰富生活！ 【总内容导图】 【关于魑魅先生】 【动漫人物介绍】 魑魅魍魉（chī mèi wǎng liǎng），形形色色妖魔鬼怪，现通指坏人。而魑魅先生乃妖中之首，掌控妖魔万物，专克魑魅魍魉，其面容清秀，千变万化，善学好思矣。其间见霸戈【BUG】，祸害人世间，便习编程术，从此不归路。 绘画作者: @YYu 【联系方式】 如有任何交流需求，请添加个人微信或个人公众号 添加前请备注来由。(๑‾ ꇴ ‾๑) 微信扫码或点击链接关注我哦︿(￣︶￣)︿ 凡事预则立，不预则废。 计划 2021-PLANS DAY 读书 每日算法 每日英语 理财，记账 WEEK 绘画 吉他 运动两次 练字 MONTH 总结 YEAR 熟练一个以上技能 目标 2021-GOALS 读书 《社会心理学》 《大问题：简明哲学导论》 《局外人》 《三体》 《我是猫》 专业 设计模式代码Demo 网络 JVM VUE项目 Linux 音乐 吉他谱扒谱学习 运动 画画 时间轴记录 本站推荐索引 技术知识点 Java并发知识点 法律法规 法律法规数据库 中华人民共和国国旗法 中华人民共和国宪法 中华人民共和国消费者权益保护法 中华人民共和国刑事诉讼法 中华人民共和国婚姻法 中华人名共和国网络安全法 中华人民共和国劳动法 其他 网易云音乐歌单分享","link":"/about/index.html"},{"title":"","text":"🎈🎈摄影-平面设计-绘画欣赏🎈🎈 每天看一点，审美高一点 😊薇薇安·迈尔😊 ​ 薇薇安·迈尔（Vivian Maier），1926年2月1日出生于美国纽约，美国业余街头摄影师、家庭保姆。薇薇安是法国人后裔，出生在纽约，但在法国长大，后回到美国先后生活在纽约和芝加哥，她一生拍摄了超过10万张照片。2007年，芝加哥当地历史学家约翰·马鲁夫发现了她的大量底片并开始整理，此后她的作品登上美国以至意大利、阿根廷和英国等地的报纸。2009年4月21日病逝于芝加哥。2010年，薇薇安的作品开始在芝加哥进行展出，成为摄影圈中热议的人物，并被认可为美国当代最重要的街头摄影师之一。 图片部分搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://mrdemonlxl.github.io/images/avatar.jpg 网站名称：魑魅先生 网站地址：https://mrdemonlxl.github.io/ 网站简介：Java全栈开发，技术分享，程序员的生活分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &nbsp;&nbsp;看看视频 ->点击以下条目开始播放视频,向下滑动查看更多","link":"/media/index.html"},{"title":"","text":"好问则裕，自用则小。——《尚书·仲虺之诰》","link":"/message/index.html"},{"title":"音乐歌单收藏","text":"个人学习食用，如有推荐请留言歌单或歌曲链接。点击左下角箭头可全局播放哦 - 🤲😎 温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"碎碎念树洞 tips：github登录后按时间正序查看、可点赞加❤️「+99次查看」 碎碎念加载中，请稍等... $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ clientID: '20ef8321698ab8c0c37b', clientSecret: '237515b7faa3e1097f2980762f772c6f02dbcbbb', id: '666666', repo: 'MrDemonlxl.github.io', owner: 'MrDemonlxl', admin: \"MrDemonlxl\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container1'); });","link":"/self-talking/index.html"}],"posts":[{"title":"TODO","text":"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 嗨，请准确无误地输入密码查看哟！","link":"/2021/02/24/Draft/2021/TODO/"},{"title":"JVM 与上层技术","text":"向下扎深，向上扩展 新学四问 WHY【与前代优化了什么，弥补了什么空白】：了解底层，优化，面试，解决底层BUG WHAT【框架，思维导图，主题框架】：结构， HOW【如何记忆，学习资源】：学习资源：尚硅谷JVM，ANKI记忆，文档 LEVEL【不是每个都学精】：了解 进度：上篇 【完12.19】 综合问题 类加载过程 整体结构 如何判断一个对象是否存活 垃圾回收算法 垃圾回收器 深拷贝浅拷贝 双亲委派机制 StringTable 优化方法 上篇：内存与垃圾回收 速查 -XX选项表 堆 选项名 选项作用 默认值 备注 -XX: +PrintFlagsInitial 查看所有的参数的默认初始值 -XX: +PrintFlagsFinal 查看所有的参数的最终值（可能会存在修改，不再是初始值) jps 查看当前运行中的进程 jinfo -flag 参数名 进程id 当前进程中参数值 -Xms 初始雄空向内存 物理内存的1/64 -Xmx :最大雄空间内存 物理内存的1/4 -Xmn :设置新生代的大小。（初始值及最大值） -XX: NewRatio :配置新生代与老年代在堆结构的占比 新老比：1:2 -XX: SurvivorRatio :设置新生代中Eden和S0/S1空间比例 8 -XX: MaxTenuringThreshold :设置新生代垃扱的最大年龄 -XX: +PrintGCdetails :输出详细GC处理日志 🌞 -XX: +PrintGC 🌞 - verbose:GC :打印筒要信息 -XX: HandlePromotionFailure :是否设置空问分配担保 -XX: Doescapeanalysis 显式开启逃逸分析 JDK 6u23后默认开启 -XX: Printescapeanalysis 查看逃逸分析的筛选结果 -XX:+EliminateAllocations :开启标量替换，允许将对象打散分配在栈上。 默认打开 方法区 选项名 选项作用 默认值 备注 垃圾回收器 选项名 选项作用 默认值 备注 一、JVM与JAVA体系结构 1.JAVA与JVM Java大事件 1990年，在sun公司中，由Patrick naughton、mikesheridan以及james Gosling领导的小组Green Team，开发出新的程序语言，命名为OAK，后期更名为Java 1995年，sun正式发布Java和hotJAVA产品，Java首次公开亮相。 1996年1月23日sun Microsystems发布了JDK1.0. 1998年，JDK1.2版本发布。同时，sun发布了JSP/Servlet、EJB规范，以及将Java分成了J2EE、J2SE和J2ME。这表明Java开始向企业、桌面应用和移动设备应用3大领域挺进。 2000年，JDK1.3发布，Java HotSpot Virtual Machine正式发布，成为Java默认的虚拟机。 2002年，JDK 1.4发布，古老的classic虚拟机退出历史舞台。 2003年底，Java平台的scala正式发布，同年Groovy也加入了Java阵营。 2004年，JDK1.5发布，同时JDK1.5改名为JavaSE5.0. 2006年，JDK 6发布，同年Java开源并建立了openJDK，顺理成章，Hotspot虚拟机成为了OpenJDK中默认的虚拟机。 2007年，Java平台迎来了新伙伴Clojure。 2008年，Oracle收购了BEA，得到了JRockit虚拟机。 2009年，Twitter宣布将后台大部分程序从ruby迁移到Scala，这是Java平台的有一次大规模应用。 2010年，Oracle收购了sun，获得Java商标和最具价值的hotspot虚拟机。此时Oracle拥有市场占用率最高的两款虚拟机hotspot和JRockit，并且计划未来进行整合：HotRockit。 2011年，JDK7发布，在JDK１.7ｕ4中，正式启用了新的垃圾回收器G1. 2017年，JDK9发布，将G1设置为默认GC，替代CMS。 2017同年，IBM的J9开源，形成了现在的open J9社区。 2018年，Android的Java侵权案判决，Google公司赔偿Oracle总计88亿美元。 2018同年，Oracle宣布JavaEE成为历史名词，JDBC、JMS、Servlet赠与Eclipse基金会。 2018同年，JDK11发布，LTS版本的JDK，发布革命性的ZGC，调整JDK授权许可。 2019年，JDK12发布，加入RedHat领导开发的shenandoah GC。 在JDK11之前，OracleJDK还会存在一些openJDK中没有的、闭源的功能。但在JDK11中，openJDK和OracleJDK代码实质上已经达到完全一致的程度。 JVM介绍 ​ 所谓虚拟机(Virtual Machine)，就是一台虚拟的计算机。它是一款软件，用来执行一系列虚拟计算机指令。大体上，虚拟机可以分为系统虚拟机和程序虚拟机。大名鼎鼎的Visual Box,VMware就属于系统虚拟机，它们完全是对物理计算机的仿真，提供了一个可运行完整操作系统的软件平台。程序虚拟机的典型代表就是Java虚拟机，它专门为执行单个计算机程序而设计，在Java虚拟机中执行的指令我们称为Java字节码指令。无论是系统虚拟机还是程序虚拟机，在上面运行的软件都被限制于虚拟机提供的资源中。 ​ Java虚拟机是一台执行Java字节码的虚拟计算机，它拥有独立的运行机制,其运行的Java字节码也未必由Java语言编译而成。JVM平台的各种语言可以共享Java虚拟机带来的跨平台性、优秀的垃圾回器，以及可靠的即时编译器。Java技术的核心就是Java虚拟机（JVM，Java Virtual Machine) ,因为所有的Java程序都运行在Java虚拟机内部。 **作用：**Java虚拟机就是二进制字节码的运行环境，负责装载字节码到其内部，解释/编译为对应平台上的机器指令执行。每一条Java指令，Java虚拟机规范都有详细定义，如怎么取操作数，怎么处理操作数，处理结果放在哪里。 **特点：**一次编译，到处运行；自动内存管理；自动垃圾回收功能 位置： JVM整体结构 Java代码执行流程 ​ JVM架构模型 Java编译器输入的指令流基本上是一种基于栈的指令集架构，另外一种指令集架构则是基于寄存器的指令集架构。 区别: ·基于栈式架构的特点 设计和实现更简单，适用于资源受限的系统; 避开了寄存器的分配难题:使用零地址指令方式分配。 指令流中的指令大部分是零地址指令，其执行过程依赖于操作栈。指令集更小，编译器容易实现。 不需要硬件支持，可移植性更好，更好实现跨平台 基于寄存器架构的特点 典型的应用是x86的二进制指令集:比如传统的PC以及Android的Davlik虚拟机。 指令集架构则完全依赖硬件,可移植性差 性能优秀和执行更高效; 花费更少的指令去完成一项操作。 在大部分情况下，基于寄存器架构的指令集往往都以一地址指令、二地址指令和三地址指令为主，而基于栈式架构的指令集却是以零地址指令为主。 总结: 由于跨平台性的设计，Java的指令都是根据栈来设计的。不同平台CPU架构不同，所以不能设计为基于寄存器的。优点是跨平台，指令集小，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。时至今日，尽管嵌入式平台已经不是Java程序的主流运行平台了（准确的来说HotSpotVM的宿主环境已经不局限于嵌入式平台了)，那么为什么不将架构更换为基于寄存器的架构呢? 栈: 跨平台性、指令集小、指令多;执行性能比寄存器差 JVM生命周期 启动 Java虚拟机的启动是通过引导类加载器(bootstrap class loader)创建一个初始类(initial class)来完成的，这个类是由虚拟机的具体实现指定的 执行 一个运行中的Java虚拟机有着一个清晰的任务:执行Java程序。 程序开始执行时他才运行，程序结束时他就停止。 执行一个所谓的Java程序的时候，真真正正在执行的是一个叫做Java虚拟机的进程。 退出 有如下的几种情况:。程序正常执行结束 ·程序在执行过程中遇到了异常或错误而异常终止·由于操作系统出现错误而导致Java虚拟机进程终止 ·某线程调用Runtime类或system类的exit方法，或Runtime类的halt方法，并且Java安全管理器也允许这次exit或halt操作。 ·除此之外，JNI ( Java Native Interface)规范描述了用JNI Invocation API来加载或卸载Java虚拟机时，Java虚拟机的退出情况。 JVM发展历程 SUN Classic VM ·早在1996年Java1.0版本的时候，sun公司发布了一款名为Sun Classic VM的Java虚拟机，它同时也是世界上第一款商用Java虚拟机，JDK1.4时完全被淘汰。 ·这款虚拟机内部只提供解释器。 ·如果使用JIT编译器，就需要进行外挂。但是一旦使用了JIT编译器JIT就会接管虚拟机的执行系统。解释器就不再工作。解释器和编译器不能配合工作。 ·现在hotspot内置了此虚拟机。 Exact VM ·为了解决上一个虚拟机问题，jdk1.2时，sun提供了此虚拟机。 Exact Memory Management:准确式内存管理 ·也可以叫Non-conservative/Accurate Memory Management：虚拟机可以知道内存中某个位置的数据具体是什么类型。 具备现代高性能虚拟机的雏形 ·热点探测 ·编译器与解释器混合工作模式I 只在solaris平台短暂使用，其他平台上还是classic vm ·英雄气短，终被Hotspot虚拟机替换 SUN公司的 HotSpot VM HotSpot历史 ·最初由一家名为“Longview Technologies&quot;的小公司设计1997年，此公司被sun收购;2009年，sun公司被甲骨文收购。 ·JDK1.3时，HotSpot VM成为默认虚拟机 ·目前Hotspot占有绝对的市场地位，称霸武林。 不管是现在仍在广泛使用的JDK6，还是使用比例较多的JDK8中，默认的虚拟机都是HotSpot sun/ oracle JDK 和 OpenJDK的默认虚拟机 因此本课程中默认介绍的虚拟机都是HotSpot，相关机制也主要是指HotSpot的GC机制。(比如其他两个商用虚拟机都没有方法区的概念) ·从服务器、桌面到移动端、嵌入式都有应用。 ·名称中的HotSpot指的就是它的热点代码探测技术。 通过计数器找到最具编译价值代码，触发即时编译或栈上替换 通过编译器与解释器协同工作，在最优化的程序响应时间与最佳执行性能中取得平衡 BEA 的JRockit ·专注于服务器端应用 它可以不太关注程序启动速度，因此JRockit内部不包含解析器实现，全部代码都靠即时编译器编译后执行。 ·大量的行业基准测试显示，JRockit JVM是世界上最快的JVM。 使用JRockit产品，客户已经体验到了显著的性能提高（一些超过了70% ）和硬件成本的减少(达50%）。 ·优势:全面的Java运行时解决方案组合 JRockit面向延迟敏感型应用的解决方案JRockit Real Time提供以毫秒或微秒级的JVM响应时间，适合财务、军事指挥、电信网络的需要MissionControl服务套件，它是一组以极低的开销来监控、管理和分析生产环境中的应用程序的工具。 2008年，BEA被oracle收购。 oracle表达了整合两大优秀虚拟机的工作，大致在JDK 8中完成。整合的方式是在HotSpot的基础上，移植JRockit的优秀特性。 ·高斯林:目前就职于谷歌，研究人工智能和水下机器人 IBM 的J9 全称:IBM Technology for Java virtual Machine，简称IT4J，内部代号:J9 ·市场定位与HotSpot接近，服务器端、桌面应用、嵌入式等多用途VM。 ·广泛用于IBM的各种Java产品。 ·目前，有影响力的三大商用虚拟机之一，也号称是世界上最快的Java虚拟机。 ·2017年左右，IBM发布了开源J9 VM，命名为openJ9，交给Eclipse基金会管理，也称为Eclipse OpenJ9 KVM和cDC/CL.DC Hotspot oracle在Java ME产品线上的两款虚拟机为:CDC/CLDC HotSpot Implementation VM KVM (Kilobyte）是CLDC-HI早期产品 ·目前移动领域地位尴尬，智能手机被Android和ioS二分天下。 .KVM简单、轻量、高度可移植，面向更低端的设备上还维持自己的一片市场 智能控制器、传感器老人手机、经济欠发达地区的功能手机 .所有的虚拟机的原则:一次编译，到处运行。 Azul VM ·前面三大“高性能Java虚拟机”使用在通用硬件平台上 ·这里Azul VM和BEA Liquid VM是与特定硬件平台绑定、软硬件配合的专有虚拟机 高性能Java虚拟机中的战斗机。 Azul VM是Azul systems公司在HotSpot基础上进行大量改进，运行于Azul systems公司的专有硬件vega系统上的Java虚拟机。 ·每个Azul VM实例都可以管理至少数十个CPU和数百GB内存的硬件资源，并提供在巨大内存范围内实现可控的GC时间的垃圾收集器、专有硬件优化的线程调度等优秀特性。 2010年，Azul systems公司开始从硬件转向软件，发布了自己的ZingJVM，可以在通用x86平台上提供接近于Vega系统的特性。 Liquid VM ·高性能Java虚拟机中的战斗机。 ·BEA公司开发的，直接运行在自家Hypervisor系统上 ·Liquid VM即是现在的JRockit VE(Virtual Edition） ,LiquidVM不需要操作系统的支持，或者说它自己本身实现了一个专用操作系统的必要功能，如线程调度、文件系统、网络支持等。 ·随着JRockit虚拟机终止开发，Liquid VM项目也停止了。 Apache Harmony Apache也曾经推出过与JDK 1.5和JDK 1.6兼容的Java运行平台Apache Harmony。 ·它是IBM和Intel联合开发的开源JVM，受到同样开源的openJDK的压制，sun坚决不让Harmony获得JCP认证，最终于2011年退役，IBM转而参与OpenJDK ·虽然目前并没有Apache Harmony被大规模商用的案例，但是它的Java类库代码吸纳进了Android SDK。 Microsoft JVM ·微软为了在IE3浏览器中支持Java Applets，开发了Microsoft JVM。·只能在window平台下运行。但确是当时windows下性能最好的Java VM.. 1997年，sun以侵犯商标、不正当竞争罪名指控微软成功，赔了sun很多钱。微软在windowsXP SP3中抹掉了其VM。现在windows上安装的jdk都是HotSpot。 TaobaoJVM ·由AliJVM团队发布。阿里，国内使用Java最强大的公司，覆盖云计算、金雷生切一电商等众多领域，需要解决高并发、高可用、分布式的复合问题。有大重的开源广的。 ·基于openJDK开发了自己的定制版本AlibabaJDK，简称AJDK。是整个阿里Java体系的基石。 ·基于openJDK HotSpot VM 发布的国内第一个优化、深度定制且开源的高性能服务器版Java虚拟机。 创新的GCIH (GC invisible heap ）技术实现了off-heap ，即将生命周期较长的Java对象从heap中移到heap之外，并且GC不能管理GCIH内部的Java 对象，以此达到降低GC的回收频率和提升GC 的回收效率的目的。 GCIH 中的对象还能够在多个Java虚拟机进程中实现共享 使用crc32指令实现JVM intrinsic降低JNI 的调用开销 PMU hardware 的Java profiling tool和诊断协助功能 针对大数据场景的ZenGC . taobao vm应用在阿里产品上性能高，硬件严重依赖intel的cpu，损失了兼容性，但提高了性能 目前已经在淘宝、天猫上线，把oracle 官方JVM版本全部替换了。 Dalvik VM : ·谷歌开发的，应用于Android系统，并在Android2.2中提供了JIT，发展迅猛。.Dalvik VM只能称作虚拟机，而不能称作“Java虚拟机”，它没有遵循 Java虚拟机规范 ·不能直接执行Java 的class 文件·基于寄存器架构，不是jvm的栈架构。 ·执行的是编译以后的dex(Dalvik Executable）文件。执行效率比较高 它执行的dex (Dalvik Executable）文件可以通过class文件转化而来，使用Java语法编写应用程序，可以直接使用大部分的Java API等。 ·Android 5.0使用支持提前编译(Ahead of Time Compilation，AOT）的ARTVM替换Dalvik VM。 Graal VM . 2018年4月，oracle Labs公开了Graal VM，号称&quot;Run Programs Faster Anywhere&quot;，勃勃野心。与1995年java的”write once，run anywhere&quot;遥相呼应。 Graal VM在HotSpot VM基础上增强而成的跨语言全栈虚拟机，可以作为“任何语言”的运行平台使用。语言包括: Java、Scala、Groovy、Kotlin;C、C++ Javascript、Ruby、Python、R等 ·支持不同语言中混用对方的接口和对象，支持这些语言使用已经编写好的本地库文件工作原理是将这些语言的源代码或源代码编译后的中间格式，通过解释器转换为能被Graal VM接受的中间表示。Graal VM提供Truffle工具集快速构建面向一种新语言的解释器。在运行时还能进行即时编译优化，获得比原生编译器更优秀的执行效率。 如果说HotSpot有一天真的被取代，Graal VM希望最大。但是Java的软件生态没有丝毫变化。 二、类加载子系统 1.内存简图 2.类加载器 ​ 类加载器子系统负责从文件系统或者网络中加载class文件，class文件在文件开头有特定的文件标识。 ClassLoader只负责class文件的加载，至于它是否可以运行，则由ExecutionEngine决定。 加载的类信息存放于一块称为方法区的内存空间。除了类的信息外，方法区中还会存放运行时常量池信息，可能还包括字符串字面量和数字常量（这部分常量信息是Class文件中常量池部分的内存映射) 3.类加载器ClassLoader角色 class file 存在于本地硬盘上，可以理解为设计师画在纸上的模板，而最终这个模板在执行的时候是要加载到JVM当中来，根据这个文件实例化出n个一模一样的实例。 class file 加载到JVM中，被称为DNA元数据模板，放在方法区。 在.class文件-&gt; JVM -&gt;最终成为元数据模板，此过程就要一个运输工具(类装载器class Loader)，扮演一个快递员的角色。 获取ClassLoader途经 4.类加载过程 加载(Loading): 1．通过一个类的全限定名获取定义此类的二进制字节流 2．将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 3．在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口 补充:加载.class文件的方式 ·从本地系统中直接加载 ·通过网络获取，典型场景: web Applet ·从zip压缩包中读取，成为日后jar、 war格式的基础·运行时计算生成，使用最多的是:动态代理技术 ·由其他文件生成，典型场景:JSP应用 ·从专有数据库中提取.class文件,比较少见 ·从加密文件中获取，典型的防class文件被反编译的保护措施 链接 验证(Verify): ·目的在于确保class文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机自身安全。 ·主要包括四种验证，文件格式验证，元数据验证，字节码验证，符号引用验证。 准备(Prepare): ·为类变量分配内存并且设置该类变量的默认初始值，即零值。 这里不包含用final修饰的static，因为final在编译的时候就会分配了，准备阶段会显式链初始化; ·这里不会为实例变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中。 解析(Resolve) : ·将常量池内的符号引用转换为直接引用的过程。 ·事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行。 ·符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《java虚拟机规范》的class文件格式中。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 ·解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的 CONSTANT Class info、CONSTANT Fieldref_info、CONSTANT _Methodref_info等.。 初始化: ·初始化阶段就是执行类构造器方法 ()的过程。 ·此方法不需定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。 ·构造器方法中指令按语句在源文件中出现的顺序执行。 . ()不同于类的构造器。(关联:构造器是虚拟机视角下的( ))·若该类具有父类，JVM会保证子类的()执行前，父类的 ()已经执行完毕。 ·虚拟机必须保证一个类的 ()方法在多线程下被同步加锁。 5.类加载器分类 JVM支持两种类型的类加载器，分别为引导类加载器（Bootstrap ClassLoader）和自定义类加载器(User-Defined ClassLoader)。 ·从概念上来讲，自定义类加载器一般指的是程序中由开发人员自定义的一类类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 ·无论类加载器的类型如何划分，在程序中我们最常见的类加载器始终只有3个，如下所示: 虚拟机自带的加载器 ·启动类加载器（引导类加载器，Bootstrap classLoader) 这个类加载使用**C/C++**语言实现的，嵌套在JVM内部。 它用来加载Java的核心库（JAVA HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容），用于提供JVM自身需要的类 并不继承自java.lang.classLoader，没有父加载器。加载扩展类和应用程序类加载器，并指定为他们的父类加载器。 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 ·扩展类加载器（Extension ClassLoader) Java语言编写，由sun.misc.Launcher$ExtClassLoader实现。派生于classLoader类 父类加载器为启动类加载器 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录（扩展目录)下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载。 ·应用程序类加载器（系统类加载器，AppClassLoader) java语言编写，由sun.misc.Launcher$AppclassLoader实现&gt;派生于classLoader类 父类加载器为扩展类加载器 它负责加载环境变量classpath或系统属性java.class.path指定路径下的类库 该类加载是程序中默认的类加载器，一般来说，Java应用的类都是由它来完成加载 通过classLoader#getSystemClassLoader ()方法可以获取到该类加载器 用户自定义类加载器 ·在Java的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互配合执行的，在必要时，我们还可以自定义类加载器，来定制类的加载方式。 .为什么要自定义类加载器? 隔离加载类 修改类加载的方式 扩展加载源 防止源码泄漏 用户自定义类加载器实现步骤: 1.开发人员可以通过继承抽象类java.lang.classLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 2.在JDK1.2之前，在自定义类加载器时，总会去继承classLoader类并重写loadClass ()方法，从而实现自定义的类加载类，但是在JDK1.2之后已不再建议用户去覆盖loadclass ()方法，而是建议把自定义的类加载逻辑写在findclass ()方法中 3．在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承URLClassLoader类，这样就可以避免自己去编写findclass ()方法及其获取字节码流的方式，使自定义类加载器编写更加简洁。 6.双亲委派机制 ​ Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，Java虚拟机采用的是双亲委派模式，即把请求交由父类处理,它是一种任务委派模式。 工作原理 1)如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行; 2)如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器; 3)如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 即向上委托，向下甩锅 双亲委派优势 避免类的重复加载 保护程序安全，防止核心API被随意篡改 自定义类:java. lang. string 自定义类: java . lang. shkStart java.lang. securityException: Prohibited package name: java.lang 沙箱安全机制 ​ 自定义string类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件(rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的是rt.jar包中的String类。这样可以保证对java核心源代码的保护，这就是沙箱安全机制。即先用java核心源码再往外层用 7.其他 ·在JVM中表示两个class对象是否为同一个类存在两个必要条件: 类的完整类名必须一致，包括包名。 加载这个类的classLoader(指classLoader实例对象)必须相同。 ·换句话说，在VM中，即使这两个类对象(class对象)来源同一个class文件，被同一个虚拟机所加载，但只要加载它们的ClassLoader实例对象不同，那么这两个类对象也是不相等的。 对类加载其的引用 JVM必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。如果一个类型是由用户类加载器加载的，那么JVM会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，JVM需要保证这两个类型的类加载器是相同的。 类的主动使用和被动使用 Java程序对类的使用方式分为:主动使用和被动使用。·主动使用，又分为七种情况: ·创建类的实例 ·访问某个类或接口的静态变量，或者对该静态变量赋值 ·调用类的静态方法 ·反射（比如: Class.forName ( &quot;com.atguigu . Test&quot;) )&gt; ·初始化一个类的子类 ·Java虚拟机启动时被标明为启动类的类 ·JDK 7开始提供的动态语言支持:java . lang.invoke.MethodHandle实例的解析结果REF getstatic、REF putstatic、REF_invokestatic句柄对应的类没有初始化，则初始化 除了以上七种情况，其他使用Java类的方式都被看作是对类的被动使用，都不会导致类的初始化。 三、运行时数据区概述及线程 1.运行时数据区内部结构 ​ 内存是非常重要的系统资源，是硬盘和CPU的中间仓库及桥梁，承载着操作系统和应用程序的实时运行。JVM内存布局规定了Java在运行过程中内存申请、分配、管理的策略，保证了JVM的高效稳定运行。不同的JVM对于内存的划分方式和管理机制存在着部分差异。结合JVM虚拟机规范，来探讨一下经典的JVM内存布局。 Error GC(Garbage Collection) OOM(OutOfMemoryError) StackOverflowError 程序计数器 ❌ ❌ ❌ 本地方法栈 ✔️ ❌ ✔️ ✔️ 虚拟机栈 ✔️ ❌ ✔️ ✔️ 堆 ✔️ ✔️ ✔️ 方法区 ✔️ ✔️ ✔️ ​ Java虚拟机定义了若干种程序运行期间会使用到的运行时数据区，其中有一些会随着虚拟机启动而创建，随着虚拟机退出而销毁。另外一些则是与线程一一对应的，这些与线程对应的数据区域会随着线程开始和结束而创建和销毁。灰色的为单独线程私有的，红色的为多个线程共享的。即:每个线程独立：包括程序计数器、栈、本地栈。每个线程共享：堆、方法区 2.线程 ​ 线程是一个程序里的运行单元。JVM允许一个应用有多个线程并行的执行。在Hotspot JVM里，每个线程都与操作系统的本地线程直接映射。 ​ 当一个Java线程准备好执行以后，此时一个操作系统的本地线程也同时创建。Java线程执行终止后，本地线程也会回收。操作系统负责所有线程的安排调度到任何一个可用的CPU上。一旦本地线程初始化成功，它就会调用Java线程中的run ()方法。 ​ 如果你使用jconsole或者是任何一个调试工具，都能看到在后台有许多线程在运行。这些后台线程不包括调用public static void main (string[])的main线程以及所有这个main线程自己创建的线程。这些主要的后台系统线程在Hotspot JVM里主要是以下几个: **虚拟机线程:**这种线程的操作是需要JVM达到安全点才会出现。这些操作必须在不同的线程中发生的原因是他们都需要JVM达到安全点，这样堆才不会变化。这种 线程的执行类型包括&quot;stop-the-world&quot;的垃圾收集，线程栈收集，线程挂起以及偏向锁撤销。 **周期任务线程:**这种线程是时间周期事件的体现(比如中断)，他们一般用于周期性操作的调度执行。 **GC线程:**这种线程对在JVM里不同种类的垃圾收集行为提供了支持。&gt;编译线程:这种线程在运行时会将字节码编译成到本地代码。 **信号调度线程:**这种线程接收信号并发送给JVM，在它内部通过调用适当的方法进行处理。 ·线程是一个程序里的运行单元。JVM允许一个应用有多个线程并行的执行。 .在Hotspot JVM里，每个线程都与操作系统的本地线程直接映射。 当一个Java线程准备好执行以后，此时一个操作系统的本地线程也同时创建。Java线程执行终止后，本地线程也会回收。 ·操作系统负责所有线程的安排调度到任何一个可用的CPU上。一旦本地线程初始化成功，它就会调用Java线程中的run ()方法。 ●守护线程、普通线程 四、程序计数器 1.Program Counter Register介绍 ​ JVM中的程序计数寄存器(Program counter Register)中， Register的命名源于CPU的寄存器，寄存器存储指令相关的现场信息。CPU只有把数据装载到寄存器才能够运行。这里，并非是广义上所指的物理寄存器，或许将其翻译为PC计数器（或指令计数器）会更加贴切(也称为程序钩子)，并且也不容易引起一些不必要的误会。JVM中的PC寄存器是对物理PC寄存器的一种抽象模拟。 作用: PC寄存器用来存储指向下一条指令的地址,也即将要执行的指令代码。由执行引擎读取下一条指令(指令代码在java栈中对应的指令)。 它是一块很小的内存空间，几乎可以忽略不记。也是运行速度最快的存储区域。 在JVM规范中，每个线程都有它自己的程序计数器，是线程私有的，生命周期与线程的生命周期保持一致。 任何时间一个线程都只有一个方法在执行，也就是所谓的当前方法。程序计数器会存储当前线程正在执行的Java方法的TVM指令地址;或者，如果是在执行native方法，则是未指定值（undefned) 。 它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令。 它是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 2.程序示例 1javap -v/verbose （class名） //反编译 3.两个问题 （1）使用PC寄存器存储字节码指令地址有什么用呢?为什么使用PC寄存器记录当前线程的执行地址呢？ 因为CPU需要不停的切换各个线程，这时候切换回来以后，就得知道接着从哪开始继续执行。 JVM的字节码解释器就需要通过改变PC寄存器的值来明确下一条应该执行什么样的字节码指令。 (2)PC寄存器为什么会被设定为线程私有? ​ 我们都知道所谓的多线程在一个特定的时间段内只会执行其中某一个线程的方法，CPU会不停地做任务切换，这样必然导致经常中断或恢复，如何保证分毫无差呢?为了能够准确地记录各个线程正在执行的当前字节码指令地址，最好的办法自然是为每一个线程都分配一个PC寄存器，这样一来各个线程之间便可以进行独立计算，从而不会出现相互千扰的情况。 ​ CPU时间片即CPU分配给各个程序的时间，每个线程被分配一个时间段，称作它的时间片。在宏观上:我们可以同时打开多个应用程序，每个程序并行不悖，同时运行。但在微观上:由于只有一个CPU，一次只能处理程序要求的一部分，如何处理公平，一种方法就是引入时间片，每个程序轮流执行。 五、虚拟机栈 1.栈主要特点 ​ 由于跨半台性的设计，Java的指令都是根据栈来设计的。不同平台CPU架构不同，所以不能设计为基于寄存器的。优点是跨平台，指令集小，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。**栈是运行时的单位，而堆是存储的单位。**即：栈解决程序的运行问题，即程序如何执行，或者说如何处理数据。堆解决的是数据存储的问题，即数据怎么放、放在哪儿。 ・Java虚拟机栈是什么？ Java虚拟机栈( Java Virtual Machine Stack),早期也叫Java栈。每个线程在创建时都会创建一个虚拟机栈，其内部保存一个个的栈帧( Stack Frame),对应着一次次的Java方法调用，是线程私有的。生命周期和线程一致。 ・作用 主管Java程序的运行，它保存方法的局部变量(8种基本数据类型、对象的引用地址)、部分结果，并参与方法的调用和返回。 局部变量vs成员变量（或属性） 基本数据变量vs引用类型变量(类、数组、接口) ・栈的特点（优点） 栈是一种快速有效的分配存储方式，访问速度仪次于程序计数器。 JVM直接对Java栈的操作只有两个： 每个方法执行，伴随着进栈(入栈、压栈) 执行结束后的出栈工作 对于栈来说不存在垃圾回收问题 2.栈中可能出现的异常 ​ Java虚拟机规范允许Java栈的大小是动态的或者是固定不变的。 ​ 如果采用固定大小的Java虚拟机栈，那每一个线程的Java虚拟机栈容量可以在线程创建的时候独立选定。如果线程请求分配的栈容量超过Java虚拟机栈允许的最大容量，Java虚拟机将会抛出一个StackOverflowError异常。 ​ 如果Java虚拟机栈可以动态扩展，并且在尝试扩展的时候无法中请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的虚拟机栈，那Java虚拟机将会抛出一个 OutOfMemoryError异常。 设置栈大小 Xss size Sets the thread stack size( in bytes). Append the letter k or K to indicate KB, m or M to indicate MB, and g or G to indicate GB. The default value depends on the platform Linux/x64(64-bit): 1024 KB macos(64-bit): 1024 KB Oracle Solaris/x64(64-bit): 1024 KB Windows: The default value depends on virtual memory The following examples set the thread stack size to 1024 KB in different units -Xss1m -Xss1024k Xss1048576 3.栈存储单位 栈中存储什么 ・每个线程都有自己的栈，栈中的数据都是以栈帧( Stack Frame)的格式存在。 ・在这个线程上正在执行的栈帧( Stack Frame) 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中的各种数据信息 复习 ・OOP的基本概念：类、对象 ・类中基本结构：field(属性、字段、域)、 method JVM直接对Java栈的操作只有两个，就是对栈帧的栈和出栈，遵循“先进后出”/“后进先出”原则。 在一条活动线程中，一个时间点上，只会有一个活动的栈帧。即只有当前正在执行的方法的栈帧（栈顶栈帧）是有效的，这个栈帧被称为当前栈帧( Current Frame),与当前栈帧相对应的方法就是当前方法（ Current Method),定义这个方法的类就是当前类( Current Class). 执行引擎运行的所有字节码指令只针对当前栈帧进行操作。 如果在该方法中调用了其他方法，对应的新的栈帧会被创建出来，放在栈的顶端，成为新的当前帧。 ・不同线程中所包含的栈帧是不允许存在相互引用的，即不可能在一个栈帧之中引用另外一个线程的栈帧 ・如果当前方法调用了其他方法，方法返回之际，当前栈帧会传回此方法的执行结果给前一个栈帧，接着，虚拟机会丢弃当前栈帧，使得前一个栈帧重新成为当前栈帧。 ・Java方法有两种返回函数的方式，一种是正常的函数返回，使用 return指令；另外一种是抛出异常。不管使用哪种方式，都会导致栈帧被弹出。 4.栈帧结构 ・局部变量表(Local variables) ・操作数栈( Operand Stack)（或表达式栈） ・动态链接( Dynamic Linking)（或指向运行时常量池的方法引用） ・方法返回地址( Return Address)（或方法正常退出或者异常退出的定义） ・一些附加信息 1.局部变量表 ・局部变量表也被称之为局部变量数组或本地变量表 ・定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量,这些数据类型包括各类基本数据类型、对象引用( reference),以及returnAddress类型。 ・由于局部变量表是建立在线程的栈上，是线程的私有数据，因此不存在数据安全问题 ・局部变量表所需的容量大小是在编译期确定下来的，并保存在方法的code属性的 maximuim local variables数据项中。在方法运行期间是不会改变局部变量表的大小的。 ・方法嵌套调用的次数由栈的大小决定。一般来说，栈越大，方法嵌套调用次数越多。对一个函数而言，它的参数和局部变量越多，使得局部变量表膨胀，它的栈帧就越大，以满足方法调用所需传递的信息増大的需求。进而函数调用就会占用更多的栈空间，导致其嵌套调用次数就会减少。 ・局部变量表中的变量只在当前方法调用中有效。在方法执行时，虚拟机通过使用局部变量表完成参数值到参数变量列表的传递过程。当方法调用结束后，随着方法栈帧的销毁，局部变量表也会随之销毁。 2.字节码中方法内部结构解析 3.Slot ・参数值的存放总是在局部变量数组的 index0开始，到数组长度-1的索引结束 ・局部变量表，最基本的存储单元是Slot（变量槽） ・局部变量表中存放编译期可知的各种基本数据类型(8种)，引用类型( reference), returnAddressa类型。 ・在局部变量表里，32位以内的类型只占用一个slot(包括returnAddressa类型），64位的类型(long和 double)占用两个slot. byte、 short、char在存储前被转换为int, boolean也被转换为int,0表示 false,非0表示true。 long和double则占据两个Slot。 ・JVM会为局部变量表中的每一个Slot都分配一个访问素引，通过这个素引即可成功访问到局部变量表中指定的局部变量值. ・当一个实例方法被调用的时候，它的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个Slot上. ・如果需要访问局部变量表中ー个64bit的局部变量值时，只需要使用前一个素引即可。(比如：访问1ong或doub1e类型变量). ・如果当前帧是由构造方法或者实例方法创建的，那么该对象引用this将会存放在 index为0的slot处，其余的参数按照参数表顺序继续排列. ​ 栈帧中的局部变量表中的槽位是可以重用的，如果一个局部变量过了其作用域，那么在其作用域之后申明的新的局部变量就很有可能会复用过期局部变量的槽位，从而达到节省资源的目的。 7.静态变量和局部变量对比 $\\triangleright$ 参数表分配完毕之后，再根据方法体内定义的变量的顺序和作用域分配 $\\triangleright$ 我们知道类变量表有两次初始化的机会，第一次是在“准备阶段”，执行系统初始化，对类变量设置零值，另一次则是在“初始化”阶段，赋予程序员在代码中定义的初始值. $\\triangleright$ 和类变量初始化不同的是，局部变量表不存在系统初始化的过程，这意味着一旦定义了局部变量则必须人为的初始化，否则无法使用。 12345//这样的代码是错误的，没有赋值不能够使用public void test() {int i;System.out .println(i);} 变量的分类： $\\triangleright$ 按照数据类型分：基本数据类型、引用数据类型 $\\triangleright$ 按照在类中声明的位置分：成员变量：在使用前，都经历过默认初始化赋值 类变量： Linking的 prepare阶段：给类变量默认赋值–&gt;initial阶段：给静态代码块赋值 实例变量：随着对象的创建，会在堆空间中分配实例变量空间，并进行默认赋值 $\\triangleright$ 局部变量：在使用前，必须要进行显式赋值的！否则，编译不通过。 补充说明 $\\triangleright$ 在栈帧中 ， 与性能调优关系最为密切的部分就是前面提到的局部变量表 。在方法执行时 ， 虚拟机使用局部变量表完成方法的传递 。 $\\triangleright$ 局部变量表中的变量也是重要的垃圾回收根节点 ， 只要被局部变量表中直接或间接引用的对象都不会被回收 。 8.操作数栈 $\\triangleright$ 每一个独立的栈帧中除了包含局部变量表以外，还包含一个后进先出(Last-In-First-0ut)的操作数栈，也可以称之为表达式栈(Expression Stack). $\\triangleright$ 操作数栈，在方法执行过程中，根据字节码指令，往栈中写入数据或提取数据，即入栈(push)/出栈(pop). 某些字节码指令将值压入操作数栈，其余的字节码指令将操作数取出栈。使用它们后再把结果压入栈。比如：执行复制、交换、求和等操作 $\\triangleright$ 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新PC寄存器中下一条需要执行的字节码指令。 $\\triangleright$ 操作数栈中元素的数据类型必须与字节码指令的序列严格匹配，这由编译器在编译器期间进行验证，同时在类加载过程中的类检验阶段的数据流分析阶段要再次验证。 $\\triangleright$ 另外，我们说Java虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈。 $\\triangleright$ 操作数栈，主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间。 $\\triangleright$ 操作数栈就是JVM执行引擎的-一个工作区，当一个方法刚开始执行的时候，一个新的栈帧也会随之被创建出来，这个方法的操作数栈是空的。 $\\triangleright$ 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译期就定义好了，保存在方法的Code属性中，为max stack的值。 $\\triangleright$ 栈中的任何一个元素都是可以任意的Java数据类型。 ➢32bit的类型占用一个栈单位深度 ➢64bit的类型占用两个栈单位深度 $\\triangleright$ 操作数栈并非采用访问索引的方式来进行数据访问的，而是只能通过标准的入栈(push) 和出栈(pop)操作来完成一次数据访问。 代码示例： 12345public void testAddOperation(){byte i=15;int j=8;int k=i + j;} 图例： 9.栈顶缓存（Top-of-Stack-Cashing）技术 $\\star$ 前面提过，基于栈式架构的虛拟机所使用的零地址指令更加紧凑，但完成一项操作的时候必然需要使用更多的入栈和出栈指令，这同时也就意味着将需要更多的指令分派( instruction dispatch)次数和内存读/写次数。 $\\star$ 由于操作数是存储在内存中的，因此频繁地执行内存读/写操作必然会影响执行速度。为了解决这个问题， Hotspot JVM的设计者们提出了栈顶缓存(ToS,Top-of- Stack Cashing)技术，将栈顶元素全部缓存在物理CPU的寄存器中，以此降低对内存的读/写次数，提升执行引擎的执行效率。 10.动态链接 $\\star$ 每一个栈帧内部都包含一个指向运行时常量池中该栈帧所属方法的引用,包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接( Dynamic Linking)。比如：invoke dynamic 指令。 $\\star$ 在Java源文件被编译到字节码文件中时，所有的变量和方法引用都作为符号引用( Symbolic Reference)保存在class文件的常量池里。比如：描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 11.方法的调用 在JVM中，将符号引用转换为调用方法的直接引用与方法的绑定机制相关。 静态链接： $\\star$ 当一个字节码文件被装载进JVM内部时，如果被调用的目标方法在编译期可知，且运行期保持不变时。这种情况下将调用方法的符号引用转换为直接引用的过程称之为静态链接 动态链接： $\\star$ 如果被调用的方法在编译期无法被确定下来，也就是说，只能够在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此也就被称之为动态链接。 对应的方法的绑定机制为：早期绑定(Early Binding)和晚期绑定( Late Binding)。绑定是一个字段、方法或者类在符号引用被替换为直接引用的过程，这仅仅发生一次。 早期绑定： $\\star$ 早期绑定就是指被调用的目标方法如果在编译期可知，且运行期保持不变时，即可将这个方法与所属的类型进行绑定，这样一来，由于明确了被调用的目标方法究竟是哪一个，因此也就可以使用静态链接的方式将符号引用转换为直接引用。 晚期绑定： $\\star$ 如果被调用的方法在编译期无法被确定下来，只能够在程序运行期根据实际的类型绑定相关的方法，这种绑定方式也就被称之为晚期绑定。 12.虚方法、非虚方法 非虚方法： 如果方法在编译期就确定了具体的调用版本，这个版本在运行时是不可变的。这样的方法称为非虚方法。静态方法、私有方法、final方法、实例构造器、父类方法都是非虚方法，其他方法称为虚方法。 ​ 随着高级语言的横空出世，类似于Javaー样的基于面向对象的编程语言如今越来越多，尽管这类编程语言在语法风格上存在一定的差别，但是它们彼此之间始终保持着一个共性，那就是都支持封装、继承和多态等面向对象特性，既然这一类的编程语言具备多态特性，那么自然也就具备早期绑定和晚期绑定两种绑定方式。 ​ Java中任何一个普通的方法其实都具备虚函数的特征，它们相当于C语言中的虚函数(C中则需要使用关键字 virtual来显式定义)。如果在Java程序中不希望某个方法拥有虚函数的特征时，则可以使用关键字final来标记这个方法。 虚拟机中提供了以下几条方法调用指令 普通调用指令 invokestatic:调用静态方法，解析阶段确定唯一方法版本 invokespecial:调用****方法、私有及父类方法，解析阶段确定唯一方法版本 involkevirtual:调用所有虚方法 invokeinterface:调用接口方法 动态调用指令 invokedynamic:动态解析出需要调用的方法，然后执行前四条指令固化在虚拟机内部，方法的调用执行不可人为干预，而 invokedynamic指令则支持由用户确定方法版本。其中 invokestatic 指令和 invokespecial 指令调用的方法称为非虚方法，其余的( final 修饰的除外)称为虚方法。 13.Invokedynamic $\\star$ JVM字节码指令集一直比较稳定，一直到Java7中オ増加了一个 invokedynamic 指令，这是 Java 为了实现「动态类型语言」支持而做的种改进。 $\\star$ 但是在 Java7 中并没有提供直接生成 invokedynamic 指令的方法，需要借助 ASM 这种底层字节码工具来产生 invokedynamic 指令。直到 Java8 的 Lambda 表达式的出现，nvokedynamic 指令的生成，在 Java 中才有了直接的生成方式。 $\\star$ Java7 中增加的动态语言类型支持的本质是对 Java 虚拟机规范的修改，而不是对 Java 语言规则的修改，这一块相对来讲比较复杂，增加了虚拟机中的方法调用，最直接的受益者就是运行在 Java 平台的动态语言的编译器。 动态类型语言和静态类型语言 动态类型语言和静态类型语言两者的区别就在于对类型的检查是在编译期还是在运行期，满足前者就是静态类型语言，反之是动态类型语言。静态类型语言是判断变量自身的类型信息；动态类型语言是判断变量值的类型信息，变量没有类型信息，变量值才有类型信息，这是动态语言（比如Python）的一个重要特征。 14.方法重写本质 Java语言中方法重写的本质 1.找到操作数栈顶的第一个元素所执行的对象的实际类型，记作C 2.如果在类型C中找到与常量中的描述符合简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，査找过程结束；如果不通过，则返回java.lang.Illegal AccessError异常。 3.否则，按照继承关系从下往上依次对C的各个父类进行第2步的搜索和验证过程。 4.如果始终没有找到合适的方法，则抛出java.lang. AbstractMethodError异常。 Illegal Accessarror介绍： 程序试图访问或修改一个属性或调用一个方法，这个属性或方法，你没有权限访问。一般的，这个会引起编译器异常。这个错误如果发生在运行时，就说明一个类发生了不兼容的改变。 ・在面向对象的编程中，会很频繁的使用到动态分派，如果在每次动态分派的过程中都要重新在类的方法元数据中搜索合适的目标的话就可能影响到执行效率。因此，为了提高性能，JVM采用在类的方法区建立一个虚方法表( virtual method tabble)（非虚方法不会出现在表中）来实现。使用索引表来代替查找。 ・每个类中都有一个虚方法表，表中存放着各个方法的实际入口 ・那么虚方法表什么时候被创建？ 虚方法表会在类加载的链接阶段被创建并开始初始化，类的变量初始值准备完成之后，JVM会把该类的方法表也初始化完毕。 15.方法返回地址 存放调用该方法的PC寄存器的值。 **一个方法的结束，有两种方式：**1.正常执行完成。2.出现未处理的异常，非正常退出 ​ 无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置。方法正常退出时，调用者的PC计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址。而通过异常退出的，返回地址是要通过异常表来确定，栈帧中一般不会保存这部分信息。 ​ 本质上，方法的退出就是当前栈帧出栈的过程。此时，需要恢复上层方法的局部变量表、操作数栈、将返回值压入调用者栈帧的操作数栈、设置PC寄存器值等，让调用者方法继续执行下去。 ​ 正常完成出口和异常完成出口的区别在于：通过异常完成出口退出的不会给他的上层调用者产生任何的返回值。 当一个方法开始执行后，只有两种方式可以退出这个方法 1、执行引擎遇到任意一个方法返回的字节码指令( return),会有返回值传递给上层的方法调用者，简称正常完成出口 ​ 一个方法在正常调用完成之后究竟需要使用哪一个返回指令还需要根据方法返回值的实际数据类型而定。 ​ 在字节码指令中，返回指令包含 lreturn(当返回值是 boolean、byte、char、shorth和int类型时使用)、lreturn、 freturn、 dreturn以及 areturn,另外还有一个 return指令供声明为void的方法、实例初始化方法、类和接口的初始化方法使用。 2、在方法执行的过程中遇到了异常( Exception),并且这个异常没有在方法内进行处理，也就是只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出。简称异常完成出口。 方法执行过程中抛出异常时的异常处理，存储在一个异常处理表，方便在发生异常的时候找到处理异常的代码。 16.一些附加信息(略) 17.面试题 举例栈溢出的情况？( StackOverflowError):2.栈中可能出现的异常 通过-Xss设置栈的大小；OOM 调整栈大小，就能保证不出现溢出吗？不能 分配的栈内存越大越好吗？不是 垃圾回收是否会涉及到虚拟机栈？不会的！ 方法中定义的局部变量是否线程安全？1.局部变量表 六、本地方法接口 什么是本地方法？ 简单地讲，一个 Native Method 就是一个Java调用非Java代码的接口。一个Native Method是这样一个Java方法：该方法的实现由非Java语言实现，比如C.这个特征并非Java所特有，很多其它的编程语言都有这一机制，比如在 C 中，你可以用 extern&quot;c&quot;告知C 编译器去调用一个C的函数。 A native method is a Java method whose Implementation is provided by non-java code 在定义ー个 native method 时，并不提供实现体(有些像定义ー个Javai nterface),因为其实现体是由非java语言在外面实现的。 本地接口的作用是融合不同的编程语言为Java所用，它的初衷是融合C/C++程序。 为什么要使用 Native Method? Java使用起来非常方便，然而有些层次的任务用Java实现起来不容易，或者我们对程序的效率很在意时，问题就来了。 ・与Java环境外交互：**有时Java应用需要与Java外面的环境交互，这是本地方法存在的主要原因。**你可以想想Java需要与一些底层系统，如操作系统或某些硬件交换信息时的情况。本地方法正是这样一种交流机制：它为我们提供了一个非常简洁的接口而且我们无需去了解Java应用之外的繁琐的细节。 与操作系统交互 JVM支持着Java语言本身和运行时库，它是Java程序赖以生存的平台，它由一个解释器（解释字节码）和一些连接到本地代码的库组成。然而不管怎样，它毕竟不是一个完整的系统，它经常依赖于一些底层系统的支持。这些底层系统常常是强大的操作系统。通过使用本地方法，我们得以用Java实现了jre的与底层系统的交互，甚至JVM的一些部分就是用C写的。还有，如果我们要使用一些Java语言本身没有提供封装的操作系统的特性时，我们也需要使用本地方法。 Sun’s Java **Sun的解释器是用C实现的，这使得它能像一些普通的C一样与外部交互。**jre大部分是用Java实现的，它也通过一些本地方法与外界交互。例如：类java.lang. hread的 setpriority（）方法是用Java实现的，但是它实现调用的是该类里的本地方法setprlority0（）。这个本地方法是用C实现的，并被植入JVM内部，在Windows 95的平台上，这个本地方法最终将调用Win32 Setpriority（）API。这是一个本地方法的具体实现由JVM直接提供，更多的情况是本地方法由外部的动态链接库( external dynamic link library)提供，然后被JVM调用。 现状 目前该方法使用的越来越少了，除非是与硬件有关的应用，比如通过Java程序驱动打印机或者Java系统管理生产设备，在企业级应用中已经比较少见。因为现在的异构领域间的通信很发达，比如可以使用 Socket通信，也可以使用网 Web Service等等，不多做介绍。 七、本地方法栈 本地方法栈( Native Method Stack) ・Java虚拟机栈用于管理Java方法的调用，而本地方法栈用于管理本地方法的调用。 ・本地方法栈，也是线程私有的。 ・允许被实现成固定或者是可动态扩展的内存大小。（在内存溢出方面是相同的） ​ 如果线程请求分配的栈容量超过本地方法栈允许的最大容量，Java虚拟机将会抛出一个 StackOverflowError异常。 ​ 如果本地方法栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的本地方法栈，那么Java虚拟机将会抛出一个 OutOfMemoryError异常。 ・本地方法栈是使用C语言实现的 ・它的具体做法是 Native Method Stack中登记 native 方法，在 Execution Engine 执行时加载本地方法库。 ・当某个线程调用一个本地方法时，它就进入了一个全新的并且不再受虚拟机限制的世界。它和虚拟机拥有同样的权限。 本地方法栈可以通过本地方法接口来访问虚拟机内部的运行时数据区。 它甚至可以直接使用本地处理器中的寄存器 直接从本地内存的堆中分配任意数量的内存。 并不是所有的JVM都支持本地方法。因为Java虚拟机规范并没有明确要求本地方法栈的使用语言、具体实现方式、数据结构等。如果JVM产品不打算支持 native方法，也可以无需实现本地方法栈。 ・在 Hotspot JVM中，直接将本地方法栈和虚拟机栈合二为一。 八、堆 1.堆的核心概述 ・一个JVM实例只存在一个堆内存，堆也是Java内存管理的核心区域。 ・Java堆区在JVM启动的时候即被创建，其空间大小也就确定了。是JVM管理的最大一块内存空间。堆内存的大小是可以调节的。《Java虚拟机规范》规定，堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的。 ・所有的线程共享Java堆，在这里还可以划分线程私有的缓冲区（ Thread Local Allocation Buffer, TLAB) ・《Java虚拟机规范》中对Java堆的描述是：所有的对象实例以及数组都应当在运行时分配在堆上。（ The heap is the run-time data area fromwhich memcry for all class instances and arrays is allocated)我要说的是：“几乎”所有的对象实例都在这里分配内存。一从实际使用角度看的。 ・数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。 ・在方法结束后，堆中的对象不会马上被移除，仅仅在垃圾收集的时候才会被移除。 ・堆，是GC( Garbage Collection,垃圾收集器)执行垃圾回收的重点区域。 2.堆的细分内存结构 3.设置堆内存大小与OOM 1.设置堆内存 Java堆区用于存储Java对象实例，那么堆的大小在JVM启动时就已经设定好了，大家可以通过选项&quot;-Xmx&quot;和&quot;-Xms&quot;来进行设置 ​ “-Xms&quot;用于表示堆区的起始内存，等价于ーXX: Initialheapsize ​ “-Xmx&quot;则用于表示堆区的最大内存，等价于ーXX: Maxheapsize 一旦堆区中的内存大小超过“-Xmx&quot;所指定的最大内存时，将会抛出 OutOfMemoryError 异常。 ・通常会将 -Xms 和 -Xmx 两个参数配置相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小，从而提高性能。默认情况下：初始内存大小：物理电脑内存大小/64，最大内存大小：物理电脑内存大小/4 4.查看设置的参数：方式一：jps/jstat -GC 进程id 方式二：-XX: +PrintGCdetails 2.OOM说明与举例 工具： jvisualvm：查看内存情况，jvisualvm命令可直接打开 安装插件 1234567891011121314151617181920212223public class OOMInstance { public static void main(String[] args){ ArrayList&lt;Picture1&gt; list=new ArrayList&lt;&gt;(); while(true) { try { Thread.sleep(20); } catch (InterruptedException e) { e.printStackTrace(); } list.add(new Picture1(new Random().nextInt(1024*1024))); } }}class Picture1{ private byte[] pixels; public Picture1(int length){ this.pixels = new byte[length]; }} 编译程序 修改堆大小 运行程序查看 jvisualvm Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at pers.lxl.mylearnproject.programbase.jvm.Picture1.(OOMInstance.java:27) at pers.lxl.mylearnproject.programbase.jvm.OOMInstance.main(OOMInstance.java:19) 4.年轻代与老年代 ・存储在JVM中的Java对象可以被划分为两类 一类是生命周期较短的瞬时对象，这类对象的创建和消亡都非常迅速 另外一类对象的生命周期却非常长，在某些极端的情况下还能够与JVM的生命周期保持一致。 ・Java堆区进一步细分的话，可以划分为年轻代( YoungGen)和老年代( OldGen) ・其中年轻代又可以划分为Eden空间、 Survivor0空间和 Survivor1空间(有时也叫做from区、to区)。 参数设置： 配置新生代与老年代在堆结构的占比。 默认**-XX: NewRatio=2**,表示新生代占1,老年代占2,新生代占整个堆的1/3 可以修改-XX: Newratio=4,表示新生代占1,老年代占4,新生代占整个堆的1/5 在 Hot Spot中，Eden究间和另外两个 Survivor'空间缺省所占的比例是8:1:1当然开发人员可以通过选项“-Xx: SurvivorRatio”调整这个空间比例。比如XX: Survivorratio=8,默认8 几乎所有的Java对象都是在Eden区被new出来的绝大部分的Java对象的销毁都在新生代进行了。IBM公司的专门研究表明，新生代中80%的对象都是“朝生夕死”的。 可以使用选项&quot;-Xmn'&quot;设置新生代最大内存大小&gt;这个参数一般使用默认值就可以了。 对象流程： 5.对象分配过程 1.一般过程 ​ Eden 满触发 YGC/Minor GC 对 Eden与 From 区回收，还在用的对象放到 From 区，Age设为1，下次回收触发时复制 From 区转到 To 区（空的区），Age+1，Age 达到阈值（可通过设置 -XX: MaxTenuringThreshold=）放入Old。 为新对象分配内存是一件非常严谨和复杂的任务，JVM的设计者们不仅需要考虑内存如何分配、在哪里分配等问题，并且由于内存分配算法与内存回收算法密切相关，所以还需要考虑GC执行完内存回收后是否会在内存空间中爬生内存碎片。 1.new的对象先放伊甸园区。此区有大小限制。 2.当伊甸园的空间填满时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收( Minor GC),将伊甸园区中的不再被其他对象所引用的对象进行销毁。再加载新的对象放到伊甸园区。 3.然后将伊甸园中的剩余对象移动到幸存者0区 4.如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，如果没有回收，就会放到幸存者1区。 5.如果再次经历垃圾回收，此时会重新放回幸存者0区，接着再去幸存者1区 6.啥时候能去养老区呢？可以设置次数。默认是15次。可以设置参数：-XX: MaxTenuringThreshold=进行设置。 总结 针对幸存者s0,s1区的总结：复制之后有交换，谁空谁是To 关于垃圾回收：频繁在新生区收集，很少在养老区收集，几乎不在永久区/元空间收集。 2.特殊情况 6.常用调优工具 ・JDK命令行 Ecllpse: Memory Analyzer Tool Jconsole Visualvm Jprofiler Java Flight Recorder GCVlewer GC Easy 7.Minor GC、 Maior GC、 Full GC JVM在进行GC时，并非每次都对上面三个内存(新生代、老年代、方法区)区域一起回收的，大部分时候回收的都是指新生代。 针对 HotSpot VM的实现，它里面的GC按照回收区域又分为两大种类型：一种是部分收集( Partial GC),一种是整堆收集(Full GC) 部分收集：不是完整收集整个JAVA堆的垃圾收集。其中又分为： 新生代收集( Minor GC / Young GC):只是新生代的垃圾收集 老年代收集( Major GC / Old GC):只是老年代的垃圾收集 目前，只有 CMS GC会有单独收集老年代的行为。 注意，很多时候 Major GC 会和 Full GC混淆使用，需要具体分辨是老年代回收还是整堆回收。 混合收集( Mixed GC):收集整个新生代以及部分老年代的垃圾收集。 目前，只有G1 GC会有这种行为 整堆收集(Full GC):收集整个java堆和方法区的垃圾收集。 1.年轻代GC( Minor GC)触发机制： ​ 当年轻代空间不足时，就会触发 Minor GC,这里的年轻代满指的是 Eden代满， Survivor满不会引发GC.(每次 Minor GC会清理年轻代的内存。） ​ 因为Java对象大多都具备朝生夕灭的特性，所以 Minor GC非常频繁，一般回收速度也比较快。这一定义既清晰又易于理解。 ​ Minor GC会引发STW,暂停其它用户的线程，等垃圾回收结東，用户线程才恢复运行。 2.老年代GC( Major GC / Full GC) 触发机制 指发生在老年代的GC,对象从老年代消失时，我们说“ Major GC”或“Full GC”发生了。 出现了 Major GC,经常会伴随至少一次的 Minor GC(但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行 Major GCE的策略选择过程） 也就是在老年代空间不足时，会先尝试触发 Minor GC。如果之后空间还不足，则触发 Major GC。 Major GC的速度一般会比 Minor GC 慢10倍以上，STW 的时间更长。 如果 Major GC后，内存还不足，就报OOM了 Major GC 的速度一般会比 Minor GC慢10倍以上。 3.Full GC 触发机制：（后面细讲触发Full GC执行的情况有如下五种： (1)调用 System.GC（）时，系统建议执行Full GC,但是不必然执行 (2)老年代空间不足 (3)方法区空间不足 (4)通过 Minor GC后进入老年代的平均大小大于老年代的可用内存 (5)由Eden区、 survivor space0( From Space)区向 survivor space1(To Space)区复制时，对象大小大于 To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 说明：Full GC是开发或调优中尽量要避免的。这样暂时时间会短一些。 8.GC举例与日志分析 添加参数：-XX:PrintGCDetails 查看日志： 9.堆空间分代思想 为什么需要把Java堆分代？不分代就不能正常工作了吗？ 其实不分代完全可以，分代的唯一理由就是优化GC性能。如果没有分代，那所有的对象都在一块，就如同把一个学校的人都关在一个教室。GC的时候要找到哪些对象没用，这样就会对堆的所有区域进行扫描。而很多对象都是朝生夕死的，如果分代的话，把新创建的对象放到某一地方，当GC的时候先把这块存储“朝生夕死”对象的区域进行回收，这样就会腾出很大的空间出来。 10.内存分配策略（对象提升【Promotion】规则） ​ 如果对象在 Eden 出生并经过第一次 Miinor GC 后仍然存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为1。对象 Survivor区中每熬过一次 MInorGC,年龄就增加1岁，当它的年龄增加到一定程度(默认为15岁，其实每个JVM、每个GC都有所不同)时，就会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过选项**-XX: MaxTenuringThreshold**来设置。 针对不同年龄段的对象分配原则如下所示： 优先分配到Eden。 大对象直接分配到老年代。 尽量避兔程序中出现过多的大对象。 长期存活的对象分配到老年代。 动态对象年龄判断 如果 Survivor区中相同年龄的所有对象大小的总和大于 Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代，无须等到 MaxTenuringThreshold中要求的年龄 空间分配担保： -XX: HandlePromotionFailure 11.对象分配过程：TLAB 1.为什么有TLAB( Thread Local Allocation Buffer)? 🎧 堆区是线程共享区域，任何线程都可以访问到堆区中的共享数据 🎧 由于对象实例的创建在JVM中非常频繁，因此在并发环境下从堆区中划分内存空间是线程不安全的 🎧 为避免多个线程操作同一地址，需要使用加锁等机制，进而影响分配速度。 2.什么是TLAB? 🐜 从内存模型而不是垃圾收集的角度，对Eden区域继续进行划分，JVM为每个线程分配了一个私有缓存区域，它包含在Eden空间内 🐜 多线程同时分配内存时，使用TLAB可以避免一系列的非线程安全问题，同时还能够提升内存分配的吞吐量，因此我们可以将这种内存分配方式称之为快速分配策略。 🐜 据我所知所有 OPENJDK行生出来的JVM都提供了TLAB的设计 🐜 尽管不是所有的对象实例都能够在TLAB中成功分配内存，但JVM确实是将TLAB作为内存分配的首选。 🐜 在程序中，开发人员可以通过选项“-XX: USETLAB”设置是否开启TLAB空间。 🐜 默认情况下，TLAB空间的内存非常小，仅占有整个Eden空间的1%,当然我们可以通过选项“-XX: TLABWasteTargetPercent”设置TLAB空间所占用Eden空间的百分比大小。 🐜 一旦对象在AB空间分配内存失败时，JVM就会尝试着通过使用加锁机制确保数据操作的原子性，从而直接在Eden空间中分配内存。 12.堆空间常见参数 -XX: +PrintFlagsInitial:查看所有的参数的默认初始值 -XX: +PrintFlagsFinal:查看所有的参数的最终值（可能会存在修改，不再是初始值) -Xms:初始雄空向内存(默认为物理内存的1/64) -Xmx:最大雄空间内存(默认为物理内存的1/4) -Xmn:设置新生代的大小。（初始值及最大值） -XX: NewRatio:配置新生代与老年代在堆结构的占比 -XX: SurvivorRatio:设置新生代中Eden和S0/S1空间比例 -XX: MaxTenuringThreshold:设置新生代垃扱的最大年龄 -XX: +PrintGCdetails:输出详细GC处理日志 打印筒要信息：🌞 -XX: +PrintGC 🌞 - verbose:GC -XX: HandlePromotionFailure:是否设置空问分配担保 是否设置空问分配担保？： 在发生 Minor GC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间。 如果大于，则此次 Minor GC是安全的 如果小于，则虚拟机会查看ーXX: HandlePromotionFailure设置值是否允许担保失败。 如果 HandlepromotionFailure=true,那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小。 如果大于，则尝试进行一次 Minor GC,但这次 Minor GC依然是有风险的。 如果小于，则改为进行一次Full GC。 如果 HandlePromotionFalure= false,则改为进行一次Full GC。 在JDK6 Update24之后， HandlePromotionFailure 参数不会再影响到虚拟机的空间分配担保策略，观察 OpenJDK中的源码变化，虽然源码中还定义了HandlePromotionFailure 参数，但是在代码中已经不会再使用它。JDK6 Update24之后的规则变为 只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小 就会进行 Minor GC,否则将进行Full GC。 13.通过逃逸分析看堆空间的对象分配 堆是分配对象存储的唯一选择吗？ ​ 在《深入理解Java虚拟机》中关于Java堆内存有这样一段描述：随着JIT编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。在Java虚拟机中，对象是在Java堆中分配内存的，这是一个普遍的常识。但是，有种特殊情况，那就是如果经过逃逸分析( Escape Analysis)后发现，一个对象并没有逃逸出方法**【NEW的对象是否有可能在方法外被调用，调用则发生逃逸】**的话，那么就可能被优化成栈上分配。这样就无需在堆上分配内存，也无须进行垃圾回收了。这也是最常见的堆外存储技术。 ​ 此外，前面提到的基于 OPENJDK深度定制的 TaoBaoVM,其中创新的GCIH(GCinvisible heap)技术实现。off-heap,将生命周期较长的Java对象从heap中移至heap外，并且GC不能管理GCIH内部的Java对象，以此达到降低GC的回收频率和提升GC的回收效率的目的。 [堆不是分配对象存储的唯一选择吗](# 4.堆不是分配对象存储的唯一选择吗) 如何将堆上的对象分配到栈，需要使用逃逸分析手段。 这是一种可以有效减少Java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。通过逃逸分析， Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。 逃逸分析的基本行为就是分析对象动态作用域： 当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。 当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。 参数设置： ・在JDK6u23版本之后， HotSpot中默认就已经开启了逃逸分析。如果使用的是较早的版本，开发人员则可以通过 选项“-XX: Doescapeanalysis&quot;显式开启逃逸分析 通过选项“-X: Printescapeanalysis&quot;查看逃逸分析的筛选结果 结论： 开发中能使用局部变量的，就不要使用在方法外定义。 14.逃逸分析：代码优化 ​ 使用逃逸分析，编译器可以对代码做如下优化： 一、栈上分配。将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 二、同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 三、分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在CPU寄存器中。 1.栈上分配 实验： 1.-Xmx1G -Xms1G -XX:-DoEscapeAnalysis -XX:+PrintGCDetails 2.-Xmx1G -Xms1G -XX:+DoEscapeAnalysis -XX:+PrintGCDetails 3.-Xmx256m -Xms256m -XX:-DoEscapeAnalysis -XX:+PrintGCDetails 4.-Xmx256m -Xms256m -XX:+DoEscapeAnalysis -XX:+PrintGCDetails 1234567891011121314151617181920212223242526272829303132/** * 栈上分配测试 * -Xmx1G -Xms1G -XX:-DoEscapeAnalysis -XX:+PrintGCDetails * @author shkstart shkstart@126.com * @create 2020 10:31 */public class StackAllocation { public static void main(String[] args) { long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) { alloc(); } // 查看执行时间 long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为： &quot; + (end - start) + &quot; ms&quot;); // 为了方便查看堆内存中对象个数，线程sleep try { Thread.sleep(1000000); } catch (InterruptedException e1) { e1.printStackTrace(); } } private static void alloc() { User user = new User();//未发生逃逸 } static class User { }} 2.同步省略（锁消除） ​ 线程同步的代价是相当高的，同步的后果是降低并发性和性能。 ​ 在动态编译同步块的时候，JIT编译器可以借助逃逸分析来判断同步块所使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。如果没有，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。这样就能大大提高并发性和性能。这个取消同步的过程就叫同步省略，也叫锁消除。 3.分离对象或标量替换 ​ 标量(Scalar)是指一个无法再分解成更小的数据的数据。Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量( Aggregate),Java中的对象就是聚合量，因为他可以分解成其他聚合暈和标量 ​ 在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。 标量替换参数设置： 参数-XX:+EliminateAllocations:开启标量替換（默认打开），允许将对象打散分配在栈上。 4.堆不是分配对象存储的唯一选择吗？ 上述代码在主函数中进行了1亿次alloc。调用进行对象创建，由于User对象实例需要占据约16字节的空间，因此累计分配空间达到将近1.5GB。如果堆空间小于这个值，就必然会发生GC。使用如下参数运行上述代码 -server -Xmx100m -xms100m -xx:+DoEscapeAnalvsis -XX: Print GC -XX:+EliminateAllocations 这里使用参数如下 ・参数- server:启动 server模式，因为在 Server模式下，オ可以启用逃逸分析。 ・参数-XX: +DoEscapeAnalysis:启用逃逸分析 ・参数-Xmx10m:指定了堆空间最大为10MB ・参数-XX: Print GC:将打印GC日志。 ・参数-XX: EliminateAllocations:开启了标量替换（默认打开），允许将对象打散分配在栈上，比如对象拥有id和name两个字段，那么这两个字段将会被视为两个独立的局部变量进行分配。 ​ 关于逃逸分析的论文在1999年就已经发表了，但直到JDK1.6オ有实现，而且这项技术到如今也并不是十分成熟的。 ​ 其根本原因就是无法保证逃逸分析的性能消耗一定能高于他的消耗。虽然经过逃逸分析可以做标量替换、栈上分配、和锁消除。但是逃逸分析自身也是需要进行一系列复杂的分析的，这其实也是一个相对耗时的过程。 ​ 一个极端的例子，就是经过逃逸分析之后，发现没有一个对象是不逃逸的。那这个逃逸分析的过程就白白浪费掉了 ​ 虽然这项技术并不十分成熟，但是它也是即时编译器优化技术中一个十分重要的手段。注意到有一些观点，认为通过逃逸分析，JVM会在栈上分配那些不会逃逸的对象，这在理论上是可行的，但是取决于JVM设计者的选择。据我所知， Oracle Hotspot JVM中并未这么做，这一点在逃逸分析相关的文档里已经说明，所以可以明确所有的对象实例都是创建在堆上。 ​ 目前很多书籍还是基于JDK7以前的版本，JDK已经发生了很大变化， intern字符串的缓存和静态变量曾经都被分配在永久代上，而永久代已经被元数据区取代。但是，intern:字符串缓存和静态变量并不是被转移到元数据区，而是直接在堆上分配，所以这一点同样符合前面一点的结论：对象实例都是分配在堆上。 九、方法区 1.栈、堆、方法区间的交互关系 2.方法区基本理解 ​ 《Java虚拟机规范》中明确说明：&quot;尽管所有的方法区在逻辑上是属于堆的一部分，但些简单的实现可能不会选择去进行垃圾收集或者进行压缩。”但对于 HotSpotJVM而言，方法区还有一个别名叫做Non-Heap（非堆），目的就是要和堆分开。所以，方法区看作是一块独立于Java堆的内存空间。 ・方法区( Method Area)与JAVA堆一样，是各个线程共享的内存区域。 ・方法区在JVM启动的时侯被创建，并且它的实际的物理内存空间中和Java堆区一样都可以是不连续的。 ・方法区的大小，跟堆空间一样，可以选择固定大小或者可扩展。 ・方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类【比如加载大量的第三方的jar包； Tomcat部署的工程过多(30-50个)；大量动态的生成反射类】，导致方法区溢出，虚拟机同样会抛出内存溢出错误：java.lang. OutOfMemoryError: PermGen space 或者 java. lang.OutOfMemoryError: Metaspace ・关闭JVM就会释放这个区域的内存。 3.HotSpot中方法区的演进 在jdk7及以前，习惯上把方法区，称为永久代。jdk8开始，使用元空间取代了永久代。 In JDK8, classes metadata is now stored in the native heap and this space is called Metaspace 本质上，方法区和永久代并不等价。仅是对 HotSpot而言的。《Java虚拟机规范》对如何实现方法区，不做统一要求。例如： BEA JRockit / IBM J9中不存在永久代的概念。现在来看，当年使用永久代，不是好的idea。导致Java程序更容易OOM(超过XX: MaxPermSize上限) 而到了JDK8,终于完全废弃了永久代的概念，改用与 Jrockit、J9一样在本地内存中实现的元空间( Metaspace)来代替 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过**元空间与永久代最大的区别在于：元空间不在虚拟机设置的内存中，而是使用本地内存。**永久代、元空间二者并不只是名字变了，内部结构也调整了。根据《Java虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出OOM异常。 4.设置方法区大小 方法区的大小不必是固定的，JVM可以根据应用的需要动态调整。 jdk7及以前： 通过**-XX: PermSize**来设置永久代初始分配空间。默认值是20.75M -XX: MaxPermSize来设定永久代最大可分配空间。32位机器默认是64M,64位机器模式是82M 当JVM加载的类信息容量超过了这个值，会报异常 OutOfMemoryError: Permgen Space。 可通过命令行查看：jps————》jinfo -flag PermSize/MaxPermSize ’num‘ jdk8及以后： ​ 元数据区大小可以使用参数**-XX: MetaspaceSize和-XX: MaxMetaspacesSize**指定，替代上述原有的两个参数。 ​ 默认值依赖于平台。 windows下，-XX: MetaspaceSize是21M,-XX: MaxMetaspaceSize的值是-1,即没有限制。 ​ 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存。如果元数据区发生溢出，虚拟机一样会抛出异常OutOfMemoryError:Metaspace ​ -XX: MetaspaceSize:设置初始的元空间大小。对于ー个64位的服务器端JVM来说，其默认的-XX: MetaspaceSize值为21MB。这就是初始的高水位线，一旦触及这个水位线，Full GC将会被触发并卸载没用的类（即这些类对应的类加载器不再存活），然后这个高水位线将会重置。新的高水位线的值取决于GC后释放了多少元空间。如果释放的空间不足，那么在不超过 MaxMetaspaceSize时，适当提高该值。如果释放空间过多，则适当降低该值。 ​ 如果初始化的高水位线设置过低，上述高水位线调整情况会发生很多次。通过垃圾回收器的日志可以观察到FullGC多次调用。为了避免频繁地GC,建议将 -XX: MetaspaceSize设置为一个相对较高的值。 5.OOM举例 1234567891011121314151617181920212223242526272829303132333435import com.sun.xml.internal.ws.org.objectweb.asm.ClassWriter;import jdk.internal.org.objectweb.asm.Opcodes;/** * jdk6/7中： * -XX:PermSize=10m -XX:MaxPermSize=10m * * jdk8中： * -XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m * * @author shkstart shkstart@126.com * @create 2020 22:24 */public class OOMTest extends ClassLoader { public static void main(String[] args) { int j = 0; try { OOMTest test = new OOMTest(); for (int i = 0; i &lt; 10000; i++) { //创建ClassWriter对象，用于生成类的二进制字节码 ClassWriter classWriter = new ClassWriter(0); //指明版本号，修饰符，类名，包名，父类，接口 classWriter.visit(Opcodes.V1_6, Opcodes.ACC_PUBLIC, &quot;Class&quot; + i, null, &quot;java/lang/Object&quot;, null); //返回byte[] byte[] code = classWriter.toByteArray(); //类的加载 test.defineClass(&quot;Class&quot; + i, code, 0, code.length);//Class对象 j++; } } finally { System.out.println(j); } }} 如何解决这些OOM？ 1、要解决OOM异常或 heap space的异常，一般的手段是首先通过内存映像分析工具(如Eclipse Memory Analyzer)对dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏（ Memory Leak)还是内存溢出( Memory Overflow) 2、如果是内存泄漏，可进一步通过工具査看泄漏对象到 GC Roots的引用链。于是就能找到泄漏对象是通过怎样的路径与 GC Roots相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息，以及 GC Roots引用链的信息，就可以比较准确地定位出泄漏代码的位置。 3、如果不存在内存泄漏，换句话说就是内存中的对象确实都还必须存活着，那就应当检査虚拟机的堆参数(-Xmx与-Xms),与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 6.方法区内部结构 ​ 《深入理解Java虚拟机》书中对方法区( Method Area)存储内容描述如下它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等。 类型信息： 对每个加载的类型(类class、接口 interface、枚举enum、注解 annotation),JVM必须在方法区中存储以下类型信息： ①这个类型的完整有效名称(全名=包名.类名) ②)这个类型直接父类的完整有效名（对于 interface或是java.lang.Object,都没有父类) ③这个类型的修饰符（pulblic, abstract,fina的某个子集) ④这个类型直接接口的一个有序列表 域（Filed）信息 JVM必须在方法区中保存类型的所有域的相关信息以及域的声明顺序。 域的相关信息包括：域名称、域类型、域修饰符（ public，private，protected，static，fina， volatile，transient的某个子集） 方法（Method）信息 JTVM必须保存所有方法的以下信息，同域信息一样包括声明顺序 🔼 方法名称 🔼 方法的返回类型(或void)方法参数的数量和类型（按顺序 🔼 方法的修饰符（ public, private, protected, static, final,synchronized, native, abstract的一个子集) 🔼 方法的字节码( bytecodes)、操作数栈、局部变量表及大小( abstract和native方法除外) 🔼 异常表( abstract和 native方法除外)：每个异常处理的开始位置、结束位置、代码处理在程序计数器中的偏移地址被捕获的异常类的常量池索引 non- final的类变量 静态变量和类关联在一起，随着类的加载而加载，它们成为类数据在逻辑上的一部分。 类变量被类的所有实例共享，即使没有类实例时你也可以访问它。 全局常量： static final 被声明为 final的类变量的处理方法则不同，每个全局常量在编译的时候就会被分配了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276import java.io.Serializable;/** * 测试方法区的内部构成 * @author shkstart shkstart@126.com * @create 2020 23:39 */public class MethodInnerStrucTest extends Object implements Comparable&lt;String&gt;,Serializable { //属性 public int num = 10; private static String str = &quot;测试方法的内部结构&quot;; //构造器 //方法 public void test1(){ int count = 20; System.out.println(&quot;count = &quot; + count); } public static int test2(int cal){ int result = 0; try { int value = 30; result = value / cal; } catch (Exception e) { e.printStackTrace(); } return result; } @Override public int compareTo(String o) { return 0; }}//运行javap-v-p MethodInnerStrucTest.class &gt; test.txtClassfile /D:/workspace_idea5/JVMDemo/out/production/chapter09/com/atguigu/java/MethodInnerStrucTest.class Last modified 2020-4-22; size 1626 bytes MD5 checksum 69643a16925bb67a96f54050375c75d0 Compiled from &quot;MethodInnerStrucTest.java&quot; //类型信息public class com.atguigu.java.MethodInnerStrucTest extends java.lang.Object implements java.lang.Comparable&lt;java.lang.String&gt;, java.io.Serializable minor version: 0 major version: 51 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #18.#52 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #17.#53 // com/atguigu/java/MethodInnerStrucTest.num:I #3 = Fieldref #54.#55 // java/lang/System.out:Ljava/io/PrintStream; #4 = Class #56 // java/lang/StringBuilder #5 = Methodref #4.#52 // java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V #6 = String #57 // count = #7 = Methodref #4.#58 // java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #8 = Methodref #4.#59 // java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; #9 = Methodref #4.#60 // java/lang/StringBuilder.toString:()Ljava/lang/String; #10 = Methodref #61.#62 // java/io/PrintStream.println:(Ljava/lang/String;)V #11 = Class #63 // java/lang/Exception #12 = Methodref #11.#64 // java/lang/Exception.printStackTrace:()V #13 = Class #65 // java/lang/String #14 = Methodref #17.#66 // com/atguigu/java/MethodInnerStrucTest.compareTo:(Ljava/lang/String;)I #15 = String #67 // 测试方法的内部结构 #16 = Fieldref #17.#68 // com/atguigu/java/MethodInnerStrucTest.str:Ljava/lang/String; #17 = Class #69 // com/atguigu/java/MethodInnerStrucTest #18 = Class #70 // java/lang/Object #19 = Class #71 // java/lang/Comparable #20 = Class #72 // java/io/Serializable #21 = Utf8 num #22 = Utf8 I #23 = Utf8 str #24 = Utf8 Ljava/lang/String; #25 = Utf8 &lt;init&gt; #26 = Utf8 ()V #27 = Utf8 Code #28 = Utf8 LineNumberTable #29 = Utf8 LocalVariableTable #30 = Utf8 this #31 = Utf8 Lcom/atguigu/java/MethodInnerStrucTest; #32 = Utf8 test1 #33 = Utf8 count #34 = Utf8 test2 #35 = Utf8 (I)I #36 = Utf8 value #37 = Utf8 e #38 = Utf8 Ljava/lang/Exception; #39 = Utf8 cal #40 = Utf8 result #41 = Utf8 StackMapTable #42 = Class #63 // java/lang/Exception #43 = Utf8 compareTo #44 = Utf8 (Ljava/lang/String;)I #45 = Utf8 o #46 = Utf8 (Ljava/lang/Object;)I #47 = Utf8 &lt;clinit&gt; #48 = Utf8 Signature #49 = Utf8 Ljava/lang/Object;Ljava/lang/Comparable&lt;Ljava/lang/String;&gt;;Ljava/io/Serializable; #50 = Utf8 SourceFile #51 = Utf8 MethodInnerStrucTest.java #52 = NameAndType #25:#26 // &quot;&lt;init&gt;&quot;:()V #53 = NameAndType #21:#22 // num:I #54 = Class #73 // java/lang/System #55 = NameAndType #74:#75 // out:Ljava/io/PrintStream; #56 = Utf8 java/lang/StringBuilder #57 = Utf8 count = #58 = NameAndType #76:#77 // append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #59 = NameAndType #76:#78 // append:(I)Ljava/lang/StringBuilder; #60 = NameAndType #79:#80 // toString:()Ljava/lang/String; #61 = Class #81 // java/io/PrintStream #62 = NameAndType #82:#83 // println:(Ljava/lang/String;)V #63 = Utf8 java/lang/Exception #64 = NameAndType #84:#26 // printStackTrace:()V #65 = Utf8 java/lang/String #66 = NameAndType #43:#44 // compareTo:(Ljava/lang/String;)I #67 = Utf8 测试方法的内部结构 #68 = NameAndType #23:#24 // str:Ljava/lang/String; #69 = Utf8 com/atguigu/java/MethodInnerStrucTest #70 = Utf8 java/lang/Object #71 = Utf8 java/lang/Comparable #72 = Utf8 java/io/Serializable #73 = Utf8 java/lang/System #74 = Utf8 out #75 = Utf8 Ljava/io/PrintStream; #76 = Utf8 append #77 = Utf8 (Ljava/lang/String;)Ljava/lang/StringBuilder; #78 = Utf8 (I)Ljava/lang/StringBuilder; #79 = Utf8 toString #80 = Utf8 ()Ljava/lang/String; #81 = Utf8 java/io/PrintStream #82 = Utf8 println #83 = Utf8 (Ljava/lang/String;)V #84 = Utf8 printStackTrace{ //域信息 public int num; descriptor: I flags: ACC_PUBLIC private static java.lang.String str; descriptor: Ljava/lang/String; flags: ACC_PRIVATE, ACC_STATIC //方法信息 public com.atguigu.java.MethodInnerStrucTest(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: aload_0 5: bipush 10 7: putfield #2 // Field num:I 10: return LineNumberTable: line 10: 0 line 12: 4 LocalVariableTable: Start Length Slot Name Signature 0 11 0 this Lcom/atguigu/java/MethodInnerStrucTest; public void test1(); descriptor: ()V flags: ACC_PUBLIC Code: stack=3, locals=2, args_size=1 0: bipush 20 2: istore_1 3: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 6: new #4 // class java/lang/StringBuilder 9: dup 10: invokespecial #5 // Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 13: ldc #6 // String count = 15: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 18: iload_1 19: invokevirtual #8 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 22: invokevirtual #9 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 25: invokevirtual #10 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 28: return LineNumberTable: line 17: 0 line 18: 3 line 19: 28 LocalVariableTable: Start Length Slot Name Signature 0 29 0 this Lcom/atguigu/java/MethodInnerStrucTest; 3 26 1 count I public static int test2(int); descriptor: (I)I flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: iconst_0 1: istore_1 2: bipush 30 4: istore_2 5: iload_2 6: iload_0 7: idiv 8: istore_1 9: goto 17 12: astore_2 13: aload_2 14: invokevirtual #12 // Method java/lang/Exception.printStackTrace:()V 17: iload_1 18: ireturn Exception table: from to target type 2 9 12 Class java/lang/Exception LineNumberTable: line 21: 0 line 23: 2 line 24: 5 line 27: 9 line 25: 12 line 26: 13 line 28: 17 LocalVariableTable: Start Length Slot Name Signature 5 4 2 value I 13 4 2 e Ljava/lang/Exception; 0 19 0 cal I 2 17 1 result I StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 12 locals = [ int, int ] stack = [ class java/lang/Exception ] frame_type = 4 /* same */ public int compareTo(java.lang.String); descriptor: (Ljava/lang/String;)I flags: ACC_PUBLIC Code: stack=1, locals=2, args_size=2 0: iconst_0 1: ireturn LineNumberTable: line 33: 0 LocalVariableTable: Start Length Slot Name Signature 0 2 0 this Lcom/atguigu/java/MethodInnerStrucTest; 0 2 1 o Ljava/lang/String; public int compareTo(java.lang.Object); descriptor: (Ljava/lang/Object;)I flags: ACC_PUBLIC, ACC_BRIDGE, ACC_SYNTHETIC Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: checkcast #13 // class java/lang/String 5: invokevirtual #14 // Method compareTo:(Ljava/lang/String;)I 8: ireturn LineNumberTable: line 10: 0 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lcom/atguigu/java/MethodInnerStrucTest; static {}; descriptor: ()V flags: ACC_STATIC Code: stack=1, locals=0, args_size=0 0: ldc #15 // String 测试方法的内部结构 2: putstatic #16 // Field str:Ljava/lang/String; 5: return LineNumberTable: line 13: 0}Signature: #49 // Ljava/lang/Object;Ljava/lang/Comparable&lt;Ljava/lang/String;&gt;;Ljava/io/Serializable;SourceFile: &quot;MethodInnerStrucTest.java&quot; 运行时常量池VS常量池 常量池 方法区内部包含了运行时常量池 ・字节码文件，内部包含了常量池。 ・要弄清楚方法区，需要理解清楚ClassFile,因为加载类的信息都在方法区。 ・要弄清楚方法区的运行时常量池，需要理解清楚ClassFile中的常量池https://docs.oracle.com/javase/specs/ivms/se8/htm1/jvms-4.html如下： 一个有效的字节码文件中除了包含类的版本信息、字段、方法以及接口等描述信息外，还包含一项信息那就是常量池表( Constant Pool Table),包括各种字面量和对类型、域和方法的符号引用。 为什么需要常量池？ 一个java源文件中的类、接口，编译后产生一个字节码文件。而Java中的字节码需要数据支持，通常这种数据会很大以至于不能直接存到字节码里，换另一种方式，可以存到常量池，这个字节码包含了指向常量池的引用。在动态链接的时候会用到运行时常量池，之前有介绍。 小结 常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名、方法名、参数类型、字面量等类型。 运行时常量池 🥑 运行时常量池( Runtime Constant Pool)是方法区的一部分。 🥑 常量池表( Constant Pool Table)是Class文件的一部分，用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 🥑 运行时常量池，在加载类和接口到虚拟机后，就会创建对应的运行时常量池。 🥑 JVM为每个已加载的类型（类或接口）都维护一个常量池。池中的数据项像数组项一样，是通过索引访问的。 🥑 运行时常量池中包含多种不同的常量，包括编译期就已经明确的数值字面量，也包括到运行期解析后才能够获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换为真实地址。 🥑 运行时常量池，相对于Class文件常量池的另一重要特征是：具备动态性。运行时常量池类似于传统编程语言中的符号表( symol table),但是它所包含的数据却比符号表要更加丰富一些。 🥑 当创建类或接口的运行时常量池时，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大值，则JVM会抛 OutOfMemoryError异常。 7.方法区的演进细节 1.首先明确：只有 HotSpotオ有永久代。 BEA JRockit、IBM J9等来说，是不存在永久代的概念的。原则上如何实现方法区属于虚拟机实现细节，不受《Java虚拟机规范》管束，并不要求统一。 2, HotSpot中方法区的变化： jak1.6及之前 有永久代( permanent generation),静态变量（变量名）存放在永久代上 jdk. 7 有永久代，但已经逐步“去永久代”，字符串常量池、静态变量移除，保存在堆中 jak1.8及之后 无永久代，类型信息、字段、方法、常量保存在本地内存的元空间，但字符串常量池、静态变量仍在堆 3.永久代为什么要被元空间替换？ ・随着Java8的到来， Hotspot Vy中再也见不到永久代了。但是这并不意味着类的元数据信息也消失了。这些数据被移到了一个与堆不相连的本地内存区域，这个区域叫做元空间( Metaspace). ・由于类的元数据分配在本地内存中，元空间的最大可分配空间就是系统可用内存空 ・这项改动是很有必要的，原因有： 1)为永久代设置空间大小是很难确定的 ​ 在某些场景下，如果动态加载类过多，容易产生Perm 区的OOM。比如某个实际Web工程中，因为功能点比较多，在运行过程中，要不断动态加载很多类，经常出现致命错误。比如Exception in thread ‘dubbo client x.x conner’ java. lang. Outommemoryemor PermGen space 而元空间和永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 2)对永久代进行调优是很困难的。 ・判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件： 该类所有的实例都已经被回收，也就是JAVA堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 ・JAVA虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。关于是否要对类型进行回收，HotSpot虚拟机提供了-Xnoclassc参数进行控制，还可以使用 -verbose:class 以及 -XX: TracedClass - Loading、-XX: +TraceClassUnLoading查看类加载和卸载信息。 ・在大量使用反射、动态代理、CGLib等字节码框架，动态生成JSP以及OSGi这类频繁自定义类加载器的场景中，通常都需要Java虚拟机具备类型卸载的能力，以保证不会对方法区造成过大的内存压力。 4、Stringtable为什么要调整？ ​ jdk7中将 string Table放到了堆空间中。因为永久代的回收效率很低，在full GC的时候才会触发。而full GC是老年代的空间不足、永久代不足时才会触发 这就导致 StringTable回收效率不高。而我们开发中会有大量的字符串被创建，回收效率低，导致永久代内存不足。放到堆里，能及时回收内存。 8.静态变量存在哪 工具： 12345678910111213141516public class StaticObjTest { static class Test { static ObjectHolder staticObj = new ObjectHolder(); ObjectHolder instanceObj = new ObjectHolder(); void foo { ObjectHolder localObj = new ObjectH01der(); System.out.println(&quot;done&quot;); private static class ObjectHolder { } public static void main (String[]args){ Test test = new StaticObjTest.Test(); test.foo(); } } staticOBJ随着Test的类型信息存放在方法区， instanceOBJ随着Test的对象实例存放在JAVA堆，localObject则是存放在foo()方法栈帧的局部变量表中。 1234hsdb&gt;scanoops 0x00007f32c7800000 0x00007f32c7b50000 JHSDB_TestCase$ObjectHolder0x00007f32c7a7c458 JHSDB_TestCase$ObjectHolder0x00007f32c7a7c480 JHSDB_TestCase$ObjectHolder0x00007f32c7a7c490 JHSDB_TestCase$ObjectHolder 测试发现：三个对象的数据在内存中的地址都落在Eden区范围内，所以结论：只要是对象实例必然会在Java堆中分配 接着，找到了一个引用该 staticon&gt;对象的地方，是在一个java.1ang.C1ass的实例里，并且给出了这个实例的地址，通过工 nspector查看该对象实例，可以清楚看到这确实是一个java.1ang.C1ass类型的对象实例，里面有一个名为 statical]的实例字段： ​ 从《Java虚拟机规范》所定义的概念模型来看，所有Class相关的信息都应该存放在方法区之中，但方法区该如何实现，《JAVA虚拟机规范》并未做出规定，这就成了一件允许不同虚拟机自己灵活把握的事情。JDK7及其以后版本的 HotSpot虚拟机选择把静态变量与类型在Java语言一端的映射class对象存放在一起，存储于Java堆之中，从我们的实验中也明确验证了这一点。 9.方法区垃圾回收 ​ 有些人认为方法区(如 HotSpot虚拟机中的元空间或者永久)是没有垃圾收集行为的，其实不然。《Java虚拟机规范》对方法区的约束是非常宽松的，提到过可以不要求虚拟机在方法区中实现垃圾收集。事实上也确实有未实现或未能完整实现方法区类型卸载的收集器存在(如JDK11时期的ZGC收集器就不支持类卸载)。 ​ 一般来说这个区域的回收效果比较难令人满意，尤其是类型的卸载，条件相当苛刻。但是这部分区域的回收有时又确实是必要的。以前Sun公司的Bug列表中，曾出现过的若干个严重的Bug就是由于低版本的 HotSpot虚拟机对此区域未完全回收而导致内存泄漏。 ​ 方法区的垃圾收集主要回收两部分内容：常量池中废弃的常量和不再使用的类型。 ​ 先来说说方法区内常量池之中主要存放的两大类常量：字面量和符号引用字面量比较接近JAVA语言层次的常量概念，如文本字符串、被声明为final的常量值等。而符号引用则属于编译原理方面的概念，包括下面三类常量： ​ 1、类和接口的全限定名 ​ 2、字段的名称和描述符 ​ 3、方法的名称和描述符 ​ HotSpot,虚拟机对常量池的回收策略是很明确的，只要常量池中的常量没有被任何地方引用，就可以被回收。 ​ 回收废弃常量与回收JAVA堆中的对象非常类似 ​ 判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件 ​ 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。 ​ 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。 ​ 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 ​ Java虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。关于是否要对类型进行回收，HotSpot虚拟机提供了-Xnoclassc参数进行控制，还可以使用 -cverbose:class以及 -XX: +TracedClass- Loading、-XX: TracedClassUnLoading查看类加载和卸载信息。 ​ 在大量使用反射、动态代理、CGLib等字节码框架，动态生成JSP以及SGi这类频繁自定义类加载器的场景中，通常都需要Java虚拟机具备类型卸载的能力，以保证不会对方法区造成过大的内存压力。 十、对象的实例化内存 布局与访问定位 1.对象实例化的几种方式 2.创建对象的步骤 1.判断对象对应的类是否加载、链接、初始化 ​ 虚拟机遇到一条new指令，首先去检査这个指令的参数能否在 Metaspace的常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化。（即判断类元信息是否存在）。如果没有，那么在双亲委派模式下，使用当前类加载器以classloader 包名 类名为Key进行查找对应的，class文件。如果没有找到文件，则抛出Classnotfound Exception异常，如果找到，则进行类加载，并生成对应的class类对象 2.为对象分配内存 ​ 首先计算对象占用空间大小，接着在堆中划分一块内存给新对象。如果实例成员变量是引用变量，仅分配引用变量空间即可，即4个字节大小。 如果内存规整，使用指针碰撞 ​ 如果内存是规整的，那么虚拟机将采用的是指针碰撞法( Bump The Pointer)来为对象分配内存。意思是所有用过的内存在一边，空闲的内存在另外一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是把指针向空闲那边挪动一段与对象大小相等的距离罢了。如果垃圾收集器选择的是 Serial、 Pardew这种基于压缩算法的，虚拟机采用这种分配方式。般使用带有 compact（整理）过程的收集器时，使用指针碰撞。 如果内存不规整，虚拟机需要维护一个列表，使用空闲列表分配 ​ 如果内存不是规整的，已使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。意思是虚拟机维护了一个列表，记录上哪些内存块是可用的，再分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的内容。这种分配方式成为“空闲列表( Free List)” 说明：选择哪种分配方式由Java堆是否规整决定，而JAVA堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 3.处理并发安全问题 ​ 在分配内存空间时，另外一个问题是及时保证new对象时候的线程安全性：创建对象是非常频繁的操作，虚拟机需要解决并发问题。虚拟机采用了两种方式解决并发问题: ・CAS( Compare And Swap)失败重试、区域加锁：保证指针更新操作的原子性； ・TLAB把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在JAVA堆中预先分配一小块内存，称为本地线程分配缓冲区，（TLAB, Thread Local All ocation Buffer)虚拟机是否使用TLAB,可以通过-XX:+/- UseTLAB参数来设定。 4,初始化分配到的空间 ​ 内存分配结束，虚拟机将分配到的内存空间都初始化为零值（不包括对象头）。这一步保证了对象的实例字段在JAVA代码中可以不用赋初始值就可以直接使用，程序能访问到这些字段的数据类型所对应的零值。 5,设置对象的对象头 ​ 将对象的所属类（即类的元数据信息）、对象的 Hashcode,和对象的GC信息、锁信息等数 据存储在对象的对象头中。这个过程的具体设置方式取决于JVM实现。 6,执行init方法进行初始化 ​ 在Java程序的视角看来，初始化才正式开始。初始化成员变量，执行实例化代码块，调用类的构造方法，并把堆内对象的首地址赋值给引用变量。因此一般来说(由字节码中是否跟随有invokespecial指令所决定)，new指令之后会接着就是执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全创建出来。 3.对象的内存布局 4.对象访问定位 十一、直接内存 ・不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。 ・直接内存是在Java堆外的、直接向系统申请的内存区间。 来源于NIO,通过存在堆中的 Directbytebuffer操作 Native内存 ・通常，访问直接内存的速度会优于Java堆。即读写性能高。因此出于性能考虑，读写频繁的场合可能会考虑使用直接内存。 Java的NIO库允许Java程序使用直接内存，用于数据缓冲区 非直接缓存：读写文件，需要与磁盘交互，需要由用户态切换到内核态。在内核态时，需要内存如右图的操作。 使用ｴO,见右图。这里需要两份内存存储重复数据，效率低。 直接缓存：使用NIO时，如图。操作系统划出的直接缓存区可以被java代码直接访问，只有一份。NIO适合对大文件的读写操作。 直接内存也可能导致 Outofmemoryerror异常，由于直接内存在Java堆外，因此它的大小不会直接受限于-Xmx指定的最大堆大小，但是系统内存是有限的，Java堆和直接内存的总和依然受限于操作系统能给出的最大内存。 缺点： 分配回收成本较高 不受JVM内存回收管理 直接内存大小可以通过 MaxDlrectMemorySize如果不指定，默认与堆的最大值一Xmx参数值一致 十二、执行引擎 十三、StringTable 1.String的基本特性 String:字符串,使用一对&quot; &quot;引起来表示. ・ string s1=&quot; atgulqu&quot;;//字面量的定义方式 ・ string s2= new String(&quot;hello&quot;)； string声明为final的,不可被继承 ・ string实现了 Serializable接口:表示字符串是支持序列化的. 实现了 Comparable接口:表示 string可以比较大小 ・ String在jdk8及以前内部定义了 final char【】 value用于存储字符串数据.jk9时改为byte【】加上编码标记 以节约空间。StringBuffer与StringBuilder同步修改。 string:代表不可变的字符序列.简称:不可变性. 当对字符串重新赋值时,需要重写指定内存区域赋值,不能使用原有的value进行赋值. 当对现有的字符串进行连接操作时,也需要重新指定内存区域赋值,不能使用原有的value进行赋值. 当调用 string的 replace()方法修改指定字符或字符串时,也需要重新指定内存区域赋值,不能使用原有的value进行赋值 通过字面量的方式(区别于new)给一个字符串赋值,此时的字符串值声明在字符串常量池中. 字符串常量池中是不会存储相同内容的字符串的 Stringl的 String Pool是一个固定大小的 Hashtable,默认值大小长度是1009.如果放进 string Pool的String非常多,就会造成Hash冲严重,从而导致链表会很长,而链表长了后直接会造成的影响就是当调用 String. internl时性能会大幅下降. 使用-XX: StringTableSize可设置 StringTable的长度 在jdk6中 StringTable是固定的,就是1009的长度,所以如果常量池中的字符串过多就会导致效率下降很快. StringTableSize设置没有要求 在jdk7中， Stringtable的长度默认值是60013, StringTableSize设置没有要求 Jdk8开始，设置 StringTable的长度的话，1009是可设置的最小值。 2.String的内存分配 ​ 在Java语言中有8种基本数据类型和一种比较特殊的类型 string。这些类型为了使它们在运行过程中速度更快、更节省内存，都提供了一种常量池的概念。常量池就类似一个Java系统级别提供的缓存。8种基本数据类型的常量池都是系统协调的，String类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 string对象会直接存储在常量池中。比如：Stringinfo=&quot;atqulgu.com&quot;; 如果不是用双引号声明的 string对象，可以使用 string提供的 Intern（）方法。 Java6及以前，字符串常量池存放在永久代。 Java7 中 Oracle的工程师对字符串池的逻辑做了很大的改变，即将字符串常量池的位置调整到Java堆内。 所有的字符串都保存在堆(Heap)中，和其他普通对象一样，这样可以让你在进行调优应用时仅需要调整堆大小就可以了。 字符串常量池概念原本使用得比较多，但是这个改动使得我们有足够的理由让我们重新考虑在Java7中使用 string.intern（） Java8元空间，字符串常量在堆。 3.String的基本操作 4.字符串拼接操作 常量与常量的拼接结果在常量池，原理是编译期优化 2.常量池中不会存在相同内容的常量。 3.只要其中有一个是变量，结果就在堆中。变量拼接的原理是 String Builder 4.如果拼接的结果调用 intern（）方法，则主动将常量池中还没有的字符串对象放入池中，并返回此对象地址。 字符串如何拼接？ 字符审拼接操作不一定使用的是 Stringbuilder! 如果拼接符号左右两边部是字符审常量戦常量引用，则仍然使编译期优化，即非 StringBuilder的方式分对于 final修飾类、方法、蒸本数类率、引用数据类型的量的结构时，能使用上 final的时候建议使用上。 体会执行效率：通过 StringBuilder的append（）的方式添加字符宇的教率要远高于使用 string的字符宇拼接方式！ 洋情： StringBuilder的 append（）的方式：自始至終中只创建过一个StringBuilder的对象 使用String的字符串拼接方式：建过多个 StringBuilder和 String的对象 使用 String的字符串拼接方式：内存中出于创建了较多 StringBuilder和 String的对象，内存占用更大；如果进行GC,需要花费额外的时间。 改进空间：在实际开发中，如果基本确定要前前后后添加的字符串长度不高手某个限定值 highlevel的况下，建议使用构造器实例化 StringBuilder s=new StringBuilder(highlevel);//new char[highlevel] 12345678String s1=&quot;a&quot;;String s2=&quot;b&quot;;String s3=&quot;ab&quot;;//s1+s2细节：//1.StringBuilder s=new StringBuilder();//s.append(&quot;a&quot;);//s.append(&quot;b&quot;);//s.toString()//---&gt;约等于 new String(&quot;ab&quot;) 5.intern（）的使用 ​ 如果不是用双引号声明的 string对象，可以使用 string提供的 intern方法： intern方法会从字符串常量池中査询当前字符串是否存在，若不存在就会将当前字符串放入常量池中。 比如：String myinfo = new string(&quot;I love atguigu&quot;).intern(） ​ 也就是说，如果在任意字符串上调用 string. intern方法，那么其返回结果所指向的那个类实例，必须和直接以常量形式出现的字符串实例完全相同。因此，下列表达式的值必定是true: ​ (“a”+“b”+“c”).intern() ==&quot;abc&quot; ​ 通俗点讲， Interned String/就是确保字符串在内存里只有一份拷贝，这样可以节约内存空间，加快字符串操作任务的执行速度。注意，这个值会被存放在字符串内部池(String Intern Pool) 问题 new String（“ab”）会创建几个对象,new String(&quot;a&quot;) + new String(&quot;b&quot;)呢？ 1234567891011121314151617181920212223242526/** * 题目： * new String(&quot;ab&quot;)会创建几个对象？看字节码，就知道是两个。 * 一个对象是：new关键字在堆空间创建的 * 另一个对象是：字符串常量池中的对象&quot;ab&quot;。 字节码指令：ldc * * * 思考： * new String(&quot;a&quot;) + new String(&quot;b&quot;)呢？ * 对象1：new StringBuilder() * 对象2： new String(&quot;a&quot;) * 对象3： 常量池中的&quot;a&quot; * 对象4： new String(&quot;b&quot;) * 对象5： 常量池中的&quot;b&quot; * * 深入剖析： StringBuilder的toString(): * 对象6 ：new String(&quot;ab&quot;) * 强调一下，toString()的调用，在字符串常量池中，没有生成&quot;ab&quot; */public class StringNewTest { public static void main(String[] args) {// String str = new String(&quot;ab&quot;); String str = new String(&quot;a&quot;) + new String(&quot;b&quot;); }} 如何保证变量s指向的是字符串常量池中的数据呢？ 123456789101112131415161718192021222324252627/** * 如何保证变量s指向的是字符串常量池中的数据呢？ * 有两种方式： * 方式一： String s = &quot;shkstart&quot;;//字面量定义的方式 * 方式二： 调用intern() * String s = new String(&quot;shkstart&quot;).intern(); * String s = new StringBuilder(&quot;shkstart&quot;).toString().intern(); */public class StringIntern { public static void main(String[] args) { String s = new String(&quot;1&quot;); s.intern();//调用此方法之前，字符串常量池中已经存在了&quot;1&quot; String s2 = &quot;1&quot;; System.out.println(s == s2);//jdk6：false jdk7/8：false String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;);//s3变量记录的地址为：new String(&quot;11&quot;) //执行完上一行代码以后，字符串常量池中，是否存在&quot;11&quot;呢？答案：不存在！！ s3.intern();//在字符串常量池中生成&quot;11&quot;。如何理解：jdk6:创建了一个新的对象&quot;11&quot;,也就有新的地址。 // jdk7:此时常量中并没有创建&quot;11&quot;,而是创建一个指向堆空间中new String(&quot;11&quot;)的地址 String s4 = &quot;11&quot;;//s4变量记录的地址：使用的是上一行代码代码执行时，在常量池中生成的&quot;11&quot;的地址 System.out.println(s3 == s4);//jdk6：false jdk7/8：true }} 12345678910public class StringIntern1 { public static void main(String[] args) { String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;);//new String(&quot;11&quot;) //执行完上一行代码以后，字符串常量池中，是否存在&quot;11&quot;呢？答案：不存在！！ String s4 = &quot;11&quot;;//在字符串常量池中生成对象&quot;11&quot; String s5 = s3.intern(); System.out.println(s3 == s4);//false System.out.println(s5 == s4);//true }} 总结 stringl的 intern（）的使用 ・jdk1.6中，将这个字符串对象尝试放入串池。 如果串池中有，则并不会放入。返回已有的串池中的对象的地址。如果没有，会把此对象复制一份，放入串池，并返回串池中的对象地址 ・Jdk1.7起，将这个字符串对象尝试放入串池。 如果串池中有，则并不会放入。返回已有的串池中的对象的地址。如果没有，则会把对象的引用地址复制一份，放入串池，并返回串池中的引用地址 12345678910111213public class StringExer1 { public static void main(String[] args) { String x = &quot;ab&quot;; String s = new String(&quot;a&quot;) + new String(&quot;b&quot;);//new String(&quot;ab&quot;) //在上一行代码执行完以后，字符串常量池中并没有&quot;ab&quot; String s2 = s.intern();//jdk6中：在串池中创建一个字符串&quot;ab&quot; //jdk8中：串池中没有创建字符串&quot;ab&quot;,而是创建一个引用，指向new String(&quot;ab&quot;)，将此引用返回 System.out.println(s2 == &quot;ab&quot;);//jdk6:true jdk8:true System.out.println(s == &quot;ab&quot;);//jdk6:false jdk8:true }} 123456789public class StringExer2 { public static void main(String[] args) { String s1 = new String(&quot;ab&quot;);//执行完以后，会在字符串常量池中会生成&quot;ab&quot;// String s1 = new String(&quot;a&quot;) + new String(&quot;b&quot;);////执行完以后，不会在字符串常量池中会生成&quot;ab&quot; s1.intern(); String s2 = &quot;ab&quot;; System.out.println(s1 == s2); }} 空间效率测试 123456789101112131415161718192021222324252627282930/** * 使用intern()测试执行效率：空间使用上 * * 结论：对于程序中大量存在存在的字符串，尤其其中存在很多重复字符串时，使用intern()可以节省内存空间。 * 大的网站平台，需要内存中存储大量的字符串。比如社交网站，很多人都存储：北京市、海淀区等信息。这时侯如果字符串都调用intern（）方 * * 法，就会明显降低内存的大小。 */public class StringIntern2 { static final int MAX_COUNT = 1000 * 10000; static final String[] arr = new String[MAX_COUNT]; public static void main(String[] args) { Integer[] data = new Integer[]{1,2,3,4,5,6,7,8,9,10}; long start = System.currentTimeMillis(); for (int i = 0; i &lt; MAX_COUNT; i++) {// arr[i] = new String(String.valueOf(data[i % data.length])); arr[i] = new String(String.valueOf(data[i % data.length])).intern(); } long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start)); try { Thread.sleep(1000000); } catch (InterruptedException e) { e.printStackTrace(); } System.GC(); }} 垃圾回收测试 1234567891011121314/** * String的垃圾回收: * -Xms15m -Xmx15m -XX:+PrintStringTableStatistics -XX:+PrintGCDetails * * @author shkstart shkstart@126.com * @create 2020 21:27 */public class StringGCTest { public static void main(String[] args) { for (int j = 0; j &lt; 100000; j++) { String.valueOf(j).intern(); } }} G1的String去重操作 背景： 对许多Java应用（有大的也有小的）做的测试得出以下结果： 堆存活数据集合里面 string,对象占了25% 堆存活数据集合里面重复的 string对象有13.5% String对象的平均长度是45 许多大规模的Java应用的瓶颈在于内存，测试表明，在这些类型的应用里面，Java堆中存活的数据集合差不多25%是 string对象。更进一步，这里面差不多一半 string对象是重复的，重复的意思是说: strinG1. equals( string2)=true。堆上存在重复的 String对象必然是一种内存的浪费。这个项目将在G1垃圾收集器中实现自动持续对重复的 string对象进行去重，这样就能避免浪费内存。 实现 当垃圾收集器工作的时候，会访问堆上存活的对象。对每一个访问的对象都会检查是否是候选的要去重的String对象。 如果是，把这个对象的一个引用插入到队列中等待后续的处理。一个去重的线程在后台运行，处理这个队列。处理队列的一个元素意味着从队列删除这个元素，然后尝试去重它引用的String对象。 使用一个 hashtable来记录所有的被 String对象使用的不重复的char数组。当去重的时候，会査这个 hashtable,来看堆上是否已经存在一个一模一样的char数组。 如果存在， string对象会被调整引用那个数组，释放对原来的数组的引用，最终会被垃圾收集器回收掉。 如果查找失败，char数组会被插入到 hashtable,这样以后的时候就可以共享这个数组了。 命令行选项 UsestringDeduplication(bool):开启 String去重，默认是不开启的，需要手动开启。 PrintstringDeduplicationStatistics(bool):打印详细的去重统计信息 StringdeduplicationAgethreshold( uintx):达到这个年龄的 string对象被认为是去重的候选对象 十四、垃圾回收概述 垃圾收集，不是Java语言的伴生产物。早在1960年，第一门开始使用内存动态分配和垃圾收集技术的Lisp语言诞生 关于垃圾收集有三个经典问题：哪些内存需要回收？&gt;什么时候回收？如何回收？ 垃圾收集机制是Java的招牌能力，极大地提高了开发效率。如今，垃圾收集几乎成为现代语言的标配，即使经过如此长时间的发展，Java的垃圾收集机制仍然在不断的演进中，不同大小的设备、不同特征的应用场景，对垃圾收集提出了新的挑战，这当然也是面试的热点。 题例： 蚂蚁金服： 你知道哪几种垃圾回收器，各自的优缺点，重点讲一下CMS和G1 面： JVM GC算法有哪些，目前的JDK版本采用什么回收算法 面：G1回收器讲下回收过程 GC是什么？为什么要有GC? 面：GC的两种判定方法?CMS收集器与G1收集器的特点。 百度： 说一下GC算法，分代回收说下垃圾收集策略和算法 天猫： 面： JVM GC原理，JVM怎么回收内存 一面：CMS特点，垃圾回收算法有哪些？各自的优缺点，他们共同的缺点是什么？ 滴滴 面：java的垃圾回收器都有哪些，说下G1的应用场景，平时你是如何搭配使用垃圾回收器的。 京东： 你知道哪几种垃圾收集器，各自的优缺点，重点讲下CMS和G1,包括原理，流程，优缺点。垃圾回收算法的实现原理阿里： 讲一讲垃圾回收算法。什么情况下触发垃圾回收？如何选择合适的垃圾收集算法？ JVM有哪三种垃圾回收器？ 字节跳动： 常见的垃圾回收器算法有哪些，各有什么优劣？system.GC（）和 runtime.GC（）会做什么事情？面： Java GC机制？ GC Roots有哪些？ 二面：Java对象的回收方式，回收算法 CMS和G1了解么，CMS解决什么问题，说一下回收的过程。 CMS回收停顿了几次，为什么要停顿两次。 1.什么是垃圾 什么是垃圾( Garbage)呢？ 垃圾是指在运行程序中没有任何指针指向的对象，这个对象就是需要被回收的垃圾。 An object is considered garbage when it can nolonger be reached from any pointer in the runningprogram 如果不及时对内存中的垃圾进行清理，那么，这些垃圾对象所占的内存空间会一直保留到应用程序结束，被保留的空间无法被其他对象使用。甚至可能导致内存溢出。 2.想要学习GC,首先需要理解为什么需要GC? ​ 对于高级语言来说，一个基本认知是如果不进行垃圾回收，内存迟早都会被消耗完因为不断地分配内存空间而不进行回收，就好像不停地生产生活垃圾而从来不打扫一样。 ​ 除了释放没用的对象，垃圾回收也可以清除内存里的记录碎片。碎片整理将所占用的堆内存移到堆的一端，以便JVM将整理出的内存分配给新的对象。 ​ 随着应用程序所应付的业务越来越庞大、复杂，用户越来越多，没有GC就不能保证应用程序的正常进行。而经常造成STW的GC又跟不上实际的需求，所以才会不断地尝试对GC进行优化。 早期垃圾回收 ​ 在早期的C/C 时代，垃圾回收基本上是手工进行的。开发人员可以使用new关键字进行内存申请，并使用 delete关键字进行内存释放。比如以下代码。 ​ 这种方式可以灵活控制内存释放的时间，但是会给开发人员带来频繁申请和释放内存的管理负担。倘若有一处内存区间由于程序员编码的问题忘记被回收，那么就会产生内存泄漏，垃圾对象永远无法被清除，随着系统运行时间的不断增长，垃圾对象所耗内存可能持续上升，直到出现内存溢出并造成应用程序崩溃。现在，除了Java以外，C#、 python、Ruby等语言都使用了自动垃圾回收的思想，也是未来发展趋势。可以说，这种自动化的内存分配和垃圾回收的方式己经成为现代开发语言必备的标准。 Java垃圾回收机制 ​ 自动内存管理，无需开发人员手动参与内存的分配与回收，这样降低内存泄漏和内存溢出的风险。没有垃圾回收器，java也会和cpp一样，各种悬垂指针，野指针，泄露问题让你头疼不已。 ​ 自动内存管理机制，将程序员从繁重的内存管理中释放出来，可以更专心地专注于业务开发 oracle官网关于垃圾回收的介绍：https://docs.oracle.com/javase/8/docs/technotes/guides/vm/GCtuning/toc.html ​ 对于Java开发人员而言，自动内存管理就像是一个黑匣子，如果过度依赖于“自动”，那么这将会是一场灾难，最严重的就会弱化Java开发人员在程序出现内存溢出时定位问题和解决问题的能力。 ​ 此时，了解JVM的自动内存分配和内存回收原理就显得非常重要，只有在真正了解JVM是如何管理内存后，我们才能够在遇见 OutOfMemoryError时，快速地根据错误异常日志定位问题和解决问题。 ​ 当需要排查各种内存溢出、内存泄漏问题时，当垃圾收集成为系统达到更高并发量的瓶颈时，我们就必须对这些“自动化”的技术实施必要的监控和调节。 垃圾回收器可以对年轻代回收，也可以对老年代回收，甚至是全堆和方法区的回收。 其中，Java堆是垃圾收集器的工作重点。 从次数上讲 频繁收集 Young区 较少收集old区 基本不动Perm区 十五、垃圾回收相关算法 1.垃圾标记阶段的算法 1.垃圾标记阶段：对象存活判断 ​ 在堆里存放着几乎所有的Java对象实例，在GC执行垃圾回收之前，首先需要区分出内存中哪些是存活对象，哪些是已经死亡的对象。只有被标记为己经死亡的对象，GCオ会在执行垃圾回收时，释放掉其所占用的内存空间，因此这个过程我们可以称为垃圾标记阶段。 那么在JVM中究竟是如何标记一个死亡对象呢？简单来说，当一个对象已经不再被任何的存活对象继续引用时，就可以宣判为已经死亡。判断对象存活一般有两种方式：引用计数算法和可达性分析算法。 方式一：引用计数算法 引用计数算法( Reference Counting)比较简单，对每个对象保存一个整型的引用计数器属性。用于记录对象被引用的情况。 对于一个对象A,只要有任何一个对象引用了A,则A的引用计数器就加1;当引用失效时，引用计数器就减1。只要对象A的引用计数器的值为0,即表示对象A不可能再被使用，可进行回收。 优点：实现简单，垃圾对象便于辨识；判定效率高，回收没有延退性。 缺点：它需要单独的字段存储计数器，这样的做法增加了存储空间的开销。每次赋值都需要更新计数器，伴随着加法和减法操作，这增加了时间开销。引用计数器有一个严重的问题，即无法处理循环引用的情况。这是一条致命缺陷，导致在Java的垃圾回收器中没有使用这类算法。 方式二：可达性分析(或根搜索算法、追踪性垃圾收集) 相对于引用计数算法而言，可达性分析算法不仅同样具备实现简单和执行高效等特点，更重要的是该算法可以有效地解决在引用计数算法中循环引用的问题，防止内存泄漏的发生。 相较于引用计数算法，这里的可达性分析就是Java、C#选择的。这种类型的垃圾收集通常也叫作追踪性垃圾收集（ Tracing Garbage Collection)。 所谓&quot; GC Roots&quot;根集合就是一组必须活跃的引用。 基本思路： 可达性分析算法是以根对象集合( GC Roots)为起始点，按照从上至下的方式搜索被根对象集合所连接的目标对象是否可达。 使用可达性分析算法后，内存中的存活对象都会被根对象集合直接或间接连接着，搜索所走过的路径称为引用链( Reference Chain) 如果目标对象没有任何引用链相连，则是不可达的，就意味着该对象己经死亡，可以标记为垃圾对象。 在可达性分析算法中，只有能够被根对象集合直接或者间接连接的对象才是存活对象。 GC ROOT 在Java语言中， GC Roots包括以下几类元素： 虚拟机栈中引用的对象。比如：各个线程被调用的方法中使用到的参数、局部变量等。 本地方法栈内JNI（通常说的本地方法）引用的对象 方法区中类静态属性引用的对象&gt;比如：Java类的引用类型静态变量方法区中常量引用的对象。比如：字符串常量池( String Table)里的引用所有被同步锁 synchronized持有的对象 Java虚拟机内部的引用。基本数据类型对应的class对象，一些常驻的异常对象（如：Null Pointerexception、 OutOfMemoryError),系统类加载器。 反映java虚拟机内部情况的 JMXbeam、JVMTI中注册的回调、本地代码缓存等。 除了这些固定的 GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整 GC Roots集合。比如：分代收集和局部回收( Partial GC)。如果只针对Java堆中的某一块区域进行垃圾回收(比如：典型的只针对新生代)，必须考虑到内存区域是虚拟机自己的实现细节，更不是孤立封闭的，这个区域的对象完全有可能被其他区域的对象所引用，这时候就需要一并将关联的区域对象也加入 GC Roots 集合中去考虑，才能保证可达性分析的准确性 小技巧：由于Root采用栈方式存放变量和指针，所以如果一个指针，它保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那它就是一个Root 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。 这点也是导致GC进行时必须 “Stop The World&quot;的一个重要原因。即使是号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 2.对象的 finalization机制 Java语言提供了对象终止( finalization)机制来允许开发人员提供对象被销毁之前的自定义处理逻辑。 当垃圾回收器发现没有引用指向一个对象，即：垃圾回收此对象之前，总会先调用这个对象的 finallize（）方法 finalize()方法允许在子类中被重写，用于在对象被回收时进行资源释放。通常在这个方法中进行一些资源释放和清理的工作，比如关闭文件、套接字和数据库连接等。 永远不要主动调用某个对象的finalize()方法，应该交给垃圾回收机制调用。理由包括下面三点： finalize() 时可能会导致对象复活。 finalize() 方法的执行时间是没有保障的，它完全由GC线程决定，极端情况下，若不发生GC,则finalize()方法将没有执行机会。 一个糟糕的 finalize() 会严重影响GC的性能 从功能上来说， finalize() 方法与 C 中的析构函数比较相似，但是Java采用的是基于垃圾回收器的自动内存管理机制，所以 finalize() 方法在本质上不同于C中的析构函数。由于 finalize() 方法的存在，虚拟机中的对象一般处于三种可能的状态。 如果从所有的根节点都无法访问到某个对象，说明对象己经不再使用了。一般来说，此对象需要被回收。但事实上，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段。一个无法触及的对象有可能在某一个条件下“复活”自己，如果这样，那么对它的回收就是不合理的，为此，定义虚拟机中的对象可能的三种状态。如下： 可触及的：从根节点开始，可以到达这个对象。 可复活的：对象的所有引用都被释放，但是对象有可能 finalize() 中复活。 不可触及的：对象的 finalize() 被调用，并且没有复活，那么就会进入不可触及状态。不可触及的对象不可能被复活，因为 finalize() 只会被调用一次。 以上3种状态中，是由于fina1ize（）方法的存在，进行的区分。只有在对象不可触及时才可以被回收。 具体过程： 判定一个对象 objA 是否可回收，至少要经历两次标记过程 1.如果对象。objA到 GC Roots没有引用链，则进行第一次标记。 2.进行筛选，判断此对象是否有必要执行finalize()方法。 ①如果对象objA没有重写 finalize()方法，或者 finalize() 方法已经被虚拟机调用过，则虚拟机视为“没有必要执行”，objA被判定为不可触及的。 ②如果对象objA重写了finalize() 方法，且还未执行过，那么objA会被插入到F- Queue队列中，由一个虚拟机自动创建的、低优先级的Finalizer线程触发其 finalize() 方法执行。 ③ finalize() 方法是对象逃脱死亡的最后机会，稍后GC会对F- Queue队列中的对象进行第二次标记。如果objA在finalize() 方法中与引用链上的任何一个对象建立了联系，那么在第二次标记时，objA会被移出“即将回收”集合。之后，对象会再次出现没有引用存在的情况。在这个情况下，finalize方法不会被再次调用，对象会直接变成不可触及的状态，也就是说，一个对象的finalize方法只会被调用一次。 3.MAT与 Jprofiler的GC Roots湖源 MAT MAT是 Memory Analyzer的简称，它是一款功能强大的Java堆内存分析器。用于査找内存泄漏以及査看内存消耗情况。MAT是基于 Eclipse开发的，是一款免费的性能分析工具大家可以在https://www.eclipse.org/mat/下载并使用MAT。 ​ 获取 dump 文件 Jprofiler 4.垃圾清除阶段算法之标记一清除算法 当成功区分出内存中存活对象和死亡对象后，GC接下来的任务就是执行垃圾回收，释放掉无用对象所占用的内存空间，以便有足够的可用内存空间为新对象分配内存。 目前在JVM中比较常见的三种垃圾收集算法是标记 - 清除算法（Mark Sweep)、复制算法( Copying)、标记 - 压缩算法（Mark Compact） 标记 - 清除算法（Mark Sweep) 停止-标记-清除 背景：标记一清除算法(Mark- Sweep)是一种非常基础和常见的垃圾收集算法，该算法被J. Mccarthy等人在1960年提出并并应用于Lisp语言。 执行过程：当堆中的有效内存空间(avail able memory)被耗尽的时候，就会停止整个程序(也被称为 stop the world),然后进行两项工作，第一项则是标记，第二项则是清除。 标记：Collector从引用根节点开始遍历，标记所有被引用的对象。一般是在对象的 Header中记录为可达对象。 清除：Collector对堆内存从头到尾进行线性的遍历，如果发现某个对象在其 Headerl中没有标记为可达对象，则将其回收. 缺点 效率不算高 在进行GC的时候，需要停止整个应用程序，导致用户体验差。 这种方式清理出来的空闲内存是不连续的，产生内存碎片。需要维护一个空闲列表注意。 何为清除？ 这里所谓的清除并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里。下次有新对象需要加载时，判断垃圾的位置空间是否够如果够，就存放。 复制算法( Copying) 两块-活连续复制到另一块-清除 背景 为了解决标记一清除算法在垃圾收集效率方面的缺陷,M,工. Minsky于1963年发表了著名的论文,&quot;使用双存储区的Lisp语言垃圾收集器CALISP Garbage Collector Algorithm Using Serial Secondary Storage)&quot;。M.L. Minsky在该论文中描述的算法被人们]称为复制( Copying)算法,它也被M.L. Minsky本人成功地引入到了Lisp语言的一个实现版本中。 执行过程 将活着的内存空间分为两块,每次只使用其中一块,在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中,之后清除正在使用的内存块中的所有对象,交换两个内存的角色,最后完成垃圾回收. 优点： 没有标记和清除过程，实现简单，运行高效 复制过去以后保证空间的连续性，不会出现“碎片”问题缺点 缺点：需要两倍的内存空间。 对于G1这种分拆成为大量 region的GC,复制而不是移动，意味着GC需要维护 region之间对象引用关系，不管是内存占用或者时间开销也不小。 特别的：如果系统中的垃圾对象很多，复制算法不会很理想，因为复制算法需要复制的存活对象数量并不会太大或者说非常低才行。 标记 - 压缩算法（Mark Compact） 标记-引用对象压缩到一端-清除 背景 复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代，更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活对象较多，复制的成本也将很高。因此，基于老年代垃圾回收的特性，需要使用其他的算法。 标记一清除算法的确可以应用在老年代中，但是该算法不仅执行效率低下，而且在执行完内存回收后还会产生内存碎片，所以JM的设计者需要在此基础之上进行改进。标记压缩(Mark- Compact)算法由此诞生。 1970年前后，G.L. Steele、C.J. Chene和D.S.Wise等研究者发布标记压缩算法。在许多现代的垃圾收集器中，人们都使用了标记-压缩算法或其改进版本。 执行过程 第一阶段和标记清除算法一样从根节点开始标记所有被引用对象 第二阶段将所有的存活对象压缩到内存的一端，按顺序排放。 之后，清理边界外所有的空间。 ​ 标记一压缩算法的最终效果等同于标记一清除算法执行完成后，再进行一次内存碎片整理，因此，也可以把它称为标记一清除一压缩（Mark- Sweep- Compact)算法。二者的本质差异在于标记一清除算法是一种非移动式的回收算法，标记-压缩是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策。可以看到，标记的存活对象将会被整理，按照内存地址依次排列，而未被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销。 优点： 消除了标记一清除算法当中，内存区域分散的缺点，我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可。消除了复制算法当中，内存减半的高额代价 缺点： 从效率上来说，标记一整理算法要低于复制算法移动对象的同时，如果对象被其他对象引用，则还需要调整引用的地址。移动过程中，需要全程暂停用户应用程序。即：STW 总结 ​ 效率上来说，复制算法是当之无愧的老大，但是却浪费了太多内存。而为了尽量兼顾上面提到的三个指标，标记一整理算法相对来说更平滑一些，但是效率上不尽如人意，它比复制算法多了一个标记的阶段，比标记一清除多了一个整理内存的阶段。 前面所有这些算法中，并没有一种算法可以完全替代其他算法，它们都具有自己独特的优势和特点。分代收集算法应运而生。 分代收集算法，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的收集方式，以便提高回收效率。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点使用不同的回收算法，以提高垃圾回收的效率 在Java程序运行的过程中，会产生大量的对象，其中有些对象是与业务信息相关，比如Http请求中的 Session对象、线程、 Socket连接，这类对象跟业务直接挂钩，因此生命周期比较长。但是还有一些对象，主要是程序运行过程中生成的临时变量，这些对象生命周期会比较短，比如： string,对象，由于其不变类的特性，系统会产生大量的这些对象，有些对象甚至只用一次即可回收。 ​ 目前几乎所有的GC都是采用分代收集( Generational Collecting)算法执行垃圾回收的。在 Hotspot中，基于分代的概念，GC所使用的内存回收算法必须结合年轻代和老年代各自的特点。 年轻代( Young Gen) ​ 年轻代特点：区域相对老年代较小，对象生命周期短、存活率低，回收频繁。这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对象大小有关，因此很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过 hotspot中的两个 survivor的设计得到缓解。 老年代( Tensured Gen) ​ 老年代特点：区域较大，对象生命周期长、存活率高，回收不及年轻代频繁。这种情况存在大量存活率高的对象，复制算法明显变得不合适。一般是由标记一清除或者是标记一清除与标记整理的混合实现。Mark阶段的开销与存活对象的数量成正比。Sweep阶段的开销与所管理区域的大小成正相关。Compact阶段的开销与存活对象的数据成正比。 ​ 以 Hotspot中的CMS回收器为例，CMS是基于Mark- Sweep实现的，对于对象的回收效率很高。而对于碎片问题，CMS采用基于Mark- Compact算法的 Serial old 回收器作为补偿措施：当内存回收不佳（碎片导致的 Concurrent Mode Failure时），将采用 Seralold执行FullGC以达到对老年代内存的整理。分代的思想被现有的虚拟机广泛使用。几乎所有的垃圾回收器都区分新生代和老年代。 分区算法 ​ 一般来说，在相同条件下，堆空间越大，一次GC时所需要的时间就越长，有关GC产生的停顿也越长。为了更好地控制GC产生的停顿时间，将一块大的内存区域分割成多个小块，根据目标的停顿时间，每次合理地回收若干个小区间，而不是整个堆空间，从而减少ー次GC所产生的停顿。分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同小区间。每一个小区间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。 增量收集算法 缺点 使用这种方式，由于在垃圾回收过程中，间断性地还执行了应用程序代码，所以能减少系统的停顿时间。但是，因为线程切换和上下文转换的消耗，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 十六、垃圾回收相关概念 1.System.GC()的理解 在默认情况下，通过 System.GC)或者 Runtime. getruntime（）.GC（）的调用，会显式触发Full GC,同时对老年代和新生代进行回收，尝试释放被丢弃对象占用的内存。 然而 system.GC（）调用附带一个免责声明，无法保证对垃圾收集器的调用。 JVM实现者可以通过 System.GC（）调用来决定JVM的GC行为。而一般情况下，垃圾回收应该是自动进行的，无须手动触发，否则就太过于麻烦了。在一些特殊情况下，如我们正在编写一个性能基准，我们可以在运行之间调用System.GC（）。 2.内存溢出与内存泄漏 内存溢出 (满了) 内存溢出相对于内存泄漏来说，尽管更容易被理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。 由于GC一直在发展，所有一般情況下，除非应用程序占用的内存增长速度非常快，造成垃圾回收已经跟不上内存消耗的速度，否则不太容易出现OOM的情况。 大多数情况下，GC会进行各种年龄段的垃圾回收，实在不行了就放大招，来一次独占式的Full GC操作，这时候会回收大量的内存，供应用程序继续使用。 javadoc中对 OutOfMemoryError的解释是，没有空闲内存，并且垃圾收集器也无法提供更多内存。 ・没有空闲内存的情况:说明java虚拟机的堆内存不够.原因有二： (1)Java虚拟机的堆内存设置不够 比如:可能存在内存泄漏问题:地很有可能就是堆的大小不合理,比如我们要处理比较可观的数据量,但是没有显式指定JVM堆大小或者指定数值偏小.我们可以通过参数-Xms、Xmx来调整. (2)代码中创建了大量大对象,并且长时间不能被垃圾收集器收集(存在被引用) s对于老版本的 Oracle JDK,因为永久代的大小是有限的,并且JVM对永久代垃圾回收(如,常量池回收、卸载不再需要的类型)非常不积极,所以当我们不断添加新类型的时候,永久代出现 OutOfMemoryError也非常多见,尤其是在运行时存在大量动态类型生成的场合;类似 l tern字符串缓存占用太多空间,也会导致OOM问题.对应的异常信息,会标记出来和永久代相关:&quot;java.lang.OutOfMemoryError: Permgen space&quot;. 随着元数据区的引入,方法区内存已经不再那么窘迫,所以相应的OOM有所改观,出现OOM,异常信息则变成了:&quot;java.lang. OutOfMemoryError: Metaspace&quot;.直接内存不足,也会导致OOM. ​ 这里面隐含着一层意思是,在抛出 OutOfMemoryError之前,通常垃圾收集器会被触发,尽其所能去清理出空间。例如:在引用机制分析中,涉及到JVM会去尝试回收软引用指向的对象等。在java.nio.BTts. reservememory()方法中,我们能清楚的看到, System.GC()会被调用,以清理空间. ​ 当然,也不是在任何情况下垃圾收集器都会被触发的。比如,我们去分配一个超大对象,类似一个超大数组超过堆的最大值,JVM可以判断出垃圾收集并不能解决这个问题,所以直接抛出 OutOfMemoryError. 内存泄漏（不能回收） ​ 也称作“存储渗漏”。严格来说，只有对象不会再被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。但实际情况很多时候一些不太好的实践（或疏忽）会导致对象的生命周期变得很长甚至导致OOM,也可以叫做宽泛意义上的“内存泄漏”。 ​ 尽管内存泄漏并不会立刻引起程序崩溃，但是一旦发生内存泄漏，程序中的可用内存就会被逐步蚕食，直至耗尽所有内存，最终出现 OutOfMemory异常，导致程序崩溃。注意，这里的存储空间并不是指物理内存，而是指虚拟内存大小，这个虚拟内存大小取决于磁盘交换区设定的大小。 举例 1、单例模式 单例的生命周期和应用程序是一样长的，所以单例程序中，如果持有对外部对象的引用的话，那么这个外部对象是不能被回收的，则会导致内存泄漏的产生。 2、一些提供close的资源未关闭导致内存泄漏 数据库连接( datasource. getconnection（）),网络连接( socket)和io连接必须手动close,否则是不能被回收的。 3.Stop The World Stop-the-World,简称 STW,指的是GC事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被暂停，没有任何响应，有点像卡死的感觉，这个停顿称为STW. 可达性分析算法中枚举根节点( GC Roots)会导致所有Java执行线程停顿。分析工作必须在一个能确保一致性的快照中进行。一致性指整个分析期间整个执行系统看起来像被冻结在某个时间点上如果出现分析过程中对象引用关系还在不断变化，则分析结果的准确性无法保证。 被STW中断的应用程序线程会在完成GC之后恢复，频繁中断会让用户感觉像是网速不快造成电影卡带一样，所以我们需要减少STW的发生 STW事件和采用哪款GC无关，所有的GC都有这个事件。哪怕是G1也不能完全避免Stop-the-world情况发生，只能说垃圾回收器越来越优秀，回收效率越来越高，尽可能地缩短了暂停时间。 STW是JVM在后台自动发起和自动完成的。在用户不可见的情况下，把用户正常的工作线程全部停掉。 开发中不要用 system.GC() 会导致Stop-the- world的发生。 4.垃圾回收的并发与并行 并发和并行，在谈论垃圾收集器的上下文语境中，它们可以解释如下： **并行( Parallel)😗*指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态。如 Pardew、 Parallel Scavenge、 Parallel Old **串行( Serial)😗*相较于并行的概念，单线程执行。如果内存不够，则程序暂停，启动JVM垃圾回收器进行垃圾回收。回收完，再启动程序的线程。 并发和并行,在谈论垃圾收集器的上下文语境中,它们可以解释如下: 并发( Concurrent):指用户线程与垃圾收集线程同时执行(但不一定是并行的,可能会交替执行),垃圾回收线程在执行时不会停顿用户程序的运行。用户程序在继续运行,而垃圾收集程序线程运行于另ー个CPU上，如:CMS、G1。 5.安全点与安全区域 安全点 程序执行时并非在所有地方都能停顿下来开始GC,只有在特定的位置才能停顿下来开始GC,这些位置称为“安全点( Safepoint)”。 Safe Pointl的选择很重要，如果太少可能导致GC等待的时间太长，如果太频繁可能导致运行时的性能问题。大部分指令的执行时间都非常短暂通常会根据“是否具有让程序长时间执行的特征”为标准。比如：选择些执行时间较长的指令作为 Safe Point,如方法调用、循环跳转和异常跳转等。 如何在GC发生时，检査所有线程都跑到最近的安全点停顿下来呢？ 抢先式中断：（目前没有虚拟机采用了) 首先中断所有线程。如果还有线程不在安全点，就恢复线程，让线程跑到安全点。 主动式中断 设置一个中断标志，各个线程运行到 Safe Point的时候主动轮询这个标志，如果中断标志为真，则将自己进行中断挂起。 安全区域 Safepoint机制保证了程序执行时，在不太长的时间内就会遇到可进入GC的 Safepoint。但是，程序“不执行”的时候呢？例如线程处于Sleep状态或Blocked状态，这时候线程无法响应JVM的中断请求，“走”到安全点去中断挂起，JVM也不太可能等待线程被唤醒。对于这种情况，就需要安全区域( Safe Region)来解决。 安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始GC都是安全的。我们也可以把 Safe Region看做是被扩展了的 Safepoint。 实际执行时： 1、当线程运行到 Safe Region的代码时，首先标识已经进入了 Safe Region,如果这段时间内发生GC,JVM会忽略标识为 Safe Region状态的线程 2、当线程即将离开 Safe Region时，会检査JVM是否已经完成GC,如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开 Safe Regiong的信号为止； 6.引用 ​ 我们希望能描述这样一类对象：当内存空间还足够时，则能保留在内存中；如果内存空间在进行垃圾收集后还是很紧张，则可以抛弃这些对象。 【既偏门又非常高频的面试题】强引用、软引用、弱引用、虚引用有什么区别？具体使用场景是什么？ ​ 在JDK1.2版之后，Java对引用的概念进行了扩充，将引用分为强引用（ Strong Reference)、软引用( Soft Reference)、弱引用( Weak Reference)和虚引用(Phantom Reference)4种，这4种引用强度依次逐渐减弱。 ​ 除强引用外，其他3种引用均可以在java.lang. ref包中找到它们的身影。如下图，显示了这3种引用类型对应的类，开发人员可以在应用程序中直接使用它们。 Reference子类中只有终结器引用是包内可见的，其他3种引用类型均为public可以在应用程序中直接使用 强引用( StrongReference)😗*【引用在就别管我】**最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似“ Object obj= new Object（）”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用( SoftReference)😗*【快满了回收我】**在系统将要发生内存溢出之前，将会把这些对象列入回收范围之中进行第二次回收。如果这次回收后还没有足够的内存，才会抛出内存溢出异常。 弱引用( WeakReference)😗*【看到就回收我】**被弱引用关联的对象只能生存到下一次垃圾收集之前。当垃圾收集器工作时，无论内存空间是否足够,都会回收掉被弱引用关联的对象. 虚引用( PhantomReference)😗*【不影响你】**一个对象是否有虚引用的存在,完全不会对其生存时间构成影响,也无法通过虚引用来获得一个对象的实例.为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 强引用 在Java程序中，最常见的引用类型是强引用(普通系统99%以上都是强引用)，也就是我们最常见的普通对象引用，也是默认的引用类型。 当在JAVA语言中使用new操作符创建一个新的对象，并将其赋值给一个变量的时候，这个变量就成为指向该对象的一个强引用。 强引用的对象是可触及的，垃圾收集器就永远不会回收掉被引用的对象 对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为null,就是可以当做垃圾被收集了，当然具体回收时机还是要看垃圾收集策略。 相对的，软引用、弱引用和虚引用的对象是软可触及、弱可触及和虚可触及的，在一定条件下，都是可以被回收的。所以，强引用是造成Java内存泄漏的主要原因之 软引用 软引用是用来描述一些还有用，但非必需的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，オ会抛出内存溢出异常。 软引用通常用来实现内存敏感的缓存。比如：高速缓存就有用到软引用。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 垃圾回收器在某个时刻决定回收软可达的对象的时候，会清理软引用，并可选地把引用存放到一个引用队列( Reference Queue)。 类似弱引用，只不过Java虚拟机会尽量让软引用的存活时间长一些，迫不得已才清理。 弱引用 弱引用也是用来描述那些非必需对象，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。在系统GC时，只要发现弱引用，不管系统堆空间使用是否充足，都会回收掉只被弱引用关联的对象 但是，由于垃圾回收器的线程通常优先级很低，因此，并不一定能很快地发现持有弱引用的对象。在这种情况下，弱引用对象可以存在较长的时间。 弱引用和软引用一样，在构造弱引用时，也可以指定一个引用队列，当弱引用对象被回收时，就会加入指定的引用队列，通过这个队列可以跟踪对象的回收情况。 软引用、弱引用都非常适合来保存那些可有可无的缓存数据。如果这么做，当系统内存不足时，这些缓存数据会被回收，不会导致内存溢出。而当内存资源充足时，这些缓存数据又可以存在相当长的时间，从而起到加速系统的作用。 弱引用对象与软引用对象的最大不同就在于，当GC在进行回收时，需要通过算法检查是否回收软引用对象，而对于弱引用对象，GC总是进行回收。弱引用对象更容易、更快被GC回收。面试题：你开发中使用过 WeakHashMap吗？ 虚引用 ​ 也称为“幽灵引用”或者“幻影引用”，是所有引用类型中最弱的一个。一个对象是否有虚引用的存在，完全不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它和没有引用几乎是一样的，随时都可能被垃圾回收器回收。它不能单独使用，也无法通过虚引用来获取被引用的对象。当试图通过虚引用的get（）方法取得对象时，总是null。为一个对象设置虚引用关联的唯一目的在于跟踪垃圾回收过程。比如：能在这个对象被收集器回收时收到一个系统通知。 ​ 虚引用必须和引用队列一起使用。虚引用在创建时必须提供一个引用队列作为参数。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象后，将这个虚引用加入引用队列，以通知应用程序对象的回收情况。由于虚引用可以跟踪对象的回收时间，因此，也可以将一些资源释放操作放置在虚引用中执行和记录。在JDK1.2版之后提供了 PhantomReference类来实现虚引用。 终结器引用 ​ 它用以实现对象的finalize()方法，也可以称为终结器引用。无需手动编码，其内部配合引用队列使用。在GC时，终结器引用入队。由Finalizer线程通过终结器引用找到被引用对象并调用它的finalize()方法，第二次GC时才能回收被引用对象。 十七、垃圾回收器 1.GC分类 ​ 垃圾收集器没有在规范中进行过多的规定，可以由不同的厂商、不同版本的JVM来实现。由于JDK的版本处于高速送代过程中，因此Java发展至今已经衍生了众多的GC版本。从不同角度分析垃圾收集器，可以将GC分为不同的类型。 ​ 按线程数分，可以分为串行垃圾回收器和并行垃圾回收器。 ​ 串行回收指的是在同一时间段内只允许有一个CPU用于执行垃圾回收操作，此时工作线程被暂停，直至垃圾收集工作结束。在诸如单CPU处理器或者较小的应用内存等硬件平台不是特别优越的场合，串行回收器的性能表现可以超过并行回收器和并发回收器。所以，串行回收默认被应用在客户端的Client模式下的JVM中在并发能力比较强的CPU上，并行回收器产生的停顿时间要短于串行回收器。 ​ 和串行回收相反，并行收集可以运用多个CPU同时执行垃圾回收，因此提升了应用的吞吐量，不过并行回收仍然与串行回收一样，采用独占式，使用了“Stop-the- world”机制。 ​ **按照工作模式分，**可以分为并发式垃圾回收器和独占式垃圾回收器。 ​ 并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间。独占式垃圾回收器( Stop the world)一旦运行，就停止应用程序中的所有用户线程，直到垃圾回收过程完全结束 ​ 按碎片处理方式分，可分为压缩式垃圾回收器和非压缩式垃圾回收器 ​ 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。非压缩式的垃圾回收器不进行这步操作。 ​ 按工作的内存区间分，又可分为年轻代垃圾回收器和老年代垃圾回收器。 2.GC性能评估指标 吞吐量：运行用户代码的时间占总运行时间的比例(总运行时间：程序的运行时间十内存回收的时间) 垃圾收集开销：吞吐量的补数，垃圾收集所用时间与总运行时间的比例 暂停时间：执行垃圾收集时，程序的工作线程被暂停的时间 收集频率：相对于应用程序的执行，收集操作发生的频率。 内存占用：JAVA堆区所占的内存大小。 快速：一个对象从诞生到被回收所经历的时间。 ​ 这三者共同构成一个“不可能三角”。三者总体的表现会随着技术进步而越来越好。一款优秀的收集器通常最多同时满足其中的两项。 ​ 这三项里，暂停时间的重要性日益凸显。因为随着硬件发展，内存占用多些越来越能容忍，硬件性能的提升也有助于降低收集器运行时对应用程序的影响，即提高了吞吐量。而内存的扩大，对延退反而带来负面效果。简单来说，主要抓住两点：吞吐量、暂停时间。 3.吞吐量与暂停时间 ​ 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)。比如：虚拟机总共运行了180分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 这种情况下，应用程序能容忍较高的暂停时间，因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的吞吐量优先，意味着在单位时间内，STW的时间最短。 ​ “暂停时间”是指一个时间段内应用程序线程暂停，让GC线程执行的状态。例如，GC期间100毫秒的暂停时间意味着在这100毫秒期间内没有应用程序线程是活动的。暂停时间优先，意味着尽可能让单次STW的时间最短。 ​ 高吞吐量较好因为这会让应用程序的最终用户感觉只有应用程序线程在做“生产性”工作。直觉上，吞吐量越高程序运行越快。 ​ 低暂停时间（低延迟）较好因为从最终用户的角度来看不管是GC还是其他原因导致一个应用被挂起始终是不好的。这取决于应用程序的类型，有时候甚至短暂的260毫秒暂停都可能打断终端用户体验。因此，具有低的较大暂停时间是非常重要的，特别是对于一个交互式应用程序。 ​ 不幸的是”高吞吐量”和”低暂停时间”是一对相互竞争的目标（矛盾）因为如果选择以吞吐量优先，那么必然需要降低内存回收的执行频率，但是这样会导致GC需要更长的暂停时间来执行内存回收。相反的，如果选择以低延迟优先为原则，那么为了降低每次执行内存回收时的暂停时间，也只能频繁地执行内存回收，但这又引起了年轻代内存的缩减和导致程序吞吐量的下降。 ​ 在设计（或使用）GC算法时，我们必须确定我们的目标：一个GC算法只可能针对两个目标之一（即只专注于较大吞吐量或最小暂停时间），或尝试找到一个二者的折衷。现在标准：在最大吞吐量优先的情况下，降低停顿时间。 4，垃圾回收器发展史 有了虚拟机，就一定需要收集垃圾的机制，这就是 Garbage Collection,对应的产品我们称为 Garbage Collector. ・1999年随JDK1.3.1一起来的是串行方式的 Serial GC,它是第一款GC. Pardew垃圾收集器是 Seria收集器的多线程版本 ・2002年2月26日，Paralle GC和 Concurrent Mark Sweep GC跟随JDK1.4.2起发布Parallel GC在JDK6之后成为 Hotspot默认GC.2012年，在JDK1.7u4版本中，G1可用。 ・2017年，JDK9中G1变成默认的垃圾收集器，以替代CMS. ・2018年3月，JDK10中G1垃圾回收器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 ・2018年9月，JDK11发布。引入Bpsi1on垃圾回收器，又被称为&quot;No-Op（无操作）回收器。同时，引入zGC:可伸缩的低延迟垃圾回收器( Experimental). ・2019年3月，JDK12发布。增强G1,自动返回未用堆内存给操作系统。同时，引入Sh. enandoah GC:低停顿时间的GC( Experimenta1). ・2019年9月，JDK13发布。增强ZGC,自动返回未用堆内存给操作系统 ・2020年3月，JDK14发布。删除CMS垃圾回收器。扩展zGC在 macos,和 Windows上的应用 串行回收器： Serial、 Serial Old 并行回收器： Pa rnew、Parallel Scavenge、 Parallel Old 并发回收器：CMS、G1 垃圾收集器的组合关系 新生代收集器： Serial、 Pardew、Parallel Scavenge 老年代收集器： Serial 0ld、Parallel 0ld、CMS 整堆收集器：G1 JDK 8—&gt;JDK 9 实线----》红虚线 —&gt;JDK 14 实线----》绿虚线 并废弃CMS 默认G1 ​ 为什么要有很多收集器，一个不够吗？因为Java的使用场景很多，移动端，服务器等。所以就需要针对不同的场景，提供不同的垃圾收集器，提高垃圾收集的性能。 ​ 虽然我们会对各个收集器进行比较，但并非为了挑选一个最好的收集器出来。没有一种放之四海皆准、任何场景下都适用的完美收集器存在更加没有万能的收集器。所以我们选择的只是对具体应用最合适的收集器。 5.查看默认垃圾回收器 **-XX: PrintCommandLineFlags:**查看命令行相关参数（包含使用的垃圾收集器） 使用命令行指令：jinfo -flag 相关垃圾回收器参数 进程ID Serial 串行回收 Serial 收集器是最基本、历史最悠久的垃圾收集器了。JDK1.3之前回收新生代唯一的选择。 Serial 收集器作为 Hotspot中Client模式下的默认新生代垃圾收集器。 Serial收集器采用复制算法、串行回收和&quot;Stop-the-World&quot;机制的方式执行内存回收。 除了年轻代之外， Serial 收集器还提供用于执行老年代垃圾收集的 Serial Old收集器。 Serial Old收集器同样也采用了串行回收和&quot; Stop the World&quot;机制，只不过内存回收算法使用的是标记一压缩算法。 Serial Old是运行在Client 模式下默认的老年代的垃圾回收器。 Serial Old在 Server,模式下主要有两个用途：①与新生代的Parallel Scavenge配合使用。②作为老年代CMS收集器的后备垃圾收集方案。 优势：简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说， Serial 收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。运行在Client 模式下的虚拟机是个不错的选择。 在用户的桌面应用场景中，可用内存一般不大(几十MB至一两百MB),可以在较短时间内完成垃圾收集(几十ms至一百多ms),只要不频繁发生使用串行回收器是可以接受的。 在 Hotspot虚拟机中，使用 -XX: UseSerialGC参数可以指定年轻代和老年代都使用串行收集器。等价于新生代用 Seral GC,且老年代用 Serial Old GC 总结：这种垃圾收集器大家了解，现在已经不用串行的了。而且在限定单核cpuオ可以用。现在都不是单核的了。对于交互较强的应用而言，这种垃圾收集器是不能接受的。一般在Java Web应用程序中是不会采用串行垃圾收集器的。 ParNew 并行回收 ​ 如果说 Serial GC是年轻代中的单线程垃圾收集器，那么 ParNewl收集器则是 Seral 收集器的多线程版本。Par是 Parallel 的缩写，New:只能处理的是新生代。ParNew收集器除了采用并行回收的方式执行内存回收外，两款垃圾收集器之间几乎没有任何区别。 ParNew收集器在年轻代中同样也是采用复制算法、&quot;Stop-the-world&quot;机制。ParNew是很多JVM运行在 Server模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行方式高效。 对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源） 由于 Pardew收集器是基于并行回收，那么是否可以断定 Parnewl收集器的回收效率在任何场景下都会比 Seral 收集器更高效？ Parnew收集器运行在多CPU的环境下，由于可以充分利用多CPU、多核心等物理硬件资源优势，可以更快速地完成垃圾收集，提升程序的吞吐量。 但是在单个CPU的环境下， Parnewl收集器不比 Seral 收集器更高效。虽然 Serlal收集器是基于串行回收，但是由于CPU不需要频繁地做任务切换，因此可以有效避免多线程交互过程中产生的一些额外开销。 因为除 Seral外，目前只有 PardNew GC能与CMS收集器配合工作。 在程序中，开发人员可以通过选项”-XX: UseParNewGC&quot;手动指定使用ParNewl收集器执行内存回收任务。它表示年轻代使用并行收集器，不影响老年代。 -XX: ParallelGCThreads限制线程数量，默认开启和CPU数据相同的线程数。 Parallel Scavenge 吞吐量优先 HotSpot的年轻代中除了拥有 ParNew收集器是基于并行回收的以外，Parallel Scavenge收集器同样也采用了复制算法、并行回收和&quot;Stop The World&quot;机制。 那么 Parallel 收集器的出现是否多此一举？ 和 Pardew收集器不同， Parallel Scavenge收集器的目标则是达到个可控制的吞吐量( Throughput),它也被称为吞吐量优先的垃圾收集器。 自适应调节策略也是 Parallel Scavenge与 ParNew一个重要区别。 高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如，那些执行批量处理、订单处理、工资支付、科学计算的应用程序。 Parallel 收集器在JDK1.6时提供了用于执行老年代垃圾收集的 Parallel Old 收集器，用来代替老年代的 Seral Old收集器。 Parallel Old 收集器采用了标记-压缩算法，但同样也是基于并行回收和&quot;Stop-the- World&quot;机制。 ​ 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间 垃圾收集时间)。比如：虚拟机总共运行了160分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 ​ 这种情况下，应用程序能容忍较高的暂停时间，因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的。吞吐量优先，意味着在单位时间内，ST的时间最短。在程序吞吐量优先的应用场景中，Parallel 收集器和 Parallel Old收集器的组合，在 Server模式下的内存回收性能很不错。在Java8中，默认是此垃圾收集器。 参数设置: -XX: +UseParallelGC手动指定年轻代使用Para1le1并行收集器执行内存回收任务 -XX: +UseParallelOldGC手动指定老年代都是使用并行回收收集器。分别适用于新生代和老年代。默认jdk8是开启的。上面两个参数，默认开启一个，另一个也会被开启。（互相激活） -XX: +ParallelGCThreads设置年轻代并行收集器的线程数。一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能。在默认情况下，当CPU数量小于8个，ParallelGCThreads的值等于CPU数量。当CPU数量大于8个，ParallelGCThreads的值等于3+[5*CPU_Count] / 8]。 -XX: MaxGCPauseMillis 设置垃圾收集器最大停顿时间(即STW的时间)。单位是毫秒。 为了尽可能地把停顿时间控制在 MaxGCPauseMillis 以内，收集器在工作时会调整Java堆大小或者其他一些参数。对于用户来讲，停顿时间越短体验越好。但是在服务器端，我们注重高并发，整体的吞吐量。所以服务器端适合 Paralle1,进行控制。该参数使用需谨慎。 -XX: GCTimeRatio 垃圾收集时间占总时间的比例 (=1/(N+1))用于衡量吞吐量的大小。 取值范围(0,100)。默认值99,也就是垃圾回收时间不超过1%。与前一个 -XX: MaxGCPauseMillis参数有一定矛盾性。暂停时间越长， Radio参数就容易超过设定的比例。 -XX: UseAdaptiveSizePoltcy 设置 Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小、Eden和 Survivor的比例、晋升老年代的对象年龄等参数会被自动调整，已达到在堆大小、吞吐量和停顿时间之间的平衡点。在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、日标的吞吐量( GCTimeRatio )和停顿时间( MaxGCPauseMillis ),让虚拟机自己完成调优工作。 CMS 低延迟 在JDK1.5时期， HotSpot推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器：CMS( Concurrent - Mark - Sweep)收集器，这款收集器是 HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。 CMS收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短（低延迟）就越适合与用户交互的程序，良好的响应速度能提升用户体验。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 CMS的垃圾收集算法采用标记一清除算法，并且也会&quot;Stop-the-world 不幸的是，CMS作为老年代的收集器，却无法与JDK1.4.8中已经存在的新生代收集器Parallel Scavenge配合工作，所以在JDK1.5中使用CMS来收集老年代的时候，新生代只能选择 Pardew或者 Sera1收集器中的一个。在G1出现之前，CMS使用还是非常广泛的。一直到今天，仍然有很多系统使用 CMS GC。 工作原理 CMS整个过程比之前的收集器要复杂，整个过程分为4个主要阶段，即初始标记阶段、并发标记阶段、重新标记阶段和并发清除阶段。 **初始标记( Initial-Mark)阶段：**在这个阶段中，程序中所有的工作线程都将会因为“Stop-the-World”机制而出现短暂的暂停，这个阶段的主要任务仅仅只是标记出GC Roots能直接关联到的对象。一旦标记完成之后就会恢复之前被暂停的所有应用线程。由于直接关联对象比较小，所以这里的速度非常快。 **并发标记( Concurrent-Mark)阶段：**从 GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 **重新标记( Remark)阶段：**由于在并发标记阶段中，程序的工作线程会和垃圾收集线程同时运行或者交叉运行，因此为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。 **并发清除( Concurrent- Sweep)阶段：**此阶段清理删除掉标记阶段判断的己经死亡的对象，释放内存空间。由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。 特点与弊端 尽管CMS收集器采用的是并发回收（非独占式），但是在其初始化标记和再次标记这两个阶段中仍然需要执行“Stop-The-Wolrd”机制暂停程序中的工作线程，不过暂停时间并不会太长，因此可以说明目前所有的垃圾收集器都做不到完全不需要 “Stop-The-Wolrd”，只是尽可能地缩短暂停时间。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 另外，由于在垃圾收集阶段用户线程没有中断，所以在CMS回收过程中，还应该确保应用程序用户线程有足够的内存可用。因此，CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在CMS工作过程中依然有足够的空间支持应用程序运行。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“ Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用 Seral Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。 CMS收集器的垃圾收集算法采用的是标记一清除算法，这意味着每次执行完内存回收后，由于被执行内存回收的无用对象所占用的内存空间极有可能是不连续的一些内存块，不可避免地将会产生一些内存碎片。那么CMS在为新对象分配内存空间时，将无法使用指针碰撞( Bump the Pointer)技术，而只能够选择空闲列表( Free List)执行内存分配。为什么不使用Mark Compact？因为当并发清除的时候，用 Compact 整理内存的话，原来的用户线程使用的内存还怎么用呢？要保证用户线程能继续执行，前提是它运行的资源不受影响。 Mark Compact更适合“ Stop the World”这种场景下使用。 CMS的优点： 并发收集、低延迟 CMS的弊端： **1)会产生内存碎片，**导致并发清除后，用户线程可用的空间不足。在无法分配大对象的情况下，不得不提前触发Full GC。 **2)CMS收集器对CPU资源非常敏感。**在并发阶段，它虽然不会导致用户停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。 **3)CMS收集器无法处理浮动垃圾。**可能出现“ Concurrent Mode Failure&quot;失败而导致另一次Full GC的产生。在并发标记阶段由于程序的工作线程和垃圾收集线程是同时运行或者交叉运行的，那么在并发标记阶段如果产生新的垃圾对象，CMS将无法对这些垃圾对象进行标记，最终会导致这些新产生的垃圾对象没有被及时回收，从而只能在下一次执行GC时释放这些之前未被回收的内存空间。 参数设置 **-XX: UseConcMarkSweepGC **手动指定使用CMS收集器执行内存回收任务。开启该参数后会自动将 -XX: UseParNEWGC打开。即： ParNew( Young区用) +CMS(Old区用) Serial Old的组合。 -XX: CMSLnitiatingOccupanyFraction 设置堆内存使用率的阈值，一旦达到该阈值，便开始进行回收。JDK5及以前版本的默认值为68,即当老年代的空间使用率达到68%时，会执行一次CMS回收。JDK6及以上版本默认值为92%。如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低CMS的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。因此通过该选项便可以有效降低Full GC的执行次数。 **-XX: UseCMSCompactAtFullCollection **用于指定在执行完Full GC后对内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。 -XX: CMSFullGCsBeforeCompaction 设置在执行多少次Full GC后对内存空间进行压缩整理。 -XX: ParalellCMSThreads 设置CMS的线程数量。CMS默认启动的线程数是(ParalellCMSThreads+3)/4。ParalellCMSThreads是年轻代并行收集器的线程数。当CPU资源比较紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。 总结 小结： HotSpot有这么多的垃圾回收器，那么如果有人问， Serial GC、ParalleGC、Concurrent Mark Sweep GC.这三个GC有什么不同呢？ 请记住以下口令： 如果你想要最小化地使用内存和并行开销，请选 Serial GC; 如果你想要最大化应用程序的吞吐量，请选 Paralle GC; 如果你想要最小化GC的中断或停顿时间，请选 CMS GC。 JDK9新特性：CMS 被标记为 Deprecate(JEP291)。如果对JDK9及以上版本的 Hotspot,虚拟机使用参数 -XX: UseConcMarkSweepGC 来开启CMS收集器的话，用户会收到一个警告信息，提示CMS未来将会被废弃。 DK14新特性：删除CMS垃圾回收器(JEP363)移除了CMS垃圾收集器，如果在JDK14中使用 -XX: UseConcMarkSweepGC的话，JVM不会报错，只是给出一个warning信息，但是不会exit。JVM会自动回退以默认GC方式启动JVM。 G1 区域分代化 为什么名字叫做 Garbage First(G1)呢？ 因为G1是一个并行回收器，它把堆内存分割为很多不相关的区域( Region)（物理上不连续的）。使用不同的 Region来表示Eden、幸存者0区，幸存者1区，老年代等。 G1 GC有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个 Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region由于这种方式的侧重点在于回收垃圾最大量的区间( Region),所以我们给G1一个名字：垃圾优先(Garbage First)。 既然我们已经有了前面几个强大的GC,为什么还要发布 Garbage First(G1)GC? 原因就在于应用程序所应对的业务越来越庞大、复杂，用户越来越多，没有GC就不能保证应用程序正常进行，而经常造成STW的GC又跟不上实际的需求，所以才会不断地尝试对GC进行优化。G1( Garbage- First)垃圾回收器是在Java7 update4之后引入的一个新的垃圾回收器，是当今收集器技术发展的最前沿成果之一。 与此同时，为了适应现在不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间( pause time),同时兼顾良好的吞吐量。官方给G1设定的目标是在延退可控的情况下获得尽可能高的吞吐量，所以才担当起“全功能收集器”的重任与期望。 ​ G1( Garbage- First)是一款面向服务端应用的垃圾收集器，主要针对配备多核CPU及大容量内存的机器，以极高概率满足GC停顿时间的同时，还兼具高吞吐量的性能特征。 ​ 在JDK1.7版本正式启用，移除了 Experimental的标识，是JDK9以后的默认垃圾回收器，取代了CMS回收器以及Paralel Parallel Old组合。被 Oracle官方称为“全功能的垃圾收集器”。与此同时，CMS已经在JDK9中被标记为废弃( deprecated)。在jdk8中还不是默认的垃圾回收器，需要使用 -XX: UseG1GC来启用。 优势 与其他GC收集器相比，G1使用了全新的分区算法，其特点如下所示： 并行与并发 并行性：G1在回收期间，可以有多个GC线程同时工作，有效利用多核计算能力。此时用户线程STW 并发性：G1拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会在整个回收阶段发生完全阻塞应用程序的情况。 分代收集 从分代上看，G1依然属于分代型垃圾回收器，它会区分年轻代和老年代，年轻代依然有Eden区和 Survivor区。但从堆的结构上看，它不要求整个Eden区、年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。将堆空间分为若干个区域( Region),这些区域中包含了逻辑上的年轻代和老年代。和之前的各类回收器不同，它同时兼顾年轻代和老年代。对比其他回收器，或者工作在年轻代，或者工作在老年代； 空间整合 ​ CMS:“标记-清除”算法、内存碎片、若干次GC后进行一次碎片整理。G1将内存划分为一个个的 region。内存的回收是以 regiont作为基本单位的。Region之间是复制算法，但整体上实际可看作是标记一压缩（Mark- Compact算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。尤其是当Java堆非常大的时候，G1的优势更加明显。 可预测的停顿时间模型(即：软实时 soft real-time) ​ 这是G1相对于CMS的另一大优势，G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。由于分区的原因，G1可以只选取部分区域进行内存回收，这样缩小了回收的范围，因此对于全局停顿情况的发生也能得到较好的控制。 ​ G1跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。相比于 CMS GC,G1未必能做到CMS在最好情况下的延时停顿，但是最差情况要好很多。 ​ 相较于CMS,G1还不具备全方位、压倒性优势。比如在用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用( Footprint)还是程序运行时的额外执行负载( overload)都要比CMS要高。从经验上来说，在小内存应用上CMS的表现大概率会优于G1,而G1在大内存应用上则发挥其优势。平衡点在6-8GB之间。 参数设置 -XX:+UseG1GC手动指定使用G1收集器执行内存回收任务 -XX:G1HeapRegionSize设置每个 Regionl的大小。值是2的幂，范围是1MB到32MB之间，目标是根据最小的Java堆大小划分出约2048个区域。默认是堆内存的1/2000 -XX: MaxGCPauseMilllis设置期望达到的最大GC停顿时间指标（JVM会尽力实现，但不保证达到）,默认值是200ms。 -XX:ParallelGCThreads设置STW工作线程数的值。最多设置为8。 -XX:ConcGCThreads设置并发标记的线程数。将n设置为并行垃圾回收线程数(ParallelGCThreadS)的1/4左右。 -XX: InitiatingHeapOccupancyPercent设置触发并发GC周期的Java。堆占用率阈值。超过此值，就触发GC。默认值是45。 G1回收器常见步骤 G1的设计原则就是简化JVM性能调优，开发人员只需要简单的三步即可完成调优 第一步：开启G1垃圾收集器 第二步：设置堆的最大内存 第三步：设置最大的停顿时间 G1中提供了三种垃圾回收模式： YoungGC、 Mixed GC和FullGC,在不同的条件下被触发。 使用场景 面向服务端应用，针对具有大内存、多处理器的机器。（在普通大小的堆里表现并不惊喜） 最主要的应用是需要低GC延迟，并具有大堆的应用程序提供解决方案；如：在堆大小约6GB或更大时，可预测的暂停时间可以低于8.5秒；（G1通过每次只清理一部分而不是全部的 Region的增量式清理来保证每次G停顿时间不会过长）用来替换掉JDK1.5中的CMS收集器 在下面的情况时，使用G1可能比CMS好： ①超过50%的Java堆被活动数据占用； ②对象分配频率或年代提升频率变化很大； ③GC停顿时间过长(长于6.5至1秒)； Hotspot垃圾收集器里，除了G1以外，其他的垃圾收集器使用内置的JVM线程执行。GC的多线程操作，而G1GC可以采用应用线程承担后台运行的GC工作，即当JVM的GC线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾回收过程。 Region ​ 使用G1收集器时，它将整个Java堆划分成约2048个大小相同的独立 Region块，每个 Region块大小根据堆空间的实际大小而定，整体被控制在1MB到32MB之间，且为2的N次幂，即1MB,2MB,4MB,8MB,16MB,32MB。可以通过**-XX:G1HeapRegionSize**设定。所有的 Region大小相同，且在JVM生命周期内不会被改变。 ​ 虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 Region（不需要连续）的集合。通过 Region的动态分配方式实现逻辑上的连续。 ​ 一个 region有可能属于Eden, Survivor或者Old/ Tenured内存区域。但是 一个 region只可能属于一个角色。图中的E表示该 region属于Eden内存区域，S表示属于 Survivor内存区域，O表示属于Old内存区域。图中空白的表示未使用的内存空间。 ​ G1垃圾收集器还增加了一种新的内存区域，叫做 Humongous内存区域，如图中的H块。主要用于存储大对象，如果超过1.5个 region,就放到H。 **设置H的原因：**对于堆中的大对象，默认直接会被分配到老年代，但是如果它是一个短期存在的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个 Humongous区。它用来专门存放大对象。如果一个H区装不下ー个大对象，那么G1会寻找连续的H区来存储。为了能找到连续的H区，有时候不得不启动Full GC.G1的大多数行为都把H区作为老年代的一部分来看待。 G1 回收过程 G1 GC的垃圾回收过程主要包括如下三个环节 年轻代GC( Young GC) 老年代并发标记过程( Concurrent Marking) 混合回收( Mixed GC) (如果需要，单线程、独占式、高强度的FullGC还是继续存在的。它针对GC的评估失败提供了一种失败保护机制，即强力回收。） ​ 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到 Survivor区间或者老年区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值(默认45%)时，开始老年代并发标记过程。标记完成马上开始混合回收过程。对于一个混合回收期，G1GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，G1的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收小部分老年代的 Region就可以了。同时，这个老年代 Region是和年轻代一起被回收的。举个例子：一个Web服务器，Java进程最大堆内存为4G,每分钟响应1500个请求，每45秒钟会新分配大约2G的内存。G1会每45秒钟进行一次年轻代回收，每31个小时整个堆的使用率会达到45%,会开始老年代并发标记过程，标记完成后开始四到五次的混合回收。 Remembered Set 一个对象被不同区域引用的问题。一个 Region不可能是孤立的，一个 Region中的对象可能被其他任意 Region中对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确。在其他的分代收集器，也存在这样的问题(而G1更突出)。回收新生代也不得不同时扫描老年代？这样的话会降低 Minor GC的效率。 解决方法： 无论G1还是其他分代收集器，JVM都是使用 Remembered Set来避免全局扫描：每个 Region都有一个对应的 Remembered Set;每次 Reference类型数据写操作时，都会产生一个 Write Barrier暂时中断操作；然后检查将要写入的引用指向的对象是否和该 Referencea类型数据在不同的 Region(其他收集器：检査老年代对象是否引用了新生代对象）；如果不同，通过 Cardtable把相关引用信息记录到引用指向对象的所在 Region对应的 Remembered Set中；当进行垃圾收集时，在GC根节点的枚举范围加入 Remembered Set:就可以保证不进行全局扫描，也不会有遗漏。 回收过程一：年轻代GC ​ JVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。年轻代垃圾回收只会回收Eden区和 Survivor区。首先61停止应用程序的执行(Stop-The-World),G1创建回收集(Collection Set),回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和 Survivor区所有的内存分段。 然后开始如下回收过程： 第一阶段，扫描根。 根是指 static变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同RSet记录的外部引用作为扫描存活对象的入口 第二阶段，更新RSet. 处理 dirty card queue（见备注）中的card,更新RSet。此阶段完成后，RSet可以准确的反映老年代对所在的内存分段中对象的引用。 第三阶段，处理RSet. 识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。 第四阶段，复制对象。 此阶段，对象树被遍历，Eden区内存段中存活的对象会被复制到 Survivor区中空的内存分段，Survivor区内存段中存活的对象如果年龄未达阈值，年龄会加1,达到阀值会被会被复制到0ld区中空的内存分段。如果 Survivor空间不够，Eden空间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理Soft,Weak, Phantom,Fina1, JNI Weak等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 回收过程二：并发标记过程 初始标记阶段：标记从根节点直接可达的对象。这个阶段是STW的，并且会触发一次年轻代GC. 根区域扫描( Root Region Scanning):G1 GC扫描 Survivor区直接可达的老年代区域对象，并标记被引用的对象。这一过程必须在 young GC之前完成 并发标记( Concurrent Marking):在整个堆中进行并发标记（和应用程序并发执行），此过程可能被 young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性（区域中存活对象的比例）。 再次标记( Remark):由于应用程序持续进行，需要修正上一次的标记结果。是STW的。G1中采用了比CMS更快的初始快照算法： snapshot-at-the- beginning(SATB)。 独占清理(cleanup,STW):计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是ST的。这个阶段并不会实际上去做垃圾的收集并发清理阶段：识别并清理完全空闲的区域。 回收阶段三：混合回收 ​ 当越来越多的对象晋升到老年代。old region时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即 Mixed GC,该算法并不是一个Old GC,除了回收整个 Young Region,还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集，从而可以对垃圾回收的耗时时间进行控制。也要注意的是 Mixed GC并不是Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存分段被回收了，部分为垃圾的内存分段被计算了出来。默认情况下，这些老年代的内存分段会分8次(可以通过XX:G1 MixedGCCountTarget设置)被回收。 混合回收的回收集(Collection Set)包括八分之一的老年代内存分段，Eden区内存分段， Survivor区内存分段。混合回收的算法和年轻代回收的算法完全一样，只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1 MixedGCliveThresholPercent,默认为65%,意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行8次。有一个阈值-XX:G1HeapWastePercent,默认值为10%,意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%,则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。 总结 G1的初衷就是要避免Full GC的出现。但是如果上述方式不能正常工作，G1会停止应用程序的执行(Stop-The-World),使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢？比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用则会回退到Full GC,这种情况可以通过増大内存解决。导致G1 Full GC的原因可能有两个：Evacuation的时候没有足够的to- space来存放晋升的对象。并发处理过程完成之前空间耗尽。 ​ 从 Oracle官方透露出来的信息可获知，回收阶段( Evacuation)其实本也有想过设计成与用户程序一起并发执行，但这件事情做起来比较复杂，考虑到G1只是回收一部分 Reglon,停顿时间是用户可控制的，所以并不迫切去实现，而选择把这个特性放到了G1之后出现的低延迟垃圾收集器(即ZGC)中。另外，还考虑到G1不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案。 优化建议 年轻代大小 避免使用-Xmn或-XX: NewRatio等相关选项显式设置年轻代大小。固定年轻代的大小会覆盖暂停时间目标暂停时间目标不要太过严苛 G1 GC的吞吐量目标是90%的应用程序时间和1%的垃圾回收时间评估G1GC的吞吐量时，暂停时间目标不要太严苛。目标太过严苛表 示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 垃圾回收器总结 Java垃圾收集器的配置对于]W써优化来说是一个很重要的选择，选择合适的垃圾收集器可以让V的性能有一个很大的提升。 怎么选择垃圾收集器？ 优先调整堆的大小让JVM自适应完成 如果内存小于100M,使用串行收集器 如果是单核、单机程序，并且没有停顿时间的要求，串行收集器 如果是多CPU、需要高吞吐量、允许停顿时间超过1秒，选择并行或者JVM自己选择 如果是多CPU、追求低停顿时间，需快速响应（比如延迟不能超过1秒，如互联网应用），使用并发收集器 官方推荐G1,性能高。现在互联网的项目，基本都是使用G1。 最后需要明确一个观点 1.没有最好的收集器，更没有万能的收集； 2.调优永远是针对特定场景、特定需求，不存在一劳永逸的收集器 对于垃圾收集，面试官可以循序渐进从理论、实践各种角度深入，也未必是要求面试者什么都懂。但如果你懂得原理，一定会成为面试中的加分项。 这里较通用、基础性的部分如下 垃圾收集的算法有哪些？如何判断一个对象是否可以回收？ 垃圾收集器工作的基本流程。 另外，大家需要多关注垃圾回收器这一章的各种常用的参数。 GC日志分析 通过阅读GC日志，我们可以了解Java虚拟机内存分配与回收策略。 内存分配与垃圾回收的参数列表 -XX:+PriintGC 输出GC日志。类似：- verbose:GC -XX:+ PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps输出GC的时间戳（以基准时间的形式） -XX:+ PrintGCDateStamps输出GC的时间戳（以日期的形式，如2013-05-04T21:53:59.234+0800) -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -XlogGC:../logs/GC.log日志文件的输出路径 &quot;[GC&quot;和&quot;[Full GC&quot;说明了这次垃圾收集的停顿类型,如果有&quot;Full&quot;则说明GC发生了Stop The World 使用 Serial收集器在新生代的名字是 Default New Generation,因此显示的是&quot;[ ParNew 使用 ParNew收集器在新生代的名字会变成&quot;[Pardew&quot;,意思是&quot; Parallel New Generation&quot; 使用Parallel Scavenge收集器在新生代的名字是&quot;Psyounggen&quot; 老年代的收集和新生代道理一样,名字也是收集器决定的 使用G1收集器的话,会显示为&quot; garbage- first heap&quot; Allocation Failure 表明本次引起GC的原因是因为在年轻代中没有足够的空间能够存储新的数据了。 [ PSZoungGen:5986K-&gt;696K(8704K)]5986K--704K(9216K) 中括号内：GC回收前年轻代大小，回收后大小，（年轻代总大小） 括号外：GC回收前年轻代和老年代大小，回收后大小，（年轻代和老年代总大小） user代表用户态回收耗时，sys内核态回收耗时，rea实际耗时。由于多核的原因，时间总和可能会超过real时间 日志分析工具 可以用一些工具去分析这些GC日志。常用的日志分析工具有： GCVlewer、 GCEsay、 GCHisto、 GCCLogViewer、Hpjmeter、 garbagecat等。 垃圾回收器的新时代发展 GC仍然处于飞速发展之中，目前的默认选项G1GC在不断的进行改进，很多我们原来认为的缺点，例如串行的FullGC、 Card Table扫描的低效等，都已经被大幅改进，例如，JDK10以后，FullGC已经是并行运行，在很多场景下，其表现还略优于 ParallelGC的并行FullGC实现。即使是 SerialGC,虽然比较古老，但是简单的设计和实现未必就是过时的，它本身的开销，不管是GC相关数据结构的开销，还是线程的开销，都是非常小的，所以随着云计算的兴起，在 Serverless:等新的应用场景下， SeralGC找到了新的舞台。比较不幸的是 CMS GC,因为其算法的理论缺陷等原因，虽然现在还有非常大的用户群体，但在3DK9中已经被标记为废弃，并在JDK14版本中移除。 Open JDK12的 Shenandoah GC:低停颠时间的cc（实验性） Shenandoah,无疑是众多GC中最孤独的一个。是第一款不由 Oracle公司团队领导开发的 Hotspot垃圾收集器。不可避免的受到官方的排挤。比如号称openJDK和oracleJDK没有区别的oracle公司仍拒绝在oracleJDK12中支持 Shenandoah。Shenandoah垃圾回收器最初由 Redhat进行的一项垃圾收集器研究项目 Pauseless GC的实现，旨在针对]WM上的内存回收实现低停顿的需求。在2014年贡献给OPENJDK. Red Hat研发 Shenandoah团队对外宣称， Shenandoah垃圾回收器的暂停时间与堆大小无关，这意味着无论将堆设置为200MB还是200GB,99.9%的目标都可以把垃圾收集的停顿时间限制在十毫秒以内。不过实际使用性能将取决于实际工作堆的大小和工作负载。 总结： Shenandoah GC的弱项：高运行负担下的吞吐量下降。 Shenandoah GC的强项：低延迟时间。 Shenandoah GC的工作过程大致分为九个阶段，这里就不再赘述。在之前 Java12新特性视频里有过介绍。 ZGC ​ ZGC与 Shenandoah目标高度相似，在尽可能对吞吐量影响不大的前提下实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。《深入理解Java虚拟机》一书中这样定义zGC:2GC收集器是一款基于Region内存布局的，（暂时）不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记一压缩算法的，以低延迟为首要目标的一款垃圾收集器2GC的工作过程可以分为4个阶段：并发标记一并发预备重分配一并发重分配并发重映射等。ZGC几乎在所有地方并发执行的，除了初始标记的是STW的。所以停顿时间几乎就耗费在初始标记上，这部分的实际时间是非常少的。 JBP364:ZGC应用在 macos上 JEP365:ZGC应用在 Windows上 JDK14之前，ZGC仅inuxオ支持。 ・尽管许多使用ZGC的用户都使用类エinux的环境，但在 Windows和 macos上，人们也需要ZGC进行开发部署和测试。许多桌面应用也可以从ZGC中受益。因此，ZGC特性被移植到了 Windows和 macos上。 ・现在mac或 Windows上也能使用ZGC了，示例如下：XX: +UnlockExperimentalVMOptions -XX: +UseZGC 复习【processon作图，完善流程过程中再次熟悉】 完成：JVM总图 - ProcessOn","link":"/2021/11/10/Draft/2021/JVM%20%E4%B8%8E%E4%B8%8A%E5%B1%82%E6%8A%80%E6%9C%AF/"},{"title":"魑魅先生 | 前端","text":"VUE、HTML、JS、、、、 VUE 准备 教程：尚硅谷Vue2.0+Vue3.0全套教程丨vuejs从入门到精通_哔哩哔哩_bilibili 简介 安装 创建新项目 vue init 是vue-cli2.x的初始化方式，可以使用github上面的一些模板来初始化项目 webpack是官方推荐的标准模板名 使用方式：vue init webpack 项目名称 electron-vue的模板 使用方式：vue init simulatedgreg/electron-vue 项目名称 vue create 是vue-cli3.x的初始化方式，模板是固定的，模板选项可自由配置 使用方式：vue create 项目名称 初始化项目文件结构说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263│ .babelrc│ .editorconfig│ .eslintignore│ .eslintrc.js│ .gitignore│ .postcssrc.js│ index.html│ package-lock.json│ package.json│ README.md│├─build│ build.js│ check-versions.js│ logo.png│ utils.js│ vue-loader.conf.js│ webpack.base.conf.js│ webpack.dev.conf.js│ webpack.prod.conf.js│├─config│ dev.env.js│ index.js│ prod.env.js│ test.env.js│├─node_modules├─src│ │ App.vue│ │ main.js│ ││ ├─assets│ │ logo.png│ ││ ├─components│ │ HelloWorld.vue│ ││ └─router│ index.js│├─static│ .gitkeep│└─test ├─e2e │ │ nightwatch.conf.js │ │ runner.js │ │ │ ├─custom-assertions │ │ elementCount.js │ │ │ └─specs │ test.js │ └─unit │ .eslintrc │ jest.conf.js │ setup.js │ └─specs HelloWorld.spec.js Src工程化文件目录结构 1234567891011121314151617├─src│ │ App.vue│ │ main.js│ ││ ├─api│ ├─assets│ │ logo.png│ ││ ├─components│ │ HelloWorld.vue│ ││ ├─layout│ ├─router│ │ index.js│ ││ ├─utils│ └─views 页面跳转 12341：router-link跳转2：this.$router.push() 3：this.$router.replace() 4：this.$router.go(n) 组件之间传值 生命周期 ES6 变量定义 ES5 var定义的变量，没有块的概念，可以跨块{}访问, 不能跨函数访问，可多次重复定义相同变量，定义的变量提前用显示undefine ES6 let定义的变量，只能在块作用域里访问，不能跨块访问，也不能跨函数访问，不可多次重复定义相同变量，定义的变量提前用显示报错未定义 const用来定义只读常量，使用时必须初始化(即必须赋值)，只能在块作用域里访问，而且不能修改指针。 this.$nextTick(()=&gt;{创建地图对象代码})顺序加载DOM Yarn/NPM UniApp https://uniapp.dcloud.io/ 一段开发多端运行，基于VUE，可开发小程序 MiniProgram HTML5 Javascript Bootstrap CSS Ajax JQuery Axios vue-axios 准备 123456npm install --save axios vue-axiosimport Vue from 'vue'import axios from 'axios'import VueAxios from 'vue-axios'Vue.use(VueAxios, axios) ​ axios 使用 post 发送数据时，默认是直接把 json 放到请求体中提交到后端的。也就是说，我们的 Content-Type 变成了 application/json;charset=utf-8 ,这是axios默认的请求头content-type类型。但是实际我们后端要求的 'Content-Type': 'application/x-www-form-urlencoded' 为多见. 1234567891011121314151617181920212223//POST: let data = Qs.stringify({ a: 2, b: &quot;11&quot;, }); await axios .post(url, params, { headers: { &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, Host: &quot;172.0.0.1:0000&quot;, }, }) .then((res) =&gt; { console.log(&quot;结果&quot;, res); if ( ) { } }) .catch((err) =&gt; { console.log(err); });//GET: axios.get(url) CLI webpack querystring A querystring parsing and stringifying library with some added security. 安装：npm install qs // 或者 npm install querystring ES6引入：import querystring from 'querystring' parse、decode 将query字符串解析成对象 escape 参数编码 unescape 参数解码 encode、stringify 将对象转换成query字符串。如果属性值不是string、boolean和number中的一种，它就不能序列化，返回内容中的关键字对应的值为空 Element-UI 123456789101112131415161718192021//安装npm i element-ui -S//引入&lt;!-- 引入样式 --&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/element-ui/lib/theme-chalk/index.css&quot;&gt; &lt;!-- 引入组件库 --&gt; &lt;script src=&quot;https://unpkg.com/element-ui/lib/index.js&quot;&gt;&lt;/script&gt;//VUE main.jsimport Vue from 'vue';import ElementUI from 'element-ui';import 'element-ui/lib/theme-chalk/index.css';import App from './App.vue';Vue.use(ElementUI);new Vue({ el: '#app', render: h =&gt; h(App)});//使用：复制 组件 tree动态新增子节点 iconfont-阿里巴巴矢量图标库 vue create 和vue init webpack的区别 vue create 是vue-cli3.x的初始化方式 vue init 是vue-cli2.x的初始化方式，可以使用github上面的一些模板来初始化项目，webpack是官方推荐的标准模板名。 vue cli2升级到vue cli3 1.先升级npm的版本 npm install -g npm 2.再卸载之前的vue cli 2.9.6 npm uninstall -g @vue/cli 3.下载最新的vue cli版本 npm install -g @vue/cli 区别 1.打包方式： cli2 运行:npm run dev cli3 运行：npm run serve 至于为什么会变，来看一下package.json 2.文件夹目录： 3.cli3 文件目录：取消掉了config目录、build目录、static目录 ,还有最重要的一点，3.0的安装项目时自动下载node-model，vue.config.js也没了，需要手动添加。 3.创建项目方式： 3.cli3的安装：vue create project 2.cli2的安装：vue init webpack project 其他 Ajax：ajax最早出现的发送后端请求技术，隶属于原始js中，核心使用XMLHttpRequest对象，多个请求之间如果有先后关系的话，就会出现回调地狱。 Jquery Ajax：是jQuery框架中的发送后端请求技术，由于jQuery是基于原始的基础上做的封装，所以，jquery Ajax自然也是原始ajax的封装。 Promise:Promise主要用于解决异步回调嵌套的问题。 Fetch：fetch号称是AJAX的替代品，是在ES6出现的，使用了ES6中的promise对象。Fetch是基于promise设计的。Fetch的代码结构比起ajax简单多了，参数有点像jQuery ajax。但是，一定记住fetch不是ajax的进一步封装，而是原生js。Fetch函数就是原生js，没有使用XMLHttpRequest对象。 axios：axios是通过promise实现对ajax技术的一种封装，就像jQuery实现ajax封装一样。即ajax技术实现了网页的局部数据刷新，axios实现了对ajax的封装。它不仅可以在客户端使用，也可以在nodejs端使用，也可以在请求和响应阶段进行拦截 开发技巧 VsCode代码片段模板 添加路径：Windows/Linux: File→Preferences→User Snippets macOS: Code→Preferences→User Snippets 示例：VUE 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&quot;Print to console&quot;: { &quot;prefix&quot;: &quot;vue&quot;, &quot;body&quot;: [ &quot;&lt;!-- $1 --&gt;&quot;, &quot;&lt;template&gt;&quot;, &quot;&lt;div class='$2'&gt;$5&lt;/div&gt;&quot;, &quot;&lt;/template&gt;&quot;, &quot;&quot;, &quot;&lt;script&gt;&quot;, &quot;//这里可以导入其他文件（比如：组件，工具js，第三方插件js，json文件，图片文件等等）&quot;, &quot;//例如：import 《组件名称》 from '《组件路径》';&quot;, &quot;&quot;, &quot;export default {&quot;, &quot;//import引入的组件需要注入到对象中才能使用&quot;, &quot;components: {},&quot;, &quot;data() {&quot;, &quot;//这里存放数据&quot;, &quot;return {&quot;, &quot;&quot;, &quot;};&quot;, &quot;},&quot;, &quot;//监听属性 类似于data概念&quot;, &quot;computed: {},&quot;, &quot;//监控data中的数据变化&quot;, &quot;watch: {},&quot;, &quot;//方法集合&quot;, &quot;methods: {&quot;, &quot;&quot;, &quot;},&quot;, &quot;//生命周期 - 创建完成（可以访问当前this实例）&quot;, &quot;created() {&quot;, &quot;&quot;, &quot;},&quot;, &quot;//生命周期 - 挂载完成（可以访问DOM元素）&quot;, &quot;mounted() {&quot;, &quot;&quot;, &quot;},&quot;, &quot;beforeCreate() {}, //生命周期 - 创建之前&quot;, &quot;beforeMount() {}, //生命周期 - 挂载之前&quot;, &quot;beforeUpdate() {}, //生命周期 - 更新之前&quot;, &quot;updated() {}, //生命周期 - 更新之后&quot;, &quot;beforeDestroy() {}, //生命周期 - 销毁之前&quot;, &quot;destroyed() {}, //生命周期 - 销毁完成&quot;, &quot;activated() {}, //如果页面有keep-alive缓存功能，这个函数会触发&quot;, &quot;}&quot;, &quot;&lt;/script&gt;&quot;, &quot;&lt;style scoped&gt;&quot;, &quot;//@import url($3); 引入公共css类&quot;, &quot;$4&quot;, &quot;&lt;/style&gt;&quot; ], &quot;description&quot;: &quot;生成vue模板&quot; }, &quot;http-get请求&quot;: { &quot;prefix&quot;: &quot;httpget&quot;, &quot;body&quot;: [ &quot;this.\\\\$http({&quot;, &quot;url: this.\\\\$http.adornUrl(''),&quot;, &quot;method: 'get',&quot;, &quot;params: this.\\\\$http.adornParams({})&quot;, &quot;}).then(({ data }) =&gt; {&quot;, &quot;})&quot; ], &quot;description&quot;: &quot;httpGET请求&quot; }, &quot;http-post请求&quot;: { &quot;prefix&quot;: &quot;httppost&quot;, &quot;body&quot;: [ &quot;this.\\\\$http({&quot;, &quot;url: this.\\\\$http.adornUrl(''),&quot;, &quot;method: 'post',&quot;, &quot;data: this.\\\\$http.adornData(data, false)&quot;, &quot;}).then(({ data }) =&gt; { });&quot; ], &quot;description&quot;: &quot;httpPOST请求&quot; } 调试技巧 侵入式：代码中添加debugger； 侵入式：代码中或控制台输入console（VS中cl快捷方式），代码中使用alert弹出信息。 非侵入式：谷歌浏览器来源调试，网络中查看网络请求相关信息，元素栏进行元素定位同左上角icon作用，应用查看缓存等 谷歌浏览器 ctrl+F5强制刷新缓存 设计之美 技术应用 iframe iframe 父子间传值通信 1、同域 iframe 父子间传值 （1）父页面 1234567891011121314151617&lt;html&gt;&lt;head&gt; &lt;script type=&quot;text/javascript&quot;&gt; function say(){ alert(&quot;parent.html&quot;); } function callChild(){ myFrame.window.say(); myFrame.window.document.getElementById(&quot;button&quot;).value=&quot;调用结束&quot;; } &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;input id=&quot;button&quot; type=&quot;button&quot; value=&quot;调用child.html中的函数say()&quot; onclick=&quot;callChild()&quot;/&gt; &lt;iframe name=&quot;myFrame&quot; src=&quot;child.html&quot;&gt;&lt;/iframe&gt;&lt;/body&gt;&lt;/html&gt; （2）子页面 12345678910111213141516&lt;html&gt;&lt;head&gt; &lt;script type=&quot;text/javascript&quot;&gt; function say(){ alert(&quot;child.html&quot;); } function callParent(){ parent.say(); parent.window.document.getElementById(&quot;button&quot;).value=&quot;调用结束&quot;; } &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;input id=&quot;button&quot; type=&quot;button&quot; value=&quot;调用parent.html中的say()函数&quot; onclick=&quot;callParent()&quot;/&gt;&lt;/body&gt;&lt;/html&gt; 总结：方法调用 123父页面调用子页面方法：FrameName.window.childMethod();子页面调用父页面方法：parent.window.parentMethod(); 2、跨域 iframe 父子间传值 （1）父页面 12345678910111213141516171819202122232425262728293031323334&lt;template&gt; &lt;div&gt; &lt;iframe :src=&quot;iframesrc&quot; id=&quot;a-page&quot;&gt;&lt;/iframe&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { computed:{ iframesrc:function(){ let iframesrc = &quot;http://b.com&quot; return iframesrc } }, created () { // 得到B传来的值 window.addEventListener('message',function(e){ console.log(&quot;B DOM的高度&quot;, e.data) },false); // 监听A页面的事件，发送给B window.addEventListener('scroll', function () { let iframe = document.getElementById('a-page'); let json = { scrollTop: scrollTop, windowHeight: windowHeight, }; iframe.contentWindow.postMessage(json, '*'); }); }}&lt;/script&gt; （2）子页面 123456789101112131415161718192021&lt;template&gt; &lt;div&gt; &lt;div id=&quot;b-page&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default { mounted() { // 获取到B页面的值，发送给A let _this = this let b_pageH = document.getElementById('b-page').scrollHeight; window.parent.postMessage(b_pageH, '*'); // 得到A页面的值 window.addEventListener('message',function(e){ console.log(&quot;e.data.scrollTop&quot;, e.data.scrollTop) console.log(&quot;e.data.windowHeight&quot;, e.data.windowHeight) },false); }}&lt;/script&gt; 接口 1234567891011121314151617181920//导入$ npm install axiosimport axios from &quot;axios&quot;;&lt;script src=&quot;https://unpkg.com/axios/dist/axios.min.js&quot;&gt;&lt;/script&gt;使用axios.request(config)axios.get(url[, config])axios.delete(url[, config])axios.head(url[, config])axios.options(url[, config])axios.post(url[, data[, config]])axios.put(url[, data[, config]])axios.patch(url[, data[, config]])//示例 axios.get(&quot;http://172.18.109.13:8083/512834d664a4d3407b4c18ba3e8b3b66373945ec/ArcGIS/MapService/Catalog/SDE.L2021PS.gis/query?&amp;f=json&quot;).then((res) =&gt; { console.log(&quot;res&quot;, res); }); VUE深拷贝浅拷贝","link":"/2021/03/01/Draft/2021/%E5%89%8D%E7%AB%AF/"},{"title":"魑魅先生 | 业务技术","text":"所有的技术都是为了更好的解决业务 菜鸟一站： 图例： 记录 进行中 完成 JSR303数据校验：javax.validation.constraints cron表达式、正则表达式 日志记录（tomcat、mysql） Excel数据操作 文件上传下载解析到数据库 集成自己的Java前后台框架 CURD（所有框架） 网络通信 单元测试 常用工具类 实际业务 加密解密【AES(CBC)】 缓存 自定义注解 单个配置类型 hutool（JAVA工具类包） 扫码登录 JAVA常用专业名词 图片处理 jeekins与部署实施 动态表单设计与实现 验证码 外部接口应用 工作流 JWT spring security 代码生成 数据库设计(见MySQL优化) 文件格式转换（word--》pdf） 数据格式转换 工具类合集 分页 跨域 单点登录 外发接口规范token验证 支付 短信 后台创建数据【创建时间，id，初始数据】 目录可参考kuangstudy 数据格式 权限管理 JDK高版本 序列化反序列化 开源相关 常用组件：日志（登录、操作）记录、角色权限、性能监控（在线用户、数据流转、服务器性能）、字典管理、参数设置、公告聊天系统、代码生成、定时任务 开源项目研究：JEECG BOOT 低代码开发平台【学习业务：代码生成，工作流，文件管理，单点登录，数据性能监控，消息中心】文档 工具类【文件【Excel、图片、Word等】、网络、安全【Xss】、格式、线程、文本【日期，json】、日志、】 前端：大屏，常用组件（） 开源阅读 开源项目 JEECG 环境： 123456789101112npm i yarn -gyarn config set registry https://registry.npm.taobao.org --globalyarn config set disturl https://npm.taobao.org/dist --globalyarn install yarn常用命令yarn / yarn install 等同于npm install 批量安装依赖yarn add xxx 等同于 npm install xxx —save 安装指定包到指定位置yarn remove xxx 等同于 npm uninstall xxx —save 卸载指定包yarn add xxx —dev 等同于 npm install xxx —save-devyarn upgrade 等同于 npm update 升级全部包yarn global add xxx 等同于 npm install xxx -g 全局安装指定包 CRUD 所有知识从CRUD开始 SSM SpringBoot SpringCloud 文件上传下载 相关对象： 1、MultipartFile 和 CommonsMultipartFile都是用来接收上传的文件流的 ！ 2、MultipartFile是一个接口，CommonsMultipartFile是MultipartFile接口的实现类 ！ 3、使用MultipartFile作为形参接收上传文件时，直接用即可。CommonsMultipartFile作为形参接收上传文件时，必需添加@RequestParam注解（否则会报错：exception is java.lang.NoSuchMethodException: org.springframework.web.multipart.commons.CommonsMultipartFile） 12MultipartFile 文件路径： 1HttpServletRequest req Servlet获取当前项目的上下文路径（web文件下的路径）： 1req.getContextPath() Servlet获取当前项目的上下文的绝对路径（web文件下的路径）： 1req.getServletContext().getRealPath(); 获取Java程序中的resources文件路径： 1Resources.getResourceAsStream() 前后端交互： 跨域 解决根本原理 什么情况会跨域 同一协议， 如http或https 同一IP地址, 如127.0.0.1 同一端口, 如8080 以上三个条件中有一个条件不同就会产生跨域问题。 解决方案 前端解决方案 使用JSONP方式实现跨域调用； 使用NodeJS服务器做为服务代理，前端发起请求到NodeJS服务器， NodeJS服务器代理转发请求到后端服务器； 后端解决方案 nginx反向代理解决跨域 服务端设置Response Header(响应头部)的Access-Control-Allow-Origin 在需要跨域访问的类和方法中设置允许跨域访问（如Spring中使用@CrossOrigin注解）； 继承使用Spring Web的CorsFilter（适用于Spring MVC、Spring Boot） 实现WebMvcConfigurer接口（适用于Spring Boot） WebService 简介 跨编程语言和跨操作系统平台的远程调用技术 原理 XML,SOAP和WSDL就是构成WebService平台的三大技术 。 WebService采用Http协议来在客户端和服务端之间传输数据。WebService使用XML来封装数据，XML主要的优点在于它是跨平台的。 WebService通过HTTP协议发送请求和接收结果时，发送的请求内容和结果内容都采用XML格式封装，并增加了一些特定的HTTP消息头，以说明HTTP消息的内容格式，这些特定的HTTP消息头和XML内容格式就是SOAP协议规定的。 WebService服务器端首先要通过一个WSDL文件来说明自己有什么服务可以对外调用。简单的说，WSDL就像是一个说明书，用于描述WebService及其方法、参数和返回值。 WSDL文件保存在Web服务器上，通过一个url地址就可以访问到它。客户端要调用一个WebService服务之前，要知道该服务的WSDL文件的地址。WebService服务提供商可以通过两种方式来暴露它的WSDL文件地址：1.注册到UDDI服务器，以便被人查找；2.直接告诉给客户端调用者。 RESTful 简介 数据格式 JSON 数据示例 1234567{ &quot;sites&quot;: [ { &quot;name&quot;:&quot;a&quot; , &quot;url&quot;:&quot;www.runoob.com&quot; }, { &quot;name&quot;:&quot;b&quot; , &quot;url&quot;:&quot;www.google.com&quot; }, { &quot;name&quot;:&quot;c&quot; , &quot;url&quot;:&quot;www.weibo.com&quot; } ]} 不同数据相互转换方式 字符串，Map，对象，JSONObject（无序），JSONArray（有序），Java数组 字符串2Json 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849```### 判断是否包含.has### CRUD### 取值```javastatic Map analysisJsonResultMap = new HashMap(); public static Map analysisJson(Object objJson) { //如果obj为json数组 if (objJson instanceof JSONArray) { JSONArray objArray = (JSONArray) objJson; for (int i = 0; i &lt; objArray.size(); i++) { analysisJson(objArray.get(i)); } } //如果为json对象 else if (objJson instanceof JSONObject) { JSONObject jsonObject = (JSONObject) objJson; Iterator it = jsonObject.keySet().iterator(); while (it.hasNext()) { String key = it.next().toString(); Object object = jsonObject.get(key); //如果得到的是数组 if (object instanceof JSONArray) { JSONArray objArray = (JSONArray) object; analysisJson(objArray); } //如果key中是一个json对象 else if (object instanceof JSONObject) { analysisJson((JSONObject) object); } //如果key中是其他 else { System.out.println(&quot;[&quot; + key + &quot;]:&quot; + object.toString() + &quot; &quot;); analysisJsonResultMap.put(key, object.toString()); } } } return analysisJsonResultMap; } 开源相关 https://www.runoob.com/w3cnote/open-source-license.html 单元测试 分层测试 SpringBoot快捷测试 鼠标放在要测试的类名上command+shift+T DAO Service Controller JUnit 非常简单地组织测试代码，并随时运行它们，JUnit就会给出成功的测试和失败的测试， 还可以生成测试报告，不仅包含测试的成功率，还可以统计测试的代码覆盖率，即被测试的代码本身有多少经过了测试。 对于高质量的代码来说，测试覆盖率应该在80%以上。 此外，几乎所有的IDE工具都集成了JUnit，这样我们就可以直接在IDE中编写并运行JUnit测试。 一是单元测试代码本身必须非常简单，能一下看明白，决不能再为测试代码编写测试； 二是每个单元测试应当互相独立，不依赖运行的顺序； 三是测试时不但要覆盖常用测试用例，还要特别注意测试边界条件，例如输入为0，null，空字符串&quot;&quot;等情况。 测试可回滚 Junit4 or 5 ? 1234567891011121314151617181920212223242526272829303132333435//junit5 Spring Boot 2.2.X以后import org.junit.jupiter.api.Test;//junit4 Spring Boot 2.2.X以后import org.junit.Test;//POM//junit4&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.13.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;//junit5&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;- JUnit — The de-facto standard for unit testing Java applications.- Spring Test &amp; Spring Boot Test — Utilities and integration test support for Spring Boot applications.- AssertJ — A fluent assertion library.- Hamcrest — A library of matcher objects (also known as constraints or predicates).- Mockito — A Java mocking framework.- JSONassert — An assertion library for JSON.- JsonPath — XPath for JSON. Spring Boot 2.2 之后的 pom.xml IDEA Ctrl+Shift+T (Window) 创建测试类 注解变化 @RunWith(SpringRunner.class) + @SpringBootTest =====》 @SpringBootTest 123456//JUnit4@RunWith(SpringRunner.class)@SpringBootTest//JUnit5//@RunWith被@ExtendWith替换。同时有下@SpringBootTest源码得知@SpringBootTest包含@ExtendWith@SpringBootTest 1234567@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@BootstrapWith(SpringBootTestContextBootstrapper.class)@ExtendWith({SpringExtension.class})public @interface SpringBootTest { 总结 Junit4 Junit5 Before BeforeEach After AfterEach BeforeClass BeforeAll AfterClass AfterAll Category Tag RunWith ExtendWith Rule ExtendWith ClassRule RegisterExtension Junit5常用注解 @BeforeClass：针对所有测试，只执行一次，且必须为static void @Before：初始化方法，执行当前测试类的每个测试方法前执行。 @Test：测试方法，在这里可以测试期望异常和超时时间 @After：释放资源，执行当前测试类的每个测试方法后执行 @AfterClass：针对所有测试，只执行一次，且必须为static void @Ignore：忽略的测试方法（只在测试类的时候生效，单独执行该测试方法无效） @RunWith:可以更改测试运行器 ，缺省值 org.junit.runner.Runner 一个单元测试类执行顺序为： @BeforeClass –&gt; @Before –&gt; @Test –&gt; @After –&gt; @AfterClass 每一个测试方法的调用顺序为： @Before –&gt; @Test –&gt; @After 断言(assertThat) 基础语法 assertThat( [value], [matcher statement] ); value 是接下来想要测试的变量值； matcher statement 是使用 Hamcrest 匹配符来表达的对前面变量所期望的值的声明，如果 value 值与 matcher statement 所表达的期望值相符，则测试成功，否则测试失败。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import static org.junit.Assert.*;字符相关匹配符/**equalTo匹配符断言被测的testedValue等于expectedValue，* equalTo可以断言数值之间，字符串之间和对象之间是否相等，相当于Object的equals方法*/assertThat(testedValue, equalTo(expectedValue));/**equalToIgnoringCase匹配符断言被测的字符串testedString*在忽略大小写的情况下等于expectedString*/assertThat(testedString, equalToIgnoringCase(expectedString));/**equalToIgnoringWhiteSpace匹配符断言被测的字符串testedString*在忽略头尾的任意个空格的情况下等于expectedString，*注意：字符串中的空格不能被忽略*/assertThat(testedString, equalToIgnoringWhiteSpace(expectedString);/**containsString匹配符断言被测的字符串testedString包含子字符串subString**/assertThat(testedString, containsString(subString) );/**endsWith匹配符断言被测的字符串testedString以子字符串suffix结尾*/assertThat(testedString, endsWith(suffix));/**startsWith匹配符断言被测的字符串testedString以子字符串prefix开始*/assertThat(testedString, startsWith(prefix));一般匹配符/**nullValue()匹配符断言被测object的值为null*/assertThat(object,nullValue());/**notNullValue()匹配符断言被测object的值不为null*/assertThat(object,notNullValue());/**is匹配符断言被测的object等于后面给出匹配表达式*/assertThat(testedString, is(equalTo(expectedValue)));/**is匹配符简写应用之一，is(equalTo(x))的简写，断言testedValue等于expectedValue*/assertThat(testedValue, is(expectedValue));/**is匹配符简写应用之二，is(instanceOf(SomeClass.class))的简写，*断言testedObject为Cheddar的实例*/assertThat(testedObject, is(Cheddar.class));/**not匹配符和is匹配符正好相反，断言被测的object不等于后面给出的object*/assertThat(testedString, not(expectedString));/**allOf匹配符断言符合所有条件，相当于“与”（&amp;&amp;）*/assertThat(testedNumber, allOf( greaterThan(8), lessThan(16) ) );/**anyOf匹配符断言符合条件之一，相当于“或”（||）*/assertThat(testedNumber, anyOf( greaterThan(16), lessThan(8) ) );数值相关匹配符/**closeTo匹配符断言被测的浮点型数testedDouble在20.0¡À0.5范围之内*/assertThat(testedDouble, closeTo( 20.0, 0.5 ));/**greaterThan匹配符断言被测的数值testedNumber大于16.0*/assertThat(testedNumber, greaterThan(16.0));/** lessThan匹配符断言被测的数值testedNumber小于16.0*/assertThat(testedNumber, lessThan (16.0));/** greaterThanOrEqualTo匹配符断言被测的数值testedNumber大于等于16.0*/assertThat(testedNumber, greaterThanOrEqualTo (16.0));/** lessThanOrEqualTo匹配符断言被测的testedNumber小于等于16.0*/assertThat(testedNumber, lessThanOrEqualTo (16.0));集合相关匹配符/**hasEntry匹配符断言被测的Map对象mapObject含有一个键值为&quot;key&quot;对应元素值为&quot;value&quot;的Entry项*/assertThat(mapObject, hasEntry(&quot;key&quot;, &quot;value&quot; ) );/**hasItem匹配符表明被测的迭代对象iterableObject含有元素element项则测试通过*/assertThat(iterableObject, hasItem (element));/** hasKey匹配符断言被测的Map对象mapObject含有键值“key”*/assertThat(mapObject, hasKey (&quot;key&quot;));/** hasValue匹配符断言被测的Map对象mapObject含有元素值value*/assertThat(mapObject, hasValue(value)); 可表达全部的测试思想。 Springboot 可自动生成测试代码 IDEA Ctrl + Shift +T 生成测试类 Service 123456789101112131415161718192021import org.junit.Assert;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import static org.hamcrest.CoreMatchers.*;@RunWith(SpringRunner.class)@SpringBootTestpublic class LearnServiceTest { @Autowired private LearnService learnService; @Test public void getLearn(){ LearnResource learnResource=learnService.selectByKey(1001L); Assert.assertThat(learnResource.getAuthor(),is(&quot;嘟嘟MD独立博客&quot;)); }} Controller MockMvc：可以不启动工程测试接口，MockMvc 实现了对 Http 请求的模拟，能够直接使用网络的形式，转换到 Controller 的调用，这样可以使得测试速度快、不依赖网络环境，而且提供了一套验证的工具，这样可以使得请求的验证统一而且很方便。 mockMvc.perform执行一个请求 MockMvcRequestBuilders.get(“/user/1”)构造一个请求，Post请求就用.post方法 contentType(MediaType.APPLICATION_JSON_UTF8)代表发送端发送的数据格式是application/json;charset=UTF-8 accept(MediaType.APPLICATION_JSON_UTF8)代表客户端希望接受的数据类型为application/json;charset=UTF-8 session(session)注入一个session，这样拦截器才可以通过 ResultActions.andExpect添加执行完成后的断言 ResultActions.andExpect(MockMvcResultMatchers.status().isOk())方法看请求的状态响应码是否为200如果不是则抛异常，测试不通过 andExpect(MockMvcResultMatchers.jsonPath(“$.author”).value(“嘟嘟MD独立博客”))这里jsonPath用来获取author字段比对是否为嘟嘟MD独立博客,不是就测试不通过 ResultActions.andDo添加一个结果处理器，表示要对结果做点什么事情，比如此处使用MockMvcResultHandlers.print()输出整个响应结果信息 单元测试回滚 测试的垃圾数据清理，添加注解 @Transactional 默认引擎是InnoDB有效，MyISAM（MySQL5.5之前默认引擎）不支持事务、也不支持外键 想关闭回滚，只要加上@Rollback(false)注解即可。@Rollback表示事务执行完回滚，支持传入一个参数value，默认true即回滚，false不回滚。 修改默认数据库引擎的步骤 MyISAM适合：(1)做很多count 的计算；(2)插入不频繁，查询非常频繁；(3)没有事务。 InnoDB适合：(1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且表锁定的机会比较大的情况。(4)性能较好的服务器，比如单独的数据库服务器，像阿里云的关系型数据库RDS就推荐使用InnoDB引擎。 show variables like '%storage_engine%'; show create table user; ALTER TABLE user ENGINE=INNODB; TextNG 注解跟junit差不多，但是功能更多。 Swagger UI 接口文档，接口测试 启动类添加注解@EnableSwagger2Doc 123456789101112131415161718192021222324252627282930313233343536@Api：用在请求的类上，表示对类的说明 tags=&quot;说明该类的作用，可以在UI界面上看到的注解&quot; value=&quot;该参数没什么意义，在UI界面上也看到，所以不需要配置&quot; @ApiOperation：用在请求的方法上，说明方法的用途、作用 value=&quot;说明方法的用途、作用&quot; notes=&quot;方法的备注说明&quot; @ApiImplicitParams：用在请求的方法上，表示一组参数说明 @ApiImplicitParam：用在@ApiImplicitParams注解中，指定一个请求参数的各个方面 name：参数名 value：参数的汉字说明、解释 required：参数是否必须传 paramType：参数放在哪个地方 · header --&gt; 请求参数的获取：@RequestHeader · query --&gt; 请求参数的获取：@RequestParam · path（用于restful接口）--&gt; 请求参数的获取：@PathVariable · body（不常用） · form（不常用） dataType：参数类型，默认String，其它值dataType=&quot;Integer&quot; defaultValue：参数的默认值 @ApiResponses：用在请求的方法上，表示一组响应 @ApiResponse：用在@ApiResponses中，一般用于表达一个错误的响应信息 code：数字，例如400 message：信息，例如&quot;请求参数没填好&quot; response：抛出异常的类 @ApiModel：用于响应类上，表示一个返回响应数据的信息 （这种一般用在post创建的时候，使用@RequestBody这样的场景， 请求参数无法使用@ApiImplicitParam注解进行描述的时候） @ApiModelProperty：用在属性上，描述响应类的属性 http转https 两者区别： https：优： 1.数据加密 2.SSL安全机制【客户端访问服务器-&gt;服务器把数字证书+公用密匙 发给客户端-&gt;客户端验证服务器，确保访问的是正确的服务器（不是钓鱼网站）-&gt;客户端生产会话密匙并用公用密匙进行加密再次发给服务器-&gt;服务器用私人密匙进行解密（也就相当于验证客户端），验证成功建立起一条安全的数据传递通道-&gt;服务器把客户端请求的数据打包加密发送给客户端-&gt;客户端浏览器接收数据并解析 3.安全标识，提高用户信任感 步骤： 申请SSL证书 安装证书。 整改网站链接 全站做301转向，减少网站权重的流失 相关 服务器 的 80、8080、443（https默认）端口开放需要备案。 nginx 【win】 购买ssl证书获得key和pem文件 123456789101112131415161718192021222324252627server { listen 8080 ssl; server_name aaa.bbb.com; ssl on; ssl_certificate E:/tys/nginx-1.12.2/key_pem//hzsgis.pem; ssl_certificate_key E:/tys/nginx-1.12.2/key_pem/hzsgis.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 20m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #charset koi8-r; #access_log logs/host.access.log main; location /main/map3d/rest { #add_header 'Access-Control-Allow-Headers' '*'; proxy_pass http://111.111.11.11:1111/main/map3d/rest; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; }} nginx 【Linux】 购买ssl证书获得key和pem文件并添加到nginx目录下 改443端口配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events { worker_connections 1024;}http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' # '$status $body_bytes_sent &quot;$http_referer&quot; ' # '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; upstream jq_one{server 10.32.250.176:8080 weight=5;server 10.32.250.177:8080 weight=5;}upstream jq_two{server 10.32.250.178:8080 weight=5;server 10.32.250.179:8080 weight=5;}server {listen 8080;server_name localhost;#charset koi8-r;#access_log logs/host.access.log main;location / {root html;index index.html index.htm;proxy_pass http://jq_two;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header REMOTE-HOST $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;}} server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; proxy_pass http://jq_one; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } server { listen 443 ssl; server_name fyditu.fuyang.gov.cn; ssl_certificate /usr/local/nginx/key_pem//fyditu.fuyang.gov.cn_bundle.pem; ssl_certificate_key /usr/local/nginx/key_pem//fyditu.fuyang.gov.cn.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 20m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; proxy_pass http://jq_one; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } # another virtual host using mix of IP-, name-, and port-based configuration # #server { # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / { # root html; # index index.html index.htm; # } #} # HTTPS server # #server { # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / { # root html; # index index.html index.htm; # } #}} 检验配置文件 12/usr/local/nginx/sbin 安装目录下执行./nginx -t nginx安装SSL模块 123456#进入nginx源码文件夹执行，/usr/local/nginx为安装文件夹./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module#替换安装 非make install覆盖安装make#备份cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak 重启 12345678910111213#查看nginx端口号ps -ef|grep nginx#关闭进程kill -QUIT 端口号#从源码文件夹复制到安装文件夹cp ./objs/nginx /usr/local/nginx/sbin/#先切换到sbin目录cd /usr/local/nginx/sbin/#检测nginx的配置文件是否有错误./niginx -t#启动/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf 开通443端口 123456789101112131415161718#1.先查看服务器防火墙开放的端口firewall-cmd --zone=public --list-ports //查看防火墙的开放端口#2.允许防火墙放行443端口# 命令含义：# --zone #作用域# --add-port=443/tcp #添加端口，格式为：端口/通讯协议# --permanent #代表永久生效，没有此参数重启后失效firewall-cmd --zone=public --add-port=443/tcp --permanent#3.重启防火墙firewall-cmd --reload1.systemctl start firewalld.service（开启防火墙）2.systemctl stop firewalld.service（开启防火墙）3.service firewalld restart（重启防火墙）4.firewall-cmd --zone=public --add-port=4400-4600/udp --permanen(指定端口范围为4400-4600通过防火墙)Warning: ALREADY_ENABLED: 3306:tcp（说明3306端口通过成功）5.firewall-cmd --zone=public --remove-port=80/tcp --permanent（关闭指定端口）6.firewall-cmd --zone=public --list-ports（查看通过的端口）7.查看防火墙状态 ：firewall-cmd --state 定时任务 SpringBoot @Scheduled注解，接口SchedulingConfigurer,Quartz 工作流 Activit 分库分表 在访问量连接数或者数据量大的组合情况下选择不同分库分表方案。根据数据访问量，单库承受量选择分多少。根据表信息选择水平【比如不同等级】或垂直拆分（比如买家买家信息）， 多数据源，主从复制 序列化反序列化 12345678//Java 序列化是指：将对象转化成一个字节序列(二进制数据)的过程。//将序列化对象写入文件之后，可以从文件中读取出来，并且对它进行反序列化。//Java 反序列化是指：将一个对象的字节序列恢复成 Java 对象的过程。//一个平台中序列化的对象，可以在另一个平台中进行反序列化，因为这个过程是在 JVM 中独立完成的，可以依赖于 Java 的可移植性。public class BaseEntity implements Serializable{ private static final long serialVersionUID = 1L;} JAVA常用专业名词 领域模型 VO （View Object）视图对象 用于展示层，它的作用是把某个指定页面（或组件）的所有数据封装起来。通常用于业务层之间的数据传递，和 PO 一样也是仅仅包含数据而已，但应是抽象出的业务对象，可以和表对应，也可以不，这根据业务的需要，用 new 关键字创建，由 GC 回收 view Object 试图对象 接受页面传递来的数据，封装对象，封装页面需要用的数据 DTO（Data Transfer Object）数据传输对象 主要用于展示层与服务层之间的数据传输对象。这个概念来源于 J2EE 的设计模式，原来的目的是为了 EJB的分布式应用提供粗粒度的数据实体，以减少分布式调用的次数，从而提高分数调用的性能和降低网络负载，但在这里，泛指用于展示层与服务层之间的数据传输对象 DO（Domain Object）领域对象 就是从现实世界中抽象出来的有形或无形的业务实体。 PO（Persistent Object）：持久化对象 它跟持久层（通常是关系型数据库）的数据结构形成一一对应的映射关系，如果持久层是关系型数据库，那么，数据表中的每个字段就对应PO的一个属性。po 就是对应数据库中某一个表的一条记录，多个记录可以用 PO 的集合，PO 中应该不包含任何对数据库到操作 TO (Transfer Object) 数据传输对象 不同的应用程序之间传输的对象 BO(business object) 业务对象 从业务模型的角度看，见 UML 原件领域模型中的领域对象，封装业务逻辑的， java 对象，通过调用 DAO 方法，结合 PO VO,进行业务操作，business object 业务对象，主要作用是把业务逻辑封装成一个对象，这个对象包括一个或多个对象，比如一个简历，有教育经历，工作经历，社会关系等等，我们可以把教育经历对应一个 PO 、工作经验对应一个 PO、 社会关系对应一个 PO, 建立一个对应简历的的 BO 对象处理简历，每 个 BO 包含这些 PO ,这样处理业务逻辑时，我们就可以针对 BO 去处理 POJO ( plain ordinary java object) 简单无规则 java 对象 传统意义的 java 对象，就是说一些 Object/Relation Mapping 工具中，能够做到维护数据库表记录的 persisent object 完全是一个符合 Java Bean 规范的 纯 java 对象，没有增加别的属性和方法，我们的理解就是最基本的 Java bean 只有属性字段 setter 和 getter 方法 POJO 时是 DO/DTO/BO/VO 的统称 DAO（data access object） 数据访问对象 是一个 sun 的一个标准 j2ee 设计模式，这个模式有个接口就是 DAO ，他负持久层的操作，为业务层提供接口，此对象用于访问数据库，通常和 PO 结合使用，DAO 中包含了各种数据库的操作方法，通过它的方法，结合 PO 对数据库进行相关操作，夹在业务逻辑与数据库资源中间，配合VO 提供数据库的 CRUD 功能 VO 与 DTO 的区别 DTO 和 VO 的属性值基本是一致的，而且他们通常都是 POJO【简单的 Java 对象（Plain Old Java Object）】,但两者存在本质上的区别；DTO 代表服务层需要接收的数据和返回的数据，而VO 代表展示层需要显示的数据。 DTO 与 DO 的区别 首先是概念上的区别，DTO 是展示层和服务层之间的数据传输对象（可以认为是两者之间的协议），而 DO是对现实世界各种业务角色的抽象，这就引出了两者在数据上的区别。 DO 与 PO 的区别 DO 和 PO 在绝大部分情况下是一一对应的，PO是只含有 get/set 方法的POJO，但某些场景还是能反映出两者在概念上存在本质区别： DO在某些场景下不需要进行显式的持久化，例如利用策略模式设计的商品折扣策略，会衍生出折扣策略的接口和不同折扣策略实现类，这些折扣策略实现类可以算是DO，但它们只会驻留在静态内存池，不需要持久化到持久层，因此，这类 DO 是不存在对应的 PO的。 同样的道理，某些场景下，PO也没有对应的DO，例如老师Teacher和学生Student存在多对多的关系，在关系数据库中，这种关系需要表现为一个中间表，也就对应有一个TeacherAndStudentPO的PO，但这个PO在业务领域没有任何现实的意义，它完全不能与任何DO对应上。 日志记录以及详解 Tomcat 一.tomcat日志文件路径 tomcat/logs 二.tomcat日志配置文件 tomcat对应日志的配置文件：tomcat目录下的/conf/logging.properties tomcat的日志等级有：SEVERE (最高级别) &gt; WARNING &gt; INFO &gt; CONFIG &gt; FINE &gt; FINER(精心) &gt; FINEST (所有内容,最低级别) 三.tomcat日志文件都有哪些 tomcat有五类日志：catalina、localhost、manager、admin、host-manager 四.使用率较高的日志文件是哪些？ 4.1 catalina.out/实时运行日志 即标准输出和标准出错，所有输出到这两个位置的都会进入catalina.out，这里包含tomcat运行自己输出的日志以及应用里向console输出的日志。默认这个日志文件是不会进行自动切割的，我们需要借助其他工具进行切割（注意：catalina.out文件如果过大会影响） 4.2 catalina.yy-mm-dd.log/tomcat运行日志 catalina.{yyyy-MM-dd}.log是tomcat自己运行的一些日志，这些日志还会输出到catalina.out，但是应用向console输出的日志不会输出到catalina.{yyyy-MM-dd}.log,它是tomcat的启动和暂停时的运行日志，注意，它和catalina.out是里面的内容是不一样的。 五.其他日志文件说明 5.1 localhost.yy-mm-dd.log localhost.{yyyy-MM-dd}.log主要是应用初始化(listener, filter, servlet)未处理的异常最后被tomcat捕获而输出的日志,它也是包含tomcat的启动和暂停时的运行日志,但它没有catalina.2018-09-19.log 日志全。它只是记录了部分日志。 5.2 localhost_access_log.yy-mm-dd.txt 这个是访问 tomcat 的日志，请求时间和资源，状态码都有记录。 5.3 manager.yy-mm-dd.log 这个是 tomcat manager 项目专有的日志文件. 5.4 host-manager.yy-mm-dd.log 这个估计是放 tomcat 的自带的 manager 项目的日志信息的，未看到有什么重要的日志信息。 六.tomcat日志文件切割 tomcat 的 catalina.out 文件 tomcat 是不会进行日志切割的，当这个文件大于2G 时，会影响tomcat的运行。那么我们需要对这个文件进行日志切割，切割的方法有很多种： 第一种：通过系统自带的切割工具：logrotate来进行切割。 第二种:使用logj4进行切割日志。 第三种：使用用cronolog分割tomcat的catalina.out文件 。 以上三种方法见：https://www.cnblogs.com/happy-king/p/9193401.html 常用工具类 类型转换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997import java.math.BigDecimal;import java.math.BigInteger;import java.nio.ByteBuffer;import java.nio.charset.Charset;import java.text.NumberFormat;import java.util.Set;import com.jeethink.common.utils.StringUtils;/** * 类型转换器 * */public class Convert{ /** * 转换为字符串&lt;br&gt; * 如果给定的值为null，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static String toStr(Object value, String defaultValue) { if (null == value) { return defaultValue; } if (value instanceof String) { return (String) value; } return value.toString(); } /** * 转换为字符串&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static String toStr(Object value) { return toStr(value, null); } /** * 转换为字符&lt;br&gt; * 如果给定的值为null，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Character toChar(Object value, Character defaultValue) { if (null == value) { return defaultValue; } if (value instanceof Character) { return (Character) value; } final String valueStr = toStr(value, null); return StringUtils.isEmpty(valueStr) ? defaultValue : valueStr.charAt(0); } /** * 转换为字符&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Character toChar(Object value) { return toChar(value, null); } /** * 转换为byte&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Byte toByte(Object value, Byte defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Byte) { return (Byte) value; } if (value instanceof Number) { return ((Number) value).byteValue(); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return Byte.parseByte(valueStr); } catch (Exception e) { return defaultValue; } } /** * 转换为byte&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Byte toByte(Object value) { return toByte(value, null); } /** * 转换为Short&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Short toShort(Object value, Short defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Short) { return (Short) value; } if (value instanceof Number) { return ((Number) value).shortValue(); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return Short.parseShort(valueStr.trim()); } catch (Exception e) { return defaultValue; } } /** * 转换为Short&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Short toShort(Object value) { return toShort(value, null); } /** * 转换为Number&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Number toNumber(Object value, Number defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Number) { return (Number) value; } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return NumberFormat.getInstance().parse(valueStr); } catch (Exception e) { return defaultValue; } } /** * 转换为Number&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Number toNumber(Object value) { return toNumber(value, null); } /** * 转换为int&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Integer toInt(Object value, Integer defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Integer) { return (Integer) value; } if (value instanceof Number) { return ((Number) value).intValue(); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return Integer.parseInt(valueStr.trim()); } catch (Exception e) { return defaultValue; } } /** * 转换为int&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Integer toInt(Object value) { return toInt(value, null); } /** * 转换为Integer数组&lt;br&gt; * * @param str 被转换的值 * @return 结果 */ public static Integer[] toIntArray(String str) { return toIntArray(&quot;,&quot;, str); } /** * 转换为Long数组&lt;br&gt; * * @param str 被转换的值 * @return 结果 */ public static Long[] toLongArray(String str) { return toLongArray(&quot;,&quot;, str); } /** * 转换为Integer数组&lt;br&gt; * * @param split 分隔符 * @param split 被转换的值 * @return 结果 */ public static Integer[] toIntArray(String split, String str) { if (StringUtils.isEmpty(str)) { return new Integer[] {}; } String[] arr = str.split(split); final Integer[] ints = new Integer[arr.length]; for (int i = 0; i &lt; arr.length; i++) { final Integer v = toInt(arr[i], 0); ints[i] = v; } return ints; } /** * 转换为Long数组&lt;br&gt; * * @param split 分隔符 * @param str 被转换的值 * @return 结果 */ public static Long[] toLongArray(String split, String str) { if (StringUtils.isEmpty(str)) { return new Long[] {}; } String[] arr = str.split(split); final Long[] longs = new Long[arr.length]; for (int i = 0; i &lt; arr.length; i++) { final Long v = toLong(arr[i], null); longs[i] = v; } return longs; } /** * 转换为String数组&lt;br&gt; * * @param str 被转换的值 * @return 结果 */ public static String[] toStrArray(String str) { return toStrArray(&quot;,&quot;, str); } /** * 转换为String数组&lt;br&gt; * * @param split 分隔符 * @param split 被转换的值 * @return 结果 */ public static String[] toStrArray(String split, String str) { return str.split(split); } /** * 转换为long&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Long toLong(Object value, Long defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Long) { return (Long) value; } if (value instanceof Number) { return ((Number) value).longValue(); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { // 支持科学计数法 return new BigDecimal(valueStr.trim()).longValue(); } catch (Exception e) { return defaultValue; } } /** * 转换为long&lt;br&gt; * 如果给定的值为&lt;code&gt;null&lt;/code&gt;，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Long toLong(Object value) { return toLong(value, null); } /** * 转换为double&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Double toDouble(Object value, Double defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Double) { return (Double) value; } if (value instanceof Number) { return ((Number) value).doubleValue(); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { // 支持科学计数法 return new BigDecimal(valueStr.trim()).doubleValue(); } catch (Exception e) { return defaultValue; } } /** * 转换为double&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Double toDouble(Object value) { return toDouble(value, null); } /** * 转换为Float&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Float toFloat(Object value, Float defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Float) { return (Float) value; } if (value instanceof Number) { return ((Number) value).floatValue(); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return Float.parseFloat(valueStr.trim()); } catch (Exception e) { return defaultValue; } } /** * 转换为Float&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Float toFloat(Object value) { return toFloat(value, null); } /** * 转换为boolean&lt;br&gt; * String支持的值为：true、false、yes、ok、no，1,0 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static Boolean toBool(Object value, Boolean defaultValue) { if (value == null) { return defaultValue; } if (value instanceof Boolean) { return (Boolean) value; } String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } valueStr = valueStr.trim().toLowerCase(); switch (valueStr) { case &quot;true&quot;: return true; case &quot;false&quot;: return false; case &quot;yes&quot;: return true; case &quot;ok&quot;: return true; case &quot;no&quot;: return false; case &quot;1&quot;: return true; case &quot;0&quot;: return false; default: return defaultValue; } } /** * 转换为boolean&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static Boolean toBool(Object value) { return toBool(value, null); } /** * 转换为Enum对象&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * * @param clazz Enum的Class * @param value 值 * @param defaultValue 默认值 * @return Enum */ public static &lt;E extends Enum&lt;E&gt;&gt; E toEnum(Class&lt;E&gt; clazz, Object value, E defaultValue) { if (value == null) { return defaultValue; } if (clazz.isAssignableFrom(value.getClass())) { @SuppressWarnings(&quot;unchecked&quot;) E myE = (E) value; return myE; } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return Enum.valueOf(clazz, valueStr); } catch (Exception e) { return defaultValue; } } /** * 转换为Enum对象&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * * @param clazz Enum的Class * @param value 值 * @return Enum */ public static &lt;E extends Enum&lt;E&gt;&gt; E toEnum(Class&lt;E&gt; clazz, Object value) { return toEnum(clazz, value, null); } /** * 转换为BigInteger&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static BigInteger toBigInteger(Object value, BigInteger defaultValue) { if (value == null) { return defaultValue; } if (value instanceof BigInteger) { return (BigInteger) value; } if (value instanceof Long) { return BigInteger.valueOf((Long) value); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return new BigInteger(valueStr); } catch (Exception e) { return defaultValue; } } /** * 转换为BigInteger&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;code&gt;null&lt;/code&gt;&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static BigInteger toBigInteger(Object value) { return toBigInteger(value, null); } /** * 转换为BigDecimal&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @param defaultValue 转换错误时的默认值 * @return 结果 */ public static BigDecimal toBigDecimal(Object value, BigDecimal defaultValue) { if (value == null) { return defaultValue; } if (value instanceof BigDecimal) { return (BigDecimal) value; } if (value instanceof Long) { return new BigDecimal((Long) value); } if (value instanceof Double) { return new BigDecimal((Double) value); } if (value instanceof Integer) { return new BigDecimal((Integer) value); } final String valueStr = toStr(value, null); if (StringUtils.isEmpty(valueStr)) { return defaultValue; } try { return new BigDecimal(valueStr); } catch (Exception e) { return defaultValue; } } /** * 转换为BigDecimal&lt;br&gt; * 如果给定的值为空，或者转换失败，返回默认值&lt;br&gt; * 转换失败不会报错 * * @param value 被转换的值 * @return 结果 */ public static BigDecimal toBigDecimal(Object value) { return toBigDecimal(value, null); } /** * 将对象转为字符串&lt;br&gt; * 1、Byte数组和ByteBuffer会被转换为对应字符串的数组 2、对象数组会调用Arrays.toString方法 * * @param obj 对象 * @return 字符串 */ public static String utf8Str(Object obj) { return str(obj, CharsetKit.CHARSET_UTF_8); } /** * 将对象转为字符串&lt;br&gt; * 1、Byte数组和ByteBuffer会被转换为对应字符串的数组 2、对象数组会调用Arrays.toString方法 * * @param obj 对象 * @param charsetName 字符集 * @return 字符串 */ public static String str(Object obj, String charsetName) { return str(obj, Charset.forName(charsetName)); } /** * 将对象转为字符串&lt;br&gt; * 1、Byte数组和ByteBuffer会被转换为对应字符串的数组 2、对象数组会调用Arrays.toString方法 * * @param obj 对象 * @param charset 字符集 * @return 字符串 */ public static String str(Object obj, Charset charset) { if (null == obj) { return null; } if (obj instanceof String) { return (String) obj; } else if (obj instanceof byte[] || obj instanceof Byte[]) { return str((Byte[]) obj, charset); } else if (obj instanceof ByteBuffer) { return str((ByteBuffer) obj, charset); } return obj.toString(); } /** * 将byte数组转为字符串 * * @param bytes byte数组 * @param charset 字符集 * @return 字符串 */ public static String str(byte[] bytes, String charset) { return str(bytes, StringUtils.isEmpty(charset) ? Charset.defaultCharset() : Charset.forName(charset)); } /** * 解码字节码 * * @param data 字符串 * @param charset 字符集，如果此字段为空，则解码的结果取决于平台 * @return 解码后的字符串 */ public static String str(byte[] data, Charset charset) { if (data == null) { return null; } if (null == charset) { return new String(data); } return new String(data, charset); } /** * 将编码的byteBuffer数据转换为字符串 * * @param data 数据 * @param charset 字符集，如果为空使用当前系统字符集 * @return 字符串 */ public static String str(ByteBuffer data, String charset) { if (data == null) { return null; } return str(data, Charset.forName(charset)); } /** * 将编码的byteBuffer数据转换为字符串 * * @param data 数据 * @param charset 字符集，如果为空使用当前系统字符集 * @return 字符串 */ public static String str(ByteBuffer data, Charset charset) { if (null == charset) { charset = Charset.defaultCharset(); } return charset.decode(data).toString(); } // ----------------------------------------------------------------------- 全角半角转换 /** * 半角转全角 * * @param input String. * @return 全角字符串. */ public static String toSBC(String input) { return toSBC(input, null); } /** * 半角转全角 * * @param input String * @param notConvertSet 不替换的字符集合 * @return 全角字符串. */ public static String toSBC(String input, Set&lt;Character&gt; notConvertSet) { char c[] = input.toCharArray(); for (int i = 0; i &lt; c.length; i++) { if (null != notConvertSet &amp;&amp; notConvertSet.contains(c[i])) { // 跳过不替换的字符 continue; } if (c[i] == ' ') { c[i] = '\\u3000'; } else if (c[i] &lt; '\\177') { c[i] = (char) (c[i] + 65248); } } return new String(c); } /** * 全角转半角 * * @param input String. * @return 半角字符串 */ public static String toDBC(String input) { return toDBC(input, null); } /** * 替换全角为半角 * * @param text 文本 * @param notConvertSet 不替换的字符集合 * @return 替换后的字符 */ public static String toDBC(String text, Set&lt;Character&gt; notConvertSet) { char c[] = text.toCharArray(); for (int i = 0; i &lt; c.length; i++) { if (null != notConvertSet &amp;&amp; notConvertSet.contains(c[i])) { // 跳过不替换的字符 continue; } if (c[i] == '\\u3000') { c[i] = ' '; } else if (c[i] &gt; '\\uFF00' &amp;&amp; c[i] &lt; '\\uFF5F') { c[i] = (char) (c[i] - 65248); } } String returnString = new String(c); return returnString; } /** * 数字金额大写转换 先写个完整的然后将如零拾替换成零 * * @param n 数字 * @return 中文大写数字 */ public static String digitUppercase(double n) { String[] fraction = { &quot;角&quot;, &quot;分&quot; }; String[] digit = { &quot;零&quot;, &quot;壹&quot;, &quot;贰&quot;, &quot;叁&quot;, &quot;肆&quot;, &quot;伍&quot;, &quot;陆&quot;, &quot;柒&quot;, &quot;捌&quot;, &quot;玖&quot; }; String[][] unit = { { &quot;元&quot;, &quot;万&quot;, &quot;亿&quot; }, { &quot;&quot;, &quot;拾&quot;, &quot;佰&quot;, &quot;仟&quot; } }; String head = n &lt; 0 ? &quot;负&quot; : &quot;&quot;; n = Math.abs(n); String s = &quot;&quot;; for (int i = 0; i &lt; fraction.length; i++) { s += (digit[(int) (Math.floor(n * 10 * Math.pow(10, i)) % 10)] + fraction[i]).replaceAll(&quot;(零.)+&quot;, &quot;&quot;); } if (s.length() &lt; 1) { s = &quot;整&quot;; } int integerPart = (int) Math.floor(n); for (int i = 0; i &lt; unit[0].length &amp;&amp; integerPart &gt; 0; i++) { String p = &quot;&quot;; for (int j = 0; j &lt; unit[1].length &amp;&amp; n &gt; 0; j++) { p = digit[integerPart % 10] + unit[1][j] + p; integerPart = integerPart / 10; } s = p.replaceAll(&quot;(零.)*零$&quot;, &quot;&quot;).replaceAll(&quot;^$&quot;, &quot;零&quot;) + unit[0][i] + s; } return head + s.replaceAll(&quot;(零.)*零元&quot;, &quot;元&quot;).replaceFirst(&quot;(零.)+&quot;, &quot;&quot;).replaceAll(&quot;(零.)+&quot;, &quot;零&quot;).replaceAll(&quot;^整$&quot;, &quot;零元整&quot;); }} UUID 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487package com.jeethink.common.core.lang;import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;import java.security.SecureRandom;import java.util.Random;import java.util.concurrent.ThreadLocalRandom;import com.jeethink.common.exception.UtilException;/** * 提供通用唯一识别码（universally unique identifier）（UUID）实现 * @author 官方网址 */public final class UUID implements java.io.Serializable, Comparable&lt;UUID&gt;{ private static final long serialVersionUID = -1185015143654744140L; /** * SecureRandom 的单例 * */ private static class Holder { static final SecureRandom numberGenerator = getSecureRandom(); } /** 此UUID的最高64有效位 */ private final long mostSigBits; /** 此UUID的最低64有效位 */ private final long leastSigBits; /** * 私有构造 * * @param data 数据 */ private UUID(byte[] data) { long msb = 0; long lsb = 0; assert data.length == 16 : &quot;data must be 16 bytes in length&quot;; for (int i = 0; i &lt; 8; i++) { msb = (msb &lt;&lt; 8) | (data[i] &amp; 0xff); } for (int i = 8; i &lt; 16; i++) { lsb = (lsb &lt;&lt; 8) | (data[i] &amp; 0xff); } this.mostSigBits = msb; this.leastSigBits = lsb; } /** * 使用指定的数据构造新的 UUID。 * * @param mostSigBits 用于 {@code UUID} 的最高有效 64 位 * @param leastSigBits 用于 {@code UUID} 的最低有效 64 位 */ public UUID(long mostSigBits, long leastSigBits) { this.mostSigBits = mostSigBits; this.leastSigBits = leastSigBits; } /** * 获取类型 4（伪随机生成的）UUID 的静态工厂。 使用加密的本地线程伪随机数生成器生成该 UUID。 * * @return 随机生成的 {@code UUID} */ public static UUID fastUUID() { return randomUUID(false); } /** * 获取类型 4（伪随机生成的）UUID 的静态工厂。 使用加密的强伪随机数生成器生成该 UUID。 * * @return 随机生成的 {@code UUID} */ public static UUID randomUUID() { return randomUUID(true); } public static void main(String[] args) { System.out.println(randomUUID()); System.out.println(fastUUID()); } /** * 获取类型 4（伪随机生成的）UUID 的静态工厂。 使用加密的强伪随机数生成器生成该 UUID。 * * @param isSecure 是否使用{@link SecureRandom}如果是可以获得更安全的随机码，否则可以得到更好的性能 * @return 随机生成的 {@code UUID} */ public static UUID randomUUID(boolean isSecure) { final Random ng = isSecure ? Holder.numberGenerator : getRandom(); byte[] randomBytes = new byte[16]; ng.nextBytes(randomBytes); randomBytes[6] &amp;= 0x0f; /* clear version */ randomBytes[6] |= 0x40; /* set to version 4 */ randomBytes[8] &amp;= 0x3f; /* clear variant */ randomBytes[8] |= 0x80; /* set to IETF variant */ return new UUID(randomBytes); } /** * 根据指定的字节数组获取类型 3（基于名称的）UUID 的静态工厂。 * * @param name 用于构造 UUID 的字节数组。 * * @return 根据指定数组生成的 {@code UUID} */ public static UUID nameUUIDFromBytes(byte[] name) { MessageDigest md; try { md = MessageDigest.getInstance(&quot;MD5&quot;); } catch (NoSuchAlgorithmException nsae) { throw new InternalError(&quot;MD5 not supported&quot;); } byte[] md5Bytes = md.digest(name); md5Bytes[6] &amp;= 0x0f; /* clear version */ md5Bytes[6] |= 0x30; /* set to version 3 */ md5Bytes[8] &amp;= 0x3f; /* clear variant */ md5Bytes[8] |= 0x80; /* set to IETF variant */ return new UUID(md5Bytes); } /** * 根据 {@link #toString()} 方法中描述的字符串标准表示形式创建{@code UUID}。 * * @param name 指定 {@code UUID} 字符串 * @return 具有指定值的 {@code UUID} * @throws IllegalArgumentException 如果 name 与 {@link #toString} 中描述的字符串表示形式不符抛出此异常 * */ public static UUID fromString(String name) { String[] components = name.split(&quot;-&quot;); if (components.length != 5) { throw new IllegalArgumentException(&quot;Invalid UUID string: &quot; + name); } for (int i = 0; i &lt; 5; i++) { components[i] = &quot;0x&quot; + components[i]; } long mostSigBits = Long.decode(components[0]).longValue(); mostSigBits &lt;&lt;= 16; mostSigBits |= Long.decode(components[1]).longValue(); mostSigBits &lt;&lt;= 16; mostSigBits |= Long.decode(components[2]).longValue(); long leastSigBits = Long.decode(components[3]).longValue(); leastSigBits &lt;&lt;= 48; leastSigBits |= Long.decode(components[4]).longValue(); return new UUID(mostSigBits, leastSigBits); } /** * 返回此 UUID 的 128 位值中的最低有效 64 位。 * * @return 此 UUID 的 128 位值中的最低有效 64 位。 */ public long getLeastSignificantBits() { return leastSigBits; } /** * 返回此 UUID 的 128 位值中的最高有效 64 位。 * * @return 此 UUID 的 128 位值中最高有效 64 位。 */ public long getMostSignificantBits() { return mostSigBits; } /** * 与此 {@code UUID} 相关联的版本号. 版本号描述此 {@code UUID} 是如何生成的。 * &lt;p&gt; * 版本号具有以下含意: * &lt;ul&gt; * &lt;li&gt;1 基于时间的 UUID * &lt;li&gt;2 DCE 安全 UUID * &lt;li&gt;3 基于名称的 UUID * &lt;li&gt;4 随机生成的 UUID * &lt;/ul&gt; * * @return 此 {@code UUID} 的版本号 */ public int version() { // Version is bits masked by 0x000000000000F000 in MS long return (int) ((mostSigBits &gt;&gt; 12) &amp; 0x0f); } /** * 与此 {@code UUID} 相关联的变体号。变体号描述 {@code UUID} 的布局。 * &lt;p&gt; * 变体号具有以下含意： * &lt;ul&gt; * &lt;li&gt;0 为 NCS 向后兼容保留 * &lt;li&gt;2 &lt;a href=&quot;http://www.ietf.org/rfc/rfc4122.txt&quot;&gt;IETF&amp;nbsp;RFC&amp;nbsp;4122&lt;/a&gt;(Leach-Salz), 用于此类 * &lt;li&gt;6 保留，微软向后兼容 * &lt;li&gt;7 保留供以后定义使用 * &lt;/ul&gt; * * @return 此 {@code UUID} 相关联的变体号 */ public int variant() { // This field is composed of a varying number of bits. // 0 - - Reserved for NCS backward compatibility // 1 0 - The IETF aka Leach-Salz variant (used by this class) // 1 1 0 Reserved, Microsoft backward compatibility // 1 1 1 Reserved for future definition. return (int) ((leastSigBits &gt;&gt;&gt; (64 - (leastSigBits &gt;&gt;&gt; 62))) &amp; (leastSigBits &gt;&gt; 63)); } /** * 与此 UUID 相关联的时间戳值。 * * &lt;p&gt; * 60 位的时间戳值根据此 {@code UUID} 的 time_low、time_mid 和 time_hi 字段构造。&lt;br&gt; * 所得到的时间戳以 100 毫微秒为单位，从 UTC（通用协调时间） 1582 年 10 月 15 日零时开始。 * * &lt;p&gt; * 时间戳值仅在在基于时间的 UUID（其 version 类型为 1）中才有意义。&lt;br&gt; * 如果此 {@code UUID} 不是基于时间的 UUID，则此方法抛出 UnsupportedOperationException。 * * @throws UnsupportedOperationException 如果此 {@code UUID} 不是 version 为 1 的 UUID。 */ public long timestamp() throws UnsupportedOperationException { checkTimeBase(); return (mostSigBits &amp; 0x0FFFL) &lt;&lt; 48// | ((mostSigBits &gt;&gt; 16) &amp; 0x0FFFFL) &lt;&lt; 32// | mostSigBits &gt;&gt;&gt; 32; } /** * 与此 UUID 相关联的时钟序列值。 * * &lt;p&gt; * 14 位的时钟序列值根据此 UUID 的 clock_seq 字段构造。clock_seq 字段用于保证在基于时间的 UUID 中的时间唯一性。 * &lt;p&gt; * {@code clockSequence} 值仅在基于时间的 UUID（其 version 类型为 1）中才有意义。 如果此 UUID 不是基于时间的 UUID，则此方法抛出 * UnsupportedOperationException。 * * @return 此 {@code UUID} 的时钟序列 * * @throws UnsupportedOperationException 如果此 UUID 的 version 不为 1 */ public int clockSequence() throws UnsupportedOperationException { checkTimeBase(); return (int) ((leastSigBits &amp; 0x3FFF000000000000L) &gt;&gt;&gt; 48); } /** * 与此 UUID 相关的节点值。 * * &lt;p&gt; * 48 位的节点值根据此 UUID 的 node 字段构造。此字段旨在用于保存机器的 IEEE 802 地址，该地址用于生成此 UUID 以保证空间唯一性。 * &lt;p&gt; * 节点值仅在基于时间的 UUID（其 version 类型为 1）中才有意义。&lt;br&gt; * 如果此 UUID 不是基于时间的 UUID，则此方法抛出 UnsupportedOperationException。 * * @return 此 {@code UUID} 的节点值 * * @throws UnsupportedOperationException 如果此 UUID 的 version 不为 1 */ public long node() throws UnsupportedOperationException { checkTimeBase(); return leastSigBits &amp; 0x0000FFFFFFFFFFFFL; } /** * 返回此{@code UUID} 的字符串表现形式。 * * &lt;p&gt; * UUID 的字符串表示形式由此 BNF 描述： * * &lt;pre&gt; * {@code * UUID = &lt;time_low&gt;-&lt;time_mid&gt;-&lt;time_high_and_version&gt;-&lt;variant_and_sequence&gt;-&lt;node&gt; * time_low = 4*&lt;hexOctet&gt; * time_mid = 2*&lt;hexOctet&gt; * time_high_and_version = 2*&lt;hexOctet&gt; * variant_and_sequence = 2*&lt;hexOctet&gt; * node = 6*&lt;hexOctet&gt; * hexOctet = &lt;hexDigit&gt;&lt;hexDigit&gt; * hexDigit = [0-9a-fA-F] * } * &lt;/pre&gt; * * &lt;/blockquote&gt; * * @return 此{@code UUID} 的字符串表现形式 * @see #toString(boolean) */ @Override public String toString() { return toString(false); } /** * 返回此{@code UUID} 的字符串表现形式。 * * &lt;p&gt; * UUID 的字符串表示形式由此 BNF 描述： * * &lt;pre&gt; * {@code * UUID = &lt;time_low&gt;-&lt;time_mid&gt;-&lt;time_high_and_version&gt;-&lt;variant_and_sequence&gt;-&lt;node&gt; * time_low = 4*&lt;hexOctet&gt; * time_mid = 2*&lt;hexOctet&gt; * time_high_and_version = 2*&lt;hexOctet&gt; * variant_and_sequence = 2*&lt;hexOctet&gt; * node = 6*&lt;hexOctet&gt; * hexOctet = &lt;hexDigit&gt;&lt;hexDigit&gt; * hexDigit = [0-9a-fA-F] * } * &lt;/pre&gt; * * &lt;/blockquote&gt; * * @param isSimple 是否简单模式，简单模式为不带'-'的UUID字符串 * @return 此{@code UUID} 的字符串表现形式 */ public String toString(boolean isSimple) { final StringBuilder builder = new StringBuilder(isSimple ? 32 : 36); // time_low builder.append(digits(mostSigBits &gt;&gt; 32, 8)); if (false == isSimple) { builder.append('-'); } // time_mid builder.append(digits(mostSigBits &gt;&gt; 16, 4)); if (false == isSimple) { builder.append('-'); } // time_high_and_version builder.append(digits(mostSigBits, 4)); if (false == isSimple) { builder.append('-'); } // variant_and_sequence builder.append(digits(leastSigBits &gt;&gt; 48, 4)); if (false == isSimple) { builder.append('-'); } // node builder.append(digits(leastSigBits, 12)); return builder.toString(); } /** * 返回此 UUID 的哈希码。 * * @return UUID 的哈希码值。 */ public int hashCode() { long hilo = mostSigBits ^ leastSigBits; return ((int) (hilo &gt;&gt; 32)) ^ (int) hilo; } /** * 将此对象与指定对象比较。 * &lt;p&gt; * 当且仅当参数不为 {@code null}、而是一个 UUID 对象、具有与此 UUID 相同的 varriant、包含相同的值（每一位均相同）时，结果才为 {@code true}。 * * @param obj 要与之比较的对象 * * @return 如果对象相同，则返回 {@code true}；否则返回 {@code false} */ public boolean equals(Object obj) { if ((null == obj) || (obj.getClass() != UUID.class)) { return false; } UUID id = (UUID) obj; return (mostSigBits == id.mostSigBits &amp;&amp; leastSigBits == id.leastSigBits); } // Comparison Operations /** * 将此 UUID 与指定的 UUID 比较。 * * &lt;p&gt; * 如果两个 UUID 不同，且第一个 UUID 的最高有效字段大于第二个 UUID 的对应字段，则第一个 UUID 大于第二个 UUID。 * * @param val 与此 UUID 比较的 UUID * * @return 在此 UUID 小于、等于或大于 val 时，分别返回 -1、0 或 1。 * */ public int compareTo(UUID val) { // The ordering is intentionally set up so that the UUIDs // can simply be numerically compared as two numbers return (this.mostSigBits &lt; val.mostSigBits ? -1 : // (this.mostSigBits &gt; val.mostSigBits ? 1 : // (this.leastSigBits &lt; val.leastSigBits ? -1 : // (this.leastSigBits &gt; val.leastSigBits ? 1 : // 0)))); } // ------------------------------------------------------------------------------------------------------------------- // Private method start /** * 返回指定数字对应的hex值 * * @param val 值 * @param digits 位 * @return 值 */ private static String digits(long val, int digits) { long hi = 1L &lt;&lt; (digits * 4); return Long.toHexString(hi | (val &amp; (hi - 1))).substring(1); } /** * 检查是否为time-based版本UUID */ private void checkTimeBase() { if (version() != 1) { throw new UnsupportedOperationException(&quot;Not a time-based UUID&quot;); } } /** * 获取{@link SecureRandom}，类提供加密的强随机数生成器 (RNG) * * @return {@link SecureRandom} */ public static SecureRandom getSecureRandom() { try { return SecureRandom.getInstance(&quot;SHA1PRNG&quot;); } catch (NoSuchAlgorithmException e) { throw new UtilException(e); } } /** * 获取随机数生成器对象&lt;br&gt; * ThreadLocalRandom是JDK 7之后提供并发产生随机数，能够解决多个线程发生的竞争争夺。 * * @return {@link ThreadLocalRandom} */ public static ThreadLocalRandom getRandom() { return ThreadLocalRandom.current(); }} 提高CPU，防止服务器降配 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.atguigu.java2;import java.io.IOException;class CPUTest { public static void main(String[] args) { CPUTestThread cpuTestThread = new CPUTestThread(); //修改for次数 for (int i = 0; i &lt; 20; i++) { Thread cpuTest = new Thread(cpuTestThread); cpuTest.start(); }//Windows Task Manager shows try { Runtime.getRuntime().exec(&quot;taskmgr&quot;); } catch (IOException e1) { e1.printStackTrace(); } }}class CPUTestThread implements Runnable { @Override public void run() { int busyTime = 10; int idleTime = busyTime; long startTime = 0; while (true) { startTime = System.currentTimeMillis(); System.out.println(System.currentTimeMillis()+&quot;,&quot;+startTime+&quot;,&quot;+(System.currentTimeMillis() - startTime));// busy loop while ((System.currentTimeMillis() - startTime) &lt;= busyTime);// idle loop try { Thread.sleep(idleTime); } catch (InterruptedException e) { System.out.println(e); } } }} vo2dto BeanUtil.copyProperties 技术细节思考","link":"/2021/03/01/Draft/2021/JAVA%E5%BA%94%E7%94%A8/"},{"title":"魑魅先生 | 设计模式","text":"学习目的：增加系统的健壮性，易修改性和可扩展性，阅读源码的通用工具，通用问题的工具箱，程序员的通用语言 学习方法:视频引入，博客弥补，书籍细化，代码实践 学习资源：大话设计模式书籍，bili视频，博客1,博客2 学习目标程度：了解即可 实际运用 JDK SSM Springboot 设计模式原则 开闭原则 ：对修改关闭，对扩展开放 由Bertrand Meyer提出的开闭原则（Open Closed Principle）是指，软件应该对扩展开放， 而对修改关闭。这里的意思是在增加新功能的时候，能不改代码就尽量不要改， 如果只增加代码就完成了新功能，那是最好的。一个类从头到尾都是自己写的可以更改，别人的要符合开闭原则 依赖倒转原则 ：针对接口和抽象编程 上层（调用别的方法的）不能依赖于下层（方法被调用的），他们都应该依赖于抽象 合成复用原则 ：多用组合少用继承 继承（实线空箭头） 依赖 关联（实线箭头）：组合（实心菱形加箭头）（鸟和翅膀）；聚合（空心菱形加箭头）（雁和雁群） 迪米特原则（最少知道原则） ：实体之间尽量少的相互作用 个对象应对其它对象尽可能少的了解，和朋友（类中字段，方法参数，方法返回值，方法实例出来的对象）通信 里式代换原则 ：基类可以出现的地方子类也可以出现。 里氏替换原则是Barbara Liskov提出的，这是一种面向对象的设计原则， 即如果我们调用一个父类的方法可以成功，那么替换成子类调用也应该完全可以运行。 重写时子类限制等级不能更高，错误不能抛出更多 单一职责原则 ：每个方法，类，框架只负责一件事 接口隔离原则 ：使用多个专门的接口比一个总接口好 ---分--- 学习设计模式，关键是学习设计思想，不能简单地生搬硬套， 也不能为了使用设计模式而过度设计，要合理平衡设计的复杂度和灵活性， 并意识到设计模式也并不是万能的。 算法更像是菜谱： 提供达成目标的明确步骤。 而模式更像是蓝图： 你可以看到最终的结果和模式的功能， 但需要自己确定实现步骤。 意图 部分简单描述问题和解决方案。 动机 部分将进一步解释问题并说明模式会如何提供解决方案。 结构 部分展示模式的每个部分和它们之间的关系。 在不同语言中的实现 提供流行编程语言的代码， 让读者更好地理解模式背后的思想。 设计原则 一句话归纳 目的 开闭原则 对扩展开放，对修改关闭 降低维护带来的新风险 依赖倒置原则 高层不应该依赖低层，要面向接口编程 更利于代码结构的升级扩展 单一职责原则 一个类只干一件事，实现类要单一 便于理解，提高代码的可读性 接口隔离原则 一个接口只干一件事，接口要精简单一 功能解耦，高聚合、低耦合 迪米特法则 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 只和朋友交流，不和陌生人说话，减少代码臃肿 里氏替换原则 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 防止继承泛滥 合成复用原则 尽量使用组合或者聚合关系实现代码复用，少使用继承 降低代码耦合 学习进度 设计原则 开闭原则 里氏替换原则 依赖倒置原则 单一职责原则 接口隔离原则 迪米特法则 合成复用原则 行为模式 职责链模式（Chain of Responsibility）😎😎😎😎😎 命令模式（Command） 迭代器模式（Iterator） 调停者（中介者）模式（Mediator） 备忘录模式（Memento） 观察者模式（Observer） 状态模式（State） 策略模式（Strategy） 模板方法模式（Template Method） 访问者模式（Visitor） 解释器模式（Interpreter） 结构型模式 适配器模式（Adapter） 桥接模式（Bridge） 组合模式（Composite） 装饰模式（Decorator） 外观模式（Facade） 享元模式（Flyweight） 代理模式（Proxy） 创建型模式 简单工厂模式（Simple Factory） 工厂方法模式（Factory Method） 抽象工厂模式（Abstract Factory） 创建者（生成器）模式（Builder） 原型模式（Prototype） 单例模式（Singleton） 进阶学习 总归纳复习 设计模式简述 模式之间的关系 Refactoringguru.cn 创建型模式 这类模式提供创建对象的机制， 能够提升已有代码的灵活性和可复用性。 工厂方法Factory Method抽象工厂Abstract Factory生成器Builder原型Prototype单例Singleton 结构型模式 这类模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效。 适配器Adapter桥接Bridge组合Composite装饰Decorator外观Facade享元Flyweight代理Proxy 行为模式 这类模式负责对象间的高效沟通和职责委派。 责任链Chain of Responsibility命令Command迭代器Iterator中介者Mediator备忘录Memento观察者Observer状态State策略Strategy模板方法Template Method访问者Visitor 创建型模式 创建型模式的主要关注点是“怎样创建对象？”，它的主要特点是“将对象的创建与使用分离”。这样可以降低系统的耦合度，使用者不需要关注对象的创建细节，对象的创建由相关的工厂来完成。就像我们去商场购买商品时，不需要知道商品是怎么生产出来一样，因为它们由专门的厂商生产。 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（FactoryMethod）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 结构型模式 结构型模式描述如何将类或对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者釆用组合或聚合来组合对象。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象结构型模式比类结构型模式具有更大的灵活性。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态地给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 行为型模式 行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。 模板方法（Template Method）模式：定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 UML 工具：processon 方法和属性的访问权限 - private # protected + public ~ package private 关系 概览 继承 | 泛化【继承并特殊】 12【泛化关系】：是一种继承关系，表示一般与特殊的关系，它指定了子类如何特化父类的所有特征和行为。例如：老虎是动物的一种，即有老虎的特性也有动物的共性。【箭头指向】：带三角箭头的实线，箭头指向父类 实现【接口全实现】 123【实现关系】：是一种类与接口的关系，表示类是接口所有特征和行为的实现【箭头指向】：带三角箭头的虚线，箭头指向接口 关联【我知你些属】 1234【关联关系】：是一种拥有的关系,它使一个类知道另一个类的属性和方法；如：老师与学生，丈夫与妻子关联可以是双向的，也可以是单向的。双向的关联可以有两个箭头或者没有箭头，单向的关联有一个箭头。【代码体现】：成员变量【箭头及指向】：带普通箭头的实心线，指向被拥有者 聚合【一团分几个】 1234【聚合关系】：是整体与部分的关系，且部分可以离开整体而单独存在。如车和轮胎是整体和部分的关系.聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。【代码体现】：成员变量【箭头及指向】：带空心菱形的实心线，菱形指向整体 组合【一个切几个】 123【组合关系】：是整体与部分的关系，但部分不能离开整体而单独存在。没有公司就不存在部门 组合关系是关联关系的一种，是比聚合关系还要强的关系，它要求普通的聚合关系中代表整体的对象负责代表部分的对象的生命周期【代码体现】：成员变量【箭头及指向】：带实心菱形的实线，菱形指向整体 依赖【我要用你的】 12345【依赖关系】：是一种使用的关系,所以要尽量不使用双向的互相依赖。【代码表现】：局部变量、方法的参数或者对静态方法的调用【箭头及指向】：带箭头的虚线，指向被使用者 各种关系的强弱顺序： 泛化= 实现&gt; 组合&gt; 聚合&gt; 关联&gt; 依赖 设计模式重点详解 Mybatis（ 1、Builder模式5、组合模式9、迭代器模式2、工厂模式3、单例模式4、代理6、模板方法模式7、适配器模式8、装饰者模式） Spring（1.简单工厂2.工厂方法3.单例模式4.适配器模式5.装饰器模式6.代理模式7.观察者模式8.策略模式9.模版方法模式） 创建型模式 《单例模式Singleton》 特点： 单例类只有一个实例对象； 该单例对象必须由单例类自行创建； 单例类对外提供一个访问该单例的全局访问点。 优点： 单例模式可以保证内存里只有一个实例，减少了内存的开销。 可以避免对资源的多重占用。 单例模式设置全局访问点，可以优化和共享资源的访问。 缺点： 单例模式一般没有接口，扩展困难。如果要扩展，则除了修改原来的代码，没有第二种途径，违背开闭原则。 在并发测试中，单例模式不利于代码调试。在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。 单例模式的功能代码通常写在一个类中，如果功能设计不合理，则很容易违背单一职责原则。 单例模式的应用场景： 需要频繁创建的一些类，使用单例可以降低系统的内存压力，减少 GC。 某类只要求生成一个对象的时候，如一个班中的班长、每个人的身份证号等。 某些类创建实例时占用资源较多，或实例化耗时较长，且经常使用。 某类需要频繁实例化，而创建的对象又频繁被销毁的时候，如多线程的线程池、网络连接池等。 频繁访问数据库或文件的对象。 对于一些控制硬件级别的操作，或者从系统上来讲应当是单一控制逻辑的操作，如果有多个实例，则系统会完全乱套。 当对象需要被共享的场合。由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度。如 Web 中的配置对象、数据库的连接池等。 分类JAVA实现： 懒汉式：类加载时没有生成单例，只有当第一次调用 getlnstance 方法时才去创建这个单例 ​ 如果编写的是多线程程序，则不要删除上例代码中的关键字 volatile 和 synchronized，否则将存在线程非安全的问题。如果不删除这两个关键字就能保证线程安全，但是每次访问时都要同步，会影响性能，且消耗更多的资源，这是懒汉式单例的缺点。 123456789101112public class LazySingleton { private static volatile LazySingleton instance = null; //保证 instance 在所有线程中同步 private LazySingleton() { } //private 避免类在外部被实例化 public static synchronized LazySingleton getInstance() { //getInstance 方法前加同步 if (instance == null) { instance = new LazySingleton(); } return instance; }} 饿汉式：类一旦加载就创建一个单例，保证在调用 getInstance 方法之前单例已经存在了。 12345678public class HungrySingleton { private static final HungrySingleton instance = new HungrySingleton(); private HungrySingleton() { } public static HungrySingleton getInstance() { return instance; }} ​ 饿汉式单例在类创建的同时就已经创建好一个静态的对象供系统使用，以后不再改变，所以是线程安全的，可以直接用于多线程而不会出现问题。 《原型模式Prototype》 特点： 用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。 原型实例指定了要创建的对象的种类。用这种方式创建对象非常高效，根本无须知道对象创建的细节。 优点： Java自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。 深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化创建对象过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。 缺点： 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。 角色： 抽象原型类：规定了具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。 访问类：使用具体原型类中的 clone() 方法来复制新的对象。 浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 原型模式的应用场景： 对象之间相同或相似，即只是个别的几个属性不同的时候。 创建对象成本较大，例如初始化时间长，占用CPU太多，或者占用网络资源太多等，需要优化资源。 创建一个对象需要繁琐的数据准备或访问权限等，需要提高性能或者提高安全性。 系统中大量使用该类对象，且各个调用者都需要给它的属性重新赋值。 JAVA实现： 浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.util.*;interface Shape extends Cloneable { public Object clone(); //拷贝 public void countArea(); //计算面积}class Circle implements Shape { public Object clone() { Circle w = null; try { w = (Circle) super.clone(); } catch (CloneNotSupportedException e) { System.out.println(&quot;拷贝圆失败!&quot;); } return w; } public void countArea() { int r = 0; System.out.print(&quot;这是一个圆，请输入圆的半径：&quot;); Scanner input = new Scanner(System.in); r = input.nextInt(); System.out.println(&quot;该圆的面积=&quot; + 3.1415 * r * r + &quot;\\n&quot;); }}class Square implements Shape { public Object clone() { Square b = null; try { b = (Square) super.clone(); } catch (CloneNotSupportedException e) { System.out.println(&quot;拷贝正方形失败!&quot;); } return b; } public void countArea() { int a = 0; System.out.print(&quot;这是一个正方形，请输入它的边长：&quot;); Scanner input = new Scanner(System.in); a = input.nextInt(); System.out.println(&quot;该正方形的面积=&quot; + a * a + &quot;\\n&quot;); }}class ProtoTypeManager { private HashMap&lt;String, Shape&gt; ht = new HashMap&lt;String, Shape&gt;(); public ProtoTypeManager() { ht.put(&quot;Circle&quot;, new Circle()); ht.put(&quot;Square&quot;, new Square()); } public void addshape(String key, Shape obj) { ht.put(key, obj); } public Shape getShape(String key) { Shape temp = ht.get(key); return (Shape) temp.clone(); }}public class ProtoTypeShape { public static void main(String[] args) { ProtoTypeManager pm = new ProtoTypeManager(); Shape obj1 = (Circle) pm.getShape(&quot;Circle&quot;); obj1.countArea(); Shape obj2 = (Shape) pm.getShape(&quot;Square&quot;); obj2.countArea(); }} 《简单工厂模式Simple Factory Pattern》 特点： 定义一个创建产品对象的工厂接口，将产品对象的实际创建工作推迟到具体子工厂类当中。这满足创建型模式中所要求的“创建与使用相分离”的特点 需要生成复杂对象的地方，都可以尝试考虑使用工厂模式。复杂对象指的是类的构造函数参数过多等对类的构造有影响的情况，因为类的构造过于复杂，如果直接在其他业务类内使用，则两者的耦合过重，后续业务更改，就需要在任何引用该类的源代码内进行更改，光是查找所有依赖就很消耗时间了，更别说要一个一个修改了。 优点： 工厂类包含必要的逻辑判断，可以决定在什么时候创建哪一个产品的实例。客户端可以免除直接创建产品对象的职责，很方便的创建出相应的产品。工厂和产品的职责区分明确。 客户端无需知道所创建具体产品的类名，只需知道参数即可。 也可以引入配置文件，在不修改客户端代码的情况下更换和添加新的具体产品类。 缺点： 简单工厂模式的工厂类单一，负责所有产品的创建，职责过重，一旦异常，整个系统将受影响。且工厂类代码会非常臃肿，违背高聚合原则。 使用简单工厂模式会增加系统中类的个数（引入新的工厂类），增加系统的复杂度和理解难度 系统扩展困难，一旦增加新产品不得不修改工厂逻辑，在产品类型较多时，可能造成逻辑过于复杂 简单工厂模式使用了 static 工厂方法，造成工厂角色无法形成基于继承的等级结构。 角色： 简单工厂（SimpleFactory）：是简单工厂模式的核心，负责实现创建所有实例的内部逻辑。工厂类的创建产品类的方法可以被外界直接调用，创建所需的产品对象。 抽象产品（Product）：是简单工厂创建的所有对象的父类，负责描述所有实例共有的公共接口。 具体产品（ConcreteProduct）：是简单工厂模式的创建目标。 简单工厂模式的应用场景： 生成复杂对象 JAVA实现 123456789101112131415161718192021222324252627282930313233343536public class Client { public static void main(String[] args) { } //抽象产品 public interface Product { void show(); } //具体产品：ProductA static class ConcreteProduct1 implements Product { public void show() { System.out.println(&quot;具体产品1显示...&quot;); } } //具体产品：ProductB static class ConcreteProduct2 implements Product { public void show() { System.out.println(&quot;具体产品2显示...&quot;); } } final class Const { static final int PRODUCT_A = 0; static final int PRODUCT_B = 1; static final int PRODUCT_C = 2; } static class SimpleFactory { public static Product makeProduct(int kind) { switch (kind) { case Const.PRODUCT_A: return new ConcreteProduct1(); case Const.PRODUCT_B: return new ConcreteProduct2(); } return null; } }} 《工厂模式Factory Pattern》 特点： 可以使系统在不修改原来代码的情况下引进新的产品，即满足开闭原则。 优点： 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程。 灵活性增强，对于新产品的创建，只需多写一个相应的工厂类。 典型的解耦框架。高层模块只需要知道产品的抽象类，无须关心其他实现类，满足迪米特法则、依赖倒置原则和里氏替换原则。 缺点： 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象产品只能生产一种产品，此弊端可使用抽象工厂模式解决。 角色： 抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法 newProduct() 来创建产品。 具体工厂（ConcreteFactory）：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。 工厂模式的应用场景： 客户只知道创建产品的工厂名，而不知道具体的产品名。如 TCL 电视工厂、海信电视工厂等。 创建对象的任务由多个具体子工厂中的某一个完成，而抽象工厂只提供创建产品的接口。 客户不关心创建产品的细节，只关心产品的品牌 JAVA实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package FactoryMethod;public class AbstractFactoryTest { public static void main(String[] args) { try { Product a; AbstractFactory af; af = (AbstractFactory) ReadXML1.getObject(); a = af.newProduct(); a.show(); } catch (Exception e) { System.out.println(e.getMessage()); } }}//抽象产品：提供了产品的接口interface Product { public void show();}//具体产品1：实现抽象产品中的抽象方法class ConcreteProduct1 implements Product { public void show() { System.out.println(&quot;具体产品1显示...&quot;); }}//具体产品2：实现抽象产品中的抽象方法class ConcreteProduct2 implements Product { public void show() { System.out.println(&quot;具体产品2显示...&quot;); }}//抽象工厂：提供了厂品的生成方法interface AbstractFactory { public Product newProduct();}//具体工厂1：实现了厂品的生成方法class ConcreteFactory1 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂1生成--&gt;具体产品1...&quot;); return new ConcreteProduct1(); }}//具体工厂2：实现了厂品的生成方法class ConcreteFactory2 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂2生成--&gt;具体产品2...&quot;); return new ConcreteProduct2(); }} 12345678910111213141516171819202122232425262728package FactoryMethod;import javax.xml.parsers.*;import org.w3c.dom.*;import java.io.*;class ReadXML1 { //该方法用于从XML配置文件中提取具体类类名，并返回一个实例对象 public static Object getObject() { try { //创建文档对象 DocumentBuilderFactory dFactory = DocumentBuilderFactory.newInstance(); DocumentBuilder builder = dFactory.newDocumentBuilder(); Document doc; doc = builder.parse(new File(&quot;src/FactoryMethod/config1.xml&quot;)); //获取包含类名的文本节点 NodeList nl = doc.getElementsByTagName(&quot;className&quot;); Node classNode = nl.item(0).getFirstChild(); String cName = &quot;FactoryMethod.&quot; + classNode.getNodeValue(); //System.out.println(&quot;新类名：&quot;+cName); //通过类名生成实例对象并将其返回 Class&lt;?&gt; c = Class.forName(cName); Object obj = c.newInstance(); return obj; } catch (Exception e) { e.printStackTrace(); return null; } }} 《抽象工厂模式Abstract Factory Pattern》 特点： 一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。 抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用抽象工厂模式一般要满足以下条件。 系统中有多个产品族，每个具体工厂创建同一族但属于不同等级结构的产品。 系统一次只可能消费其中某一族产品，即同族的产品一起使用。 优点： 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程。 灵活性增强，对于新产品的创建，只需多写一个相应的工厂类。 典型的解耦框架。高层模块只需要知道产品的抽象类，无须关心其他实现类，满足迪米特法则、依赖倒置原则和里氏替换原则。 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。 当需要产品族时，抽象工厂可以保证客户端始终只使用同一个产品的产品组。 抽象工厂增强了程序的可扩展性，当增加一个新的产品族时，不需要修改原代码，满足开闭原则。 缺点： 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象产品只能生产一种产品，此弊端可使用抽象工厂模式解决。 当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改。增加了系统的抽象性和理解难度。 角色： 抽象工厂中方法个数不同，抽象产品的个数也不同 抽象工厂（Abstract Factory）：提供了创建产品的接口，它包含多个创建产品的方法 newProduct()，可以创建多个不同等级的产品。 具体工厂（Concrete Factory）：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 抽象工厂模式的应用场景： 抽象工厂模式最早的应用是用于创建属于不同操作系统的视窗构件。如 Java 的 AWT 中的 Button 和 Text 等构件在 Windows 和 UNIX 中的本地实现是不同的。 抽象工厂模式通常适用于以下场景： 当需要创建的对象是一系列相互关联或相互依赖的产品族时，如电器工厂中的电视机、洗衣机、空调等。 系统中有多个产品族，但每次只使用其中的某一族产品。如有人只喜欢穿某一个品牌的衣服和鞋。 系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。 JAVA实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package FactoryMethod;public class AbstractFactoryTest { public static void main(String[] args) { try { Product a; AbstractFactory af; af = (AbstractFactory) ReadXML1.getObject(); a = af.newProduct(); a.show(); } catch (Exception e) { System.out.println(e.getMessage()); } }}//抽象产品：提供了产品的接口interface Product { public void show();}//具体产品1：实现抽象产品中的抽象方法class ConcreteProduct1 implements Product { public void show() { System.out.println(&quot;具体产品1显示...&quot;); }}//具体产品2：实现抽象产品中的抽象方法class ConcreteProduct2 implements Product { public void show() { System.out.println(&quot;具体产品2显示...&quot;); }}//抽象工厂：提供了厂品的生成方法interface AbstractFactory { public Product newProduct();}//具体工厂1：实现了厂品的生成方法class ConcreteFactory1 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂1生成--&gt;具体产品1...&quot;); return new ConcreteProduct1(); }}//具体工厂2：实现了厂品的生成方法class ConcreteFactory2 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂2生成--&gt;具体产品2...&quot;); return new ConcreteProduct2(); }} 12345678910111213141516171819202122232425262728package FactoryMethod;import javax.xml.parsers.*;import org.w3c.dom.*;import java.io.*;class ReadXML1 { //该方法用于从XML配置文件中提取具体类类名，并返回一个实例对象 public static Object getObject() { try { //创建文档对象 DocumentBuilderFactory dFactory = DocumentBuilderFactory.newInstance(); DocumentBuilder builder = dFactory.newDocumentBuilder(); Document doc; doc = builder.parse(new File(&quot;src/FactoryMethod/config1.xml&quot;)); //获取包含类名的文本节点 NodeList nl = doc.getElementsByTagName(&quot;className&quot;); Node classNode = nl.item(0).getFirstChild(); String cName = &quot;FactoryMethod.&quot; + classNode.getNodeValue(); //System.out.println(&quot;新类名：&quot;+cName); //通过类名生成实例对象并将其返回 Class&lt;?&gt; c = Class.forName(cName); Object obj = c.newInstance(); return obj; } catch (Exception e) { e.printStackTrace(); return null; } }} 《建造者模式 Bulider》 特点： 将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示 它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 优点： 封装性好，构建和表示分离。 扩展性好，各个具体的建造者相互独立，有利于系统的解耦。 客户端不必知道产品内部组成的细节，建造者可以对创建过程逐步细化，而不对其它模块产生任何影响，便于控制细节风险。 缺点： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，如果产品内部发生变化，则建造者也要同步修改，后期维护成本较大。 角色： 产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 建造者模式的应用场景： 相同的方法，不同的执行顺序，产生不同的结果。 多个部件或零件，都可以装配到一个对象中，但是产生的结果又不相同。 产品类非常复杂，或者产品类中不同的调用顺序产生不同的作用。 初始化一个对象特别复杂，参数多，而且很多参数都具有默认值。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package pers.lxl.mylearnproject.programbase.designpatterns.creationalpattern.bulider;/**产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。*/class Product { private String partA; private String partB; private String partC; public void setPartA(String partA) { this.partA = partA; } public void setPartB(String partB) { this.partB = partB; } public void setPartC(String partC) { this.partC = partC; } public void show() { System.out.println(partA+' '+partB+' '+partC); }}/**抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。*/abstract class Builder { //创建产品对象 protected Product product = new Product(); public abstract void buildPartA(); public abstract void buildPartB(); public abstract void buildPartC(); //返回产品对象 public Product getResult() { return product; }}/**具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。*/class ConcreteBuilder1 extends Builder { @Override public void buildPartA() { product.setPartA(&quot;建造 PartA&quot;); } @Override public void buildPartB() { product.setPartB(&quot;建造 PartB&quot;); } @Override public void buildPartC() { product.setPartC(&quot;建造 PartC&quot;); }}/**指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。*/class Director { private Builder builder; public Director(Builder builder) { this.builder = builder; } //产品构建与组装方法 public Product construct() { builder.buildPartA(); builder.buildPartB(); builder.buildPartC(); return builder.getResult(); }}public class ConcreteBuilder { public static void main(String[] args) { Builder builder = new ConcreteBuilder1(); Director director = new Director(builder); Product product = director.construct(); product.show(); }} 结构型模式 《代理模式 Proxy 》 特点： 中介 由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 优点： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度，增加了程序的可扩展性 缺点：(动态代理方式解决) 代理模式会造成系统设计中类的数量增加 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢； 增加了系统的复杂度； 角色： 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 根据代理的创建时期，代理模式分为静态代理和动态代理。 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。 动态：在程序运行时，运用反射机制动态创建而成 代理模式的应用场景： 远程代理，这种方式通常是为了隐藏目标对象存在于不同地址空间的事实，方便客户端访问。例如，用户申请某些网盘空间时，会在用户的文件系统中建立一个虚拟的硬盘，用户访问虚拟硬盘时实际访问的是网盘空间。 虚拟代理，这种方式通常用于要创建的目标对象开销很大时。例如，下载一幅很大的图像需要很长时间，因某种计算比较复杂而短时间无法完成，这时可以先用小比例的虚拟代理替换真实的对象，消除用户对服务器慢的感觉。 安全代理，这种方式通常用于控制不同种类客户对真实对象的访问权限。 智能指引，主要用于调用目标对象时，代理附加一些额外的处理功能。例如，增加计算真实对象的引用次数的功能，这样当该对象没有被引用时，就可以自动释放它。 延迟加载，指为了提高系统的性能，延迟对目标的加载。例如，Hibernate 中就存在属性的延迟加载和关联表的延时加载。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435package proxy;public class ProxyTest { public static void main(String[] args) { Proxy proxy = new Proxy(); proxy.Request(); }}//抽象主题interface Subject { void Request();}//真实主题class RealSubject implements Subject { public void Request() { System.out.println(&quot;访问真实主题方法...&quot;); }}//代理class Proxy implements Subject { private RealSubject realSubject; public void Request() { if (realSubject == null) { realSubject = new RealSubject(); } preRequest(); realSubject.Request(); postRequest(); } public void preRequest() { System.out.println(&quot;访问真实主题之前的预处理。&quot;); } public void postRequest() { System.out.println(&quot;访问真实主题之后的后续处理。&quot;); }} 《适配器模式 Adapter 》 特点： 将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。 优点： 客户端通过适配器可以透明地调用目标接口。 复用了现存的类，程序员不需要修改原有代码而重用现有的适配者类。 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题。 在很多业务场景中符合开闭原则。 缺点： 适配器编写过程需要结合业务场景全面考虑，可能会增加系统的复杂性。 增加代码阅读难度，降低代码可读性，过多使用适配器会使系统代码变得凌乱。 角色： 目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 适配器模式的应用场景： 以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致。 使用第三方提供的组件，但组件接口定义和自己要求的接口定义不同。 JAVA实现： 对象适配器 1234567891011121314151617181920212223242526package pers.lxl.mylearnproject.programbase.designpatterns.structuralpattern.adapter;//对象适配器类class ObjectAdapter implements Target{ private Adaptee adaptee; public ObjectAdapter(Adaptee adaptee) { this.adaptee=adaptee; } @Override public void request() { adaptee.specificRequest(); }}//客户端代码public class objectAdapterPattern{ public static void main(String[] args) { System.out.println(&quot;对象适配器模式测试：&quot;); Adaptee adaptee = new Adaptee(); Target target = new ObjectAdapter(adaptee); target.request(); }} 类适配器 1234567891011121314151617181920212223242526272829303132333435package pers.lxl.mylearnproject.programbase.designpatterns.structuralpattern.adapter;//目标接口interface Target{ public void request();}//适配者接口class Adaptee{ public void specificRequest() { System.out.println(&quot;适配者中的业务代码被调用！&quot;); }}//类适配器类 extends Adaptee implements Targetclass ClassAdapter extends Adaptee implements Target{ @Override public void request() { specificRequest(); }}//客户端代码public class ClassAdapterPattern{ public static void main(String[] args) { System.out.println(&quot;类适配器模式测试：&quot;); Target target = new ClassAdapter(); target.request(); }} 适配器模式（Adapter）可扩展为双向适配器模式，双向适配器类既可以把适配者接口转换成目标接口，也可以把目标接口转换成适配者接口，其结构图如图所示。 《桥接模式 Bridge》 特点： 将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 优点： 抽象与实现分离，扩展能力强 符合开闭原则 符合合成复用原则 其实现细节对客户透明 缺点： 由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，能正确地识别出系统中两个独立变化的维度，这增加了系统的理解与设计难度。 角色： 抽象化（Abstraction）角色：定义抽象类，并包含一个对实现化对象的引用。 扩展抽象化（Refined Abstraction）角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化（Implementor）角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化（Concrete Implementor）角色：给出实现化角色接口的具体实现。 桥接模式的应用场景： 当一个类存在两个独立变化的维度，且这两个维度都需要进行扩展时。 当一个系统不希望使用继承或因为多层次继承导致系统类的个数急剧增加时。 当一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性时。 JAVA实现： 123456789101112131415161718192021222324252627282930313233343536package bridge;public class BridgeTest { public static void main(String[] args) { Implementor imple = new ConcreteImplementorA(); Abstraction abs = new RefinedAbstraction(imple); abs.Operation(); }}//实现化角色interface Implementor { public void OperationImpl();}//具体实现化角色class ConcreteImplementorA implements Implementor { public void OperationImpl() { System.out.println(&quot;具体实现化(Concrete Implementor)角色被访问&quot;); }}//抽象化角色abstract class Abstraction { protected Implementor imple; protected Abstraction(Implementor imple) { this.imple = imple; } public abstract void Operation();}//扩展抽象化角色class RefinedAbstraction extends Abstraction { protected RefinedAbstraction(Implementor imple) { super(imple); } public void Operation() { System.out.println(&quot;扩展抽象化(Refined Abstraction)角色被访问&quot;); imple.OperationImpl(); }} 《装饰器模式 Decorator Pattern》 特点： 不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。 结构修饰 优点： 装饰器是继承的有力补充，比继承灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用 通过使用不用装饰类及这些装饰类的排列组合，可以实现不同效果 装饰器模式完全遵守开闭原则 缺点： 装饰器模式会增加许多子类，过度使用会增加程序得复杂性 角色： 抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。 具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。 抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 装饰者模式的应用场景： 当需要给一个现有类添加附加职责，而又不能采用生成子类的方法进行扩充时。例如，该类被隐藏或者该类是终极类或者采用继承方式会产生大量的子类。 当需要通过对现有的一组基本功能进行排列组合而产生非常多的功能时，采用继承关系很难实现，而采用装饰器模式却很好实现。 当对象的功能要求可以动态地添加，也可以再动态地撤销时。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package decorator;public class DecoratorPattern { public static void main(String[] args) { Component p = new ConcreteComponent(); p.operation(); System.out.println(&quot;---------------------------------&quot;); Component d = new ConcreteDecorator(p); d.operation(); }}//抽象构件角色interface Component { public void operation();}//具体构件角色class ConcreteComponent implements Component { public ConcreteComponent() { System.out.println(&quot;创建具体构件角色&quot;); } public void operation() { System.out.println(&quot;调用具体构件角色的方法operation()&quot;); }}//抽象装饰角色class Decorator implements Component { private Component component; public Decorator(Component component) { this.component = component; } public void operation() { component.operation(); }}//具体装饰角色class ConcreteDecorator extends Decorator { public ConcreteDecorator(Component component) { super(component); } public void operation() { super.operation(); addedFunction(); } public void addedFunction() { System.out.println(&quot;为具体构件角色增加额外的功能addedFunction()&quot;); }} 《外观模式 Facade Pattern》 特点： 门面模式，是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。 结构修饰 优点： 降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。 对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。 降低了大型软件系统中的编译依赖性，简化了系统在不同平台之间的移植过程，因为编译一个子系统不会影响其他的子系统，也不会影响外观对象。 缺点： 不能很好地限制客户使用子系统类，很容易带来未知风险。 增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则” 角色： 外观（Facade）角色：为多个子系统对外提供一个共同的接口。 子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。 客户（Client）角色：通过一个外观角色访问各个子系统的功能。 外观 模式的应用场景： 对分层结构系统构建时，使用外观模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。 当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。 当客户端与多个子系统之间存在很大的联系时，引入外观模式可将它们分离，从而提高子系统的独立性和可移植性。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435public class FacadePattern { public static void main(String[] args) { Facade f = new Facade(); f.method(); }}//外观角色class Facade { private SubSystem01 obj1 = new SubSystem01(); private SubSystem02 obj2 = new SubSystem02(); private SubSystem03 obj3 = new SubSystem03(); public void method() { obj1.method1(); obj2.method2(); obj3.method3(); }}//子系统角色class SubSystem01 { public void method1() { System.out.println(&quot;子系统01的method1()被调用！&quot;); }}//子系统角色class SubSystem02 { public void method2() { System.out.println(&quot;子系统02的method2()被调用！&quot;); }}//子系统角色class SubSystem03 { public void method3() { System.out.println(&quot;子系统03的method3()被调用！&quot;); }} 《享元模式 Flyweight Pattern》 特点： 运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。 优点： 相同对象只要保存一份，这降低了系统中对象的数量，从而降低了系统中细粒度对象给内存带来的压力。 缺点： 为了使对象可以共享，需要将一些不能共享的状态外部化，这将增加程序的复杂性。 读取享元模式的外部状态会使得运行时间稍微变长。 角色： ​ 享元模式的定义提出了两个要求，细粒度和共享对象。因为要求细粒度，所以不可避免地会使对象数量多且性质相近，此时我们就将这些对象的信息分为两个部分：内部状态和外部状态。 ​ 内部状态指对象共享出来的信息，存储在享元信息内部，并且不回随环境的改变而改变； ​ 外部状态指对象得以依赖的一个标记，随环境的改变而改变，不可共享。 比如，连接池中的连接对象，保存在连接对象中的用户名、密码、连接URL等信息，在创建对象的时候就设置好了，不会随环境的改变而改变，这些为内部状态。而当每个连接要被回收利用时，我们需要将它标记为可用状态，这些为外部状态。享元模式的本质是缓存共享对象，降低内存消耗。 抽象享元角色（Flyweight）：是所有的具体享元类的基类，为具体享元规范需要实现的公共接口，非享元的外部状态以参数的形式通过方法传入。 具体享元（Concrete Flyweight）角色：实现抽象享元角色中所规定的接口。 非享元（Unsharable Flyweight)角色：是不可以共享的外部状态，它以参数的形式注入具体享元的相关方法中。 享元工厂（Flyweight Factory）角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。 享元模式的应用场景： 系统中存在大量相同或相似的对象，这些对象耗费大量的内存资源。 大部分的对象可以按照内部状态进行分组，且可将不同部分外部化，这样每一个组只需保存一个内部状态。 由于享元模式需要额外维护一个保存享元的数据结构，所以应当在有足够多的享元实例时才值得使用享元模式。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class FlyweightPattern { public static void main(String[] args) { FlyweightFactory factory = new FlyweightFactory(); Flyweight f01 = factory.getFlyweight(&quot;a&quot;); Flyweight f02 = factory.getFlyweight(&quot;a&quot;); Flyweight f03 = factory.getFlyweight(&quot;a&quot;); Flyweight f11 = factory.getFlyweight(&quot;b&quot;); Flyweight f12 = factory.getFlyweight(&quot;b&quot;); f01.operation(new UnsharedConcreteFlyweight(&quot;第1次调用a。&quot;)); f02.operation(new UnsharedConcreteFlyweight(&quot;第2次调用a。&quot;)); f03.operation(new UnsharedConcreteFlyweight(&quot;第3次调用a。&quot;)); f11.operation(new UnsharedConcreteFlyweight(&quot;第1次调用b。&quot;)); f12.operation(new UnsharedConcreteFlyweight(&quot;第2次调用b。&quot;)); }}//非享元角色class UnsharedConcreteFlyweight { private String info; UnsharedConcreteFlyweight(String info) { this.info = info; } public String getInfo() { return info; } public void setInfo(String info) { this.info = info; }}//抽象享元角色interface Flyweight { public void operation(UnsharedConcreteFlyweight state);}//具体享元角色class ConcreteFlyweight implements Flyweight { private String key; ConcreteFlyweight(String key) { this.key = key; System.out.println(&quot;具体享元&quot; + key + &quot;被创建！&quot;); } public void operation(UnsharedConcreteFlyweight outState) { System.out.print(&quot;具体享元&quot; + key + &quot;被调用，&quot;); System.out.println(&quot;非享元信息是:&quot; + outState.getInfo()); }}//享元工厂角色class FlyweightFactory { private HashMap&lt;String, Flyweight&gt; flyweights = new HashMap&lt;String, Flyweight&gt;(); public Flyweight getFlyweight(String key) { Flyweight flyweight = (Flyweight) flyweights.get(key); if (flyweight != null) { System.out.println(&quot;具体享元&quot; + key + &quot;已经存在，被成功获取！&quot;); } else { flyweight = new ConcreteFlyweight(key); flyweights.put(key, flyweight); } return flyweight; }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106import javax.swing.*;import java.awt.*;import java.awt.event.MouseAdapter;import java.awt.event.MouseEvent;import java.util.ArrayList;public class WzqGame { public static void main(String[] args) { new Chessboard(); }}//棋盘class Chessboard extends MouseAdapter { WeiqiFactory wf; JFrame f; Graphics g; JRadioButton wz; JRadioButton bz; private final int x = 50; private final int y = 50; private final int w = 40; //小方格宽度和高度 private final int rw = 400; //棋盘宽度和高度 Chessboard() { wf = new WeiqiFactory(); f = new JFrame(&quot;享元模式在五子棋游戏中的应用&quot;); f.setBounds(100, 100, 500, 550); f.setVisible(true); f.setResizable(false); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); JPanel SouthJP = new JPanel(); f.add(&quot;South&quot;, SouthJP); wz = new JRadioButton(&quot;白子&quot;); bz = new JRadioButton(&quot;黑子&quot;, true); ButtonGroup group = new ButtonGroup(); group.add(wz); group.add(bz); SouthJP.add(wz); SouthJP.add(bz); JPanel CenterJP = new JPanel(); CenterJP.setLayout(null); CenterJP.setSize(500, 500); CenterJP.addMouseListener(this); f.add(&quot;Center&quot;, CenterJP); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } g = CenterJP.getGraphics(); g.setColor(Color.BLUE); g.drawRect(x, y, rw, rw); for (int i = 1; i &lt; 10; i++) { //绘制第i条竖直线 g.drawLine(x + (i * w), y, x + (i * w), y + rw); //绘制第i条水平线 g.drawLine(x, y + (i * w), x + rw, y + (i * w)); } } public void mouseClicked(MouseEvent e) { Point pt = new Point(e.getX() - 15, e.getY() - 15); if (wz.isSelected()) { ChessPieces c1 = wf.getChessPieces(&quot;w&quot;); c1.DownPieces(g, pt); } else if (bz.isSelected()) { ChessPieces c2 = wf.getChessPieces(&quot;b&quot;); c2.DownPieces(g, pt); } }}//抽象享元角色：棋子interface ChessPieces { public void DownPieces(Graphics g, Point pt); //下子}//具体享元角色：白子class WhitePieces implements ChessPieces { public void DownPieces(Graphics g, Point pt) { g.setColor(Color.WHITE); g.fillOval(pt.x, pt.y, 30, 30); }}//具体享元角色：黑子class BlackPieces implements ChessPieces { public void DownPieces(Graphics g, Point pt) { g.setColor(Color.BLACK); g.fillOval(pt.x, pt.y, 30, 30); }}//享元工厂角色class WeiqiFactory { private ArrayList&lt;ChessPieces&gt; qz; public WeiqiFactory() { qz = new ArrayList&lt;ChessPieces&gt;(); ChessPieces w = new WhitePieces(); qz.add(w); ChessPieces b = new BlackPieces(); qz.add(b); } public ChessPieces getChessPieces(String type) { if (type.equalsIgnoreCase(&quot;w&quot;)) { return (ChessPieces) qz.get(0); } else if (type.equalsIgnoreCase(&quot;b&quot;)) { return (ChessPieces) qz.get(1); } else { return null; } }} 《组合模式 Composite Pattern》 特点： 将对象组合成树状的层次结构的模式，用来表示“整体-部分”的关系，使用户对单个对象和组合对象具有一致的访问性 部分-整体 优点： 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 缺点： 设计较复杂，客户端需要花更多时间理清类之间的层次关系； 不容易限制容器中的构件； 不容易用继承的方法来增加构件的新功能； 角色： 分类： 透明方式 安全方式 抽象构件（Component）角色：它的主要作用是为树叶构件和树枝构件声明公共接口，并实现它们的默认行为。在透明式的组合模式中抽象构件还声明访问和管理子类的接口；在安全式的组合模式中不声明访问和管理子类的接口，管理工作由树枝构件完成。（总的抽象类或接口，定义一些通用的方法，比如新增、删除） 树叶构件（Leaf）角色：是组合中的叶节点对象，它没有子节点，用于继承或实现抽象构件。 树枝构件（Composite）角色 / 中间构件：是组合中的分支节点对象，它有子节点，用于继承和实现抽象构件。它的主要作用是存储和管理子部件，通常包含 Add()、Remove()、GetChild() 等方法。 组合模式的应用场景： 在需要表示一个对象整体与部分的层次结构的场合。 要求对用户隐藏组合对象与单个对象的不同，用户可以用统一的接口使用组合结构中的所有对象的场合。 JAVA实现： 安全组合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.util.ArrayList;public class Safe { public static void main(String[] args) { Composite c0 = new Composite(); Composite c1 = new Composite(); Component leaf1 = new Leaf(&quot;1&quot;); Component leaf2 = new Leaf(&quot;2&quot;); Component leaf3 = new Leaf(&quot;3&quot;); c0.add(leaf1); c0.add(c1); c1.add(leaf2); c1.add(leaf3); c0.operation(); }}//抽象构件interface Component {// public void add(Component c);// public void remove(Component c);// public Component getChild(int i); public void operation();}//树叶构件class Leaf implements Component { private String name; public Leaf(String name) { this.name = name; } public void add(Component c) { } public void remove(Component c) { } public Component getChild(int i) { return null; } @Override public void operation() { System.out.println(&quot;树叶&quot; + name + &quot;：被访问！&quot;); }}//树枝构件class Composite implements Component { private ArrayList&lt;Component&gt; children = new ArrayList&lt;Component&gt;(); public void add(Component c) { children.add(c); } public void remove(Component c) { children.remove(c); } public Component getChild(int i) { return children.get(i); } @Override public void operation() { for (Object obj : children) { ((Component) obj).operation(); } }} 透明组合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.util.ArrayList;public class transparent { public static void main(String[] args) { Component c0 = new Composite(); Component c1 = new Composite(); Component leaf1 = new Leaf(&quot;1&quot;); Component leaf2 = new Leaf(&quot;2&quot;); Component leaf3 = new Leaf(&quot;3&quot;); c0.add(leaf1); c0.add(c1); c1.add(leaf2); c1.add(leaf3); c0.operation(); }}//抽象构件interface Component { public void add(Component c); public void remove(Component c); public Component getChild(int i); public void operation();}//树叶构件class Leaf implements Component { private String name; public Leaf(String name) { this.name = name; } @Override public void add(Component c) { } @Override public void remove(Component c) { } @Override public Component getChild(int i) { return null; } @Override public void operation() { System.out.println(&quot;树叶&quot; + name + &quot;：被访问！&quot;); }}//树枝构件class Composite implements Component { private ArrayList&lt;Component&gt; children = new ArrayList&lt;Component&gt;(); @Override public void add(Component c) { children.add(c); } @Override public void remove(Component c) { children.remove(c); } @Override public Component getChild(int i) { return children.get(i); } @Override public void operation() { for (Object obj : children) { ((Component) obj).operation(); } }} 行为型模式 《**模板方法模式 **Template method pattern》 特点： 定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 优点： 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 缺点： 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象，间接地增加了系统实现的复杂度。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。 由于继承关系自身的缺点，如果父类添加新的抽象方法，则所有子类都要改一遍。 角色： 1）抽象类/抽象模板（Abstract Class） 抽象模板类，负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下。 ① 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。 ② 基本方法：是整个算法中的一个步骤，包含以下几种类型。 抽象方法：在抽象类中声明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。 2）具体子类/具体实现（Concrete Class） 具体实现类，实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤。 模板方法模式的应用场景： 算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。 当多个子类存在公共的行为时，可以将其提取出来并集中到一个公共父类中以避免代码重复。首先，要识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。 当需要控制子类的扩展时，模板方法只在特定点调用钩子操作，这样就只允许在这些点进行扩展。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334public class TemplateMethodPattern { public static void main(String[] args) { AbstractClass tm = new ConcreteClass(); tm.TemplateMethod(); }}//抽象类abstract class AbstractClass { //模板方法 public void TemplateMethod() { SpecificMethod(); abstractMethod1(); abstractMethod2(); } //具体方法 public void SpecificMethod() { System.out.println(&quot;抽象类中的具体方法被调用...&quot;); } //抽象方法1 public abstract void abstractMethod1(); //抽象方法2 public abstract void abstractMethod2();}//具体子类class ConcreteClass extends AbstractClass { @Override public void abstractMethod1() { System.out.println(&quot;抽象方法1的实现被调用...&quot;); } @Override public void abstractMethod2() { System.out.println(&quot;抽象方法2的实现被调用...&quot;); }} 《**策略模式 **Strategy Pattern》 特点： 该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。 优点： 多重条件语句不易维护，而使用策略模式可以避免使用多重条件语句，如 if...else 语句、switch...case 语句。 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码。 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的。 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法。 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离。 缺点： 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类。 策略模式造成很多的策略类，增加维护难度。 角色： 抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。 环境（Context）类：持有一个策略类的引用，最终给客户端调用。 策略模式的应用场景： 一个系统需要动态地在几种算法中选择一种时，可将每个算法封装到策略类中。 一个类定义了多种行为，并且这些行为在这个类的操作中以多个条件语句的形式出现，可将每个条件分支移入它们各自的策略类中以代替这些条件语句。 系统中各算法彼此完全独立，且要求对客户隐藏具体算法的实现细节时。 系统要求使用算法的客户不应该知道其操作的数据时，可使用策略模式来隐藏与算法相关的数据结构。 多个类只区别在表现行为不同，可以使用策略模式，在运行时动态选择具体要执行的行为。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041public class StrategyPattern { public static void main(String[] args) { Context c = new Context(); Strategy s = new ConcreteStrategyA(); c.setStrategy(s); c.strategyMethod(); System.out.println(&quot;-----------------&quot;); s = new ConcreteStrategyB(); c.setStrategy(s); c.strategyMethod(); }}//抽象策略类interface Strategy { public void strategyMethod(); //策略方法}//具体策略类Aclass ConcreteStrategyA implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略A的策略方法被访问！&quot;); }}//具体策略类Bclass ConcreteStrategyB implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略B的策略方法被访问！&quot;); }}//环境类class Context { private Strategy strategy; public Strategy getStrategy() { return strategy; } public void setStrategy(Strategy strategy) { this.strategy = strategy; } public void strategyMethod() { strategy.strategyMethod(); }} 《**命令模式 **Command Pattern》 特点： 将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 优点： 通过引入中间件（抽象接口）降低系统的耦合度。 扩展性良好，增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，且满足“开闭原则”。 可以实现宏命令。命令模式可以与组合模式结合，将多个命令装配成一个组合命令，即宏命令。 方便实现 Undo 和 Redo 操作。命令模式可以与后面介绍的备忘录模式结合，实现命令的撤销与恢复。 可以在现有命令的基础上，增加额外功能。比如日志记录，结合装饰器模式会更加灵活。 缺点： 可能产生大量具体的命令类。因为每一个具体操作都需要设计一个具体命令类，这会增加系统的复杂性。 命令模式的结果其实就是接收方的执行结果，但是为了以命令的形式进行架构、解耦请求与实现，引入了额外类型结构（引入了请求方与抽象命令接口），增加了理解上的困难。不过这也是设计模式的通病，抽象必然会额外增加类的数量，代码抽离肯定比代码聚合更加难理解 角色： 抽象命令类（Command）角色：声明执行命令的接口，拥有执行命令的抽象方法 execute()。 具体命令类（Concrete Command）角色：是抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作。 实现者/接收者（Receiver）角色：执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）角色：是请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者。 命令模式的应用场景： 请求调用者需要与请求接收者解耦时，命令模式可以使调用者和接收者不直接交互。 系统随机请求命令或经常增加、删除命令时，命令模式可以方便地实现这些功能。 当系统需要执行一组操作时，命令模式可以定义宏命令来实现该功能。 当系统需要支持命令的撤销（Undo）操作和恢复（Redo）操作时，可以将命令对象存储起来，采用备忘录模式来实现。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041public class StrategyPattern { public static void main(String[] args) { Context c = new Context(); Strategy s = new ConcreteStrategyA(); c.setStrategy(s); c.strategyMethod(); System.out.println(&quot;-----------------&quot;); s = new ConcreteStrategyB(); c.setStrategy(s); c.strategyMethod(); }}//抽象策略类interface Strategy { public void strategyMethod(); //策略方法}//具体策略类Aclass ConcreteStrategyA implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略A的策略方法被访问！&quot;); }}//具体策略类Bclass ConcreteStrategyB implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略B的策略方法被访问！&quot;); }}//环境类class Context { private Strategy strategy; public Strategy getStrategy() { return strategy; } public void setStrategy(Strategy strategy) { this.strategy = strategy; } public void strategyMethod() { strategy.strategyMethod(); }} 《**责任/职责 链模式 **Chain of Responsibility pattern》 特点： 为了避免请求发送者与多个请求处理者耦合在一起，于是将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 优点： 降低了对象之间的耦合度。该模式使得一个对象无须知道到底是哪一个对象处理其请求以及链的结构，发送者和接收者也无须拥有对方的明确信息。 增强了系统的可扩展性。可以根据需要增加新的请求处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态地改变链内的成员或者调动它们的次序，也可动态地新增或者删除责任。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 缺点： 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 角色： 抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。 责任/职责 链模式的应用场景： 多个对象可以处理一个请求，但具体由哪个对象处理该请求在运行时自动确定。 可动态指定一组对象处理请求，或添加新的处理者。 需要在不明确指定请求处理者的情况下，向多个处理者中的一个提交请求。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ChainOfResponsibilityPattern { public static void main(String[] args) { //组装责任链 Handler handler1 = new ConcreteHandler1(); Handler handler2 = new ConcreteHandler2(); handler1.setNext(handler2); //提交请求 handler1.handleRequest(&quot;two&quot;); }}//抽象处理者角色abstract class Handler { private Handler next; public void setNext(Handler next) { this.next = next; } public Handler getNext() { return next; } //处理请求的方法 public abstract void handleRequest(String request);}//具体处理者角色1class ConcreteHandler1 extends Handler { @Override public void handleRequest(String request) { if (request.equals(&quot;one&quot;)) { System.out.println(&quot;具体处理者1负责处理该请求！&quot;); } else { if (getNext() != null) { getNext().handleRequest(request); } else { System.out.println(&quot;没有人处理该请求！&quot;); } } }}//具体处理者角色2class ConcreteHandler2 extends Handler { @Override public void handleRequest(String request) { if (request.equals(&quot;two&quot;)) { System.out.println(&quot;具体处理者2负责处理该请求！&quot;); } else { if (getNext() != null) { getNext().handleRequest(request); } else { System.out.println(&quot;没有人处理该请求！&quot;); } } }} 《**状态模式 **State pattern》 特点： 对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。 优点： 结构清晰，状态模式将与特定状态相关的行为局部化到一个状态中，并且将不同状态的行为分割开来，满足“单一职责原则”。 将状态转换显示化，减少对象间的相互依赖。将不同的状态引入独立的对象中会使得状态转换变得更加明确，且减少对象间的相互依赖。 状态类职责明确，有利于程序的扩展。通过定义新的子类很容易地增加新的状态和转换。 缺点： 状态模式的使用必然会增加系统的类与对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当会导致程序结构和代码的混乱。 状态模式对开闭原则的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源码，否则无法切换到新增状态，而且修改某个状态类的行为也需要修改对应类的源码。 角色： 环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。 状态模式的应用场景： 当一个对象的行为取决于它的状态，并且它必须在运行时根据状态改变它的行为时，就可以考虑使用状态模式。 一个操作中含有庞大的分支结构，并且这些分支决定于对象的状态时。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class StatePatternClient { public static void main(String[] args) { Context context = new Context(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); }}//环境类class Context { private State state; //定义环境类的初始状态 public Context() { this.state = new ConcreteStateA(); } //设置新状态 public void setState(State state) { this.state = state; } //读取状态 public State getState() { return (state); } //对请求做处理 public void Handle() { state.Handle(this); }}//抽象状态类abstract class State { public abstract void Handle(Context context);}//具体状态A类class ConcreteStateA extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 A.&quot;); context.setState(new ConcreteStateB()); }}//具体状态B类class ConcreteStateB extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 B.&quot;); context.setState(new ConcreteStateA()); }} 《观察者模式 Observer pattern》 特点： 指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式。 优点： 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。符合依赖倒置原则。 目标与观察者之间建立了一套触发机制。 缺点： 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。 角色： 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。 观察者模式的应用场景： 对象间存在一对多关系，一个对象的状态发生改变会影响其他对象。 当一个抽象模型有两个方面，其中一个方面依赖于另一方面时，可将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。 实现类似广播机制的功能，不需要知道具体收听者，只需分发广播，系统中感兴趣的对象会自动接收该广播。 多层级嵌套使用，形成一种链式触发机制，使得事件具备跨域（跨越两种观察者类型）通知。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class StatePatternClient { public static void main(String[] args) { Context context = new Context(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); }}//环境类class Context { private State state; //定义环境类的初始状态 public Context() { this.state = new ConcreteStateA(); } //设置新状态 public void setState(State state) { this.state = state; } //读取状态 public State getState() { return (state); } //对请求做处理 public void Handle() { state.Handle(this); }}//抽象状态类abstract class State { public abstract void Handle(Context context);}//具体状态A类class ConcreteStateA extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 A.&quot;); context.setState(new ConcreteStateB()); }}//具体状态B类class ConcreteStateB extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 B.&quot;); context.setState(new ConcreteStateA()); }} 《中介者模式 Mediator Pattern》 特点： 定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。 优点： 类之间各司其职，符合迪米特法则。 降低了对象之间的耦合性，使得对象易于独立地被复用。 将对象间的一对多关联转变为一对一的关联，提高系统的灵活性，使得系统易于维护和扩展。 缺点： 中介者模式将原本多个对象直接的相互依赖变成了中介者和多个同事类的依赖关系。当同事类越多时，中介者就会越臃肿，变得复杂且难以维护。 角色： 抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者（Concrete Mediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。 抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。 具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。 中介者模式的应用场景： 当对象之间存在复杂的网状结构关系而导致依赖关系混乱且难以复用时。 当想创建一个运行于多个类之间的对象，又不想生成新的子类时 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.*;public class MediatorPattern { public static void main(String[] args) { Mediator md = new ConcreteMediator(); Colleague c1, c2; c1 = new ConcreteColleague1(); c2 = new ConcreteColleague2(); md.register(c1); md.register(c2); c1.send(); System.out.println(&quot;-------------&quot;); c2.send(); }}//抽象中介者abstract class Mediator { public abstract void register(Colleague colleague); public abstract void relay(Colleague cl); //转发}//具体中介者class ConcreteMediator extends Mediator { private List&lt;Colleague&gt; colleagues = new ArrayList&lt;Colleague&gt;(); public void register(Colleague colleague) { if (!colleagues.contains(colleague)) { colleagues.add(colleague); colleague.setMedium(this); } } public void relay(Colleague cl) { for (Colleague ob : colleagues) { if (!ob.equals(cl)) { ((Colleague) ob).receive(); } } }}//抽象同事类abstract class Colleague { protected Mediator mediator; public void setMedium(Mediator mediator) { this.mediator = mediator; } public abstract void receive(); public abstract void send();}//具体同事类class ConcreteColleague1 extends Colleague { public void receive() { System.out.println(&quot;具体同事类1收到请求。&quot;); } public void send() { System.out.println(&quot;具体同事类1发出请求。&quot;); mediator.relay(this); //请中介者转发 }}//具体同事类class ConcreteColleague2 extends Colleague { public void receive() { System.out.println(&quot;具体同事类2收到请求。&quot;); } public void send() { System.out.println(&quot;具体同事类2发出请求。&quot;); mediator.relay(this); //请中介者转发 }} 《迭代器模式 Iterator Pattern》 特点： 提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 优点： 访问一个聚合对象的内容而无须暴露它的内部表示。 遍历任务交由迭代器完成，这简化了聚合类。 它支持以不同方式遍历一个聚合，甚至可以自定义迭代器的子类以支持新的遍历。 增加新的聚合类和迭代器类都很方便，无须修改原有代码。 封装性良好，为遍历不同的聚合结构提供一个统一的接口。 缺点： 增加了类的个数，这在一定程度上增加了系统的复杂性。 角色： 抽象聚合（Aggregate）角色：定义存储、添加、删除聚合对象以及创建迭代器对象的接口。 具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、first()、next() 等方法。 具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。 中介者模式的应用场景： 当需要为聚合对象提供多种遍历方式时。 当需要为遍历不同的聚合结构提供一个统一的接口时。 当访问一个聚合对象的内容而无须暴露其内部细节的表示时。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import java.util.*;public class IteratorPattern { public static void main(String[] args) { Aggregate ag = new ConcreteAggregate(); ag.add(&quot;中山大学&quot;); ag.add(&quot;华南理工&quot;); ag.add(&quot;韶关学院&quot;); System.out.print(&quot;聚合的内容有：&quot;); Iterator it = ag.getIterator(); while (it.hasNext()) { Object ob = it.next(); System.out.print(ob.toString() + &quot;\\t&quot;); } Object ob = it.first(); System.out.println(&quot;\\nFirst：&quot; + ob.toString()); }}//抽象聚合 定义存储、添加、删除聚合对象以及创建迭代器对象的接口。interface Aggregate { public void add(Object obj); public void remove(Object obj); public Iterator getIterator();}//具体聚合 实现抽象聚合类，返回一个具体迭代器的实例。class ConcreteAggregate implements Aggregate { private List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); @Override public void add(Object obj) { list.add(obj); } @Override public void remove(Object obj) { list.remove(obj); } @Override public Iterator getIterator() { return (new ConcreteIterator(list)); }}//抽象迭代器interface Iterator { Object first(); Object next(); boolean hasNext();}//具体迭代器class ConcreteIterator implements Iterator { private List&lt;Object&gt; list = null; private int index = -1; public ConcreteIterator(List&lt;Object&gt; list) { this.list = list; } @Override public boolean hasNext() { if (index &lt; list.size() - 1) { return true; } else { return false; } } @Override public Object first() { index = 0; Object obj = list.get(index); ; return obj; } @Override public Object next() { Object obj = null; if (this.hasNext()) { obj = list.get(++index); } return obj; }} 《访问者模式 Visitor Pattern》 特点： 将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离，是行为类模式中最复杂的一种模式。 优点： 扩展性好。能够在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。 复用性好。可以通过访问者来定义整个对象结构通用的功能，从而提高系统的复用程度。 灵活性好。访问者模式将数据结构与作用于结构上的操作解耦，使得操作集合可相对自由地演化而不影响系统的数据结构。 符合单一职责原则。访问者模式把相关的行为封装在一起，构成一个访问者，使每一个访问者的功能都比较单一。 缺点： 增加新的元素类很困难。在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。 破坏封装。访问者模式中具体元素对访问者公布细节，这破坏了对象的封装性。 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 角色： 抽象访问者（Visitor）角色：定义一个访问具体元素的接口，为每个具体元素类对应一个访问操作 visit() ，该操作中的参数类型标识了被访问的具体元素。 具体访问者（ConcreteVisitor）角色：实现抽象访问者角色中声明的各个访问操作，确定访问者访问一个元素时该做什么。 抽象元素（Element）角色：声明一个包含接受操作 accept() 的接口，被接受的访问者对象作为 accept() 方法的参数。 具体元素（ConcreteElement）角色：实现抽象元素角色提供的 accept() 操作，其方法体通常都是 visitor.visit(this) ，另外具体元素中可能还包含本身业务逻辑的相关操作。 对象结构（Object Structure）角色：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。 访问者模式的应用场景： 对象结构相对稳定，但其操作算法经常变化的程序。 对象结构中的对象需要提供多种不同且不相关的操作，而且要避免让这些操作的变化影响对象的结构。 对象结构包含很多类型的对象，希望对这些对象实施一些依赖于其具体类型的操作。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.util.*;public class VisitorPattern { public static void main(String[] args) { ObjectStructure os = new ObjectStructure(); os.add(new ConcreteElementA()); os.add(new ConcreteElementB()); Visitor visitor = new ConcreteVisitorA(); os.accept(visitor); System.out.println(&quot;------------------------&quot;); visitor = new ConcreteVisitorB(); os.accept(visitor); }}//抽象访问者interface Visitor { void visit(ConcreteElementA element); void visit(ConcreteElementB element);}//具体访问者A类class ConcreteVisitorA implements Visitor { public void visit(ConcreteElementA element) { System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationA()); } public void visit(ConcreteElementB element) { System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationB()); }}//具体访问者B类class ConcreteVisitorB implements Visitor { public void visit(ConcreteElementA element) { System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationA()); } public void visit(ConcreteElementB element) { System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationB()); }}//抽象元素类interface Element { void accept(Visitor visitor);}//具体元素A类class ConcreteElementA implements Element { public void accept(Visitor visitor) { visitor.visit(this); } public String operationA() { return &quot;具体元素A的操作。&quot;; }}//具体元素B类class ConcreteElementB implements Element { public void accept(Visitor visitor) { visitor.visit(this); } public String operationB() { return &quot;具体元素B的操作。&quot;; }}//对象结构角色class ObjectStructure { private List&lt;Element&gt; list = new ArrayList&lt;Element&gt;(); public void accept(Visitor visitor) { Iterator&lt;Element&gt; i = list.iterator(); while (i.hasNext()) { ((Element) i.next()).accept(visitor); } } public void add(Element element) { list.add(element); } public void remove(Element element) { list.remove(element); }} 《备忘录/快照 模式 Memento Pattern》 特点： 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。 优点： 提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。 简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 缺点： 资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。 角色： 发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。 备忘录/快照 模式的应用场景： JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MementoPattern { public static void main(String[] args) { Originator or = new Originator(); Caretaker cr = new Caretaker(); or.setState(&quot;S0&quot;); System.out.println(&quot;初始状态:&quot; + or.getState()); cr.setMemento(or.createMemento()); //保存状态 or.setState(&quot;S1&quot;); System.out.println(&quot;新的状态:&quot; + or.getState()); or.restoreMemento(cr.getMemento()); //恢复状态 System.out.println(&quot;恢复状态:&quot; + or.getState()); }}//备忘录class Memento { private String state; public Memento(String state) { this.state = state; } public void setState(String state) { this.state = state; } public String getState() { return state; }}//发起人class Originator { private String state; public void setState(String state) { this.state = state; } public String getState() { return state; } public Memento createMemento() { return new Memento(state); } public void restoreMemento(Memento m) { this.setState(m.getState()); }}//管理者class Caretaker { private Memento memento; public void setMemento(Memento m) { memento = m; } public Memento getMemento() { return memento; }} 《解释器模式 Interpreter Pattern》 特点： 给分析对象定义一个语言，并定义该语言的文法表示，再设计一个解析器来解释语言中的句子。也就是说，用编译语言的方式来分析应用中的实例。这种模式实现了文法表达式处理的接口，该接口解释一个特定的上下文。 优点： 扩展性好。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。 容易实现。在语法树中的每个表达式节点类都是相似的，所以实现其文法较为容易。 缺点： 执行效率较低。解释器模式中通常使用大量的循环和递归调用，当要解释的句子较复杂时，其运行速度很慢，且代码的调试过程也比较麻烦。 会引起类膨胀。解释器模式中的每条规则至少需要定义一个类，当包含的文法规则很多时，类的个数将急剧增加，导致系统难以管理与维护。 可应用的场景比较少。在软件开发中，需要定义语言文法的应用实例非常少，所以这种模式很少被使用到。 角色： 文法（主谓宾）、句子、语法树 抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。 终结符表达式（Terminal Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。 非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。 环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。 客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。 解释器模式的应用场景： 当语言的文法较为简单，且执行效率不是关键问题时。 当问题重复出现，且可以用一种简单的语言来进行表达时。 当一个语言需要解释执行，并且语言中的句子可以表示为一个抽象语法树的时候，如 XML 文档解释。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.*;/*文法规则 &lt;expression&gt; ::= &lt;city&gt;的&lt;person&gt; &lt;city&gt; ::= 韶关|广州 &lt;person&gt; ::= 老人|妇女|儿童*/public class InterpreterPatternDemo { public static void main(String[] args) { Context bus = new Context(); bus.freeRide(&quot;韶关的老人&quot;); bus.freeRide(&quot;韶关的年轻人&quot;); bus.freeRide(&quot;广州的妇女&quot;); bus.freeRide(&quot;广州的儿童&quot;); bus.freeRide(&quot;山东的儿童&quot;); }}//抽象表达式类interface Expression { public boolean interpret(String info);}//终结符表达式类class TerminalExpression implements Expression { private Set&lt;String&gt; set = new HashSet&lt;String&gt;(); public TerminalExpression(String[] data) { for (int i = 0; i &lt; data.length; i++) set.add(data[i]); } @Override public boolean interpret(String info) { if (set.contains(info)) { return true; } return false; }}//非终结符表达式类class AndExpression implements Expression { private Expression city = null; private Expression person = null; public AndExpression(Expression city, Expression person) { this.city = city; this.person = person; } @Override public boolean interpret(String info) { String s[] = info.split(&quot;的&quot;); return city.interpret(s[0]) &amp;&amp; person.interpret(s[1]); }}//环境类class Context { private String[] citys = {&quot;韶关&quot;, &quot;广州&quot;}; private String[] persons = {&quot;老人&quot;, &quot;妇女&quot;, &quot;儿童&quot;}; private Expression cityPerson; public Context() { Expression city = new TerminalExpression(citys); Expression person = new TerminalExpression(persons); cityPerson = new AndExpression(city, person); } public void freeRide(String info) { boolean ok = cityPerson.interpret(info); if (ok) { System.out.println(&quot;您是&quot; + info + &quot;，您本次乘车免费！&quot;); } else { System.out.println(info + &quot;，您不是免费人员，本次乘车扣费2元！&quot;); } }} 全模式总结 一句话全模式总结 分类 设计模式 简述 一句话归纳 目的 生活案例 创建型设计模式 （简单来说就是用来创建对象的） [工厂模式（Factory Pattern）](#《工厂模式Factory Pattern》) 不同条件下创建不同实例 产品标准化，生产更高效 封装创建细节 实体工厂 单例模式（Singleton Pattern） 保证一个类仅有一个实例，并且提供一个全局访问点 世上只有一个我 保证独一无二 CEO 原型模式（Prototype Pattern） 通过拷贝原型创建新的对象 拔一根猴毛，吹出千万个 高效创建对象 克隆 建造者模式（Builder Pattern） 用来创建复杂的复合对象 高配中配和低配，想选哪配就哪配 开放个性配置步骤 选配 结构型设计模式 （关注类和对象的组合） [代理模式（Proxy Pattern）](#《代理模式 Proxy 》) 为其他对象提供一种代理以控制对这个对象的访问 没有资源没时间，得找别人来帮忙 增强职责 媒婆 [外观模式（Facade Pattern）](#《外观模式 Facade Pattern》) 对外提供一个统一的接口用来访问子系统 打开一扇门，通向全世界 统一访问入口 前台 [装饰器模式（Decorator Pattern）](#《装饰器模式 Decorator Pattern》) 为对象添加新功能 他大舅他二舅都是他舅 灵活扩展、同宗同源 煎饼 [享元模式（Flyweight Pattern）](#《享元模式 Flyweight Pattern》) 使用对象池来减少重复对象的创建 优化资源配置，减少重复浪费 共享资源池 全国社保联网 [组合模式（Composite Pattern）](#《组合模式 Composite Pattern》) 将整体与局部（树形结构）进行递归组合，让客户端能够以一种的方式对其进行处理 人在一起叫团伙，心在一起叫团队 统一整体和个体 组织架构树 [适配器模式（Adapter Pattern）](#《适配器模式 Adapter 》) 将原来不兼容的两个类融合在一起 万能充电器 兼容转换 电源适配 [桥接模式（Bridge Pattern）](#《桥接模式 Bridge》) 将两个能够独立变化的部分分离开来 约定优于配置 不允许用继承 桥 行为型设计模式 （关注对象之间的通信） 模板模式（Template Pattern） 定义一套流程模板，根据需要实现模板中的操作 流程全部标准化，需要微调请覆盖 逻辑复用 把大象装进冰箱 策略模式（Strategy Pattern） 封装不同的算法，算法之间能互相替换 条条大道通罗马，具体哪条你来定 把选择权交给用户 选择支付方式 责任链模式（Chain of Responsibility Pattern） 拦截的类都实现统一接口，每个接收者都包含对下一个接收者的引用。将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。 各人自扫门前雪，莫管他们瓦上霜 解耦处理逻辑 踢皮球 迭代器模式（Iterator Pattern） 提供一种方法顺序访问一个聚合对象中的各个元素 流水线上坐一天，每个包裹扫一遍 统一对集合的访问方式 逐个检票进站 命令模式（Command Pattern） 将请求封装成命令，并记录下来，能够撤销与重做 运筹帷幄之中，决胜千里之外 解耦请求和处理 遥控器 状态模式（State Pattern） 根据不同的状态做出不同的行为 状态驱动行为，行为决定状态 绑定状态和行为 订单状态跟踪 备忘录模式（Memento Pattern） 保存对象的状态，在需要时进行恢复 失足不成千古恨，想重来时就重来 备份、后悔机制 草稿箱 中介者模式（Mediator Pattern） 将对象之间的通信关联关系封装到一个中介类中单独处理，从而使其耦合松散 联系方式我给你，怎么搞定我不管 统一管理网状资源 朋友圈 解释器模式（Interpreter Pattern） 给定一个语言，定义它的语法表示，并定义一个解释器，这个解释器使用该标识来解释语言中的句子 我想说”方言“，一切解释权都归我 实现特定语法解析 摩斯密码 观察者模式（Observer Pattern） 状态发生改变时通知观察者，一对多的关系 到点就通知我 解耦观察者与被观察者 闹钟 访问者模式（Visitor Pattern） 稳定数据结构，定义新的操作行为 横看成岭侧成峰，远近高低各不同 解耦数据结构和数据操作 KPI考核 委派模式（Delegate Pattern） 允许对象组合实现与继承相同的代码重用，负责任务的调用和分配 这个需求很简单，怎么实现我不管 只对结果负责 授权委托书 资料来源 编程帮 Refactoringguru.cn","link":"/2021/03/01/Draft/2021/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"魑魅先生 | Redis","text":"简介 Redis（Remote Dictionary Server）远程字典服务 免费开源的，C语言编写，提供多种语言使用，支持网络，可持久化，遵守 BSD 协议，是一个高性能的 key-value 数据库。 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 综合问题 Redis多线程还是单线程？为什么这样？ ​ 单线程，Redis 6.0版本之后开始引入了多线程处理网络请求，将读取网络数据到输入缓冲区、协议解析和将执行结果写入输出缓冲区的过程变为了多线程，执行命令仍然是单线程。 ​ 为什么不使用多线程？因为Redis数据在内存中，多线程会增加上下文切换的代价，没有才是效率最高的。单线程没有数据一致性问题，不需要竞争锁和CAS操作。 数据类型？ 常用的五种数据类型： String：可以存储字符串、整数、浮点数，允许设置过期时间自动删除。 Hash：包含键值对的无序散列表。通过 “数组 + 链表” 的链地址法来解决部分哈希冲突，value 只能是字符串。 List：双向链表，可以充当队列和栈的角色。 Set：相当于Java 中的 HashSet ，内部的键值对是无序、唯一的。key 就是元素的值，value 为 null。 Zset：相当于 Java 中的 SortedSet 和 HashMap 的结合体，内部的键值对是有序、唯一的。可以为每个元素赋予一个 score 值，用来代表排序的权重。 不常用的4种数据类型： BitMap：即位图，其实也就是 byte 数组，用二进制表示，只有 0 和 1 两个数字。实际上来说是归属于 String 类型下面。 Hyperloglogs：用来做基数统计的算法。 Geospatial：将用户给定的地理位置信息储存起来， 并对这些信息进行操作。 Pub/Sub：支持多播的可持久化的消息队列，用于实现发布订阅功能。 Redis缓存异常方案 缓存血崩？ 缓存血崩是什么，如何解决？ 缓存预热？ 缓存穿透？ 缓存击穿？ 缓存降级？ Redis事务？ Redis事务是否有原子性，相关命令？ Redis 持久化机制 ？AOF 和 RDB 有什么区别？ Redis 淘汰策略有哪些？ Redis集群架构，主从复制？ springmvc的控制器是单例的吗?是线程安全的吗？ 学习工具 命令查询 狂神视频 Redis 最全面试题及答案_岁月安然-CSDN博客_redis题库 数据类型概览 数据类型 String Hash List Set ZSet Geospatial Hyperloglog Bitmap [Stream 5.0](#Redis Stream) 特点 字符串，整数或浮点数，二进制安全 哈希，包含键（string ）值对的无序散列表，适合存储对象 列表，链表上的节点字符串元素，插入有序 集合，不同无序字符串元素,哈希表 有序集合，不同字符串，还有带double类型的可重复分数的有序集合 地理位置 基数统计不占内存记录网站UV Bitmap 位图，数据结构！二进制位记录，0 和 1 两个状态 弥补发布订阅无法持久化的缺点 容量 512MB 232 -1 键值对（40多亿） 232 -1 232 -1 232 -1 增 set hset/hmset lpush，rpush sadd zadd geoadd pfadd setbit 取 get hget （取单个）/hmget (取多个) lrange取区间值lpop左取rpop 右取 smembers 取所有 zscore单取 zrange 范围取 zrevrange 升序取zrangebyscore 所有降序取zrevrangebyscore 所有升序取 geoposgeodist（两点距离）georadius（点附近点，可数量限制） getbit 删 del hdel Lpop、Rpop、lrem srem zrem 取旧改新 getset 自增 incr （可指定 HINCRBY增加指定步长 自减 decr （可指定 追加 append 长度 strlen hlen llen zcard pfcount（不重复数量） bitcount(计算状态数量) 批量同时设置一个或多个 key-value 对 mset hmset 返回所有一个或多个给定 key 的值 mget hmget 获取对象中所有的键值对 hgetall （取所有） 判断元素是否存在 hexists sismember 获取字段名 hkeys 获取字段值 hvals 集合运算 sunion 并集，sinter 交集，sdiff 交差 pfmerge(合并) 应用场景 与MongoDB对比 MongoDB Redis Nosql由来 单机mysql mysql+缓存+垂直拆分 主从读写分离 分表分库+水平拆分+mysql集群 现在用户社交网络数据成倍增长，sql难以支撑 NoSQL（Not Only SQL）非关系型数据库，不需要固定模式，无需多于操作就可以横向操作 简介 Redis（Remote Dictionary Server）远程字典服务 免费开源，C语言编写，提供多种语言使用，支持网络，可持久化，遵守 BSD 协议，是一个高性能的 key-value 数据库。 类型多样，不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis 优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 数据类型多样 – Redis支持二进制的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 数据的备份，即master-slave模式的数据备份。 Redis与其他key-value存储有什么不同？ Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 redis的应用场景 ​ 1、会话高速缓存（最常用） ​ 2、消息队列，比如支付 ​ 3、活动排行榜或计数 ​ 4、发布、订阅消息（消息通知） ​ 5、商品列表、评论列表、计时器、计数器 ​ 6、地图信息分析 ​ 7、内存存储、持久化（rdb，aof） 3V+3高 大数据时代3V（描述问题） 海量Volume 多样Variety 实时Velocity 互联网需求3高（对程序要求） 高并发 高可扩 高性能 经典应用 阿里巴巴 数据库四大分类 KV键值对 文档型数据库 列存储数据库 图关系数据库（图结构） CAP C：Consistency（强一致性） A：Availability（可用性） P：Partition tolerance（分区容错性） 三进二（C必实现） CA 单点集群，满足一致性 CP AP BASE 基本可用（Basically Available） 软状态（Soft state） 最终一致性（Eventually consistent） 分布式与集群 ACID关系型数据库 Redis 安装 windows 企业里面几乎不用windows开发Redis https://github.com/tporadowski/redis/releases redis-server.exe redis.windows.conf 另启一个 cmd 窗口，原来的不要关闭，不然就无法访问服务端了（可直接设置为服务自启动如下） 设置redis环境变量之后在解压文件夹中执行命令 redis-server.exe --service-install redis.windows.conf --loglevel verbose 修改服务为自动启动 redis-cli.exe -h 127.0.0.1 -p 6379 set myKey abc get myKey Linux ​ 下载 == 解压（opt） == 基本环境安装【yum install gcc-c++】== make == make install == redis默认安装路径/usr/local/bin == 拷贝redis的conf文件使用拷贝配置文件启动 == daemonize改为yes改为后台启动 == 启动服务redis-server redisconfig/redis.conf == 客户端连接 redis-cli -p 6379 == ping 测试连接 === 查看redis进程是否开启 ps -ef|grep redis == shutdown关闭服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187####新建文件夹[root@iZpwlt9baenyc8Z ~]# mkdir /usr/local/redis[root@iZpwlt9baenyc8Z ~]# cd /usr/local/redis[root@iZpwlt9baenyc8Z redis]# ls####下载源码[root@iZpwlt9baenyc8Z redis]# wget http://download.redis.io/releases/redis-6.0.5.tar.gz--2022-02-10 14:00:45-- http://download.redis.io/releases/redis-6.0.5.tar.gzredis-6.0.5.tar.gz 100%[==================================================================================&gt;] 2.11M 42.0KB/s in 57s 。。。####解压缩[root@iZpwlt9baenyc8Z redis]# tar -zxvf redis-6.0.5.tar.gz####安装[root@iZpwlt9baenyc8Z redis]# lsredis-6.0.5 redis-6.0.5.tar.gz[root@iZpwlt9baenyc8Z redis]# cd redis-6.0.5[root@iZpwlt9baenyc8Z redis-6.0.5]# ls00-RELEASENOTES CONTRIBUTING deps Makefile README.md runtest runtest-moduleapi sentinel.conf tests utilsBUGS COPYING INSTALL MANIFESTO redis.conf runtest-cluster runtest-sentinel src TLS.md[root@iZpwlt9baenyc8Z redis-6.0.5]# makecd src &amp;&amp; make allmake[1]: Entering directory '/usr/local/redis/redis-6.0.5/src' 。。。Hint: It's a good idea to run 'make test' ;)make[1]: Leaving directory '/usr/local/redis/redis-6.0.5/src'[root@iZpwlt9baenyc8Z redis-6.0.5]# ls00-RELEASENOTES CONTRIBUTING deps Makefile README.md runtest runtest-moduleapi sentinel.conf tests utilsBUGS COPYING INSTALL MANIFESTO redis.conf runtest-cluster runtest-sentinel src TLS.md[root@iZpwlt9baenyc8Z redis-6.0.5]# cd src####迁出可执行程序，与源码分离[root@iZpwlt9baenyc8Z src]# make install PREFIX=/opt/redis6 CC Makefile.depHint: It's a good idea to run 'make test' ;)####创建redis配置文件目录并复制配置文件[root@iZpwlt9baenyc8Z src]# mkdir /etc/redis[root@iZpwlt9baenyc8Z src]# cd ..[root@iZpwlt9baenyc8Z redis-6.0.5]# cd ..[root@iZpwlt9baenyc8Z redis]# lsredis-6.0.5 redis-6.0.5.tar.gz[root@iZpwlt9baenyc8Z redis]# cd ..[root@iZpwlt9baenyc8Z local]# lsaegis bin etc games include lib lib64 libexec redis sbin share src[root@iZpwlt9baenyc8Z local]# cd etc[root@iZpwlt9baenyc8Z etc]# ls[root@iZpwlt9baenyc8Z etc]# cd /etc[root@iZpwlt9baenyc8Z etc]# cd redis[root@iZpwlt9baenyc8Z redis]# ls[root@iZpwlt9baenyc8Z redis]# cp /usr/local/redis/redis-6.0.5/redis.conf /etc/redis/6379.conf[root@iZpwlt9baenyc8Z redis]# cd ..[root@iZpwlt9baenyc8Z etc]# cd ..[root@iZpwlt9baenyc8Z /]# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var[root@iZpwlt9baenyc8Z ~]# cd /etc/redis/[root@iZpwlt9baenyc8Z redis]# ls6379.conf[root@iZpwlt9baenyc8Z redis]# cd /opt/redis6[root@iZpwlt9baenyc8Z redis6]# lsbin###创建redis服务，复制redis示例服务并修改[root@iZpwlt9baenyc8Z bin]# cp /usr/local/redis/redis-6.0.5/utils/systemd-redis_server.service /lib/systemd/system/redis_6379.service[root@iZpwlt9baenyc8Z bin]# cd /lib/systemd/system/[root@iZpwlt9baenyc8Z system]# vim /lib/systemd/system/redis_6379.service########################### example systemd service unit file for redis-server## In order to use this as a template for providing a redis service in your# environment, _at the very least_ make sure to adapt the redis configuration# file you intend to use as needed (make sure to set &quot;supervised systemd&quot;), and# to set sane TimeoutStartSec and TimeoutStopSec property values in the unit's# &quot;[Service]&quot; section to fit your needs.## Some properties, such as User= and Group=, are highly desirable for virtually# all deployments of redis, but cannot be provided in a manner that fits all# expectable environments. Some of these properties have been commented out in# this example service unit file, but you are highly encouraged to set them to# fit your needs.## Please refer to systemd.unit(5), systemd.service(5), and systemd.exec(5) for# more information.[Unit]Description=Redis_6379After=network.target#Documentation=https://redis.io/documentation#Before=your_application.service another_example_application.service#AssertPathExists=/var/lib/redis[Service]Type=forkingPIDFile=/var/run/redis_6379.pidExecStart=/opt/redis6/bin/redis-server /etc/redis/6379.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDPrivateTmp=true## Alternatively, have redis-server load a configuration file:#ExecStart=/usr/local/bin/redis-server /path/to/your/redis.conf#LimitNOFILE=10032#NoNewPrivileges=yes#OOMScoreAdjust=-900#PrivateTmp=yes#TimeoutStartSec=infinity#TimeoutStopSec=infinity#UMask=0077#User=redis#Group=redis#WorkingDirectory=/var/lib/redis[Install]WantedBy=multi-user.target:wq!#######[root@iZpwlt9baenyc8Z system]# cd /###使用systemctl 开启关闭服务#如果报错：Job for redis-server.service failed because a timeout was exceeded.[root@iZpwlt9baenyc8Z ~]# systemctl status redis_6379● redis_6379.service - Redis_6379 └─151178 /opt/redis6/bin/redis-server 127.0.0.1:6379Feb 10 14:28:06 iZpwlt9baenyc8Z redis-server[151178]: `-.__.-'。。。Feb 10 14:28:06 iZpwlt9baenyc8Z redis-server[151178]: 151178:M 10 Feb 2022 14:28:06.493 * Ready to accept connectionslines 1-19/19 (END)[root@iZpwlt9baenyc8Z /]# cd /opt/redis6[root@iZpwlt9baenyc8Z redis6]# lsbin[root@iZpwlt9baenyc8Z redis6]# cd bin[root@iZpwlt9baenyc8Z bin]# lsredis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server###cd /opt/redis6/bin 跳转到命令目录后使用redis-server开启服务##全局使用redis命令解决方法#修改profile文件： #vi /etc/profile #在最后行添加: #export PATH=$PATH:/opt/redis6/bin#注意：/opt/software/redis/src 表示的是redis-cli 命令存在的目录路径#重新加载/etc/profile #source /etc/profile [root@iZpwlt9baenyc8Z bin]# redis-server151212:C 10 Feb 2022 14:38:14.346 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo151212:C 10 Feb 2022 14:38:14.346 # Redis version=6.0.5, bits=64, commit=00000000, modified=0, pid=151212, just started151212:C 10 Feb 2022 14:38:14.346 # Warning: no config file specified, using the default config. In order to specify a config file use ./redis-server /path/to/redis.conf _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 6.0.5 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 151212 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 151212:M 10 Feb 2022 14:38:14.347 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.151212:M 10 Feb 2022 14:38:14.347 # Server initialized151212:M 10 Feb 2022 14:38:14.347 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.151212:M 10 Feb 2022 14:38:14.347 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.151212:M 10 Feb 2022 14:38:14.348 * Ready to accept connections[root@iZpwlt9baenyc8Z ~]# cd /opt/redis6/bin#使用 ./redis-cli 打开终端[root@iZpwlt9baenyc8Z bin]# ./redis-cli ###检测服务是否启动127.0.0.1:6379&gt; pingPONG redis-benchmark性能测试 可选参数： 测试：100个并发连接 100000请求 ./redis-benchmark -h localhost -p 6379 -c 100 -n 100000 基础知识 redis默认有16个数据库，默认使用的是第0个 可以使用 select 进行切换数据库！select 3 DBSIZE # 查看DB大小！ keys * # 查看数据库所有的key 清除当前数据库 flushdb 清除全部数据库的内容 FLUSHALL Redis 为什么单线程还这么快？ 1、误区1：高性能的服务器一定是多线程的？ 2、误区2：多线程（CPU上下文会切换！）一定比单线程效率高！ 先去CPU&gt;内存&gt;硬盘的速度要有所了解！ 核心：redis 是将所有的数据全部放在内存中的，所以说使用单线程去操作效率就是最高的，多线程 （CPU上下文会切换：耗时的操作！！！），对于内存系统来说，如果没有上下文切换效率就是最高 的！多次读写都是在一个CPU上的，在内存情况下，这个就是最佳的方案！ IO多路复用（Epoll）原理 简单描述： ​ 执行 epoll_create 函数会在内核的高速缓存区中建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着应用程序执行 epoll_ctl 函数添加文件描述符会在红黑树上增加相应的结点。 ​ 执行 epoll_ctl 的 add 操作时，不仅将文件描述符放到红黑树上，而且也注册了 callBack 函数。内核在检测到某文件描述符可读/可写时会调用回调函数，该回调函数将文件描述符放在就绪链表中。 ​ 执行 epoll_wait 函数只用观察就绪链表中有无数据即可，最后将链表的数据及就绪的数量返回给应用程序，应用程序只需要遍历依次处理即可。这里返回的文件描述符是通过内存映射函数 mmap 让内核和用户空间共享同一块内存实现传递的，减少了不必要的拷贝。 ​ mmap：将用户空间的一段内存区域映射到内核空间，映射成功后，用户对这段内存区域的修改可以直接反映到内核空间。同样，内核空间对这段区域的修改也直接反映用户空间。那么对于内核空间、用户空间两者之间需要大量数据传输等操作的话效率是非常高的。 具体原理可以查看下面这篇博客： https://blog.csdn.net/armlinuxww/article/details/92803381 Redis 配置 Redis 的配置文件位于 Redis 安装目录下，文件名为 redis.conf(Windows 名为 redis.windows.conf)。 你可以通过 CONFIG 命令查看或设置配置项。 CONFIG GET CONFIG_SETTING_NAME CONFIG GET loglevel CONFIG GET * 编辑配置 可以通过修改 redis.conf 文件或使用 CONFIG set 命令来修改配置。 CONFIG SET CONFIG_SETTING_NAME NEW_CONFIG_VALUE CONFIG SET loglevel &quot;notice&quot; 参数说明 redis.conf 配置项说明如下： https://www.runoob.com/redis/redis-conf.html Redis 数据类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950设置密码：config set requirepass 123456授权密码：auth 123456127.0.0.1:6379&gt; keys * # 查看所有的key(empty list or set)127.0.0.1:6379&gt; set name xiaobai # set keyOK127.0.0.1:6379&gt; keys *1) &quot;name&quot;127.0.0.1:6379&gt; set age 1OK127.0.0.1:6379&gt; keys *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:6379&gt; EXISTS name # 判断当前的key是否存在(integer) 1127.0.0.1:6379&gt; EXISTS name1(integer) 0127.0.0.1:6379&gt; move name 1 # 移除当前的key(integer) 1127.0.0.1:6379&gt; keys *1) &quot;age&quot;127.0.0.1:6379&gt; set name qinjiangOK127.0.0.1:6379&gt; keys *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:6379&gt; clear127.0.0.1:6379&gt; keys *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:6379&gt; get name&quot;qinjiang&quot;127.0.0.1:6379&gt; EXPIRE name 10 # 设置key的过期时间，单位是秒(integer) 1127.0.0.1:6379&gt; ttl name # 查看当前key的剩余时间(integer) 4127.0.0.1:6379&gt; ttl name(integer) 3127.0.0.1:6379&gt; ttl name(integer) 2127.0.0.1:6379&gt; ttl name(integer) 1127.0.0.1:6379&gt; ttl name(integer) -2127.0.0.1:6379&gt; get name(nil)127.0.0.1:6379&gt; type name # 查看当前key的一个类型！string127.0.0.1:6379&gt; type agestring String（字符串） string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。 string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。 SET runoob &quot;菜鸟教程&quot; GET runoob 123456keys 正则：获取符合正则的键的值exist：是否存在type：值类型del：删除expire：设置过期时间ttl：查看有效时间 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131##########################################################################127.0.0.1:6379&gt; set key1 v1 # 设置值OK127.0.0.1:6379&gt; get key1 # 获得值&quot;v1&quot;127.0.0.1:6379&gt; keys * # 获得所有的key1) &quot;key1&quot;127.0.0.1:6379&gt; EXISTS key1 # 判断某一个key是否存在(integer) 1127.0.0.1:6379&gt; APPEND key1 &quot;hello&quot; # 追加字符串，如果当前key不存在，就相当于set key(integer) 7127.0.0.1:6379&gt; get key1&quot;v1hello&quot;127.0.0.1:6379&gt; STRLEN key1 # 获取字符串的长度！(integer) 7127.0.0.1:6379&gt; APPEND key1 &quot;,xiaobai&quot;(integer) 17127.0.0.1:6379&gt; STRLEN key1(integer) 17127.0.0.1:6379&gt; get key1&quot;v1hello,xiaobai&quot;########################################################################### i++# 步长 i+=127.0.0.1:6379&gt; set views 0 # 初始浏览量为0OK127.0.0.1:6379&gt; get views&quot;0&quot;127.0.0.1:6379&gt; incr views # 自增1 浏览量变为1(integer) 1127.0.0.1:6379&gt; incr views(integer) 2127.0.0.1:6379&gt; get views&quot;2&quot;127.0.0.1:6379&gt; decr views # 自减1 浏览量-1(integer) 1127.0.0.1:6379&gt; decr views(integer) 0127.0.0.1:6379&gt; decr views(integer) -1127.0.0.1:6379&gt; get views&quot;-1&quot;127.0.0.1:6379&gt; INCRBY views 10 # 可以设置步长，指定增量！(integer) 9127.0.0.1:6379&gt; INCRBY views 10(integer) 19127.0.0.1:6379&gt; DECRBY views 5(integer) 14########################################################################### 字符串范围 range127.0.0.1:6379&gt; set key1 &quot;hello,xiaobai&quot; # 设置 key1 的值OK127.0.0.1:6379&gt; get key1&quot;hello,xiaobai&quot;127.0.0.1:6379&gt; GETRANGE key1 0 3 # 截取字符串 [0,3]&quot;hell&quot;127.0.0.1:6379&gt; GETRANGE key1 0 -1 # 获取全部的字符串 和 get key是一样的&quot;hello,xiaobai&quot;# 替换！127.0.0.1:6379&gt; set key2 abcdefgOK127.0.0.1:6379&gt; get key2&quot;abcdefg&quot;127.0.0.1:6379&gt; SETRANGE key2 1 xx # 替换指定位置开始的字符串！(integer) 7127.0.0.1:6379&gt; get key2&quot;axxdefg&quot;########################################################################### setex (set with expire) # 设置过期时间# setnx (set if not exist) # 不存在在设置 （在分布式锁中会常常使用！）127.0.0.1:6379&gt; setex key3 30 &quot;hello&quot; # 设置key3 的值为 hello,30秒后过期OK127.0.0.1:6379&gt; ttl key3(integer) 26127.0.0.1:6379&gt; get key3&quot;hello&quot;127.0.0.1:6379&gt; setnx mykey &quot;redis&quot; # 如果mykey 不存在，创建mykey(integer) 1127.0.0.1:6379&gt; keys *1) &quot;key2&quot;2) &quot;mykey&quot;3) &quot;key1&quot;127.0.0.1:6379&gt; ttl key3(integer) -2127.0.0.1:6379&gt; setnx mykey &quot;MongoDB&quot; # 如果mykey存在，创建失败！(integer) 0127.0.0.1:6379&gt; get mykey&quot;redis&quot;##########################################################################msetmget127.0.0.1:6379&gt; mset k1 v1 k2 v2 k3 v3 # 同时设置多个值OK127.0.0.1:6379&gt; keys *1) &quot;k1&quot;2) &quot;k2&quot;3) &quot;k3&quot;127.0.0.1:6379&gt; mget k1 k2 k3 # 同时获取多个值1) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; msetnx k1 v1 k4 v4 # msetnx 是一个原子性的操作，要么一起成功，要么一起失败！(integer) 0127.0.0.1:6379&gt; get k4(nil)# 对象set user:1 {name:zhangsan,age:3} # 设置一个user:1 对象 值为 json字符来保存一个对象！# 这里的key是一个巧妙的设计： user:{id}:{filed} , 如此设计在Redis中是完全OK了！127.0.0.1:6379&gt; mset user:1:name zhangsan user:1:age 2OK127.0.0.1:6379&gt; mget user:1:name user:1:age1) &quot;zhangsan&quot;2) &quot;2&quot;##########################################################################getset # 先get然后在set127.0.0.1:6379&gt; getset db redis # 如果不存在值，则返回 nil(nil)127.0.0.1:6379&gt; get db&quot;redis127.0.0.1:6379&gt; getset db mongodb # 如果存在值，获取原来的值，并设置新的值&quot;redis&quot;127.0.0.1:6379&gt; get db&quot;mongodb&quot; 数据结构是相同的！ String类似的使用场景：value除了是我们的字符串还可以是我们的数字！ 计数器 统计多单位的数量 粉丝数 对象缓存存储！ Hash（哈希） Redis hash 是一个键值(key=&gt;value)对集合。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 - DEL runoob 用于删除前面测试用过的 key，不然会报错：(error) WRONGTYPE Operation against a key holding the wrong kind of value - HMSET 设置了两个 field=&gt;value 对, HGET 获取对应 field 对应的 value。 每个 hash 可以存储 232 -1 键值对（40多亿）。 HMSET runoob field1 &quot;Hello&quot; field2 &quot;World&quot; HGET runoob field1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Map集合，key-map! 时候这个值是一个map集合！ 本质和String类型没有太大区别，还是一个简单的key-vlaue！set myhash field xiaobai5) &quot;d&quot;##########################################################################127.0.0.1:6379&gt; hset myhash field1 xiaobai # set一个具体 key-vlaue(integer) 1127.0.0.1:6379&gt; hget myhash field1 # 获取一个字段值&quot;xiaobai&quot;127.0.0.1:6379&gt; hmset myhash field1 hello field2 world # set多个 key-vlaueOK127.0.0.1:6379&gt; hmget myhash field1 field2 # 获取多个字段值1) &quot;hello&quot;2) &quot;world&quot;127.0.0.1:6379&gt; hgetall myhash # 获取全部的数据，1) &quot;field1&quot;2) &quot;hello&quot;3) &quot;field2&quot;4) &quot;world&quot;127.0.0.1:6379&gt; hdel myhash field1 # 删除hash指定key字段！对应的value值也就消失了！(integer) 1127.0.0.1:6379&gt; hgetall myhash1) &quot;field2&quot;2) &quot;world&quot;##########################################################################hlen127.0.0.1:6379&gt; hmset myhash field1 hello field2 worldOK127.0.0.1:6379&gt; HGETALL myhash1) &quot;field2&quot;2) &quot;world&quot;3) &quot;field1&quot;4) &quot;hello&quot;127.0.0.1:6379&gt; hlen myhash # 获取hash表的字段数量！(integer) 2##########################################################################127.0.0.1:6379&gt; HEXISTS myhash field1 # 判断hash中指定字段是否存在！(integer) 1127.0.0.1:6379&gt; HEXISTS myhash field3(integer) 0########################################################################### 只获得所有field# 只获得所有value127.0.0.1:6379&gt; hkeys myhash # 只获得所有field1) &quot;field2&quot;2) &quot;field1&quot;hash变更的数据 user name age,尤其是是用户信息之类的，经常变动的信息！ hash 更适合于对象的存储，String更加适合字符串存储！ List（列表） Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 列表最多可存储232 -1元素 (4294967295, 每个列表可存储40多亿)。 lpush runoob redis lpush runoob mongodb lrange runoob 0 10 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180基本的数据类型，列表127.0.0.1:6379&gt; msetnx k1 v1 k4 v4 # msetnx 是一个原子性的操作，要么一起成功，要么一起失败！(integer) 0127.0.0.1:6379&gt; get k4(nil)# 对象set user:1 {name:zhangsan,age:3} # 设置一个user:1 对象 值为 json字符来保存一个对象！# 这里的key是一个巧妙的设计： user:{id}:{filed} , 如此设计在Redis中是完全OK了！127.0.0.1:6379&gt; mset user:1:name zhangsan user:1:age 2OK127.0.0.1:6379&gt; mget user:1:name user:1:age1) &quot;zhangsan&quot;2) &quot;2&quot;##########################################################################getset # 先get然后在set127.0.0.1:6379&gt; getset db redis # 如果不存在值，则返回 nil(nil)127.0.0.1:6379&gt; get db&quot;redis127.0.0.1:6379&gt; getset db mongodb # 如果存在值，获取原来的值，并设置新的值&quot;redis&quot;127.0.0.1:6379&gt; get db&quot;mongodb&quot;在redis里面，我们可以把list玩成 ，栈、队列、阻塞队列！所有的list命令都是用l开头的，Redis不区分大小命令##########################################################################127.0.0.1:6379&gt; LPUSH list one # 将一个值或者多个值，插入到列表头部 （左）(integer) 1127.0.0.1:6379&gt; LPUSH list two(integer) 2127.0.0.1:6379&gt; LPUSH list three(integer) 3127.0.0.1:6379&gt; LRANGE list 0 -1 # 获取list中值！1) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;127.0.0.1:6379&gt; LRANGE list 0 1 # 通过区间获取具体的值！1) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; Rpush list righr # 将一个值或者多个值，插入到列表位部 （右）(integer) 4127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;4) &quot;righr&quot;##########################################################################LPOPRPOP127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;4) &quot;righr&quot;127.0.0.1:6379&gt; Lpop list # 移除list的第一个元素&quot;three&quot;127.0.0.1:6379&gt; Rpop list # 移除list的最后一个元素&quot;righr&quot;127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;two&quot;2) &quot;one&quot;##########################################################################Lindex127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;two&quot;2) &quot;one&quot;127.0.0.1:6379&gt; lindex list 1 # 通过下标获得 list 中的某一个值！&quot;one&quot;127.0.0.1:6379&gt; lindex list 0&quot;two&quot;##########################################################################Llen127.0.0.1:6379&gt; Lpush list one(integer) 1127.0.0.1:6379&gt; Lpush list two(integer) 2127.0.0.1:6379&gt; Lpush list three(integer) 3127.0.0.1:6379&gt; Llen list # 返回列表的长度(integer) 3##########################################################################移除指定的值！取关 uidLrem127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;three&quot;3) &quot;two&quot;4) &quot;one&quot;127.0.0.1:6379&gt; lrem list 1 one # 移除list集合中指定个数的value，精确匹配(integer) 1127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;three&quot;3) &quot;two&quot;127.0.0.1:6379&gt; lrem list 1 three(integer) 1127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; Lpush list three(integer) 3127.0.0.1:6379&gt; lrem list 2 three(integer) 2127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;two&quot;##########################################################################trim 修剪。； list 截断!127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; Rpush mylist &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; Rpush mylist &quot;hello1&quot;(integer) 2127.0.0.1:6379&gt; Rpush mylist &quot;hello2&quot;(integer) 3127.0.0.1:6379&gt; Rpush mylist &quot;hello3&quot;(integer) 4127.0.0.1:6379&gt; ltrim mylist 1 2 # 通过下标截取指定的长度，这个list已经被改变了，截断了只剩下截取的元素！OK127.0.0.1:6379&gt; LRANGE mylist 0 -11) &quot;hello1&quot;2) &quot;hello2&quot;##########################################################################rpoplpush # 移除列表的最后一个元素，将他移动到新的列表中！127.0.0.1:6379&gt; rpush mylist &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist &quot;hello1&quot;(integer) 2127.0.0.1:6379&gt; rpush mylist &quot;hello2&quot;(integer) 3127.0.0.1:6379&gt; rpoplpush mylist myotherlist # 移除列表的最后一个元素，将他移动到新的列表中！&quot;hello2&quot;127.0.0.1:6379&gt; lrange mylist 0 -1 # 查看原来的列表1) &quot;hello&quot;2) &quot;hello1&quot;127.0.0.1:6379&gt; lrange myotherlist 0 -1 # 查看目标列表中，确实存在改值！1) &quot;hello2&quot;###########################################################################lset 将列表中指定下标的值替换为另外一个值，更新操作127.0.0.1:6379&gt; EXISTS list # 判断这个列表是否存在(integer) 0127.0.0.1:6379&gt; lset list 0 item # 如果不存在列表我们去更新就会报错(error) ERR no such key127.0.0.1:6379&gt; lpush list value1(integer) 1127.0.0.1:6379&gt; LRANGE list 0 01) &quot;value1&quot;127.0.0.1:6379&gt; lset list 0 item # 如果存在，更新当前下标的值OK127.0.0.1:6379&gt; LRANGE list 0 01) &quot;item&quot;127.0.0.1:6379&gt; lset list 1 other # 如果不存在，则会报错！(error) ERR index out of range##########################################################################linsert # 将某个具体的value插入到列把你中某个元素的前面或者后面！127.0.0.1:6379&gt; Rpush mylist &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; Rpush mylist &quot;world&quot;(integer) 2127.0.0.1:6379&gt; LINSERT mylist before &quot;world&quot; &quot;other&quot;(integer) 3127.0.0.1:6379&gt; LRANGE mylist 0 -11) &quot;hello&quot;2) &quot;other&quot;3) &quot;world&quot;127.0.0.1:6379&gt; LINSERT mylist after world new(integer) 4127.0.0.1:6379&gt; LRANGE mylist 0 -11) &quot;hello&quot;2) &quot;other&quot;3) &quot;world&quot;4) &quot;new&quot;#小结#他实际上是一个链表，before Node after ， left，right 都可以插入值#如果key 不存在，创建新的链表#如果key存在，新增内容 Set（集合） Redis 的 Set 是 string 类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 sadd 命令 添加一个 string 元素到 key 对应的 set 集合中，成功返回 1，如果元素已经在集合中返回 0。 sadd key member 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 sadd runoob redis smembers runoob 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100set中的值是不能重读的！##########################################################################127.0.0.1:6379&gt; sadd myset &quot;hello&quot; # set集合中添加匀速(integer) 1127.0.0.1:6379&gt; sadd myset &quot;xiaobai&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;lovexiaobai&quot;(integer) 1127.0.0.1:6379&gt; SMEMBERS myset # 查看指定set的所有值1) &quot;hello&quot;2) &quot;lovexiaobai&quot;3) &quot;xiaobai&quot;127.0.0.1:6379&gt; SISMEMBER myset hello # 判断某一个值是不是在set集合中！(integer) 1127.0.0.1:6379&gt; SISMEMBER myset world(integer) 0##########################################################################127.0.0.1:6379&gt; scard myset # 获取set集合中的内容元素个数！(integer) 4##########################################################################rem127.0.0.1:6379&gt; srem myset hello # 移除set集合中的指定元素(integer) 1127.0.0.1:6379&gt; scard myset(integer) 3127.0.0.1:6379&gt; SMEMBERS myset1) &quot;lovexiaobai2&quot;2) &quot;lovexiaobai&quot;3) &quot;xiaobai&quot;##########################################################################set 无序不重复集合。抽随机！127.0.0.1:6379&gt; SMEMBERS myset1) &quot;lovexiaobai2&quot;2) &quot;lovexiaobai&quot;3) &quot;xiaobai&quot;127.0.0.1:6379&gt; SRANDMEMBER myset # 随机抽选出一个元素&quot;xiaobai&quot;127.0.0.1:6379&gt; SRANDMEMBER myset&quot;xiaobai&quot;127.0.0.1:6379&gt; SRANDMEMBER myset&quot;xiaobai&quot;127.0.0.1:6379&gt; SRANDMEMBER myset&quot;xiaobai&quot;127.0.0.1:6379&gt; SRANDMEMBER myset 2 # 随机抽选出指定个数的元素1) &quot;lovexiaobai&quot;2) &quot;lovexiaobai2&quot;127.0.0.1:6379&gt; SRANDMEMBER myset 21) &quot;lovexiaobai&quot;2) &quot;lovexiaobai2&quot;127.0.0.1:6379&gt; SRANDMEMBER myset # 随机抽选出一个元素&quot;lovexiaobai2&quot;##########################################################################删除定的key，随机删除key！127.0.0.1:6379&gt; SMEMBERS myset1) &quot;lovexiaobai2&quot;2) &quot;lovexiaobai&quot;3) &quot;xiaobai&quot;127.0.0.1:6379&gt; spop myset # 随机删除一些set集合中的元素！&quot;lovexiaobai2&quot;127.0.0.1:6379&gt; spop myset&quot;lovexiaobai&quot;127.0.0.1:6379&gt; SMEMBERS myset1) &quot;xiaobai&quot;##########################################################################将一个指定的值，移动到另外一个set集合！127.0.0.1:6379&gt; sadd myset &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;world&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;xiaobai&quot;(integer) 1127.0.0.1:6379&gt; sadd myset2 &quot;set2&quot;(integer) 1127.0.0.1:6379&gt; smove myset myset2 &quot;xiaobai&quot; # 将一个指定的值，移动到另外一个set集合！(integer) 1127.0.0.1:6379&gt; SMEMBERS myset1) &quot;world&quot;2) &quot;hello&quot;127.0.0.1:6379&gt; SMEMBERS myset21) &quot;xiaobai&quot;2) &quot;set2&quot;##########################################################################数字集合类：- 差集 SDIFF- 交集- 并集127.0.0.1:6379&gt; SDIFF key1 key2 # 差集1) &quot;b&quot;2) &quot;a&quot;127.0.0.1:6379&gt; SINTER key1 key2 # 交集 共同好友就可以这样实现1) &quot;c&quot;127.0.0.1:6379&gt; SUNION key1 key2 # 并集1) &quot;b&quot;2) &quot;c&quot;3) &quot;e&quot;4) &quot;a&quot;微博，A用户将所有关注的人放在一个set集合中！将它的粉丝也放在一个集合中！共同关注，共同爱好，二度好友，推荐好友！（六度分割理论） ZSet(有序集合) Redis zset (sorted set：有序集合)和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但**分数(score)**却可以重复。 zadd 命令 添加元素到集合，元素在集合中存在则更新对应score zadd key score member zadd runoob 0 redis ZRANGEBYSCORE runoob 0 1000 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879在set的基础上，增加了一个值，set k1 v1 zset k1 score1 v1127.0.0.1:6379&gt; hvals myhash # 只获得所有value1) &quot;world&quot;2) &quot;hello&quot;##########################################################################incr decr127.0.0.1:6379&gt; hset myhash field3 5 #指定增量！(integer) 1127.0.0.1:6379&gt; HINCRBY myhash field3 1(integer) 6127.0.0.1:6379&gt; HINCRBY myhash field3 -1(integer) 5127.0.0.1:6379&gt; hsetnx myhash field4 hello # 如果不存在则可以设置(integer) 1127.0.0.1:6379&gt; hsetnx myhash field4 world # 如果存在则不能设置(integer) 0127.0.0.1:6379&gt; zadd myset 1 one # 添加一个值(integer) 1127.0.0.1:6379&gt; zadd myset 2 two 3 three # 添加多个值(integer) 2127.0.0.1:6379&gt; ZRANGE myset 0 -11) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;##########################################################################排序如何实现127.0.0.1:6379&gt; zadd salary 2500 xiaohong # 添加三个用户(integer) 1127.0.0.1:6379&gt; zadd salary 5000 zhangsan(integer) 1127.0.0.1:6379&gt; zadd salary 500 xiaobai(integer) 1# ZRANGEBYSCORE key min max127.0.0.1:6379&gt; ZRANGEBYSCORE salary -inf +inf # 显示全部的用户 从小到大！1) &quot;xiaobai&quot;2) &quot;xiaohong&quot;3) &quot;zhangsan&quot;127.0.0.1:6379&gt; ZREVRANGE salary 0 -1 # 从大到进行排序！1) &quot;zhangsan&quot;2) &quot;xiaobai&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE salary -inf +inf withscores # 显示全部的用户并且附带成绩1) &quot;xiaobai&quot;2) &quot;500&quot;3) &quot;xiaohong&quot;4) &quot;2500&quot;5) &quot;zhangsan&quot;6) &quot;5000&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE salary -inf 2500 withscores # 显示工资小于2500员工的升序排序！1) &quot;xiaobai&quot;2) &quot;500&quot;3) &quot;xiaohong&quot;4) &quot;2500&quot;########################################################################### 移除rem中的元素127.0.0.1:6379&gt; zrange salary 0 -11) &quot;xiaobai&quot;2) &quot;xiaohong&quot;3) &quot;zhangsan&quot;127.0.0.1:6379&gt; zrem salary xiaohong # 移除有序集合中的指定元素(integer) 1127.0.0.1:6379&gt; zrange salary 0 -11) &quot;xiaobai&quot;2) &quot;zhangsan&quot;127.0.0.1:6379&gt; zcard salary # 获取有序集合中的个数(integer) 2##########################################################################127.0.0.1:6379&gt; zadd myset 1 hello(integer) 1127.0.0.1:6379&gt; zadd myset 2 world 3 xiaobai(integer) 2127.0.0.1:6379&gt; zcount myset 1 3 # 获取指定区间的成员数量！(integer) 3127.0.0.1:6379&gt; zcount myset 1 2(integer) 2案例思路：set 排序 存储班级成绩表，工资表排序！普通消息，1， 重要消息 2，带权重进行判断！排行榜应用实现，取Top N 测试！ Geospatial （地理位置） 朋友的定位，附近的人，打车距离计算？ Redis 的 Geo 在Redis3.2 版本就推出了！ 这个功能可以推算地理位置的信息，两地之间的距离，方圆几里的人！ 可以查询一些测试数据：http://www.jsons.cn/lngcodeinfo/0706D99C19A781A3/ 只有 六个命令： 官方文档：https://www.redis.net.cn/order/3685.html getadd getpos 获得当前定位：一定是一个坐标值！ GEODIST Geohash字符串属性 该命令将返回11个字符的Geohash字符串，所以没有精度Geohash，损失相比，使用内部52位表示。返回的geohashes具有以下特性： 他们可以缩短从右边的字符。它将失去精度，但仍将指向同一地区。 它可以在 geohash.org 网站使用，网址 http://geohash.org/&lt;geohash-string&gt;。查询例子：http://geohash.org/sqdtr74hyu0. 与类似的前缀字符串是附近，但相反的是不正确的，这是可能的，用不同的前缀字符串附近。 123456GEOHASH 返回一个或多个 geohash GEOPOS 返回一个或多个坐标数组，不存在为空GEODIST 返回所查两个点的距离，可指定单位GEORADIUS 根据所给点（非存在数据）以及要求范围和返回数据，返回对应范围内相关点信息GEOADD 添加一个过多个点信息（经纬度和，名称）GEORADIUSBYMEMBER 根据已有点以及所给范围查询范围内所有位置元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# getadd 添加地理位置# 规则：两级无法直接添加，我们一般会下载城市数据，直接通过java程序一次性导入！# 有效的经度从-180度到180度。# 有效的纬度从-85.05112878度到85.05112878度。# 当坐标位置超出上述指定范围时，该命令将会返回一个错误。# 127.0.0.1:6379&gt; geoadd china:city 39.90 116.40 beijing(error) ERR invalid longitude,latitude pair 39.900000,116.400000# 参数 key 值（）127.0.0.1:6379&gt; geoadd china:city 116.40 39.90 beijing(integer) 1127.0.0.1:6379&gt; geoadd china:city 121.47 31.23 shanghai(integer) 1127.0.0.1:6379&gt; geoadd china:city 106.50 29.53 chongqi 114.05 22.52 shengzhen(integer) 2127.0.0.1:6379&gt; geoadd china:city 120.16 30.24 hangzhou 108.96 34.26 xian(integer) 2127.0.0.1:6379&gt; GEOPOS china:city beijing # 获取指定的城市的经度和纬度！1) 1) &quot;116.39999896287918091&quot;2) &quot;39.90000009167092543&quot;127.0.0.1:6379&gt; GEOPOS china:city beijing chongqi1) 1) &quot;116.39999896287918091&quot;2) &quot;39.90000009167092543&quot;2) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;#两人之间的距离！#单位：#m 表示单位为米。#km 表示单位为千米。#mi 表示单位为英里。#ft 表示单位为英尺。#georadius 以给定的经纬度为中心， 找出某一半径内的元素#我附近的人？ （获得所有附近的人的地址，定位！）通过半径来查询！#获得指定数量的人，200#所有数据应该都录入：china:city ，才会让结果更加请求！127.0.0.1:6379&gt; GEODIST china:city beijing shanghai km # 查看上海到北京的直线距离&quot;1067.3788&quot;127.0.0.1:6379&gt; GEODIST china:city beijing chongqi km # 查看重庆到北京的直线距离&quot;1464.0708&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 1000 km # 以110，30 这个经纬度为中心，寻#找方圆1000km内的城市1) &quot;chongqi&quot;2) &quot;xian&quot;3) &quot;shengzhen&quot;4) &quot;hangzhou&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km1) &quot;chongqi&quot;2) &quot;xian&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withdist # 显示到中间距离的位置1) 1) &quot;chongqi&quot;2) &quot;341.9374&quot;2) 1) &quot;xian&quot;2) &quot;483.8340&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withcoord # 显示他人的定位信息1) 1) &quot;chongqi&quot;2) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;2) 1) &quot;xian&quot;2) 1) &quot;108.96000176668167114&quot;2) &quot;34.25999964418929977&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withdist withcoord count 1 ##筛选出指定的结果！1) 1) &quot;chongqi&quot;2) &quot;341.9374&quot;3) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withdist withcoord count 21) 1) &quot;chongqi&quot;2) &quot;341.9374&quot;3) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;2) 1) &quot;xian&quot;2) &quot;483.8340&quot;3) 1) &quot;108.96000176668167114&quot;2) &quot;34.25999964418929977&quot;#GEORADIUSBYMEMBER#GEOHASH 命令 - 返回一个或多个位置元素的 Geohash 表示#该命令将返回11个字符的Geohash字符串!#GEO 底层的实现原理其实就是 Zset！我们可以使用Zset命令来操作geo！ Hyperloglog（基数统计） 什么是基数？ A B 基数（不重复的元素） = 5，可以接受误差！ 简介 Redis 2.8.9 版本就更新了 Hyperloglog 数据结构！ Redis Hyperloglog 基数统计的算法！ 优点：占用的内存是固定，2^64 不同的元素的技术，只需要废 12KB内存！如果要从内存角度来比较的话 Hyperloglog 首选！ 网页的 UV （一个人访问一个网站多次，但是还是算作一个人！） 传统的方式， set 保存用户的id，然后就可以统计 set 中的元素数量作为标准判断 ! 这个方式如果保存大量的用户id，就会比较麻烦！我们的目的是为了计数，而不是保存用户id； 0.81% 错误率！ 统计UV任务，可以忽略不计的！ 测试使用 1234567891011121314127.0.0.1:6379&gt; PFadd mykey a b c d e f g h i j # 创建第一组元素 mykey(integer) 1127.0.0.1:6379&gt; PFCOUNT mykey # 统计 mykey 元素的基数数量(integer) 10127.0.0.1:6379&gt; PFadd mykey2 i j z x c v b n m # 创建第二组元素 mykey2(integer) 1127.0.0.1:6379&gt; PFCOUNT mykey2(integer) 9127.0.0.1:6379&gt; PFMERGE mykey3 mykey mykey2 # 合并两组 mykey mykey2 =&gt; mykey3 并集OK127.0.0.1:6379&gt; PFCOUNT mykey3 # 看并集的数量！(integer) 15如果允许容错，那么一定可以使用 Hyperloglog ！如果不允许容错，就使用 set 或者自己的数据类型即可！ Bitmap（位图） 位存储 统计用户信息，活跃，不活跃！ 登录 、 未登录！ 打卡，365打卡！ 两个状态的，都可以使用 Bitmaps！ Bitmap 位图，数据结构！ 都是操作二进制位来进行记录，就只有0 和 1 两个状态！ 365 天 = 365 bit 1字节 = 8bit 46 个字节左右！ 123456789101112#存setbit key offset value#取getbit key#and，or，xor, not操作bitop operation【and, or, xor, not】 destkey key [key]#统计1个数bitcount key [start end] #返回第一个0或1的位置bitpos key bit [start end]#对二进制位宽操作bitfield Redis 命令 Redis 命令 语法 Redis 客户端的基本语法为： $ redis-cli 以下实例讲解了如何启动 redis 客户端： 启动 redis 服务器，打开终端并输入命令 redis-cli，该命令会连接本地的 redis 服务。 $ redis-cli redis 127.0.0.1:6379&gt; redis 127.0.0.1:6379&gt; PING PONG 在以上实例中我们连接到本地的 redis 服务并执行 PING 命令，该命令用于检测 redis 服务是否启动。 在远程服务上执行命令 $ redis-cli -h host -p port -a password Redis keys 命令大全 https://redis.io/commands Redis Psubscribe 命令_订阅一个或多个符合给定模式的频道。 Redis HyperLogLog Redis 在 2.8.9 版本添加了 HyperLogLog 结构。 Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 什么是基数? 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 PFADD runoobkey &quot;redis&quot; PFCOUNT runoobkey Redis 发布订阅 123456Unsubscribe 取消订阅一个或多个Subscribe 订阅一个或多个Pubsub 查看订阅与发布状态Punsubscribe 退订所有给定的频道Publish 发送相关信息到所给的频道Psubscribe 订阅一个或多个 可用匹配符 * Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 订阅/发布消息图： 第一个：消息发送者， 第二个：频道 第三个：消息订阅者！ 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的 关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户 端： 命令 这些命令被广泛用于构建即时通信应用，比如网络聊天室(chatroom)和实时广播、实时提醒等 测试 123456789101112131415161718192021订阅端：127.0.0.1:6379&gt; SUBSCRIBE xiaobaishuo # 订阅一个频道 xiaobaishuoReading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;xiaobaishuo&quot;3) (integer) 1# 等待读取推送的信息1) &quot;message&quot; # 消息2) &quot;xiaobaishuo&quot; # 那个频道的消息3) &quot;hello,xiaobai&quot; # 消息的具体内容1) &quot;message&quot;2) &quot;xiaobaishuo&quot;3) &quot;hello,redis&quot;发送端：127.0.0.1:6379&gt; PUBLISH xiaobaishuo &quot;hello,xiaobai&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; PUBLISH xiaobaishuo &quot;hello,redis&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; 原理 Redis是使用C实现的，通过分析 Redis 源码里的 pubsub.c 文件，了解发布和订阅机制的底层实现，借此加深对 Redis 的理解。 Redis 通过 PUBLISH 、SUBSCRIBE 和 PSUBSCRIBE 等命令实现发布和订阅功能。 微信： 通过 SUBSCRIBE 命令订阅某频道后，redis-server 里维护了一个字典，字典的键就是一个个 频道！， 而字典的值则是一个链表，链表中保存了所有订阅这个 channel 的客户端。SUBSCRIBE 命令的关键， 就是将客户端添加到给定 channel 的订阅链表中。 通过 PUBLISH 命令向订阅者发送消息，redis-server 会使用给定的频道作为键，在它所维护的 channel 字典中查找记录了订阅这个频道的所有客户端的链表，遍历这个链表，将消息发布给所有订阅者。Pub/Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个 key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应 的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 使用场景： 1、实时消息系统！ 2、事实聊天！（频道当做聊天室，将信息回显给所有人即可！） 3、订阅，关注系统都是可以的！ 稍微复杂的场景我们就会使用 消息中间件 MQ （） SUBSCRIBE runoobChat PUBLISH runoobChat &quot;Redis PUBLISH test&quot; 重新开启个 redis 客户端在同一个频道 runoobChat 发布两次消息，订阅者就能接收到消息。 Redis 事务(不回滚，批量执行作用) Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证： 批量操作在发送 EXEC 命令前被放入队列缓存。 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行,代码错误，所有命令都不会执行。 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。 DISCARD放弃事物，所有不执行一个事务从开始到执行会经历以下三个阶段： 开始事务。 命令入队。 执行事务。 MULTI 开始一个事务 将多个命令入队到事务中 SET book-name &quot;Mastering C++ in 21 days&quot; GET book-name SADD tag &quot;C++&quot; &quot;Programming&quot; &quot;Mastering Series&quot; SMEMBERS tag EXEC/DISCARD 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。 悲观锁： 很悲观，认为什么时候都会出问题，无论做什么都会加锁！ 乐观锁： 很乐观，认为什么时候都不会出问题，所以不会上锁！ 更新数据的时候去判断一下，在此期间是否有人修改过这个数据， 获取version 更新的时候比较 version Redis测监视测试 正常执行成功！ 测试多线程修改值 , 使用watch 可以当做redis的乐观锁操作！ 123456789101112131415127.0.0.1:6379&gt; set money 100OK127.0.0.1:6379&gt; set out 0OK127.0.0.1:6379&gt; watch money # 监视 money 对象OK127.0.0.1:6379&gt; multi # 事务正常结束，数据期间没有发生变动，这个时候就正常执行成功！OK127.0.0.1:6379&gt; DECRBY money 20QUEUED127.0.0.1:6379&gt; INCRBY out 20QUEUED127.0.0.1:6379&gt; exec1) (integer) 802) (integer) 20 测试多线程修改值 , 使用watch 可以当做redis的乐观锁操作！ 1234567891011127.0.0.1:6379&gt; watch money # 监视 moneyOK127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; DECRBY money 10QUEUED127.0.0.1:6379&gt; INCRBY out 10QUEUED127.0.0.1:6379&gt; exec # 执行之前，另外一个线程，修改了我们的值，这个时候，就会导致事务执行失败！(nil) Redis 脚本 Redis 脚本使用 Lua 解释器来执行脚本。 Redis 2.6 版本通过内嵌支持 Lua 环境。执行脚本的常用命令为 EVAL。 EVAL script numkeys key [key ...] arg [arg ...] EVAL &quot;return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}&quot; 2 key1 key2 first second Redis 连接 Redis 连接命令主要是用于连接 redis 服务。 客户端如何通过密码验证连接到 redis 服务，并检测服务是否在运行： AUTH &quot;password&quot; PING 验证密码是否正确 1 AUTH password 打印字符串 2 ECHO message 查看服务是否运行 3 PING 关闭当前连接 4 QUIT 切换到指定的数据库 5 SELECT index Redis服务器 Redis服务器命令主要是用于管理redis服务 Redis GEO Redis GEO 主要用于存储地理位置信息，并对存储的信息进行操作，该功能在 Redis 3.2 版本新增。 Redis GEO 操作方法有： geoadd：添加地理位置的坐标。 geopos：获取地理位置的坐标。 geodist：计算两个位置之间的距离。 georadius：根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。 georadiusbymember：根据储存在位置集合里面的某个地点获取指定范围内的地理位置集合。 geohash：返回一个或多个位置对象的 geohash 值。 geoadd geoadd 用于存储指定的地理空间位置，可以将一个或多个经度(longitude)、纬度(latitude)、位置名称(member)添加到指定的 key 中。 geoadd 语法格式如下： GEOADD key longitude latitude member [longitude latitude member ...] - GEOADD Sicily 13.361389 38.115556 &quot;Palermo&quot; 15.087269 37.502669 &quot;Catania&quot; - GEODIST Sicily Palermo Catania - geodist 用于返回两个给定位置之间的距离。 geodist 语法格式如下： GEODIST key member1 member2 [m|km|ft|mi] member1 member2 为两个地理位置。 最后一个距离单位参数说明： m ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 - GEORADIUS Sicily 15 37 100 km - georadius 以给定的经纬度为中心， 返回键包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素。 georadiusbymember 和 GEORADIUS 命令一样， 都可以找出位于指定范围内的元素， 但是 georadiusbymember 的中心点是由给定的位置元素决定的， 而不是使用经度和纬度来决定中心点。 georadius 与 georadiusbymember 语法格式如下： GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] 参数说明： m ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 WITHDIST: 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 WITHCOORD: 将位置元素的经度和维度也一并返回。 WITHHASH: 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 COUNT 限定返回的记录数。 ASC: 查找结果根据距离从近到远排序。 DESC: 查找结果根据从远到近排序。 - GEOPOS Sicily Palermo Catania NonExisting - geopos 用于从给定的 key 里返回所有指定名称(member)的位置（经度和纬度），不存在的返回 nil。 geopos 语法格式如下： GEOPOS key member [member ...] - GEOHASH Sicily Palermo Catania - geohash Redis GEO 使用 geohash 来保存地理位置的坐标。 geohash 用于获取一个或多个位置元素的 geohash 值。 geohash 语法格式如下： GEOHASH key member [member ...] Redis Stream Redis Stream 是 Redis 5.0 版本新增加的数据结构。 Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅 (pub/sub) 来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。 简单来说发布订阅 (pub/sub) 可以分发消息，但无法记录历史消息。 而 Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。 Redis Stream 的结构如下所示，它有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容： 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。 上图解析： Consumer Group ：消费组，使用 XGROUP CREATE 命令创建，一个消费组有多个消费者(Consumer)。 last_delivered_id ：游标，每个消费组会有个游标 last_delivered_id，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动。 pending_ids ：消费者(Consumer)的状态变量，作用是维护消费者的未确认的 id。 pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack (Acknowledge character：确认字符）。 消息队列相关命令： XADD - 添加消息到末尾 XTRIM - 对流进行修剪，限制长度 XDEL - 删除消息 XLEN - 获取流包含的元素数量，即消息长度 XRANGE - 获取消息列表，会自动过滤已经删除的消息 XREVRANGE - 反向获取消息列表，ID 从大到小 XREAD - 以阻塞或非阻塞方式获取消息列表 消费者组相关命令： XGROUP CREATE - 创建消费者组 XREADGROUP GROUP - 读取消费者组中的消息 XACK - 将消息标记为&quot;已处理&quot; XGROUP SETID - 为消费者组设置新的最后递送消息ID XGROUP DELCONSUMER - 删除消费者 XGROUP DESTROY - 删除消费者组 XPENDING - 显示待处理消息的相关信息 XCLAIM - 转移消息的归属权 XINFO - 查看流和消费者组的相关信息； XINFO GROUPS - 打印消费者组的信息； XINFO STREAM - 打印流信息 XADD 使用 XADD 向队列添加消息，如果指定的队列不存在，则创建一个队列，XADD 语法格式： XADD key ID field value [field value ...] key ：队列名称，如果不存在就创建 ID ：消息 id，我们使用 * 表示由 redis 生成，可以自定义，但是要自己保证递增性。 field value ： 记录。 - XADD mystream * name Sara surname OConnor - XADD mystream * field1 value1 field2 value2 field3 value3 - XLEN mystream - XRANGE mystream - + XTRIM 使用 XTRIM 对流进行修剪，限制长度， 语法格式： XTRIM key MAXLEN [~] count key ：队列名称 MAXLEN ：长度 count ：数量 - XADD mystream * field1 A field2 B field3 C field4 D - XTRIM mystream MAXLEN 2 - XRANGE mystream - + XDEL 使用 XDEL 删除消息，语法格式： XDEL key ID [ID ...] key：队列名称 ID ：消息 ID 使用 XDEL 删除消息，语法格式： XLEN 使用 XLEN 获取流包含的元素数量，即消息长度，语法格式： XLEN key key：队列名称 - XADD mystream * item 1 - XLEN mystream XRANGE 使用 XRANGE 获取消息列表，会自动过滤已经删除的消息 ，语法格式： XRANGE key start end [COUNT count] key ：队列名 start ：开始值， - 表示最小值 end ：结束值， + 表示最大值 count ：数量 - XADD writers * name Ngozi surname Adichie - XLEN writers - XRANGE writers - + COUNT 2 XREVRANGE 使用 XREVRANGE 获取消息列表，会自动过滤已经删除的消息 ，语法格式： XREVRANGE key end start [COUNT count] key ：队列名 end ：结束值， + 表示最大值 start ：开始值， - 表示最小值 count ：数量 - XADD writers * name Virginia surname Woolf - XLEN writers - XREVRANGE writers + - COUNT 1 XREAD 使用 XREAD 以阻塞或非阻塞方式获取消息列表 ，语法格式： XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key ...] id [id ...] count ：数量 milliseconds ：可选，阻塞毫秒数，没有设置就是非阻塞模式 key ：队列名 id ：消息 ID - # 从 Stream 头部读取两条消息 XREAD COUNT 2 STREAMS mystream writers 0-0 0-0 XGROUP CREATE 使用 XGROUP CREATE 创建消费者组，语法格式： XGROUP [CREATE key groupname id-or-$] [SETID key groupname id-or-$] [DESTROY key groupname] [DELCONSUMER key groupname consumername] key ：队列名称，如果不存在就创建 groupname ：组名。 $ ： 表示从尾部开始消费，只接受新消息，当前 Stream 消息会全部忽略。 从头开始消费: XGROUP CREATE mystream consumer-group-name 0-0 从尾部开始消费: XGROUP CREATE mystream consumer-group-name $ XREADGROUP GROUP 使用 XREADGROUP GROUP 读取消费组中的消息，语法格式： XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...] group ：消费组名 consumer ：消费者名。 count ： 读取数量。 milliseconds ： 阻塞毫秒数。 key ： 队列名。 ID ： 消息 ID。 XREADGROUP GROUP consumer-group-name consumer-name COUNT 1 STREAMS mystream &gt; Redis 高级教程 Redis主从复制 概念 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点 (master/leader)，后者称为从节点(slave/follower)；数据的复制是单向的，只能由主节点到从节点。 Master以写为主，Slave 以读为主。 默认情况下，每台Redis服务器都是主节点； 且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。 主从复制的作用： 1、数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 2、故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务 的冗余。 3、负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务 （即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写 少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 4、高可用（集群）基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。 一般来说，要将Redis运用于工程项目中，只使用一台Redis是万万不能的（宕机），原因如下： 1、从结构上，单个Redis服务器会发生单点故障，并且一台服务器需要处理所有的请求负载，压力较大； 2、从容量上，单个Redis服务器内存容量有限，就算一台Redis服务器内存容量为256G，也不能将所有 内存用作Redis存储内存，一般来说，单台Redis最大使用内存不应该超过20G。 电商网站上的商品，一般都是一次上传，无数次浏览的，说专业点也就是&quot;多读少写&quot;。 对于这种场景，我们可以使如下这种架构： 主从复制，读写分离！ 80% 的情况下都是在进行读操作！减缓服务器的压力！架构中经常使用！ 一主二从！ 只要在公司中，主从复制就是必须要使用的，因为在真实的项目中不可能单机使用Redis！ 环境配置 只配置从库，不用配置主库！ 123456789101112127.0.0.1:6379&gt; info replication # 查看当前库的信息# Replicationrole:master # 角色 masterconnected_slaves:0 # 没有从机master_replid:b63c90e6c501143759cb0e7f450bd1eb0c70882amaster_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 复制3个配置文件，然后修改对应的信息 1、端口 2、pid 名字 3、log文件名字 4、dump.rdb 名字 修改完毕之后，启动我们的3个redis服务器，可以通过进程信息查看！ 一主二从 SLAVEOF 127.0.0.1 6379 # SLAVEOF host 6379 找谁当自己的老大！ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152默认情况下，每台Redis服务器都是主节点； 我们一般情况下只用配置从机就好了！认老大！ 一主 （79）二从（80，81）127.0.0.1:6379&gt; info replication # 查看当前库的信息# Replicationrole:master # 角色 masterconnected_slaves:0 # 没有从机master_replid:b63c90e6c501143759cb0e7f450bd1eb0c70882amaster_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0127.0.0.1:6380&gt; SLAVEOF 127.0.0.1 6379 # SLAVEOF host 6379 找谁当自己的老大！OK127.0.0.1:6380&gt; info replication# Replicationrole:slave # 当前角色是从机master_host:127.0.0.1 # 可以的看到主机的信息master_port:6379master_link_status:upmaster_last_io_seconds_ago:3master_sync_in_progress:0slave_repl_offset:14slave_priority:100slave_read_only:1connected_slaves:0master_replid:a81be8dd257636b2d3e7a9f595e69d73ff03774emaster_replid2:0000000000000000000000000000000000000000master_repl_offset:14second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:14# 在主机中查看！127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:1 # 多了从机的配置slave0:ip=127.0.0.1,port=6380,state=online,offset=42,lag=1 # 多了从机的配置master_replid:a81be8dd257636b2d3e7a9f595e69d73ff03774emaster_replid2:0000000000000000000000000000000000000000master_repl_offset:42second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:42如果两个都配置完了，就是有两个从机的真实的从主配置应该在配置文件中配置，这样的话是永久的，我们这里使用的是命令，暂时的！ 细节 主机可以写，从机不能写只能读！主机中的所有信息和数据，都会自动被从机保存！ 测试：主机断开连接，从机依旧连接到主机的，但是没有写操作，这个时候，主机如果回来了，从机依旧可以直接获取到主机写的信息！ 如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从主机中获取值！ 复制原理 Slave 启动成功连接到 master 后会发送一个sync同步命令 Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行 完毕之后，master将传送整个数据文件到slave，并完成一次完全同步。 全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master 继续将新的所有收集到的修改命令依次传给slave，完成同步 但是只要是重新连接master，一次完全同步（全量复制）将被自动执行！ 我们的数据一定可以在从机中 看到！ 层层链路 上一个M链接下一个 S！这时候也可以完成我们的主从复制！如果没有老大了，这个时候能不能选择一个老大出来呢？ 手动！谋朝篡位 如果主机断开了连接，我们可以使用 SLAVEOF no one 让自己变成主机！其他的节点就可以手动连 接到最新的这个主节点（手动）！如果这个时候老大修复了，那就重新连接！ 哨兵模式 概述 （自动选举老大的模式） 主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑 哨兵模式。Redis从2.8开始正式提供了Sentinel（哨兵） 架构来解决这个问题。 谋朝篡位的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。 哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独 立运行。其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。 这里的哨兵有两个作用 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。 当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服 务器，修改配置文件，让它们切换主机。 然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。 各个哨兵之间还会进行监控，这样就形成了多哨兵模式。 假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认 为主服务器不可用，这个现象成为主观下线。当后面的哨兵也检测到主服务器不可用，并且数量达到一 定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover[故障转移]操作。 切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为 客观下线。 测试 我们目前的状态是 一主二从！ 1、配置哨兵配置文件 sentinel.conf 12# sentinel monitor 被监控的名称 host port 1sentinel monitor myredis 127.0.0.1 6379 1 后面的这个数字1，代表主机挂了，slave投票看让谁接替成为主机，票数最多的，就会成为主机！ 2、启动哨兵！ 123456789101112131415161718192021222324252627282930313233[root@xiaobai bin]# redis-sentinel kconfig/sentinel.conf26607:X 31 Mar 2020 21:13:10.027 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo26607:X 31 Mar 2020 21:13:10.027 # Redis version=5.0.8, bits=64,commit=00000000, modified=0, pid=26607, just started26607:X 31 Mar 2020 21:13:10.027 # Configuration loaded_.__.-``__ ''-.__.-`` `. `_. ''-._ Redis 5.0.8 (00000000/0) 64 bit.-`` .-```. ```\\/ _.,_ ''-._( ' , .-` | `, ) Running in sentinel mode|`-._`-...-` __...-.``-._|'` _.-'| Port: 26379| `-._ `._ / _.-' | PID: 26607`-._ `-._ `-./ _.-' _.-'|`-._`-._ `-.__.-' _.-'_.-'|| `-._`-._ _.-'_.-' | http://redis.io`-._ `-._`-.__.-'_.-' _.-'|`-._`-._ `-.__.-' _.-'_.-'|| `-._`-._ _.-'_.-' |`-._ `-._`-.__.-'_.-' _.-'`-._ `-.__.-' _.-'`-._ _.-'`-.__.-'26607:X 31 Mar 2020 21:13:10.029 # WARNING: The TCP backlog setting of 511cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower valueof 128.26607:X 31 Mar 2020 21:13:10.031 # Sentinel ID is4c780da7e22d2aebe3bc20c333746f202ce7299626607:X 31 Mar 2020 21:13:10.031 # +monitor master myredis 127.0.0.1 6379 quorum126607:X 31 Mar 2020 21:13:10.031 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @myredis 127.0.0.1 637926607:X 31 Mar 2020 21:13:10.033 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @myredis 127.0.0.1 6379 如果Master 节点断开了，这个时候就会从从机中随机选择一个服务器！ （这里面有一个投票算法！） 哨兵模式 优点： 1、哨兵集群，基于主从复制模式，所有的主从配置优点，它全有 2、 主从可以切换，故障可以转移，系统的可用性就会更好 3、哨兵模式就是主从模式的升级，手动到自动，更加健壮！ 缺点： 1、Redis 不好啊在线扩容的，集群容量一旦到达上限，在线扩容就十分麻烦！ 2、实现哨兵模式的配置其实是很麻烦的，里面有很多选择！ 哨兵模式的全部配置！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Example sentinel.conf# 哨兵sentinel实例运行的端口 默认26379port 26379# 哨兵sentinel的工作目录dir /tmp# 哨兵sentinel监控的redis主节点的 ip port# master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。# quorum 配置多少个sentinel哨兵统一认为master主节点失联 那么这时客观上认为主节点失联了# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;sentinel monitor mymaster 127.0.0.1 6379 2# 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass mymaster MySUPER--secret-0123passw0rd# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;sentinel down-after-milliseconds mymaster 30000# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;sentinel parallel-syncs mymaster 1# 故障转移的超时时间 failover-timeout 可以用在以下这些方面：#1. 同一个sentinel对同一个master两次failover之间的间隔时间。#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。#3.当想要取消一个正在进行的failover所需要的时间。#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了# 默认三分钟# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;sentinel failover-timeout mymaster 180000# SCRIPTS EXECUTION#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。#对于脚本的运行结果有以下规则：#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。#通知脚本# shell编程# sentinel notification-script &lt;master-name&gt; &lt;script-path&gt;sentinel notification-script mymaster /var/redis/notify.sh# 客户端重新配置主节点参数脚本# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。# 以下参数将会在调用脚本时传给脚本:# &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt;# 目前&lt;state&gt;总是“failover”,# &lt;role&gt;是“leader”或者“observer”中的一个。# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt;sentinel client-reconfig-script mymaster /var/redis/reconfig.sh # 一般都是由运维来配置！ Redis缓存穿透和雪崩 服务的高可用问题！ 在这里我们不会详细的区分析解决方案的底层！ Redis缓存的使用，极大的提升了应用程序的性能和效率，特别是数据查询方面。但同时，它也带来了一 些问题。其中，最要害的问题，就是数据的一致性问题，从严格意义上讲，这个问题无解。如果对数据 的一致性要求很高，那么就不能使用缓存。 另外的一些典型问题就是，缓存穿透、缓存雪崩和缓存击穿。目前，业界也都有比较流行的解决方案。 缓存穿透（查不到） 概念 **【查不到的数据去查数据库】**缓存穿透的概念很简单，用户想要查询一个数据，发现redis内存数据库没有，也就是缓存没有命中，于是向持久层数据库查询。发现也没有，于是本次查询失败。当用户很多的时候，缓存都没有命中（秒杀），于是都去请求了持久层数据库。这会给持久层数据库造成很大的压力，这时候就相当于出现了缓存穿透。 解决方案 布隆过滤器 布隆过滤器是一种数据结构，对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则丢弃，从而避免了对底层存储系统的查询压力； 缓存空对象 当存储层不命中后，即使返回的空对象也将其缓存起来，同时会设置一个过期时间，之后再访问这个数 据将会从缓存中获取，保护了后端数据源； 但是这种方法会存在两个问题： 1、如果空值能够被缓存起来，这就意味着缓存需要更多的空间存储更多的键，因为这当中可能会有很多 的空值的键； 2、即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。 解决： 查询前先做规则校验，不合法的数据直接拦截返回 查询数据库没有数据也写一个 NULL 值到缓存里面并设置一个过期时间(不要超过1分钟，避免正常情况下也不能使用)，下一次查询在缓存失效前就能命中缓存直接返回 使用布隆过滤器，利用高效的数据结构和算法快速判断出这个 Key 是否在数据库中存在（需要提前将数据库所有数据放入到布隆过滤器的集合上，且布隆过滤器最大缺点就是无法删除数据。替换方案：布谷鸟过滤器） 缓存击穿（查崩了） 概述 **【过频繁访问热点数据】**这里需要注意和缓存击穿的区别，缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中 对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一 个屏障上凿开了一个洞。 当某个key在过期的瞬间，有大量的请求并发访问，这类数据一般是热点数据，由于缓存过期，会同时访 问数据库来查询最新数据，并且回写缓存，会导使数据库瞬间压力过大。 解决方案 设置热点数据永不过期 从缓存层面来看，没有设置过期时间，所以不会出现热点 key 过期后产生的问题。 加互斥锁 分布式锁：使用分布式锁，保证对于每个key同时只有一个线程去查询后端服务，其他线程没有获得分布式锁的权限，因此只需要等待即可。这种方式将高并发的压力转移到了分布式锁，因此对分布式锁的考验很大。 解决： 热点数据可以设置永不过期 增加分布式锁，等待抢到锁的请求构建完缓存后再释放锁。其他未抢到锁的请求进行阻塞等待，被唤醒后重新请求缓存获取数据 缓存雪崩（一片蹦） 概念 **【大量失效】**缓存雪崩，是指在某一个时间段，缓存集中过期失效。Redis 宕机！ 产生雪崩的原因之一，比如在写本文的时候，马上就要到双十二零点，很快就会迎来一波抢购，这波商品时间比较集中的放入了缓存，假设缓存一个小时。那么到了凌晨一点钟的时候，这批商品的缓存就都过期了。而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。 其实集中过期，倒不是非常致命，比较致命的缓存雪崩，是缓存服务器某个节点宕机或断网。因为自然 形成的缓存雪崩，一定是在某个时间段集中创建缓存，这个时候，数据库也是可以顶住压力的。无非就 是对数据库产生周期性的压力而已。而缓存服务节点的宕机，对数据库服务器造成的压力是不可预知 的，很有可能瞬间就把数据库压垮。 解决方案 redis高可用 这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续 工作，其实就是搭建的集群。（异地多活！） 限流降级（在SpringCloud讲解过！） 这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对 某个key只允许一个线程查询数据和写缓存，其他线程等待。 数据预热 数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数 据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让 缓存失效的时间点尽量均匀。 解决： 设置随机过期时间，避免同一时间大面积失效 如果是集群部署，将热点数据均匀分布在不同的 Redis 库上避免全部失效 设置热点数据永不过期，有更新缓存 对源服务访问进行限流、资源隔离（熔断）、降级等 如果是时点数据，可以进行预加载。等时点达到后，进行切换 Redis持久化 面试和工作，持久化都是重点！ Redis 是内存数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中 的数据库状态也会消失。所以 Redis 提供了持久化功能！ RDB**（Redis DataBase）** 什么是RDB ：在主从复制中，rdb就是备用了！从机上面！ 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。 Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程 都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的。 这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那 RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。我们默认的就是 RDB，一般情况下不需要修改这个配置！ 有时候在生产环境我们会将这个文件进行备份！ rdb保存的文件是dump.rdb 都是在我们的配置文件中快照中进行配置的！ 触发机制 1、save的规则满足的情况下，会自动触发rdb规则 2、执行 flushall 命令，也会触发我们的rdb规则！ 3、退出redis，也会产生 rdb 文件！ 备份就自动生成一个 dump.rdb 如何恢复rdb文件 1、只需要将rdb文件放在我们redis启动目录就可以，redis启动的时候会自动检查dump.rdb 恢复其中 的数据！ 2、查看需要存在的位置 几乎就他自己默认的配置就够用了，但是我们还是需要去学习！ RDB优点： 1、适合大规模的数据恢复！ 2、对数据的完整性要不高！ RDB缺点： 1、需要一定的时间间隔进程操作！如果redis意外宕机了，这个最后一次修改数据就没有的了！ 2、fork进程的时候，会占用一定的内容空间！！ AOF**（Append Only File）** 将我们的所有命令都记录下来，history，恢复的时候就把这个文件全部在执行一遍！ 是什么 127.0.0.1:6379&gt; config get dir 1) &quot;dir&quot; 2) &quot;/usr/local/bin&quot; # 如果在这个目录下存在 dump.rdb 文件，启动就会自动恢复其中的数据 以日志的形式来记录每个写操作，将Redis执行过的所有指令记录下来（读操作不记录），只许追加文件 但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件 的内容将写指令从前到后执行一次以完成数据的恢复工作 Aof保存的是 appendonly.aof 文件 默认是不开启的，我们需要手动进行配置！我们只需要将 appendonly 改为yes就开启了 aof！ 再重启redis 就可以生效了！ 如果这个 aof 文件有错位，这时候 redis 是启动不起来的吗，我们需要修复这个aof文件 redis 给我们提供了一个工具 redis-check-aof --fix aof 默认就是文件的无限追加，文件会越来越大！ 如果 aof 文件大于 64m，太大了！ fork一个新的进程来将我们的文件进行重写！ 优点和缺点！ 123456appendonly no # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！appendfilename &quot;appendonly.aof&quot; # 持久化的文件的名字# appendfsync always # 每次修改都会 sync。消耗性能appendfsync everysec # 每秒执行一次 sync，可能会丢失这1s的数据！# appendfsync no # 不执行 sync，这个时候操作系统自己同步数据，速度最快！# rewrite 重写， AOF优点： 1、每一次修改都同步，文件的完整会更加好！ 2、每秒同步一次，可能会丢失一秒的数据 3、从不同步，效率最高的！ AOF缺点： 1、相对于数据文件来说，aof远远大于 rdb，修复的速度也比 rdb慢！ 2、Aof 运行效率也要比 rdb 慢，所以我们redis默认的配置就是rdb持久化！ 扩展： 1、RDB 持久化方式能够在指定的时间间隔内对你的数据进行快照存储 2、AOF 持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始 的数据，AOF命令以Redis 协议追加保存每次写的操作到文件末尾，Redis还能对AOF文件进行后台重写，使得AOF文件的体积不至于过大。 3、只做缓存，如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化 4、同时开启两种持久化方式 在这种情况下，当redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF 文件保存的数据集要比RDB文件保存的数据集要完整。 RDB 的数据不实时，同时使用两者时服务器重启也只会找AOF文件，那要不要只使用AOF呢？作者 建议不要，因为RDB更适合用于备份数据库（AOF在不断变化不好备份），快速重启，而且不会有 AOF可能潜在的Bug，留着作为一个万一的手段。 5、性能建议 因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够 了，只保留 save 900 1 这条规则。 如果Enable AOF ，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自 己的AOF文件就可以了，代价一是带来了持续的IO，二是AOF rewrite 的最后将 rewrite 过程中产 生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite 的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上，默认超过原大小100%大小重 写可以改到适当的数值。 如果不Enable AOF ，仅靠 Master-Slave Repllcation 实现高可用性也可以，能省掉一大笔IO，也减少了rewrite时带来的系统波动。代价是如果Master/Slave 同时倒掉，会丢失十几分钟的数据， 启动脚本也要比较两个 Master/Slave 中的 RDB文件，载入较新的那个，微博就是这种架构。 命令 1234567891011121314151617181920测试订阅端：127.0.0.1:6379&gt; SUBSCRIBE xiaobaishuo # 订阅一个频道 xiaobaishuoReading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;xiaobaishuo&quot;3) (integer) 1# 等待读取推送的信息1) &quot;message&quot; # 消息2) &quot;xiaobaishuo&quot; # 那个频道的消息3) &quot;hello,xiaobai&quot; # 消息的具体内容1) &quot;message&quot;2) &quot;xiaobaishuo&quot;3) &quot;hello,redis&quot;发送端：127.0.0.1:6379&gt; PUBLISH xiaobaishuo &quot;hello,xiaobai&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; PUBLISH xiaobaishuo &quot;hello,redis&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; 原理 Redis是使用C实现的，通过分析 Redis 源码里的 pubsub.c 文件，了解发布和订阅机制的底层实现，借此加深对 Redis 的理解。 Redis 通过 PUBLISH 、SUBSCRIBE 和 PSUBSCRIBE 等命令实现发布和订阅功能。 微信： 通过 SUBSCRIBE 命令订阅某频道后，redis-server 里维护了一个字典，字典的键就是一个个 频道！， 而字典的值则是一个链表，链表中保存了所有订阅这个 channel 的客户端。SUBSCRIBE 命令的关键， 就是将客户端添加到给定 channel 的订阅链表中。 Pub/Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个 key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应 的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 使用场景： 1、实时消息系统！ 2、事实聊天！（频道当做聊天室，将信息回显给所有人即可！） 3、订阅，关注系统都是可以的！ 稍微复杂的场景我们就会使用 消息中间件 MQ （） Redis 数据淘汰策略 redis 提供 6种数据淘汰策略： voltile-lru：voltile（挥发性的）从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：(Time to Live)从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 常用的淘汰算法： 1.FIFO：First In First Out，先进先出。判断被存储的时间，离目前最远的数据优先被淘汰。 2.LRU：Least Recently Used，最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。 3.LFU：Least Frequently Used，最不经常使用。在一段时间内，数据被使用次数最少的，优先被淘汰。 Redis保持数据库一致性 Cache Aside Pattern 最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern。 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 为什么是删除缓存，而不是更新缓存？ 原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？ 举个栗子，一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。 其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。像 mybatis，hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 list，没有必要说每次查询部门，都把里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。 最初级的缓存不一致问题及解决方案 问题：先更新数据库，再删除缓存。如果删除缓存失败了，那么会导致数据库中是新数据，缓存中是旧数据，数据就出现了不一致。 解决思路：先删除缓存，再更新数据库。如果数据库更新失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致。因为读的时候缓存没有，所以去读了数据库中的旧数据，然后更新到缓存中。 比较复杂的数据不一致问题分析 数据发生了变更，先删除了缓存，然后要去修改数据库，此时还没修改。一个请求过来，去读缓存，发现缓存空了，去查询数据库，查到了修改前的旧数据，放到了缓存中。随后数据变更的程序完成了数据库的修改。完了，数据库和缓存中的数据不一样了... 为什么上亿流量高并发场景下，缓存会出现这个问题？ 只有在对一个数据在并发的进行读写的时候，才可能会出现这种问题。其实如果说你的并发量很低的话，特别是读并发很低，每天访问量就 1 万次，那么很少的情况下，会出现刚才描述的那种不一致的场景。但是问题是，如果每天的是上亿的流量，每秒并发读是几万，每秒只要有数据更新的请求，就可能会出现上述的数据库+缓存不一致的情况。 解决方案如下： 更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个 jvm 内部队列中。读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个 jvm 内部队列中。 一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行。这样的话，一个数据变更的操作，先删除缓存，然后再去更新数据库，但是还没完成更新。此时如果一个读请求过来，没有读到缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。 这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。 待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中。 如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回；如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值。 高并发的场景下，该解决方案要注意的问题： 读请求长时阻塞 由于读请求进行了非常轻度的异步化，所以一定要注意读超时的问题，每个读请求必须在超时时间范围内返回。 该解决方案，最大的风险点在于说，可能数据更新很频繁，导致队列中积压了大量更新操作在里面，然后读请求会发生大量的超时，最后导致大量的请求直接走数据库。务必通过一些模拟真实的测试，看看更新数据的频率是怎样的。 另外一点，因为一个队列中，可能会积压针对多个数据项的更新操作，因此需要根据自己的业务情况进行测试，可能需要部署多个服务，每个服务分摊一些数据的更新操作。如果一个内存队列里居然会挤压 100 个商品的库存修改操作，每个库存修改操作要耗费 10ms 去完成，那么最后一个商品的读请求，可能等待 10 * 100 = 1000ms = 1s 后，才能得到数据，这个时候就导致读请求的长时阻塞。 一定要做根据实际业务系统的运行情况，去进行一些压力测试，和模拟线上环境，去看看最繁忙的时候，内存队列可能会挤压多少更新操作，可能会导致最后一个更新操作对应的读请求，会 hang 多少时间，如果读请求在 200ms 返回，如果你计算过后，哪怕是最繁忙的时候，积压 10 个更新操作，最多等待 200ms，那还可以的。 如果一个内存队列中可能积压的更新操作特别多，那么你就要加机器，让每个机器上部署的服务实例处理更少的数据，那么每个内存队列中积压的更新操作就会越少。 其实根据之前的项目经验，一般来说，数据的写频率是很低的，因此实际上正常来说，在队列中积压的更新操作应该是很少的。像这种针对读高并发、读缓存架构的项目，一般来说写请求是非常少的，每秒的 QPS 能到几百就不错了。 我们来实际粗略测算一下。 如果一秒有 500 的写操作，如果分成 5 个时间片，每 200ms 就 100 个写操作，放到 20 个内存队列中，每个内存队列，可能就积压 5 个写操作。每个写操作性能测试后，一般是在 20ms 左右就完成，那么针对每个内存队列的数据的读请求，也就最多 hang 一会儿，200ms 以内肯定能返回了。 经过刚才简单的测算，我们知道，单机支撑的写 QPS 在几百是没问题的，如果写 QPS 扩大了 10 倍，那么就扩容机器，扩容 10 倍的机器，每个机器 20 个队列。 读请求并发量过高 这里还必须做好压力测试，确保恰巧碰上上述情况的时候，还有一个风险，就是突然间大量读请求会在几十毫秒的延时 hang 在服务上，看服务能不能扛的住，需要多少机器才能扛住最大的极限情况的峰值。 但是因为并不是所有的数据都在同一时间更新，缓存也不会同一时间失效，所以每次可能也就是少数数据的缓存失效了，然后那些数据对应的读请求过来，并发量应该也不会特别大。 多服务实例部署的请求路由 可能这个服务部署了多个实例，那么必须保证说，执行数据更新操作，以及执行缓存更新操作的请求，都通过 Nginx 服务器路由到相同的服务实例上。 比如说，对同一个商品的读写请求，全部路由到同一台机器上。可以自己去做服务间的按照某个请求参数的 hash 路由，也可以用 Nginx 的 hash 路由功能等等。 热点商品的路由问题，导致请求的倾斜 万一某个商品的读写请求特别高，全部打到相同的机器的相同的队列里面去了，可能会造成某台机器的压力过大。就是说，因为只有在商品数据更新的时候才会清空缓存，然后才会导致读写并发，所以其实要根据业务系统去看，如果更新频率不是太高的话，这个问题的影响并不是特别大，但是的确可能某些机器的负载会高一些。 Session、Cookie、Redis缓存对比 Redis 过期设置 EXPIRE key PERSIST key Redis 数据备份与恢复 SAVE 该命令将在 redis 安装目录中创建dump.rdb文件。 如果需要恢复数据，只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可。获取 redis 目录可以使用 CONFIG 命令 CONFIG GET dir 以上命令 CONFIG GET dir 输出的 redis 安装目录为 /usr/local/redis/bin。 Bgsave 创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行。 实例 127.0.0.1:6379&gt; BGSAVE Background saving started Redis 安全 通过 redis 的配置文件设置密码参数，这样客户端连接到 redis 服务就需要密码验证，这样可以让你的 redis 服务更安全。 查看是否设置了密码验证： CONFIG get requirepass默认情况下 requirepass 参数是空的，这就意味着你无需通过密码验证就可以连接到 redis 服务。 CONFIG set requirepass &quot;runoob&quot; AUTH &quot;runoob&quot; SET mykey &quot;Test value&quot; GET mykey Redis 性能测试 Redis 性能测试是通过同时执行多个命令实现的。 redis-benchmark [option] [option value] 注意：该命令是在 redis 的目录下执行的，而不是 redis 客户端的内部指令 以下实例同时执行 10000 个请求来检测性能： redis-benchmark -n 10000 -q - redis 性能测试工具可选参数如下所示： 序号 选项 描述 默认值 1 -h 指定服务器主机名 127.0.0.1 2 -p 指定服务器端口 6379 3 -s 指定服务器 socket 4 -c 指定并发连接数 50 5 -n 指定请求数 10000 6 -d 以字节的形式指定 SET/GET 值的数据大小 2 7 -k 1=keep alive 0=reconnect 1 8 -r SET/GET/INCR 使用随机 key, SADD 使用随机值 9 -P 通过管道传输 请求 1 10 -q 强制退出 redis。仅显示 query/sec 值 11 --csv 以 CSV 格式输出 12 -l 生成循环，永久执行测试 13 -t 仅运行以逗号分隔的测试命令列表。 14 -I Idle 模式。仅打开 N 个 idle 连接并等待。 - redis-benchmark -h 127.0.0.1 -p 6379 -t set,lpush -n 10000 -q Redis 客户端连接 Redis 通过监听一个 TCP 端口或者 Unix socket 的方式来接收来自客户端的连接，当一个连接建立后，Redis 内部会进行以下一些操作： 首先，客户端 socket 会被设置为非阻塞模式，因为 Redis 在网络事件处理上采用的是非阻塞多路复用模型。 然后为这个 socket 设置 TCP_NODELAY 属性，禁用 Nagle 算法 然后创建一个可读的文件事件用于监听这个客户端 socket 的数据发送 最大连接数 在 Redis2.4 中，最大连接数是被直接硬编码在代码里面的，而在2.6版本中这个值变成可配置的。 maxclients 的默认值是 10000，你也可以在 redis.conf 中对这个值进行修改。 config get maxclients redis-server --maxclients 100000 S.N. 命令 描述 1 CLIENT LIST 返回连接到 redis 服务的客户端列表 2 CLIENT SETNAME 设置当前连接的名称 3 CLIENT GETNAME 获取通过 CLIENT SETNAME 命令设置的服务名称 4 CLIENT PAUSE 挂起客户端连接，指定挂起的时间以毫秒计 5 CLIENT KILL 关闭客户端连接 Redis管道技术 Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。这意味着通常情况下一个请求会遵循以下步骤： 客户端向服务端发送一个查询请求，并监听套接字返回，通常以一对模式，等待服务端响应。 服务端处理命令，可以结果返回给客户端。 Redis管道技术可以在服务端未响应时，客户端可以继续向服务端发送请求，并最终一次性读取所有服务端的响应。 查看redis管道，只需要启动redis实例并输入以下命令： $（echo -en“ PING \\ r \\ n SET runoobkey redis \\ r \\ nGET runoobkey \\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ n”；睡眠10）| 数控本地主机6379（echo - zh - cn “ PING \\ r \\ n SET redoobkey redis \\ r \\ nGET runoobkey \\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ n” ；睡眠10 ）| 数控本地主机6379 以上实例中我们通过使用PING命令查看redis服务是否可用，之后我们设置了runoobkey的变量redis，然后我们获取runoobkey的值并使其访客自增3次。 在返回的结果中我们可以看到这些命令一次性向redis服务提交，并最终一次性读取所有服务端的响应 管道技术的优势 管道技术最显着的优势是提高了redis服务的性能。 Redis 分区 分区是分割数据到多个Redis实例的处理过程，因此每个实例只保存key的一个子集。 分区的优势 通过利用多台计算机内存的和值，允许我们构造更大的数据库。 通过多核和多台计算机，允许我们扩展计算能力；通过多台计算机和网络适配器，允许我们扩展网络带宽。 分区的不足 redis的一些特性在分区方面表现的不是很好： 涉及多个key的操作通常是不被支持的。举例来说，当两个set映射到不同的redis实例上时，你就不能对这两个set执行交集操作。 涉及多个key的redis事务不能使用。 当使用分区时，数据处理较为复杂，比如你需要处理多个rdb/aof文件，并且从多个实例和主机备份持久化文件。 增加或删除容量也比较复杂。redis集群大多数支持在运行时增加、删除节点的透明数据平衡的能力，但是类似于客户端分区、代理等其他系统则不支持这项特性。然而，一种叫做presharding的技术对此是有帮助的。 分区类型 Redis 有两种类型分区。 假设有4个Redis实例 R0，R1，R2，R3，和类似user:1，user:2这样的表示用户的多个key，对既定的key有多种不同方式来选择这个key存放在哪个实例中。也就是说，有不同的系统来映射某个key到某个Redis服务。 范围分区 最简单的分区方式是按范围分区，就是映射一定范围的对象到特定的Redis实例。 比如，ID从0到10000的用户会保存到实例R0，ID从10001到 20000的用户会保存到R1，以此类推。 这种方式是可行的，并且在实际中使用，不足就是要有一个区间范围到实例的映射表。这个表要被管理，同时还需要各 种对象的映射表，通常对Redis来说并非是好的方法。 哈希分区 另外一种分区方法是hash分区。这对任何key都适用，也无需是object_name:这种形式，像下面描述的一样简单： 用一个hash函数将key转换为一个数字，比如使用crc32 hash函数。对key foobar执行crc32(foobar)会输出类似93024922的整数。 对这个整数取模，将其转化为0-3之间的数字，就可以将这个整数映射到4个Redis实例中的一个了。93024922 % 4 = 2，就是说key foobar应该被存到R2实例中。注意：取模操作是取除的余数，通常在多种编程语言中用%操作符实现。 Redis 扩容 ​ 当做缓存使用，使用一致性哈希实现动态扩容缩容。 当做一个**持久化存储**使用，必须使用**固定的 keys-to-nodes 映射关系**，节点的数量一旦确定不能变化。否则的话(即 Redis 节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有 Redis 集群可以做到这样。 Java 使用 Redis jedis 安装 开始在 Java 中使用 Redis 前， 我们需要确保已经安装了 redis 服务及 Java redis 驱动，且你的机器上能正常使用 Java。 Java的安装配置可以参考我们的 Java 开发环境配置 接下来让我们安装 Java redis 驱动： 首先你需要下载驱动包 下载 jedis.jar，确保下载最新驱动包。 在你的 classpath 中包含该驱动包。 maven jedis 连接到 redis 服务 12345678910111213- import redis.clients.jedis.Jedis;public class RedisJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); // 如果 Redis 服务设置来密码，需要下面这行，没有就不需要 // jedis.auth(&quot;123456&quot;); System.out.println(&quot;连接成功&quot;); //查看服务是否运行 System.out.println(&quot;服务正在运行: &quot;+jedis.ping()); }} Redis Java String(字符串) 实例 12345678910111213- import redis.clients.jedis.Jedis;public class RedisStringJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); System.out.println(&quot;连接成功&quot;); //设置 redis 字符串数据 jedis.set(&quot;runoobkey&quot;, &quot;www.runoob.com&quot;); // 获取存储的数据并输出 System.out.println(&quot;redis 存储的字符串为: &quot;+ jedis.get(&quot;runoobkey&quot;)); }} Redis Java List(列表) 实例 12345678910111213141516171819- import java.util.List; import redis.clients.jedis.Jedis;public class RedisListJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); System.out.println(&quot;连接成功&quot;); //存储数据到列表中 jedis.lpush(&quot;site-list&quot;, &quot;Runoob&quot;); jedis.lpush(&quot;site-list&quot;, &quot;Google&quot;); jedis.lpush(&quot;site-list&quot;, &quot;Taobao&quot;); // 获取存储的数据并输出 List&lt;String&gt; list = jedis.lrange(&quot;site-list&quot;, 0 ,2); for(int i=0; i&lt;list.size(); i++) { System.out.println(&quot;列表项为: &quot;+list.get(i)); } }} Redis Java Keys 实例 1234567891011121314151617 - import java.util.Iterator; import java.util.Set; import redis.clients.jedis.Jedis;public class RedisKeyJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); System.out.println(&quot;连接成功&quot;); // 获取数据并输出 Set&lt;String&gt; keys = jedis.keys(&quot;*&quot;); Iterator&lt;String&gt; it=keys.iterator() ; while(it.hasNext()){ String key = it.next(); System.out.println(key); }}} Jedis事务 12345678910111213141516171819202122232425public static void main(String[] args) {Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379);jedis.flushDB();JSONObject jsonObject = new JSONObject();jsonObject.put(&quot;hello&quot;,&quot;world&quot;);jsonObject.put(&quot;name&quot;,&quot;xiaobai&quot;);// 开启事务Transaction multi = jedis.multi();String result = jsonObject.toJSONString();// jedis.watch(result)try {multi.set(&quot;user1&quot;,result);multi.set(&quot;user2&quot;,result);int i = 1/0 ; // 代码抛出异常事务，执行失败！multi.exec(); // 执行事务！} catch (Exception e) {multi.discard(); // 放弃事务e.printStackTrace();} finally {System.out.println(jedis.get(&quot;user1&quot;));System.out.println(jedis.get(&quot;user2&quot;));jedis.close(); // 关闭连接}} SpringBoot整合Redis SpringBoot 操作数据：spring-data jpa jdbc mongodb redis！ SpringData 也是和 SpringBoot 齐名的项目！ 说明： 在 SpringBoot2.x 之后，原来使用的jedis 被替换为了 lettuce? jedis : 采用的直连，多个线程操作的话，是不安全的，如果想要避免不安全的，使用 jedis pool 连接池！ 更像 BIO 模式 lettuce : 采用netty，实例可以再多个线程中进行共享，不存在线程不安全的情况！可以减少线程数据了，更像 NIO 模式 源码分析： 12345678910111213141516171819202122@Bean@ConditionalOnMissingBean(name = &quot;redisTemplate&quot;) // 我们可以自己定义一个redisTemplate来替换这个默认的！public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactoryredisConnectionFactory)throws UnknownHostException {// 默认的 RedisTemplate 没有过多的设置，redis 对象都是需要序列化！// 两个泛型都是 Object, Object 的类型，我们后使用需要强制转换 &lt;String, Object&gt;RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;();template.setConnectionFactory(redisConnectionFactory);return template;}@Bean@ConditionalOnMissingBean // 由于 String 是redis中最常使用的类型，所以说单独提出来了一个bean！public StringRedisTemplate stringRedisTemplate(RedisConnectionFactoryredisConnectionFactory)throws UnknownHostException {StringRedisTemplate template = new StringRedisTemplate();template.setConnectionFactory(redisConnectionFactory);return template;} 1234567891011121314151617181920212223242526272829303132333435361、导入依赖&lt;!-- 操作redis --&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;2、配置连接# 配置redisspring.redis.host=127.0.0.1spring.redis.port=63793、测试！@SpringBootTestclass Redis02SpringbootApplicationTests {@Autowiredprivate RedisTemplate redisTemplate;@Testvoid contextLoads() {// redisTemplate 操作不同的数据类型，api和我们的指令是一样的// opsForValue 操作字符串 类似String// opsForList 操作List 类似List// opsForSet// opsForHash// opsForZSet// opsForGeo// opsForHyperLogLog// 除了进本的操作，我们常用的方法都可以直接通过redisTemplate操作，比如事务，和基本的CRUD// 获取redis的连接对象// RedisConnection connection =redisTemplate.getConnectionFactory().getConnection();// connection.flushDb();// connection.flushAll();redisTemplate.opsForValue().set(&quot;mykey&quot;,&quot;关注狂神说公众号&quot;);System.out.println(redisTemplate.opsForValue().get(&quot;mykey&quot;));}} 应用场景 计算器、限速器、好友关系 缓存——提升热点数据的访问速度 共享数据——数据的存储和共享的问题 全局 ID —— 分布式全局 ID 的生成方案（分库分表） 分布式锁——进程间共享数据的原子操作保证 在线用户统计和计数 —— 使用位图进行位运算 队列、栈——跨进程的队列/栈 消息队列——异步解耦的消息机制 服务注册与发现 —— RPC 通信机制的服务协调中心（Dubbo 支持 Redis） 共享用户 Session —— 用户Session的更新和获取都可以快速完成 排行榜—— 通过分配元素 score 值进行排序 （1）、会话缓存（Session Cache） 最常用的一种使用 Redis 的情景是会话缓存（session cache）。用 Redis 缓存会话比其他 存储（如 Memcached）的优势在于：Redis 提供持久化。当维护一个不是严格要求一致性 的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样 吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用 Redis 来缓存会话的文 档。甚至广为人知的商业平台 Magento 也提供 Redis 的插件。 （2）、全页缓存（FPC） 除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题，即使重启 了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极 大改进，类似 PHP 本地 FPC。 再次以 Magento 为例，Magento 提供一个插件来使用 Redis 作为全页缓存后端。 此外，对 WordPress 的用户来说，Pantheon 有一个非常好的插件 wp-Redis，这个插件 能帮助你以最快速度加载你曾浏览过的页面。 （3）、队列 Reids 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个 很好的消息队列平台来使用。Redis 作为队列使用的操作，就类似于本地程序语言（如 Python）对 list 的 push/pop 操作。 如果你快速的在 Google 中搜索“Redis queues”，你马上就能找到大量的开源项目，这些 项目的目的就是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一个后台就是使用 Redis 作为 broker，你可以从这里去查看。 （4）、排行榜/计数器 Redis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis 只是正好提供了这两种数据 结构。所以，我们要从排序集合中获取到排名最靠前的 10 个用户–我们称之为 “user_scores”，我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你 关注微信公众号：牛牛架构师，回复：面试题，获取最新版面试题 需要这样执行： ZRANGE user_scores 0 10 WITHSCORES Agora Games 就是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储 数据的，你可以在这里看到。 （5）、发布/订阅 最后（但肯定不是最不重要的）是 Redis 的发布/订阅功能。发布/订阅的使用场景确实非 常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至 用 Redis 的发布/订阅功能来建立聊天系统！（不，这是真的，你可以去核实）","link":"/2021/03/01/Draft/2021/Redis/"},{"title":"快查","text":"Linux,Shell,软件等常用命令快捷键 技术官方文档 JAVA Spring全家桶 MYSQL JAVA 1234567891011121314151617181920212223242526272829303132333435java -选项其中选项包括: -d32 使用 32 位数据模型 (如果可用) -d64 使用 64 位数据模型 (如果可用) -server 选择 &quot;server&quot; VM 默认 VM 是 server. -cp &lt;目录和 zip/jar 文件的类搜索路径&gt; -classpath &lt;目录和 zip/jar 文件的类搜索路径&gt; 用 ; 分隔的目录, JAR 档案 和 ZIP 档案列表, 用于搜索类文件。 -D&lt;名称&gt;=&lt;值&gt; 设置系统属性 -verbose:[class|gc|jni] 启用详细输出 -version 输出产品版本并退出 -showversion 输出产品版本并继续 -ea[:&lt;packagename&gt;...|:&lt;classname&gt;] -enableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 按指定的粒度启用断言 -da[:&lt;packagename&gt;...|:&lt;classname&gt;] -disableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 禁用具有指定粒度的断言 -esa | -enablesystemassertions 启用系统断言 -dsa | -disablesystemassertions 禁用系统断言 -agentlib:&lt;libname&gt;[=&lt;选项&gt;] 加载本机代理库 &lt;libname&gt;, 例如 -agentlib:hprof 另请参阅 -agentlib:jdwp=help 和 -agentlib:hprof=help -agentpath:&lt;pathname&gt;[=&lt;选项&gt;] 按完整路径名加载本机代理库 -javaagent:&lt;jarpath&gt;[=&lt;选项&gt;] 加载 Java 编程语言代理, 请参阅 java.lang.instrument -splash:&lt;imagepath&gt; 使用指定的图像显示启动屏幕有关详细信息, 请参阅 http://www.oracle.com/technetwork/java/javase/documentation/index.html。 常用 123#查看JDK安装路径 结果第一行java -verbose#[Opened C:\\Program Files\\Java\\jdk1.8.0_181\\jre\\lib\\rt.jar] CMD 电脑信息 1234#查看电脑信息systeminfo#查看开机启动文件夹shell:common startup 文件信息 12#查看文件树不加/f为查看文件目录树tree /f cmd窗口点击内容会暂停，右键属性关闭快速编辑即可，但是关闭后无法复制cmd内容。 网络 123456789101112131415161718#查找对应端口pidnetstat -aon|findstr &quot;59207&quot;#查看对应pid程序tasklist|findstr &quot;1396&quot;#关闭对应端口taskkill /t /f /pid &quot;8888&quot;#启动远程桌面mstsc #策略组gpedit.msc#给同局域网发送消息弹窗msg /server:126.11.9.213 * 消息 #查看域名IPnslookup 域名#查看端口是否互通，需要控制面板打开 telnet功能telnet ip 端口#查看主板支持最大内存wmic memphysical get maxcapacity 网络切换 12345678@ echo off%1 %2ver|find &quot;5.&quot;&gt;nul&amp;&amp;goto :Adminmshta vbscript:createobject(&quot;shell.application&quot;).shellexecute(&quot;%~s0&quot;,&quot;goto :Admin&quot;,&quot;&quot;,&quot;runas&quot;,1)(window.close)&amp;goto :eof:Adminnetsh interface set interface &quot;WLAN&quot; enablednetsh interface set interface &quot;INSEIDE&quot; disabled windows快捷弹窗 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#计算器 calc#剪贴簿查看器 clipbrd#设备管理器 devmgmt.msc#显示属性 control desktop (desk.cpl)#Internet属性 inetcpl.cpl#IP配置实用程序(显示连接配置) ipconfig /all#IP配置实用程序(显示DNS缓存内容) ipconfig /displaydns#IP配置实用程序(删除DNS缓存内容) ipconfig /flushdns#IP配置实用程序(释放全部(或指定)适配器的由DHCP分配的动态IP地址) ipconfig /release#IP配置实用程序(为全部适配器重新分配IP地址) ipconfig /renew#IP配置实用程序(刷新DHCP并重新注册DNS) ipconfig /registerdns#IP配置实用程序(显示DHCP Class ID) ipconfig /showclassid#IP配置实用程序(修改DHCP Class ID) ipconfig /setclassid#Java控制面板(如果已经安装) jpicpl32.cpl#本地安全设置 secpol.msc#从Windows注销 logoff#网络连接 control netconnections (ncpa.cpl)#记事本 notepad#远程桌面 mstsc#Windows安全中心 wscui.cpl#服务 services.msc#共享文件夹 fsmgmt.msc#声音和音频设备属性 mmsys.cpl#系统配置实用程序 msconfig#任务管理器 taskmgr HEXO Hexo c 清理 Hexo g编译 Hexo s运行 Hexo d部署 hexo new draft（scaffolds中模板名字）“标题”新建草稿 在source/_drafts目录 hexo new “标题”新建文档 hexo publish “标题”草稿移动到source/_post目录 IDEA ctrl +R 替换 ctrl +shift + 数字标记文件内位置 ctrl+数字回到对应位置 双击shift全局搜索 ctrl + shift +F 全局文字搜索 ctrl + shift +y 搜索插件翻译选中文字 ctrl + shift +o codota搜索所选代码示例 不选中代码时显示翻译输入框 alt + F7 快速匹配项目中所有此用法 Ctrl+Alt+Shift+U UML图 Shift+Alt 选中文件汇总所有当前字符 Ctrl+Shift+E 可获取最近查看或更改的代码段的列表 自定义模板后，方法名上/* 然后tab 添加注释 WINDOWS alt + tab 两应用之间切换 ctrl+w关闭当前窗口 服务自动重启 12345678910111213141516171819202122#nginxhttps://www.jb51.net/article/211867.htm#Redisredis-server --service-install redis.windows-service.conf --loglevel verbose#tomcat1、进入到tomcat\\bin目录下，打开cmd命令窗口2、输入 service install tomcat服务名(服务名称自定义）在运行窗口输入：net start 服务名称这样tomcat服务就启动了做成开机主启动在我的电脑右击--&gt;管理--&gt;服务和应用程序--&gt;选择服务或在cmd命令窗口输入：service.msc找到Apache Tomcat XXXX [服务名称]将手动修改成自动后就可以开机自启动了。如果要删除，和修改服务！在cmd命令窗口输入：删除：sc delete 服务名修改：sc config 服务名 [参数] LINUX yum install net-tools 安装网络工具 linux可直接复制给别人，复制VMware工作目录然后直接打开 su 回车输入密码转换为超级管理员 ifconfig查询网络相关 clear清空显示 ifconfig eth0 192.16.。。。设置IP地址 命令多数为临时生效，写入配置文件为永久生效 ls [-选项][参数]：ls -la /etc 目录处理命令（list） ls -a/l/lh/ld/i 隐藏文件/文件详细信息/大小单位显示/查看目录信息/查看文件id d l 文件开头 文件、目录、软连接 权限 mkdir【make directories】 创建目录 -p递归创建 创建没有此目录下的目录，可同时创建多个mkdir /tmp/a/b /tmp/a/c cd 【change directory】切换目录 cd ..返回上一级目录 pwd【print working directory】 cd ./.. 当前目录/上级目录 remdir【remove empty dirctories】 删除空目录 cp【copy】复制 源文件 目录 -r复制目录 -p复制并保留文件属性（比如创建时间） 复制并改名 mv【move】 剪切 rm【remove】删除文件，目录 -r删除目录 -f强制删除 -rf直接删除 ctrl+c终止操作 touch 创建文件 可创建多个，创建空格文件名使用双引号但是不建议 cat 浏览文件内容 -n显示行号 tac 倒置浏览文件内容 more 分页显示长文件 空格翻页，回车换行，q退出， less 显示文件内容，可向上翻页pageup，可搜索：/关键词 n显示下一个搜索结果 head 只看文件前几行 -n 7 文件 显示前7行，默认前十行 tail 同上 -f动态显示变化 ctrl+c退出 ln【link】 -s创建软连接 不写-s创建硬链接【软连接相当于快捷方式rwxrwxrwx，权限跟源文件权限无关，硬链接相当于拷贝cp -p并且同步更新，硬链接通过i节点区分，不能跨分区（相当于不能c盘复制到d盘），不能针对目录使用】 yum -y install git sudo -s1获得管理员权限 命令：sudo -s回车 输入密码 编辑保存文件----------------------------------- 第一步：cd到该文件的目录下 第二步：vi 要编辑的文件名，进入普通模式，（可以查看文件内容） sudo gedit /etc/apt/sources.list 可视化编辑 第三步：输入 i 进入编辑模式，开始编辑文本 第四步：编辑之后，按ESC退出到普通模式。 第五步：在普通模式下，输入 : 进入命令模式 第六步：在命令模式下输入wq, 即可保存并退出 【#】代表 root权限 【$】代表普通用户 reboot重启 BUG————————————————————————————– centos7 cannot find a valid baseurl for repo············· https://blog.csdn.net/jiankunking/article/details/82770502 zlib.h: No such file or directory························ yum install zlib-devel CentOS Name or service not known·································· vi /etc/sysconfig/network-scripts/ifcfg-ens33命令来编辑配置文件 Job for network.service failed because the control process exited with error code········································································ 关闭 NetworkManger 服务就好了， service NetworkManager stop 禁止开机启动 chkconfig NetworkManager off su: Authentication failure····················································· sudo passwd root 网络————————————————————————————— 网络不行都用桥接 1systemctl restart network //重启网卡 ====ifup eth0（网卡名称） ip addr查询网络信息 postgresql——————————————————————————- sudo -u postgres psql 进入psql交互环境 alter user postgres with password'密码'; 修改postgres用户的密码 \\q 退出数据库 ? \\password：设置密码 \\h：查看SQL命令的解释，比如\\h select。 ?：查看psql命令列表。 \\l：列出所有数据库。 \\c [database_name]：连接其他数据库。 \\d：列出当前数据库的所有对象，如Table,View等。 \\dt: 列出当前数据库的所有Table。 \\d [table_name]：列出某一张表格的结构。 \\du：列出所有用户。 \\e：打开文本编辑器。 \\conninfo：列出当前数据库和连接的信息。 df 命令 来检查当前磁盘利用率 SecureCRT============================== alt+p进入sftp put -r 路径传目录 shp导入到postgresql——————————————————————— shp2pgsql -s 4544 -c -W &quot;UTF-8&quot; ADDRESS.shp public.rrrrr| psql -h 192.168.22.128 -d postgres -U postgres -W 转换格式——————————————————————— ogr2ogr -f &quot;GeoJSON&quot; ./natural.json PG:&quot;host=localhost dbname=postgres user=postgres password=724111&quot; -sql &quot;select * from natural&quot; 切片—————————————————————————————— tippecanoe -z 14 -Z 5 -ps -Bg -o buildings.mbtiles buildings.json tippecanoe -e lakepbf -pC -Z1 -z17 -f natural.json 各种常用软件环境安装 NPM npm install ---install dependencies npm run dev---serve with hot reload at localhost:8080 npm run build---build for production with minification npm run build --report----build for production and view the bundle analyzer report npm run unit---run unit tests npm run e2e---run e2e tests npm test----run all tests REDIDS ./redis-server redis.windows.conf 启动服务 server.exe --service-install redis.windows.conf --loglevel verbose 开机启动 windows redis-server --service-install redis.windows-service.conf --loglevel verbose VSCODE ctrl+~ ：打开终端 F1 或 Ctrl+Shift+P（俗称万能键） ：打开命令面板。在打开的输入框内，可以输入任何命令 代码格式化: Shift+Alt+F VUE 脚手架 npm config set registry https://registry.npm.taobao.org vue init webpack projectName cd projectName npm install npm run dev npm i element-ui -S npm install axios GIT Git常用命令及方法大全 下面是我整理的常用 Git 命令清单。几个专用名词的译名如下。 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 本地分支关联远程 1git branch --set-upstream-to=origin/分支名 分支名 代码库修改密码后push不上去怎么办？ 1234// 重新输入密码git config --system --unset credential.helper// 密码存储同步git config --global credential.helper store 一、新建代码库 123456# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url] 二、配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 1234567# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name &quot;[name]&quot;$ git config [--global] user.email &quot;[email address]&quot; 三、增加/删除文件 12345678910111213141516# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交 $ git add -p# 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file]# 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 四、代码提交 12345678910111213# 提交暂存区到仓库区$ git commit -m [message]# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a# 提交时显示所有diff信息$ git commit -v # 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 五、分支 1234567891011121314151617181920212223242526272829# 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name] # 以远程分支为基础新建一个分支，并切换到该分支$ git checkout -b [branch] origin/[remote-branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 六、标签 123456789101112131415161718# 列出所有tag$ git tag# 新建一个tag在当前commit$ git tag [tag]# 新建一个tag在指定commit$ git tag [tag] [commit]# 删除本地tag$ git tag -d [tag] # 删除远程tag$ git push origin :refs/tags/[tagName]# 查看tag信息$ git show [tag]# 提交指定tag$ git push [remote] [tag]# 提交所有tag$ git push [remote] --tags# 新建一个分支，指向某个tag$ git checkout -b [branch] [tag] 七、查看信息 1234567891011121314151617181920212223242526272829303132333435363738394041# 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码 $ git diff --shortstat &quot;@{0 day ago}&quot;# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog 八、远程同步 12345678910111213141516# 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all 九、撤销 123456789101112131415161718192021# 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash $ git stash pop 十、其他 12# 生成一个可供发布的压缩包$ git archive Git分支管理策略 一、主分支Master 首先，代码库应该有一个、且仅有一个主分支。所有提供给用户使用的正式版本，都在这个主分支上发布。 Git主分支的名字，默认叫做Master。它是自动建立的，版本库初始化以后，默认就是在主分支在进行开发。 二、开发分支Develop 主分支只用来分布重大版本，日常开发应该在另一条分支上完成。我们把开发用的分支，叫做Develop。 这个分支可以用来生成代码的最新隔夜版本（nightly）。如果想正式对外发布，就在Master分支上，对Develop分支进行&quot;合并&quot;（merge）。 Git创建Develop分支的命令： git checkout -b develop master 将Develop分支发布到Master分支的命令： # 切换到Master分支 git checkout master # 对Develop分支进行合并 git merge --no-ff develop 这里稍微解释一下，上一条命令的--no-ff参数是什么意思。默认情况下，Git执行&quot;快进式合并&quot;（fast-farward merge），会直接将Master分支指向Develop分支。 使用--no-ff参数后，会执行正常合并，在Master分支上生成一个新节点。为了保证版本演进的清晰，我们希望采用这种做法。关于合并的更多解释，请参考Benjamin Sandofsky的《Understanding the Git Workflow》。 三、临时性分支 前面讲到版本库的两条主要分支：Master和Develop。前者用于正式发布，后者用于日常开发。其实，常设分支只需要这两条就够了，不需要其他了。 但是，除了常设分支以外，还有一些临时性分支，用于应对一些特定目的的版本开发。临时性分支主要有三种： * 功能（feature）分支 * 预发布（release）分支 * 修补bug（fixbug）分支 这三种分支都属于临时性需要，使用完以后，应该删除，使得代码库的常设分支始终只有Master和Develop。 四、 功能分支 接下来，一个个来看这三种&quot;临时性分支&quot;。 第一种是功能分支，它是为了开发某种特定功能，从Develop分支上面分出来的。开发完成后，要再并入Develop。 功能分支的名字，可以采用feature-*的形式命名。 创建一个功能分支： git checkout -b feature-x develop 开发完成后，将功能分支合并到develop分支： git checkout develop git merge --no-ff feature-x 删除feature分支： git branch -d feature-x 五、预发布分支 第二种是预发布分支，它是指发布正式版本之前（即合并到Master分支之前），我们可能需要有一个预发布的版本进行测试。 预发布分支是从Develop分支上面分出来的，预发布结束以后，必须合并进Develop和Master分支。它的命名，可以采用release-*的形式。 创建一个预发布分支： git checkout -b release-1.2 develop 确认没有问题后，合并到master分支： git checkout master git merge --no-ff release-1.2 # 对合并生成的新节点，做一个标签 git tag -a 1.2 再合并到develop分支： git checkout develop git merge --no-ff release-1.2 最后，删除预发布分支： git branch -d release-1.2 六、修补bug分支 最后一种是修补bug分支。软件正式发布以后，难免会出现bug。这时就需要创建一个分支，进行bug修补。 修补bug分支是从Master分支上面分出来的。修补结束以后，再合并进Master和Develop分支。它的命名，可以采用fixbug-*的形式。 创建一个修补bug分支： git checkout -b fixbug-0.1 master 修补结束后，合并到master分支： git checkout master git merge --no-ff fixbug-0.1 git tag -a 0.1.1 再合并到develop分支： git checkout develop git merge --no-ff fixbug-0.1 最后，删除&quot;修补bug分支&quot;： git branch -d fixbug-0.1 版本回退-撤销文件修改 工作区修改一个文件后，又想回到修改前(git add前) \\1. 当然可以直接手动再在工作区中将文件修改回去 \\2. 修改后，通过命令git status查看 123456789101112131415161718192021222324252627282930313233$ git status# On branch master# Changes not staged for commit:# (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)# (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)## modified: readme.txt#no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 这时Git会告诉你，git checkout -- file可以丢弃工作区的修改： 1$ git checkout -- readme.txt Note: \\1. git checkout -- file命令中的--很重要，没有--，就变成了“切换到另一个分支”的命令，我们在后面的分支管理中会再次遇到git checkout命令。 \\2. 命令git checkout -- readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 \\3. 工作区、暂存区的概念不清楚的可见于Git版本控制教程 - Git本地仓库 如果在工作区中修改了文件还git add到暂存区（但是在commit之前） 用git status查看一下，修改只是添加到了暂存区，还没有提交： 12345678910111213141516171819202122232425$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)## modified: readme.txt# Git同样告诉我们，用命令git reset HEAD file可以把暂存区的修改撤销掉（unstage），重新放回工作区： 123456789$ git reset HEAD readme.txtUnstaged changes after reset:M readme.txt git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。 再用git status查看一下，现在暂存区是干净的，工作区有修改。 然后丢弃工作区的修改 1234567891011121314151617$ git checkout -- readme.txt $ git status# On branch masternothing to commit (working directory clean) 不但修改了文件还从暂存区提交commit到了版本库 - 版本回退 版本回退可以回退到上一个版本。不过，这是有条件的，就是你还没有把自己的本地版本库推送到远程。Git是分布式版本控制系统。 在工作中对某个文件（如readme.txt）进行多次修改交commit。 可以通过版本控制系统命令告诉我们提交的历史记录，在Git中，我们用git log命令查看： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869$ git logcommit 3628164fb26d48395383f8f31179f24e0882e1e0Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Tue Aug 20 15:11:49 2013 +0800 append GPL commit ea34578d5496d7dd233c827ed32a8cd576c5ee85Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file Note: \\1. git log命令显示从最近到最远的提交日志，我们可以看到3次提交，最近的一次是append GPL，上一次是add distributed，最早的一次是wrote a readme file。 \\2. 如果嫌输出信息太多，看得眼花缭乱的，可以试试加上--pretty=oneline参数： 12345678910111213$ git log --pretty=oneline3628164fb26d48395383f8f31179f24e0882e1e0 append GPLea34578d5496d7dd233c827ed32a8cd576c5ee85 add distributedcb926e7ea50ad11b8f9e909c05226233bf755030 wrote a readme file \\3. 你看到的一大串类似3628164...882e1e0的是commit id（版本号），和SVN不一样，Git的commit id不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示，而且你看到的commit id和我的肯定不一样，以你自己的为准。为什么commit id需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。 \\4. 每提交一个新版本，实际上Git就会把它们自动串成一条时间线。如果使用可视化工具（如GitX、github的客户端、pycharm）查看Git历史，就可以更清楚地看到提交历史的时间线。 现在我们想要把readme.txt回退到上一个版本 如“add distributed”的那个版本，怎么做呢？首先，Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交3628164...882e1e0（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD，上上一个版本就是HEAD，当然往上100个版本写100个比较容易数不过来，所以写成HEAD~100。 现在，我们要把当前版本“append GPL”回退到上一个版本“add distributed”，就可以使用git reset命令： 12345$ git reset --hard HEAD^HEAD is now at ea34578 add distributed 这时readme.txt的内容就成了版本add distributed 我们用git log再看看现在版本库的状态： 123456789101112131415161718192021222324252627282930313233343536373839404142434445$ git logcommit ea34578d5496d7dd233c827ed32a8cd576c5ee85Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file 最新的那个版本append GPL已经看不到了！ 恢复文件后，要是我们又想回到修改后的文件呢？（命令行窗口还没有被关掉） 只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个append GPL的commit id是3628164...，于是就可以指定回到未来的某个版本： 12345$ git reset --hard 3628164HEAD is now at 3628164 append GPL 版本号没必要写全，前几位就可以了，Git会自动去找。 Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL： 改为指向add distributed： 然后顺便把工作区的文件更新了。所以你让HEAD指向哪个版本号，你就把当前版本定位在哪。 恢复文件后，要是我们又想回到修改后的文件呢？（命令行窗口早就关掉了） 想恢复到新版本怎么办？找不到新版本的commit id怎么办？当你用$ git reset --hard HEAD^回退到add distributed版本时，再想恢复到append GPL，就必须找到append GPL的commit id。 Git提供了一个命令git reflog用来记录你的每一次命令：[Git高级教程:git log与git reflog] 1234567891011121314151617$ git reflogea34578 HEAD@{0}: reset: moving to HEAD^3628164 HEAD@{1}: commit: append GPLea34578 HEAD@{2}: commit: add distributedcb926e7 HEAD@{3}: commit (initial): wrote a readme file 第二行显示append GPL的commit id是3628164，现在，你又可以乘坐时光机回到未来了。","link":"/2021/02/25/Draft/2021/%E5%BF%AB%E6%9F%A5/"},{"title":"MYSQL优化","text":"MySql知识及优化 相关资源：优化视频、官方文档、基础视频 MySQL基础 为什么要使用数据库 内存 优： 存取速度快；缺： 数据不能永久保存 文件 优： 数据永久保存；缺：1）速度比内存操作慢，频繁的IO操作 2）查询不方便 数据库 1）数据永久保存 2）使用SQL语句，查询方便效率高。3）管理数据方便 什么是SQL？ 结构化查询语言(Structured Query Language)，用于存取数据、查询、更新和管理关系数据库系统的数据库查询语言 什么是MySQL? 关系型数据库管理系统 数据库三大范式是什么 第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 mysql有关权限的表都有哪几个 MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容： user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。 db权限表：记录各个帐号在各个数据库上的操作权限。 table_priv权限表：记录数据表级的操作权限。 columns_priv权限表：记录数据列级的操作权限。 host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。 MySQL的binlog有有几种录入格式？分别有什么区别？ 有三种格式，statement，row和mixed。 statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。 row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。 mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。 执行顺序 ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png) CREATE（drop，use） DATABASE 数据库名; CREATE TABLE IF NOT EXISTS runoob_tbl( runoob_id INT UNSIGNED AUTO_INCREMENT, runoob_title VARCHAR(100) NOT NULL, runoob_author VARCHAR(40) NOT NULL, submission_date DATE, PRIMARY KEY ( runoob_id ) )ENGINE=InnoDB DEFAULT CHARSET=utf8; 增删改查 • INSERT INTO table_name ( field1, field2,...fieldN ) VALUES ( value1, value2,...valueN )， ( value11, value22,...valueNN ); • SELECT column_name,column_name FROM table_name [WHERE condition1 [AND [OR]] condition2 field1 LIKE condition1 [AND [OR]] filed2 = 'somevalue'][LIMIT N][ OFFSET M] 查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分割，并使用WHERE语句来设定查询条件。 SELECT 命令可以读取一条或者多条记录。 你可以使用星号（*）来代替其他字段，SELECT语句会返回表的所有字段数据 你可以使用 WHERE 语句来包含任何条件，使用 AND 或者 OR 指定一个或多个条件。 你可以使用LIKE子句代替等号 =。- 代表单个，% 表示任意 0 个或多个字符。可匹配任意类型和长度的字符，有些情况下若是中文，请使用两个百分号（%%）表示。[]：表示括号内所列字符中的一个（类似正则表达式）。指定一个字符、字符串或范围，要求所匹配对象为它们中的任一个。[^] ：表示不在括号所列之内的单个字符。其取值和 [] 相同，但它要求所匹配对象为指定字符以外的任一个字符。 查询内容包含通配符时,由于通配符的缘故，导致我们查询特殊字符 “%”、“_”、“[” 的语句无法正常实现，而把特殊字符用 “[ ]” 括起便可正常查询。 你可以使用 LIMIT 属性来设定返回的记录数。 你可以通过OFFSET指定SELECT语句开始查询的数据偏移量。默认情况下偏移量为0。 • SELECT expression1, expression2, ... expression_n FROM tables [WHERE conditions] UNION [ALL | DISTINCT] SELECT expression1, expression2, ... expression_n FROM tables [WHERE conditions]; UNION 操作符用于连接两个以上的 SELECT 语句的结果组合到一个结果集合中。多个 SELECT 语句会删除重复的数据。 expression1, expression2, ... expression_n: 要检索的列。 tables: 要检索的数据表。 WHERE conditions: 可选， 检索条件。 DISTINCT: 可选，删除结果集中重复的数据。默认情况下 UNION 操作符已经删除了重复数据，所以 DISTINCT 修饰符对结果没啥影响。 ALL: 可选，返回所有结果集，包含重复数据。 • SELECT field1, field2,...fieldN FROM table_name1, table_name2...ORDER BY field1 [ASC [DESC][默认 ASC]], [field2...] [ASC [DESC][默认 ASC]] • SELECT column_name, function(column_name) FROM table_name WHERE column_name operator value GROUP BY column_name; • 根据一个或多个列对结果集进行分组。 在分组的列上我们可以使用 COUNT, SUM, AVG,等函数。 WITH ROLLUP 可以实现在分组统计数据基础上再进行相同的统计（SUM,AVG,COUNT…） • 使用聚合函数后需要group by分组，having只能用于group by 后面 • INNER JOIN（内连接,或等值连接）【交集】：获取两个表中字段匹配关系的记录。 LEFT JOIN（左连接）【左交】：获取左表所有记录，即使右表没有对应匹配的记录。 RIGHT JOIN（右连接）【左交】： 与 LEFT JOIN 相反，用于获取右表所有记录，即使左表没有对应匹配的记录。 • select p.id,p.name,t.content from person p left join task t on p.id=t.person_id order by p.id • UPDATE table_name SET field1=new-value1, field2=new-value2 [WHERE Clause] 你可以同时更新一个或多个字段。 你可以在 WHERE 子句中指定任何条件。 你可以在一个单独表中同时更新数据。 • update titles_test set emp_no=replace(emp_no,'10001','10005') where id=5 • DELETE FROM table_name [WHERE Clause] 如果没有指定 WHERE 子句，MySQL 表中的所有记录将被删除。 你可以在 WHERE 子句中指定任何条件 您可以在单个表中一次性删除记录。 DROP TABLE table_name ; ALTER TABLE table_name DROP i; ADD i INT ; 删除，添加或修改表字段 MODIFY c CHAR(10) ; CHANGE i j BIGINT; 修改字段类型及名称 ALTER i SET DEFAULT 1000; ALTER i DROP DEFAULT; 修改,删除字段的默认值 ALTER TABLE testalter_tbl RENAME TO alter_tbl;改表名 · 表中只剩余一个字段则无法使用DROP来删除字段 SQL语句主要分为哪几类 **数据定义语言 DDL（Data Ddefinition Language）**CREATE，DROP，ALTER 对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言 DQL（Data Query Language）SELECT 查询操作，以 select 关键字。简单查询，连接查询等 都属于 DQL。 **数据操纵语言 DML（Data Manipulation Language）**INSERT，UPDATE，DELETE 数据操作，对应上面所说的查询操作 DQL 与 DML 共同构建了多数初级程序员常用的增删改查操作。查询较为特殊划分到 DQL 中。 **数据控制功能 DCL（Data Control Language）**GRANT，REVOKE，COMMIT，ROLLBACK 对数据库安全性完整性等有操作的，权限控制等 DQMC 超键、候选键、主键、外键分别是什么？ 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 SQL 约束有哪几种？ NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK: 用于控制字段的值范围。 六种关联查询 交叉连接（CROSS JOIN） 内连接（INNER JOIN） 外连接（LEFT JOIN/RIGHT JOIN） 联合查询（UNION 与 UNION ALL） 全连接（FULL JOIN） · 交叉连接（CROSS JOIN） SELECT * FROM A,B(,C) 或者 SELECT * FROM A CROSS JOIN B (CROSS JOIN C) #没有任何关联条件，结果是笛卡尔积，结果集会很大，没有意义，很少使用内连接（INNER JOIN）SELECT * FROM A,B WHERE A.id=B.id或者SELECT * FROM A INNER JOIN B ON A.id=B.id多表中同时符合某种条件的数据记录的集合，INNER JOIN可以缩写为JOIN · 内连接分为三类 等值连接：ON A.id=B.id 不等值连接：ON A.id &gt; B.id 自连接：SELECT * FROM A T1 INNER JOIN A T2 ON T1.id=T2.pid · 外连接（LEFT JOIN/RIGHT JOIN） 左外连接：LEFT OUTER JOIN, 以左表为主，先查询出左表，按照 ON 后的关联条件匹配右表，没有匹配到的用 NULL 填充，可以简写成 LEFT JOIN 右外连接：RIGHT OUTER JOIN, 以右表为主，先查询出右表，按照 ON 后的关联条件匹配左表，没有匹配到的用 NULL 填充，可以简写成 RIGHT JOIN · 联合查询（UNION 与 UNION ALL） SELECT * FROM A UNION SELECT * FROM B UNION ... 就是把多个结果集集中在一起，UNION 前的结果为基准，需要注意的是联合查询的列数要相等，相同的记录行会合并 如果使用 UNION ALL，不会合并重复的记录行 效率 UNION 高于 UNION ALL · 全连接（FULL JOIN） MySQL 不支持全连接 可以使用 LEFT JOIN 和 UNION 和 RIGHT JOIN 联合使用 SELECT * FROM A LEFT JOIN B ON A.id=B.id UNIONSELECT * FROM A RIGHT JOIN B ON A.id=B.id 什么是子查询 条件：一条 SQL 语句的查询结果做为另一条查询语句的条件或查询结果 嵌套：多条 SQL 语句嵌套使用，内部的 SQL 查询语句称为子查询。 子查询的三种情况 子查询是单行单列的情况：结果集是一个值，父查询使用：=、 &lt;、&gt; 等运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行单列的情况：结果集类似于一个数组，父查询使用：in 运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行多列的情况：结果集类似于一张虚拟表，不能用于 where 条件，用于 select 子句中做为子表 -- 1) 查询出2011年以后入职的员工信息 -- 2) 查询所有的部门信息，与上面的虚拟表中的信息比对，找出所有部门ID相等的员工。 select * from dept d, (select * from employee where join_date &gt; '2011-1-1') e where e.dept_id = d.id; -- 使用表连接： select d., e. from dept d inner join employee e on d.id = e.dept_id where e.join_date &gt; '2011-1-1' mysql中 in 和 exists 区别 mysql 中的 in 语句是把外表和内表作 hash 连接，而 exists 语句是对外表作 loop 循环，每次 loop 循环再对内表进行查询。一直大家都认为 exists 比 in 语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用 in 和 exists 差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用 exists，子查询表小的用 in。 not in 和 not exists：如果查询语句使用了 not in，那么内外表都进行全表扫描，没有用到索引；而 not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用 not exists 都比 not in 要快。 varchar与char的区别 char 的特点 char 表示定长字符串，长度是固定的； 如果插入数据的长度小于 char 的固定长度时，则用空格填充； 因为长度固定，所以存取速度要比 varchar 快很多，甚至能快 50%，但正因为其长度固定，所以会占据多余的空间，是空间换时间的做法； 对于 char 来说，最多能存放的字符个数为 255，和编码无关 varchar 的特点 varchar 表示可变长字符串，长度是可变的； 插入的数据是多长，就按照多长来存储； varchar 在存取方面与 char 相反，它存取慢，因为长度不固定，但正因如此，不占据多余的空间，是时间换空间的做法； 对于 varchar 来说，最多能存放的字符个数为 65532 总之，结合性能角度（char 更快）和节省磁盘空间角度（varchar 更小），具体情况还需具体来设计数据库才是妥当的做法。 varchar(50)中50的涵义 最多存放 50 个字符，varchar(50)和 (200) 存储 hello 所占空间一样，但后者在排序时会消耗更多内存，因为 order by col 采用 fixed_length 计算 col 长度(memory 引擎也一样)。在早期 MySQL 版本中， 50 代表字节数，现在代表字符数。 int(20)中20的涵义 是指显示字符的长度。20 表示最大显示宽度为 20，但仍占 4 字节存储，存储范围不变； 不影响内部存储，只是影响带 zerofill 定义的 int 时，前面补多少个 0，易于报表展示 mysql为什么这么设计 对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1) 和 int(20) 存储和计算均一样； mysql中int(10)和char(10)以及varchar(10)的区别 int(10) 的 10 表示显示的数据的长度，不是存储数据的大小；chart(10) 和 varchar(10) 的 10 表示存储数据的大小，即表示存储多少个字符。 int(10) 10 位的数据长度 9999999999，占 32 个字节，int 型 4 位 char(10) 10 位固定字符串，不足补空格 最多 10 个字符 varchar(10) 10 位可变字符串，不足补空格 最多 10 个字符 char(10) 表示存储定长的 10 个字符，不足 10 个就用空格补齐，占用更多的存储空间 varchar(10) 表示存储 10 个变长的字符，存储多少个就是多少个，空格也按一个字符存储，这一点是和 char(10) 的空格不同的，char(10) 的空格表示占位不算一个字符 FLOAT和DOUBLE的区别是什么？ FLOAT 类型数据可以存储至多 8 位十进制数，并在内存中占 4 字节。 DOUBLE 类型数据可以存储至多 18 位十进制数，并在内存中占 8 字节。 drop、delete与truncate的区别 ​ Delete Truncate Drop 类型 属于 DML 属于 DDL 属于 DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据行 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行，索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 因此，在不再需要一张表的时候，用 drop；在想删除部分数据行时候，用 delete；在保留表而删除所有数据的时候用 truncate。 UNION与UNION ALL的区别？ 如果使用 UNION ALL，不会合并重复的记录行 效率 UNION 高于 UNION ALL 数据类型 1bit 位 1字节=8bit 1k=1024字节 1兆=1024k 1G=1023M 1T=1024G 引擎 MySQL存储引擎MyISAM与InnoDB区别 存储引擎 Storage engine：MySQL 中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。 MyISAM索引与InnoDB索引的区别？ Innodb 引擎：Innodb 引擎提供了对数据库 ACID 事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。 MyIASM 引擎 (原本 Mysql 的默认引擎)：不提供事务的支持，也不支持行级锁和外键。 MEMORY 引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。 InnoDB 索引是聚簇索引，MyISAM 索引是非聚簇索引。 InnoDB 的主键索引的叶子节点存储着行数据，因此主键索引非常高效。 MyISAM 索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。 InnoDB 非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。 InnoDB引擎的4大特性 插入缓冲（insert buffer) 二次写 (double write) 自适应哈希索引 (ahi) 预读 (read ahead) 存储引擎选择 如果没有特别的需求，使用默认的Innodb即可。 MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。 Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如 OA 自动化办公系统。 索引 什么是索引？ 索引是一种特殊的文件 (InnoDB 数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用 B 树及其变种 B + 树。 更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。 索引有哪些优缺点？ 优 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 缺 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增 / 改 / 删的执行效率； 空间方面：索引需要占物理空间 索引使用场景（重点） where 可以尝试在一个字段未建立索引时，根据该字段查询的效率，然后对该字段建立索引（alter table 表名 add index(字段名)），同样的 SQL 执行的效率，你会发现查询效率会有明显的提升（数据量越大越明显）。 order by 使用order by将查询结果按照某个字段排序时，如果该字段没有建立索引，那么执行计划会将查询出的所有数据使用外部排序（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），这操作很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。 但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可。而且如果分页的，那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的） join 对join语句匹配关系（on）涉及的字段建立索引能够提高效率 索引覆盖 如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。 这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。 索引有哪几种类型？ 1）从存储结构上来划分 · Btree 索引（B+tree，B-tree) · 哈希索引 · full-index 全文索引 · RTree 2）从应用层次上来划分 · 普通索引: 基本的索引类型，没有唯一性的限制，允许为 NULL 值。 CREATE INDEX indexName ON table_name (column_name)或 ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引 • ALTER TABLE testalter_tbl ADD/DROP INDEX (c); · 唯一索引: 数据列不允许重复，允许为 NULL 值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 · 主键索引: 数据列不允许重复，不允许为 NULL，一个表只能有一个主键。 ALTER TABLE table_name ADD PRIMARY KEY ( column ) 3）从表记录的排列顺序和索引的排列顺序是否一致来划分 · **聚集索引：**表记录的排列顺序和索引的排列顺序一致。 • 就是以主键创建的索引。 · **非聚集索引：**表记录的排列顺序和索引的排列顺序不一致。 • 以非主键创建的索引（也叫做二级索引）。 索引的数据结构（b树，hash） 我们经常使用的 InnoDB 存储引擎的默认索引实现为：B + 树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择 BTree 索引。 B 树索引 · mysql 通过存储引擎取数据，基本上 90% 的人用的就是 InnoDB 了，按照实现方式分，InnoDB 的索引类型目前只有两种：BTREE（B 树）索引和 HASH 索引。B 树索引是 Mysql 数据库中使用最频繁的索引类型，基本所有存储引擎都支持 BTree 索引。通常我们说的索引不出意外指的就是（B 树）索引（实际是用 B + 树实现的，因为在查看表索引时，mysql 一律打印 BTREE，所以简称为 B 树索引） • 查询方式： 主键索引区: PI(关联保存的时数据的地址) 按主键查询, 普通索引区: si(关联的 id 的地址, 然后再到达上面的地址)。所以按主键查询, 速度最快 B+tree 性质： 1.）n 棵子 tree 的节点包含 n 个关键字，不用来保存数据而是保存数据的索引。 2.）所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.）所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 4.）B+ 树中，数据对象的插入和删除仅在叶节点上进行。 5.）B + 树有 2 个头指针，一个是树的根节点，一个是最小关键码的叶节点。 哈希索引 ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png) 简要说下，类似于数据结构中简单实现的 HASH 表（散列表）一样，当我们在 mysql 中用哈希索引时，主要就是通过 Hash 算法（常见的 Hash 算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置；如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以链表形式存储。当然这只是简略模拟图。 索引的基本原理 索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理很简单，就是把无序的数据变成有序的查询 把创建了索引的列的内容进行排序 对排序结果生成倒排表 在倒排表内容上拼上数据地址链 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据 索引算法有哪些？ BTree 算法 BTree 是最常用的 mysql 数据库索引算法，也是 mysql 默认的算法。因为它不仅可以被用在 =,&gt;,&gt;=,&lt;,&lt;= 和 between 这些比较操作符上，而且还可以用于 like 操作符，只要它的查询条件是一个不以通配符开头的常量， 例如： -- 只要它的查询条件是一个不以通配符开头的常量 select * from user where name like 'jack%'; -- 如果一通配符开头，或者没有使用常量，则不会使用索引，例如： select * from user where name like '%jack'; Hash 算法 Hash Hash 索引只能用于对等比较，例如 =,&lt;=&gt;（相当于 =）操作符。由于是一次定位数据，不像 BTree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次 IO 访问，所以检索效率远高于 BTree 索引。 索引设计的原则？ 适合索引的列是出现在 where 子句中的列，或者连接子句中指定的列 基数较小的类，索引效果较差，没有必要在此列建立索引 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。 创建索引的原则（重中之重） 索引虽好，但也不是无限制的使用，最好符合一下几个原则 1） 最左前缀匹配原则，组合索引非常重要的原则，mysql 会一直向右匹配直到遇到范围查询 (&gt;、&lt;、between、like) 就停止匹配，比如 a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立 (a,b,c,d) 顺序的索引，d 是用不到索引的，如果建立 (a,b,d,c) 的索引则都可以用到，a,b,d 的顺序可以任意调整。 2）较频繁作为查询条件的字段才去创建索引 3）更新频繁字段不适合创建索引 4）若是不能有效区分数据的列不适合做索引列 (如性别，男女未知，最多也就三种，区分度实在太低) 5）尽量的扩展索引，不要新建索引。比如表中已经有 a 的索引，现在要加 (a,b) 的索引，那么只需要修改原来的索引即可。 6）定义有外键的数据列一定要建立索引。 7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 8）对于定义为 text、image 和 bit 的数据类型的列不要建立索引。 创建索引的三种方式，删除索引 第一种方式：在执行 CREATE TABLE 时创建索引 CREATE TABLE user_index2 ( id INT auto_increment PRIMARY KEY, first_name VARCHAR (16), last_name VARCHAR (16), id_card VARCHAR (18), information text, KEY name (first_name, last_name), FULLTEXT KEY (information), UNIQUE KEY (id_card) ); 第二种方式：使用 ALTER TABLE 命令去增加索引 ALTER TABLE table_name ADD INDEX index_name (column_list); ALTER TABLE 用来创建普通索引、UNIQUE 索引或 PRIMARY KEY 索引。 其中 table_name 是要增加索引的表名，column_list 指出对哪些列进行索引，多列时各列之间用逗号分隔。 索引名 index_name 可自己命名，缺省时，MySQL 将根据第一个索引列赋一个名称。另外，ALTER TABLE 允许在单个语句中更改多个表，因此可以在同时创建多个索引。 第三种方式：使用 CREATE INDEX 命令创建 CREATE INDEX index_name ON table_name (column_list); CREATE INDEX 可对表增加普通索引或 UNIQUE 索引。（但是，不能创建 PRIMARY KEY 索引） 删除索引 根据索引名删除普通索引、唯一索引、全文索引：alter table 表名 drop KEY 索引名 alter table user_index drop KEY name; alter table user_index drop KEY id_card; alter table user_index drop KEY information; 删除主键索引：alter table 表名 drop primary key（因为主键只有一个）。这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）： 需要取消自增长再行删除： alter table user_index -- 重新定义字段 MODIFY id int, drop PRIMARY KEY 但通常不会删除主键，因为设计主键一定与业务逻辑无关。 ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png) 创建索引时需要注意什么？ 非空字段：应该指定列为 NOT NULL，除非你想存储 NULL。在 mysql 中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用 0、一个特殊的值或者一个空串代替空值； 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过 count() 函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次 IO 操作获取的数据越大效率越高。 使用索引查询一定能提高查询的性能吗？为什么 通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。 索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。 这意味着每条记录的 INSERT，DELETE，UPDATE 将为此多付出 4，5 次的磁盘 I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能，索引范围查询 (INDEX RANGE SCAN) 适用于两种情况: 基于一个范围的检索，一般查询返回结果集小于表中记录数的 30% 基于非唯一性索引的检索 索引失效的几种情况 什么时候没用 or，%，算，函数，转换，复合索引 1.有or必全有索引; 2.复合索引未用左列字段; 3.like以%开头; 4.需要类型转换; 5.where中索引列有运算; 6.where中索引列使用了函数; 7.如果mysql觉得全表扫描更快时（数据少）; 什么时没必要用 1.唯一性差; 2.频繁更新的字段不用（更新索引消耗）; 3.where中不用的字段; 4.索引使用&lt;&gt;时，效果一般; 详述（转） 索引并不是时时都会生效的，比如以下几种情况，将导致索引失效： 如果条件中有or，即使其中有部分条件带索引也不会使用(这也是为什么尽量少用or的原因)，例子中user_id无索引 注意：要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 对于复合索引，如果不使用前列，后续列也将无法使用，类电话簿。 like查询是以%开头 存在索引列的数据类型隐形转换，则用不上索引，比如列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 where 子句里对索引列上有数学运算，用不上索引 where 子句里对有索引列使用函数，用不上索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 比如数据量极少的表 什么情况下不推荐使用索引？ \\1) 数据唯一性差（一个字段的取值只有几种时）的字段不要使用索引 比如性别，只有两种可能数据。意味着索引的二叉树级别少，多是平级。这样的二叉树查找无异于全表扫描。 \\2) 频繁更新的字段不要使用索引 比如logincount登录次数，频繁变化导致索引也频繁变化，增大数据库工作量，降低效率。 \\3) 字段不在where语句出现时不要添加索引,如果where后含IS NULL /IS NOT NULL/ like ‘%输入符%’等条件，不建议使用索引 只有在where语句出现，mysql才会去使用索引 4） where 子句里对索引列使用不等于（&lt;&gt;），使用索引效果一般 百万级别或以上的数据如何删除 关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件, 所以当我们对数据的增加, 修改, 删除, 都会产生额外的对索引文件的操作, 这些操作需要消耗额外的 IO, 会降低增 / 改 / 删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询 MySQL 官方手册得知删除数据的速度和创建的索引数量是成正比的。 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟） 然后删除其中无用数据（此过程需要不到两分钟） 删除完成后重新创建索引 (此时数据较少了) 创建索引也非常快，约十分钟左右。 与之前的直接删除绝对是要快速很多，更别说万一删除中断, 一切删除会回滚。那更是坑了。 前缀索引 语法：index(field(10))，使用字段值的前 10 个字符建立索引，默认是使用字段的全部内容建立索引。 前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。 实操的难度：在于前缀截取的长度。 我们可以利用select count(*)/count(distinct left(password,prefixLen));，通过从调整prefixLen的值（从 1 自增）查看不同前缀长度的一个平均匹配度，接近 1 时就可以了（表示一个密码的前prefixLen个字符几乎能确定唯一一条记录） 什么是最左前缀原则？什么是最左匹配原则 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where 子句中使用最频繁的一列放在最左边。 最左前缀匹配原则，非常重要的原则，mysql 会一直向右匹配直到遇到范围查询 (&gt;、&lt;、between、like) 就停止匹配，比如 a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立 (a,b,c,d) 顺序的索引，d 是用不到索引的，如果建立 (a,b,d,c) 的索引则都可以用到，a,b,d 的顺序可以任意调整。 = 和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立 (a,b,c) 索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式 B树和B+树的区别 在 B 树中，你可以将键和值存放在内部节点和叶子节点；但在 B + 树中，内部节点都是键，没有值，叶子节点同时存放键和值。 B + 树的叶子节点有一条链相连，而 B 树的叶子节点各自独立。 · ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png) 使用B树的好处 B 树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得 B 树在特定数据重复多次查询的场景中更加高效。 使用B+树的好处 由于 B + 树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。 B + 树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B + 树只需要使用 O(logN) 时间找到最小的一个节点，然后通过链进行 O(N) 的顺序遍历即可。而 B 树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间 Hash索引和B+树所有有什么区别或者说优劣呢? 首先要知道 Hash 索引和 B + 树索引的底层实现原理： hash 索引底层就是 hash 表，进行查找时，调用一次 hash 函数就可以获取到相应的键值，之后进行回表查询获得实际数据。B + 树底层实现是多路平衡查找树。对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。 那么可以看出他们有以下的不同： hash 索引进行等值查询更快 (一般情况下)，但是却无法进行范围查询。 因为在 hash 索引中经过 hash 函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。而 B + 树的的所有节点皆遵循 (左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。 hash 索引不支持使用索引进行排序，原理同上。 hash 索引不支持模糊查询以及多列索引的最左前缀匹配。原理也是因为 hash 函数的不可预测。AAAA 和 AAAAB 的索引没有相关性。 hash 索引任何时候都避免不了回表查询数据，而 B + 树在符合某些条件 (聚簇索引，覆盖索引等) 的时候可以只通过索引完成查询。 hash 索引虽然在等值查询上较快，但是不稳定。性能不可预测，当某个键值存在大量重复的时候，发生 hash 碰撞，此时效率可能极差。而 B + 树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。 因此，在大多数情况下，直接选择 B + 树索引可以获得稳定且较好的查询速度。而不需要使用 hash 索引。 数据库为什么使用B+树而不是B树 B 树只适合随机检索，而 B + 树同时支持随机检索和顺序检索； B + 树空间利用率更高，可减少 I/O 次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I/O 消耗。B + 树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比 B 树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO 读写次数也就降低了。而 IO 读写次数是影响索引检索效率的最大因素； B + 树的查询效率更加稳定。B 树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在 B + 树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 B - 树在提高了磁盘 IO 性能的同时并没有解决元素遍历的效率低下的问题。B + 树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而 B 树不支持这样的操作。 增删文件（节点）时，效率更高。因为 B + 树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。 B+树在满足聚簇索引和覆盖索引的时候不需要回表查询数据， 在 B + 树的索引中，叶子节点可能存储了当前的 key 值，也可能存储了当前的 key 值以及整行的数据，这就是聚簇索引和非聚簇索引。 在 InnoDB 中，只有主键索引是聚簇索引，如果没有主键，则挑选一个唯一键建立聚簇索引。如果没有唯一键，则隐式的生成一个键来建立聚簇索引。 当查询使用聚簇索引时，在对应的叶子节点，可以获取到整行数据，因此不用再次进行回表查询。 什么是聚簇索引？何时使用聚簇索引与非聚簇索引 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam 通过 key_buffer 把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在 key buffer 命中时，速度慢的原因 澄清一个概念：innodb 中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值 何时使用聚簇索引与非聚簇索引 · ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png) 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。 举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了 age 信息，不会再次进行回表查询。 联合索引是什么？为什么需要注意联合索引中的顺序？ MySQL 可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 具体原因为: MySQL 使用索引时需要索引有序，假设现在建立了 &quot;name，age，school&quot; 的联合索引，那么索引的排序为: 先按照 name 排序，如果 name 相同，则按照 age 排序，如果 age 的值也相等，则按照 school 进行排序。 当进行查询时，此时索引仅仅按照 name 严格有序，因此必须首先使用 name 字段进行等值查询，之后对于匹配到的列而言，其按照 age 字段严格有序，此时可以使用 age 字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 事务 什么是数据库事务？ 1、用 BEGIN, ROLLBACK, COMMIT来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 2、直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 事物的四大特性(ACID)介绍一下? **原子性 (Atomicity)：**一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 **一致性（Consistency）：**在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 **隔离性（Isolation）：**数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 **持久性（Durability）：**事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 什么是脏读？幻读？不可重复读？ 什么是事务的隔离级别？MySQL的默认隔离级别是什么？ 锁 对MySQL的锁了解吗 隔离级别与锁的关系 按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法 从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了 MySQL中InnoDB引擎的行锁是怎么实现的？ InnoDB存储引擎的锁的算法有三种 什么是死锁？怎么解决？ 数据库的乐观锁和悲观锁是什么？怎么实现的？ 视图 1234567#创建视图create view 视图名称 as select语句;#--查看视图结构：desc user_view; show create view user_view;#--查看视图内容：select * from user_view; 为什么要使用视图？什么是视图？ 由基表通过SQL动态生成的虚拟表，本身不包含数据，基表和视图数据会互相影响。只有基表和视图字段一一对应才可以增删改查。 在Mysql中视图的类型分为： 1、MERGE 将视图的sql语句和引用视图的sql语句合并在一起，最后一起执行。 2、TEMPTABLE 将视图的结果集存放在临时表中，每次执行时从临时表中操作。 3、UNDEFINED 默认的视图类型，DBMS倾向于选择而不是必定选择MERGE，因为MERGE的效率更高，更重要的是临时表视图不能更新。 所以，这里推荐使用MERGE算法类型视图。 视图有哪些特点？ 1）简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 2）安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 3）数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。 总而言之，使用视图的大部分情况是为了保障数据安全性，提高查询效率。 视图的使用场景有哪些？ 分表 启用计算列 如有订单数量和单价，一般数据库设计不会有总价字段，视图可有 导入导出表 使用视图导入导出数据 权限控制，提高安全性 创建视图控制显示字段，用户从视图查询只会显示控制的字段，可以把权限限定到行列级别 解耦合 创建视图后基表结构改变不影响视图，重构数据库不影响程序，向后兼容。 简化数据操作 将复杂的联合等查询创建为视图，可只查询视图来简化操作 视图的优点 视图的缺点 查询性能降低，修改视图会直接修改原数据，基表视图相互依赖。 什么是游标？ 游标的特性 不敏感：数据库可以选择不复制结果集 只读 不滚动：游标只能向一方向前进，并且不可以跳过任何一行数据 游标的优点 游标是针对行操作的，对从数据库中 select 查询得到的结果集的 每一行可以 进行分开的独立的相同或者不相同的操作，是一种分离的思想。 游标的缺点 性能不高 只能一行一行操作 使用游标会产生死锁，造成内存开销大 游标的适用场景 存储过程 函数 触发器 事件 游标的操作 1、游标的定义 DECLARE 光标名称 CURSOR FOR 查询语法 declare cursor_name cursor for select_statement 2、打开游标 OPEN 光标名称 open cursor_name 3、取游标中的数据 FETCH 光标名称 INFO var_name [，var_name ]..... fetch cursor_name info var_name 4、关闭游标 CLOSE curso_name; close 光标名称 5、释放游标 DEALLOCATE 光标名称 deallocate cursor_name; 存储过程与函数 什么是存储过程？有哪些优缺点？ 触发器 什么是触发器？触发器的使用场景有哪些？ 触发器是针对每一行的；对增删改非常频繁的表上切记不要使用触发器，因为它会非常消耗资源。 触发器是针对每一行的；对增删改非常频繁的表上切记不要使用触发器，因为它会非常消耗资源。 1234567891011121314151617181920212223242526CREATE [DEFINER = { user | CURRENT_USER }]TRIGGER trigger_nametrigger_time trigger_eventON tbl_name FOR EACH ROW [trigger_order]trigger_bodytrigger_time: { BEFORE | AFTER }trigger_event: { INSERT | UPDATE | DELETE }trigger_order: { FOLLOWS | PRECEDES } other_trigger_name&gt; BEFORE和AFTER参数指定了触发执行的时间，在事件之前或是之后。&gt; FOR EACH ROW表示任何一条记录上的操作满足触发事件都会触发该触发器，也就是说触发器的触发频率是针对每一行数据触发一次。&gt; tigger_event详解： ①INSERT型触发器：插入某一行时激活触发器，可能通过INSERT、LOAD DATA、REPLACE 语句触发(LOAD DAT语句用于将一个文件装入到一个数据表中，相当与一系列的INSERT操作)； ②UPDATE型触发器：更改某一行时激活触发器，可能通过UPDATE语句触发； ③DELETE型触发器：删除某一行时激活触发器，可能通过DELETE、REPLACE语句触发。&gt; trigger_order是MySQL5.7之后的一个功能，用于定义多个触发器，使用follows(尾随)或precedes(在…之先)来选择触发器执行的先后顺序。 NEW与OLD详解 MySQL 中定义了 NEW 和 OLD，用来表示触发器的所在表中，触发了触发器的那一行数据，来引用触发器中发生变化的记录内容，具体地： ①在INSERT型触发器中，NEW用来表示将要（BEFORE）或已经（AFTER）插入的新数据； ②在UPDATE型触发器中，OLD用来表示将要或已经被修改的原数据，NEW用来表示将要或已经修改为的新数据； ③在DELETE型触发器中，OLD用来表示将要或已经被删除的原数据； 查看触发器 1select * from information_schema.triggers MySQL中都有哪些触发器？ Linux下RPM版MYSQL安装、启停 MySQL启动问题、配置文件、编码问题 MYSQL分层、存储引擎 连接层（提供与客户端连接的服务） 服务层（提供各种用户使用的接口，提供SQL优化器【MySQL Query Optimizer】）） 引擎层（提供各种存储数据的方式【InnoDB：事物优先，适合高并发，行锁；MyISAM：性能优先，表锁】） show engines; 显示支持引擎 +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ show variables like ‘%storage_engine%’; 查看当前使用引擎 12345678-- 指定数据库引擎create table tb(​ id int(4) auto_increment,​ name varchar(5),​ dept varchar(5),​ primary key(id))ENGINE=MyISAM AUTO_INCREMENT=1DEFAULT CHARSET=utf-8 存储层（存储数据） SQL解析过程、索引、B树 sql优化： 原因：性能低，执行或等待时间太长，SQL语句欠佳（连接查询）、索引失效、服务器参数设置不对 SQL编写过程与解析过程： 编写:select dinstinct ..from …join ..on ..where ..group by ..having ..order by ..limit 解析:form ..on ..join ..where ..group by ..having ..select dinstinct ..order by limit.. 从哪些表构成的总表中，通过字段筛选出相应数据然后分组、排序、限制数量。 索引 简介 sql优化主要是优化索引，索引（index）相当于书的目录。索引是数据结构（默认B树【小左大右】） 优点 提高查询效率（降低IO、CPU使用率） 缺点 索引本身很大（可以存放在内存/硬盘） 索引不是所有情况都适用：少量数据，频繁更新字段，很少使用的字段 索引会降低增删改查效率 分类 单值索引：单字段 唯一索引：不能重复 主键索引：不能重复，不能为null，主键默认是主键索引 复合索引：相当于二级目录（name，age） 创建索引 方式一 create 索引类型 索引名 on 表（字段） 单值索引：create index dept_index on tb(dept); 唯一索引：create unique index name-index on tb(name) 复合索引：create index dept_name_index on tb(dept,name); 方式二 alter table 表名 add 索引类型 索引名（字段） 单值索引：alter table tb add index dept_index(dept); 唯一索引：alter table tb add unique index name-index(name) 复合索引：alter table tb add index dept_name_index(dept,name); 删除索引 drop index 索引名 on 表名 查询索引 show index from 表名\\G SQL优化 为什么要优化 数据库结构优化 MySQL数据库cpu飙升到500%的话怎么处理？ 大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？ 垂直分表 适用场景 缺点 水平分表： 适用场景 水平切分的缺点 MySQL的复制原理以及流程 读写分离有哪些解决方案？ 备份计划，mysqldump以及xtranbackup的实现原理 数据表损坏的修复方式有哪些？ 如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？ SQL的生命周期？ 大表数据查询，怎么优化 超大分页怎么处理？ mysql 分页 慢查询日志 关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？ 为什么要尽量设定一个主键？ 主键使用自增ID还是UUID？各自优缺点？ 字段为什么要求定义为not null？ 如果要存储用户的密码散列，应该使用什么字段进行存储？ 优化查询过程中的数据访问 优化长难的查询语句 优化特定类型的查询语句 优化关联查询 优化子查询 优化LIMIT分页 优化UNION查询 优化WHERE子句 SQL优化准备 分析SQL的执行计划：explain+sql语句，模拟sql优化器执行sql语句，从而让开发人员知道自己SQL语句情况 id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user const PRIMARY PRIMARY 152 const 1 100.00 id 编号 子查询id不同，关联查询相同 **select_type ** 查询类型 **table ** 查询表 id相同数据量越少越优先查询，id不通id越大越优先 partitions **type ** 类型 **possible_keys ** 预测用到的索引 **key ** 实际用到的索引 **key_len ** 实际使用索引的长度 **ref ** 表之间的引用 **rows ** 通过索引查询到的数据量 filtered **Extra ** 额外的信息 id、table 先查内层，再查外层。子查询id不同，关联查询id相同。 id相同数据量越少越优先查询，id不同id越大越优先 explain SELECT * from teacher where tid=(select tid from course where cid='2') or tcid=(select tcid from teachercard where tcid='3') explain select t.* from teacher t,course c,teacherCard tc where t.tid=c.tid and t.tcid=tc.tcid and (c.cid='2' or tc.tcid='3') select_type（查询类型） PRIMARY：包含子查询SQL中的 主查询（最外层） SUBQUERY：包含子查询SQL中的子查询（非最外层） SIMPLE：简单查询（不包含子查询、union） union： derived：衍生查询（使用到了临时表） 在form子查询中只有一张表 explain select cr.cname from (select * from course where tid in (1,2)) cr; form子查询中，如果有table1 union table2，则table1 就是的riverd explain select cr.cname from (select * from course where tid =1 union select * from course where tid =2) cr; type（索引类型） system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all 对TYPE优化前提：有索引 system，const知识理想情况，实际能达到 ref &gt;range system（忽略）：只有一条数据的系统表；或衍生表只有一条数据的主查询 const：仅仅能查到一条数据的SQL，用于Primayr key 或unique索引（类型与索引类型有关） eq_ref：唯一性索引：对于每个索引建的查询，返回匹配唯一行数据（有且只有1个，不能多、不能0） select ·· from ··where name=···常见唯一索引和主键索引 ref：非唯一性索引，对于每个索引建的查询，返回匹配的所有行（0，多） range：检索指定范围的行，where后面是一个范围查询（between，in，&gt; &lt; &gt;=，in有时候会失效，从而转为all） index：查询全部索引中的数据，只需要扫描索引表，不需要所有表中的所有数据 all：查询全部表中的数据，需要全表所有，即需要所有表中的所有数据 possible_keys 可能用到的索引，是一种预测，不准 key 实际用到的索引 key_len： 索引长度，用于判断复合索引是否完全使用，如果索引字段可为null，则会使用1个字节用于标识，索引字段为可变长度，则会使用2个字节 ref 注意与type值中的ref值区分 作用：指明当前表所参照的字段 select 。。。 where a.c=b.x （） rows 被索引优化查询的数据个数（实际通过索引而查询到的数据个数） extra using filesort：性能消耗大,需要额外的一次排序（查询） 单索引：如果排序和查找是同一个字段，则不会出现using filesort ；如果排序和查找不是同一个字段，则会出现。 避免：where哪些字段就order by哪些字段 复合索引：不能跨列（最佳左前缀） 避免：where和order by 按照复合索引的顺序适应，不要跨列或无序使用 using temporary：性能损耗较大，用到了临时表。一般出现在 group by 语句中； explain select a1 from test02 where a1 in(‘1’,’2’,’3’) group by a1; explain select a1 from test02 where a1 in(‘1’,’2’,’3’) group by a2; —–using tempporary 避免：查询哪些列，就根据哪些列group by using temporary：性能提升，索引覆盖，原因：不读取源文件，只从索引中获取数据，使用到的列全部都在索引中 using where：需要回原表查询 impossible where：where子句永远为false 优化示例 create table test03 (a1 int(4) not null, a2 int(4) not null, a3 int(4) not null, a4 int(4) not null ) alter table test03 add index idx_a1_a2_a3_a4(a1,a2,a3,a4); explain select a1,a2,a3,a4 from test03 where a1=1 and a2=2 and a3=3 and a4=4;—-推荐写法 explain select a1,a2,a3,a4 from test03 where a4=1 and a2=2 and a1=3 and a4=4;—-虽然编写后的顺序和索引顺序不一致，但是sql执行前经过了sql优化器的调整，结果与上条sql是一致的。 —–以上两个sql使用了全部的索引 explain select a1,a2,a3,a4 from test03 where a4=1 and a2=2 and a4=4 order by and a1; —-以上SQL用到了a1 a2两个索引，该两个字段 不需要回表查询using index ；a4跨列使用造成了该索引失效，所以需要回表查询，因此是using where ，以上可以通过key_len验证 explain select a1,a2,a3,a4 from test03 where a4=1 and a4=4 order by and a3; —-以上SQL出现了using filesort（文件内排序，”多了一次额外的查找/排序“）不要跨列使用（where 和order by拼接起来使用） explain select a1,a2,a3,a4 from test03 where a4=1 and a4=4 order by and a2,a3; ===这样便不会出现 using filesort，如果复合索引和使用顺序全部一致，则复合索引全部使用，部分一致则使用部分索引。 单表优化及总结 （1）单表优化 create table book ( bid int(4) primary key, name varchar(20) not null, authorid int(4) not null, publicid int(4) not null, typeid int(4) not null ); insert into book values(1,'tjava',1,1,2) ; insert into book values(2,'tc',2,1,2) ; insert into book values(3,'wx',3,2,1) ; insert into book values(4,'math',4,2,3) ; commit; 查询authorid=1且 typeid为2或3的 bid explain select bid from book where typeid in(2,3) and authorid=1 order by typeid desc ; (a,b,c) (a,b) 优化：加索引 alter table book add index idx_bta (bid,typeid,authorid); 索引一旦进行 升级优化，需要将之前废弃的索引删掉，防止干扰。 drop index idx_bta on book; 根据SQL实际解析的顺序，调整索引的顺序： alter table book add index idx_tab (typeid,authorid,bid); --虽然可以回表查询bid，但是将bid放到索引中 可以提升使用using index ; 再次优化（之前是index级别）：思路。因为范围查询in有时会实现，因此交换 索引的顺序，将typeid in(2,3) 放到最后。 drop index idx_tab on book; alter table book add index idx_atb (authorid,typeid,bid); explain select bid from book where authorid=1 and typeid in(2,3) order by typeid desc ; ​ ​ --小结： a.最佳做前缀，保持索引的定义和使用的顺序一致性 b.索引需要逐步优化 c.将含In的范围查询 放到where条件的最后，防止失效。 ​ ​ 本例中同时出现了Using where（需要回原表）; Using index（不需要回原表）：原因，where authorid=1 and typeid in(2,3)中authorid在索引(authorid,typeid,bid)中，因此不需要回原表（直接在索引表中能查到）；而typeid虽然也在索引(authorid,typeid,bid)中，但是含in的范围查询已经使该typeid索引失效，因此相当于没有typeid这个索引，所以需要回原表（using where）； ​ 例如以下没有了In，则不会出现using where ​ explain select bid from book where authorid=1 and typeid =3 order by typeid desc ; ​ ​ 还可以通过key_len证明In可以使索引失效。 ​ （2）两表优化 create table teacher2 ( tid int(4) primary key, cid int(4) not null ); insert into teacher2 values(1,2); insert into teacher2 values(2,1); insert into teacher2 values(3,3); create table course2 ( cid int(4) , cname varchar(20) ); insert into course2 values(1,'java'); insert into course2 values(2,'python'); insert into course2 values(3,'kotlin'); commit; 左连接： explain select *from teacher2 t left outer join course2 c on t.cid=c.cid where c.cname='java'; 索引往哪张表加？ -小表驱动大表 -索引建立经常使用的字段上 （本题 t.cid=c.cid可知，t.cid字段使用频繁，因此给该字段加索引） [一般情况对于左外连接，给左表加索引；右外连接，给右表加索引] 小表：10 大表：300 where 小表.x 10 = 大表.y 300; --循环了几次？10 大表.y 300=小表.x 10 --循环了300次 小表:10 大表:300 select ...where 小表.x10=大表.x300 ; for(int i=0;i&lt;小表.length10;i++) { for(int j=0;j&lt;大表.length300;j++) { ... } } select ...where 大表.x300=小表.x10 ; for(int i=0;i&lt;大表.length300;i++) { for(int j=0;j&lt;小表.length10;j++) { ... } } --以上2个FOR循环，最终都会循环3000次；但是 对于双层循环来说：一般建议 将数据小的循环 放外层；数据大的循环放内存。 --当编写 ..on t.cid=c.cid 时，将数据量小的表 放左边（假设此时t表数据量小） alter table teacher2 add index index_teacher2_cid(cid) ; alter table course2 add index index_course2_cname(cname); Using join buffer:extra中的一个选项，作用：Mysql引擎使用了 连接缓存。 （3）三张表优化 ​ a.小表驱动大表 b.索引建立在经常查询的字段上 示例： create table test03 ( a1 int(4) not null, a2 int(4) not null, a3 int(4) not null, a4 int(4) not null ); alter table test03 add index idx_a1_a2_a3_4(a1,a2,a3,a4) ; 12345678910explain select a1,a2,a3,a4 from test03 where a1=1 and a2=2 and a3=3 and a4 =4 ; --推荐写法，因为 索引的使用顺序（where后面的顺序） 和 复合索引的顺序一致explain select a1,a2,a3,a4 from test03 where a4=1 and a3=2 and a2=3 and a1 =4 ; --虽然编写的顺序 和索引顺序不一致，但是 sql在真正执行前 经过了SQL优化器的调整，结果与上条SQL是一致的。--以上 2个SQL，使用了 全部的复合索引explain select a1,a2,a3,a4 from test03 where a1=1 and a2=2 and a4=4 order by a3; --以上SQL用到了a1 a2两个索引，该两个字段 不需要回表查询using index ;而a4因为跨列使用，造成了该索引失效，需要回表查询 因此是using where；以上可以通过 key_len进行验证explain select a1,a2,a3,a4 from test03 where a1=1 and a4=4 order by a3; --以上SQL出现了 using filesort(文件内排序，“多了一次额外的查找/排序”) ：不要跨列使用( where和order by 拼起来，不要跨列使用) 1explain select a1,a2,a3,a4 from test03 where a1=1 and a4=4 order by a2 , a3; --不会using filesort 123--总结：i.如果 (a,b,c,d)复合索引 和使用的顺序全部一致(且不跨列使用)，则复合索引全部使用。如果部分一致(且不跨列使用)，则使用部分索引。select a,c where a = and b= and d= ii.where和order by 拼起来，不要跨列使用 1using temporary:需要额外再多使用一张表. 一般出现在group by语句中；已经有表了，但不适用，必须再来一张表。 解析过程： from .. on.. join ..where ..group by ....having ...select dinstinct ..order by limit ... a. explain select * from test03 where a2=2 and a4=4 group by a2,a4 ;--没有using temporary b. explain select * from test03 where a2=2 and a4=4 group by a3 ; 索引失效七种情况 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#（1）复合索引# a.复合索引，不要跨列或无序使用（最佳左前缀）# (a,b,c) # b.复合索引，尽量使用全索引匹配# (a,b,c) #（2）不要在索引上进行任何操作（计算、函数、类型转换），否则索引失效# select ..where A.x = .. ; --假设A.x是索引# 不要：select ..where A.x*3 = .. ; explain select * from book where authorid = 1 and typeid = 2 ;--用到了at2个索引 explain select * from book where authorid = 1 and typeid*2 = 2 ;--用到了a1个索引 explain select * from book where authorid*2 = 1 and typeid*2 = 2 ;----用到了0个索引 explain select * from book where authorid*2 = 1 and typeid = 2 ;----用到了0个索引,原因：对于复合索引，如果左边失效，#右侧全部失效。(a,b,c)，例如如果 b失效，则b c同时失效。 drop index idx_atb on book ; alter table book add index idx_authroid (authorid) ; alter table book add index idx_typeid (typeid) ; explain select * from book where authorid*2 = 1 and typeid = 2 ;#（3）复合索引不能使用不等于（!= &lt;&gt;）或is null (is not null)，否则自身以及右侧所有全部失效。# 复合索引中如果有&gt;，则自身和右侧索引全部失效。explain select * from book where authorid = 1 and typeid =2 ;# SQL优化，是一种概率层面的优化。至于是否实际使用了我们的优化，需要通过explain进行推测。explain select * from book where authorid != 1 and typeid =2 ;explain select * from book where authorid != 1 and typeid !=2 ;#体验概率情况(&lt; &gt; =)：原因是服务层中有SQL优化器，可能会影响我们的优化。drop index idx_typeid on book;drop index idx_authroid on book;alter table book add index idx_book_at (authorid,typeid);explain select * from book where authorid = 1 and typeid =2 ;--复合索引at全部使用explain select * from book where authorid &gt; 1 and typeid =2 ; --复合索引中如果有&gt;，则自身和右侧索引全部失效。explain select * from book where authorid = 1 and typeid &gt;2 ;--复合索引at全部使用#----明显的概率问题---explain select * from book where authorid &lt; 1 and typeid =2 ;--复合索引at只用到了1个索引explain select * from book where authorid &lt; 4 and typeid =2 ;--复合索引全部失效-- 我们学习索引优化 ，是一个大部分情况适用的结论，但由于SQL优化器等原因 该结论不是100%正确。-- 一般而言， 范围查询（&gt; &lt; in），之后的索引失效。#（4）补救。尽量使用索引覆盖（using index）# （a,b,c）select a,b,c from xx..where a= .. and b =.. ;#(5) like尽量以“常量”开头，不要以'%'开头，否则索引失效select * from xx where name like '%x%' ; --name索引失效explain select * from teacher where tname like '%x%'; --tname索引失效explain select * from teacher where tname like 'x%'; explain select tname from teacher where tname like '%x%'; --如果必须使用like '%x%'进行模糊查询，可以使用索引覆盖 挽救一部分。#（6）尽量不要使用类型转换（显示、隐式），否则索引失效explain select * from teacher where tname = 'abc' ;explain select * from teacher where tname = 123 ;//程序底层将 123 -&gt; '123'，即进行了类型转换，因此索引失效#（7）尽量不要使用or，否则索引失效explain select * from teacher where tname ='' or tcid &gt;1 ; --将or左侧的tname 失效。 常见优化方法及慢查询SQL排查 exist和 in select 。。from table where exist/in(子查询)； 如果主查询的数据集大，则使用In效率更高 如果子查询的数据集大，则使用exist效率更高 exist语法：将主查询的结果，放到子查询结果中进行条件校验（看子查询是否有数据，如果有数据，则校验成功，保留数据） order by using filesort 有两种算法：双路排序、单路排序 （根据IO的次数） MySQL4.1之前 默认使用 双路排序；双路：扫描2次磁盘（1：从磁盘读取排序字段 ,对排序字段进行排序（在buffer中进行的排序） 2：扫描其他字段 ） --IO较消耗性能 MySQL4.1之后 默认使用 单路排序 ： 只读取一次（全部字段），在buffer中进行排序。但种单路排序 会有一定的隐患 （不一定真的是“单路|1次IO”，有可能多次IO）。原因：如果数据量特别大，则无法 将所有字段的数据 一次性读取完毕，因此 会进行“分片读取、多次读取”。 注意：单路排序 比双路排序 会占用更多的buffer。 单路排序在使用时，如果数据大，可以考虑调大buffer的容量大小： set max_length_for_sort_data = 1024 单位byte 如果max_length_for_sort_data值太低，则mysql会自动从 单路-&gt;双路 （太低：需要排序的列的总大小超过了max_length_for_sort_data定义的字节数） 提高order by查询的策略： a.选择使用单路、双路 ；调整buffer的容量大小； b.避免select * ... （*会多花一些时间，且无法索引覆盖） c.复合索引 不要跨列使用 ，避免using filesort d.保证全部的排序字段 排序的一致性（都是升序 或 降序） 慢查询阀值和mysqldumpslow工具 :MySQL提供的一种日志记录，用于记录MySQL种响应时间超过阀值的SQL语句 （long_query_time，默认10秒） 慢查询日志默认是关闭的；建议：开发调优是 打开，而 最终部署时关闭。 检查是否开启了 慢查询日志 ： show variables like '%slow_query_log%' ; 临时开启： set global slow_query_log = 1 ; --在内存种开启 exit service mysql restart 永久开启： /etc/my.cnf 中追加配置： vi /etc/my.cnf [mysqld] slow_query_log=1 slow_query_log_file=/var/lib/mysql/localhost-slow.log ​ ​ 慢查询阀值： ​ show variables like '%long_query_time%' ; ​ ​ 临时设置阀值： ​ set global long_query_time = 5 ; --设置完毕后，重新登陆后起效 （不需要重启服务） ​ ​ 永久设置阀值： ​ ​ /etc/my.cnf 中追加配置： ​ vi /etc/my.cnf ​ [mysqld] ​ long_query_time=3 select sleep(4); select sleep(5); select sleep(3); select sleep(3); --查询超过阀值的SQL数量： show global status like '%slow_queries%' ; (1)慢查询的sql被记录在了日志中，因此可以通过日志 查看具体的慢SQL。 cat /var/lib/mysql/localhost-slow.log (2)通过mysqldumpslow工具查看慢SQL,可以通过一些过滤条件 快速查找出需要定位的慢SQL mysqldumpslow --help s：排序方式 r:逆序 l:锁定时间 g:正则匹配模式 --获取返回记录最多的3个SQL mysqldumpslow -s r -t 3 /var/lib/mysql/localhost-slow.log --获取访问次数最多的3个SQL mysqldumpslow -s c -t 3 /var/lib/mysql/localhost-slow.log --按照时间排序，前10条包含left join查询语句的SQL mysqldumpslow -s t -t 10 -g &quot;left join&quot; /var/lib/mysql/localhost-slow.log 语法： mysqldumpslow 各种参数 慢查询日志的文件 模拟并通过profiles分析海量数据 a.模拟海量数据 a.模拟海量数据 存储过程（无return）/存储函数（有return） create database testdata ; use testdata 1234567891011121314151617create table dept(dno int(5) primary key default 0,dname varchar(20) not null default '',loc varchar(30) default '')engine=innodb default charset=utf8;create table emp(eid int(5) primary key,ename varchar(20) not null default '',job varchar(20) not null default '',deptno int(5) not null default 0)engine=innodb default charset=utf8; 通过存储函数 插入海量数据： 创建存储函数： randstring(6) -&gt;aXiayx 用于模拟员工名称 1234567891011121314delimiter $ #声明分号不是结束符create function randstring(n int) returns varchar(255) begin declare all_str varchar(100) default 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' ; declare return_str varchar(255) default '' ; declare i int default 0 ; while i&lt;n do set return_str = concat( return_str, substring(all_str, FLOOR(1+rand()*52) ,1) ); set i=i+1 ; end while ; return return_str; end $ --如果报错：You have an error in your SQL syntax，说明SQL语句语法有错，需要修改SQL语句； 如果报错This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you might want to use the less safe log_bin_trust_function_creators variable) 是因为 存储过程/存储函数在创建时 与之前的 开启慢查询日志冲突了 解决冲突： 临时解决( 开启log_bin_trust_function_creators ) show variables like '%log_bin_trust_function_creators%'; set global log_bin_trust_function_creators = 1; 永久解决： /etc/my.cnf [mysqld] log_bin_trust_function_creators = 1 12345678--产生随机整数create function ran_num() returns int(5)begin declare i int default 0; set i =floor( rand()*100 ) ; return i ;end $ 1234567891011121314--通过存储过程插入海量数据：emp表中 ， 10000, 100000create procedure insert_emp( in eid_start int(10),in data_times int(10))begin declare i int default 0; set autocommit = 0 ; repeat insert into emp values(eid_start + i, randstring(5) ,'other' ,ran_num()) ; set i=i+1 ; until i=data_times end repeat ; commit ;end $ 12345678910111213--通过存储过程插入海量数据：dept表中 create procedure insert_dept(in dno_start int(10) ,in data_times int(10)) begin declare i int default 0; set autocommit = 0 ; repeat insert into dept values(dno_start+i ,randstring(6),randstring(8)) ; set i=i+1 ; until i=data_times end repeat ; commit ; end$ 1234--插入数据 delimiter ; call insert_emp(1000,800000) ; call insert_dept(10,30) ; ​ b.分析海量数据: ​ （1）profiles ​ show profiles ; --默认关闭 ​ show variables like '%profiling%'; ​ set profiling = on ; ​ show profiles ：会记录所有profiling打开之后的 全部SQL查询语句所花费的时间。缺点：不够精确，只能看到 总共消费的时间，不能看到各个硬件消费的时间（cpu io ） ​ ​ (2)精确分析:sql诊断 ​ show profile all for query 上一步查询的的Query_Id ​ show profile cpu,block io for query 上一步查询的的Query_Id ​ ​ (3)全局查询日志 ：记录开启之后的 全部SQL语句。 （这次全局的记录操作 仅仅在调优、开发过程中打开即可，在最终的部署实施时 一定关闭） ​ show variables like '%general_log%'; ​ 12345678--执行的所有SQL记录在表中set global general_log = 1 ;--开启全局日志set global log_output='table' ; --设置 将全部的SQL 记录在表中--执行的所有SQL记录在文件中set global log_output='file' ;set global general_log = on ;set global general_log_file='/tmp/general.log' ; 12开启后，会记录所有SQL ： 会被记录 mysql.general_log表中。 select * from mysql.general_log ; 锁机制详解 锁机制 ：解决因资源共享 而造成的并发问题。 示例：买最后一件衣服X A: X 买 ： X加锁 -&gt;试衣服...下单..付款..打包 -&gt;X解锁 B: X 买：发现X已被加锁，等待X解锁， X已售空 分类： 操作类型： a.读锁（共享锁）： 对同一个数据（衣服），多个读操作可以同时进行，互不干扰。 b.写锁（互斥锁）： 如果当前写操作没有完毕（买衣服的一系列操作），则无法进行其他的读操作、写操作 操作范围： a.表锁 ：一次性对一张表整体加锁。如MyISAM存储引擎使用表锁，开销小、加锁快；无死锁；但锁的范围大，容易发生锁冲突、并发度低。 b.行锁 ：一次性对一条数据加锁。如InnoDB存储引擎使用行锁，开销大，加锁慢；容易出现死锁；锁的范围较小，不易发生锁冲突，并发度高（很小概率 发生高并发问题：脏读、幻读、不可重复度、丢失更新等问题）。 c.页锁 示例： （1）表锁 ： --自增操作 MYSQL/SQLSERVER 支持；oracle需要借助于序列来实现自增 create table tablelock ( id int primary key auto_increment , name varchar(20) )engine myisam; insert into tablelock(name) values('a1'); insert into tablelock(name) values('a2'); insert into tablelock(name) values('a3'); insert into tablelock(name) values('a4'); insert into tablelock(name) values('a5'); commit; 1234567891011121314151617181920212223增加锁：locak table 表1 read/write ,表2 read/write ,...查看加锁的表：show open tables ;会话：session :每一个访问数据的dos命令行、数据库客户端工具 都是一个会话===加读锁： 会话0： lock table tablelock read ; select * from tablelock; --读（查），可以 delete from tablelock where id =1 ; --写（增删改），不可以 select * from emp ; --读，不可以 delete from emp where eid = 1; --写，不可以 结论1： --如果某一个会话 对A表加了read锁，则 该会话 可以对A表进行读操作、不能进行写操作； 且 该会话不能对其他表进行读、写操作。 --即如果给A表加了读锁，则当前会话只能对A表进行读操作。 会话1（其他会话）： select * from tablelock; --读（查），可以 delete from tablelock where id =1 ; --写，会“等待”会话0将锁释放 会话1（其他会话）： select * from emp ; --读（查），可以 delete from emp where eno = 1; --写，可以 结论2： --总结： 会话0给A表加了锁；其他会话的操作：a.可以对其他表（A表以外的表）进行读、写操作 b.对A表：读-可以； 写-需要等待释放锁。 释放锁: unlock tables ; 写锁示例与MyISAM模式特征 ===加写锁： 会话0： lock table tablelock write ; 当前会话（会话0） 可以对加了写锁的表 进行任何操作（增删改查）；但是不能 操作（增删改查）其他表 其他会话： 对会话0中加写锁的表 可以进行增删改查的前提是：等待会话0释放写锁 MySQL表级锁的锁模式 MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁， 在执行更新操作（DML）前，会自动给涉及的表加写锁。 所以对MyISAM表进行操作，会有以下情况： a、对MyISAM表的读操作（加读锁），不会阻塞其他进程（会话）对同一表的读请求， 但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。 b、对MyISAM表的写操作（加写锁），会阻塞其他进程（会话）对同一表的读和写操作， 只有当写锁释放后，才会执行其它进程的读写操作。 表锁情况分析及行锁解析 分析表锁定： 查看哪些表加了锁： show open tables ; 1代表被加了锁 分析表锁定的严重程度： show status like 'table%' ; Table_locks_immediate :立刻能获取到的锁数 Table_locks_waited：需要等待的表锁数(如果该值越大，说明存在越大的锁竞争) 一般建议： Table_locks_immediate/Table_locks_waited &gt; 5000， 建议采用InnoDB引擎，否则MyISAM引擎 ​ （2）行表（InnoDB） create table linelock( id int(5) primary key auto_increment, name varchar(20) )engine=innodb ; insert into linelock(name) values('1') ; insert into linelock(name) values('2') ; insert into linelock(name) values('3') ; insert into linelock(name) values('4') ; insert into linelock(name) values('5') ; --mysql默认自动commit; oracle默认不会自动commit ; 为了研究行锁，暂时将自动commit关闭; set autocommit =0 ; 以后需要通过commit 会话0： 写操作 insert into linelock values( 'a6') ; 会话1： 写操作 同样的数据 update linelock set name='ax' where id = 6; 对行锁情况： 1.如果会话x对某条数据a进行 DML操作（研究时：关闭了自动commit的情况下），则其他会话必须等待会话x结束事务(commit/rollback)后 才能对数据a进行操作。 2.表锁 是通过unlock tables，也可以通过事务解锁 ; 行锁 是通过事务解锁。 ​ 行锁，操作不同数据： 会话0： 写操作 insert into linelock values(8,'a8') ; 会话1： 写操作， 不同的数据 update linelock set name='ax' where id = 5; 行锁，一次锁一行数据；因此 如果操作的是不同数据，则不干扰。 行锁的注意事项及使用情况分析 ​ 行锁的注意事项： ​ a.如果没有索引，则行锁会转为表锁 ​ show index from linelock ; ​ alter table linelock add index idx_linelock_name(name); ​ 会话0： 写操作 ​ update linelock set name = 'ai' where name = '3' ; ​ ​ 会话1： 写操作， 不同的数据 ​ update linelock set name = 'aiX' where name = '4' ; ​ 会话0： 写操作 ​ update linelock set name = 'ai' where name = 3 ; ​ ​ 会话1： 写操作， 不同的数据 ​ update linelock set name = 'aiX' where name = 4 ; ​ ​ --可以发现，数据被阻塞了（加锁） ​ -- 原因：如果索引类 发生了类型转换，则索引失效。 因此 此次操作，会从行锁 转为表锁。 ​ ​ b.行锁的一种特殊情况：间隙锁：值在范围内，但却不存在 ​ --此时linelock表中 没有id=7的数据 ​ update linelock set name ='x' where id &gt;1 and id&lt;9 ; --即在此where范围中，没有id=7的数据，则id=7的数据成为间隙。 ​ 间隙：Mysql会自动给 间隙 加索 -&gt;间隙锁。即 本题 会自动给id=7的数据加 间隙锁（行锁）。 ​ 行锁：如果有where，则实际加索的范围 就是where后面的范围（不是实际的值） 查询行锁 ​ 如何仅仅是查询数据，能否加锁？ 可以 for update ​ 研究学习时，将自动提交关闭： ​ set autocommit =0 ; ​ start transaction ; ​ begin ; ​ select * from linelock where id =2 for update ; ​ ​ 通过for update对query语句进行加锁。 ​ ​ 行锁： ​ InnoDB默认采用行锁； ​ 缺点： 比表锁性能损耗大。 ​ 优点：并发能力强，效率高。 ​ 因此建议，高并发用InnoDB，否则用MyISAM。 ​ ​ 行锁分析： ​ show status like '%innodb_row_lock%' ; ​ Innodb_row_lock_current_waits :当前正在等待锁的数量 ​ Innodb_row_lock_time：等待总时长。从系统启到现在 一共等待的时间 ​ Innodb_row_lock_time_avg ：平均等待时长。从系统启到现在平均等待的时间 ​ Innodb_row_lock_time_max ：最大等待时长。从系统启到现在最大一次等待的时间 ​ Innodb_row_lock_waits ： 等待次数。从系统启到现在一共等待的次数 主从复制 ​ windows:mysql 主 ​ linux:mysql从 安装windows版mysql: 如果之前计算机中安装过Mysql，要重新再安装 则需要：先卸载 再安装 先卸载： 通过电脑自带卸载工具卸载Mysql (电脑管家也可以) 删除一个mysql缓存文件C:\\ProgramData\\MySQL 删除注册表regedit中所有mysql相关配置 --重启计算机 安装MYSQL： 安装时，如果出现未响应： 则重新打开D:\\MySQL\\MySQL Server 5.5\\bin\\MySQLInstanceConfig.exe 图形化客户端： SQLyog, Navicat 如果要远程连接数据库，则需要授权远程访问。 授权远程访问 :(A-&gt;B,则再B计算机的Mysql中执行以下命令) GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; FLUSH PRIVILEGES; 如果仍然报错：可能是防火墙没关闭 ： 在B关闭防火墙 service iptables stop 实现主从同步（主从复制）：图 1.master将改变的数 记录在本地的 二进制日志中（binary log） ；该过程 称之为：二进制日志件事 2.slave将master的binary log拷贝到自己的 relay log（中继日志文件）中 3.中继日志事件，将数据读取到自己的数据库之中 MYSQL主从复制 是异步的，串行化的， 有延迟 master:slave = 1:n 配置： windows(mysql: my.ini) linux(mysql: my.cnf) 配置前，为了无误，先将权限(远程访问)、防火墙等处理： 关闭windows/linux防火墙： windows：右键“网络” ,linux: service iptables stop Mysql允许远程连接(windowos/linux)： GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; FLUSH PRIVILEGES; 主机（以下代码和操作 全部在主机windows中操作）： my.ini [mysqld] #id server-id=1 #二进制日志文件（注意是/ 不是\\） log-bin=&quot;D:/MySQL/MySQL Server 5.5/data/mysql-bin&quot; #错误记录文件 log-error=&quot;D:/MySQL/MySQL Server 5.5/data/mysql-error&quot; #主从同步时 忽略的数据库 binlog-ignore-db=mysql #(可选)指定主从同步时，同步哪些数据库 binlog-do-db=test windows中的数据库 授权哪台计算机中的数据库 是自己的从数据库： GRANT REPLICATION slave,reload,super ON . TO 'root'@'192.168.2.%' IDENTIFIED BY 'root'; flush privileges ; 查看主数据库的状态（每次在左主从同步前，需要观察 主机状态的最新值） show master status; （mysql-bin.000001、 107） 从机（以下代码和操作 全部在从机linux中操作）： my.cnf [mysqld] server-id=2 log-bin=mysql-bin replicate-do-db=test linux中的数据 授权哪台计算机中的数控 是自己的主计算机 CHANGE MASTER TO MASTER_HOST = '192.168.2.2', MASTER_USER = 'root', MASTER_PASSWORD = 'root', MASTER_PORT = 3306, master_log_file='mysql-bin.000001', master_log_pos=107; 如果报错：This operation cannot be performed with a running slave; run STOP SLAVE first 解决：STOP SLAVE ;再次执行上条授权语句 开启主从同步： 从机linux: start slave ; 检验 show slave status \\G 主要观察： Slave_IO_Running和 Slave_SQL_Running，确保二者都是yes；如果不都是yes，则看下方的 Last_IO_Error。 本次 通过 Last_IO_Error发现错误的原因是 主从使用了相同的server-id， 检查:在主从中分别查看serverid: show variables like 'server_id' ; 可以发现，在Linux中的my.cnf中设置了server-id=2，但实际执行时 确实server-id=1，原因：可能是 linux版Mysql的一个bug，也可能是 windows和Linux版本不一致造成的兼容性问题。 解决改bug： set global server_id =2 ; stop slave ; set global server_id =2 ; start slave ; show slave status \\G 演示： 主windows =&gt;从 windows: 将表，插入数据 观察从数据库中该表的数据 数据库+后端 spring boot（企业级框架,目前使用较多） 命令集合 mysql -u root -p 登录 show engines; 显示支持引擎 show variables like ‘%storage_engine%’; 查看当前使用引擎 12345678-- 指定数据库引擎create table tb(​ id int(4) auto_increment,​ name varchar(5),​ dept varchar(5),​ primary key(id))ENGINE=MyISAM AUTO_INCREMENT=1DEFAULT CHARSET=utf-8 常用语句 数据库 1234567891011121314151617181920212223#1.查看已有库 show databases;#2.创建库(指定字符集) create database 库名 character set utf8 ；或 charset=utf8; create database 库名 default charset utf8 collate utf8_general_ci;#案例：创建stu数据库，编码为utf8 create database stu character set utf8; create database stu charset=utf8;#3.查看创建库的语句(字符集) show create database 库名;#案例：查看stu创建方法 show create database stu;#4.查看当前所在库 select database();#5.切换库 use 库名;#案例：使用stu数据库 use stu;#6.删除库 drop database 库名;#案例：删除test数据库 drop database test; #7.库名的命名规则##数字、字母、下划线,但不能使用纯数字##库名区分字母大小写##不能使用特殊字符和mysql关键字 表 12345678910111213141516#创建CREATE TABLE IF NOT EXISTS `表名`( `字段名` INT UNSIGNED AUTO_INCREMENT, `字段名` VARCHAR(100) NOT NULL, `字段名` VARCHAR(40) NOT NULL, `字段名` DATE, PRIMARY KEY ( `主键字段名` ))ENGINE=InnoDB DEFAULT CHARSET=utf8;#对创建后的表添加列alter table 表名 add column 新增列名 类型 not null DEFAULT 默认值 after 前一个列名#对创建表后添加主外键alter table 表名 add constraint primary key 表名（主键字段名）foreign key (外建名) references 外建字段所在表名 (外建字段名) on delete cascade; #on delete cascade 级联删除 数据 1234#增#删#改#查 连接 1234567#内连接 inner join on 交集#外连接##左连接 left join on / left outer join on 左表+右表符合条件数据，记录不足的地方为null##右连接 right join on / right outer join on 右表+左表符合条件数据，记录不足的地方为null#全连接 union /union all 索引 123456#创建索引create 索引类型 索引名 on 表名(字段名);alter table 表名 add 索引类型 索引名(字段名);#使用强制索引select * from 表名 FORCE INDEX (索引名) where 条件; 视图 1234567#创建视图create view 视图名称 as select语句;#--查看视图结构：desc user_view; show create view user_view;#--查看视图内容：select * from user_view; 游标 12345678910111213141516171819#定义DECLARE 光标名称 CURSOR FOR 查询语法declare cursor_name cursor for select_statement#打开OPEN 光标名称open cursor_name#取游标中的数据FETCH 光标名称 INFO var_name [，var_name ].....fetch cursor_name info var_name#关闭游标CLOSE curso_name;close 光标名称#释放游标DEALLOCATE 光标名称deallocate cursor_name; 触发器 1234567891011121314#创建##只有一个执行语句CREATE TRIGGER 触发器名 BEFORE|AFTER 触发事件 ON 表名 FOR EACH ROW 执行语句;##多个个执行语句CREATE TRIGGER 触发器名 BEFORE|AFTER 触发事件ON 表名 FOR EACH ROWBEGIN 执行语句列表END;#示例create TRIGGER audit_log after INSERT on employees_test for each row begin INSERT INTO audit VALUES (NEW.ID, NEW.NAME);end 其他 1234#查看表字段信息desc 表名#查看创建表sql（表所有信息）show create table 表名; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#输出格式date_format(date,&quot;%Y-%m&quot;) #插入存在忽略insert ignore存在忽略#去除重复distinct字段去重#分组group by 。。having##group by 在where 最后一个限制条件后面，不能在限制条件中间，HAVING是在分组后找到特定的分组。select user_id from order_info where date &gt; '2025-10-15' and product_name in ('C++','Java','Python') and status='completed' group by user_id HAVING count(user_id)&gt;1 order by user_id # in exists##in先内再外not in##EXISTS先外再内SELECT 字段 from 表名 where not EXISTS (select emp_no from dept_emp where employees.emp_no = dept_emp.emp_no)#连接,concat(last_name,'\\'',first_name)SELECT dept_no,GROUP_CONCAT(emp_no SEPARATOR ',') as employees from dept_emp group by dept_no#语法: group_concat([distinct] 字段 [order by 排序字段 asc/desc] separator '分隔符')#函数默认的分隔符是 &quot;,&quot;，默认可以省略#替换replaceupdate titles_test set emp_no=replace(emp_no,'10001','10005') where id=5#更换表名alter table titles_test rename to titles_2017round(avg(score),3)#正则匹配，替换，计算长度select id,length( regexp_replace(STRING, '[A-Z0-9]', '')) from strings#截取select 字段 from 表名 order by right/left(字段,2)substring(str,index)substring_index(str,delim,count)subdate(date,day)subtime(expr1,expr2)#分页select * from employees LIMIT 5 offset 5select * from employees LIMIT 5,5#窗口函数# 排序函数#mysql—排序函数rank() over()、dense_rank() over()、row_num() over()#条件相同排名相同， 间断不连续 rank() over() 跳过重复，比如 113446#条件相同排名相同， 间断连续 dense_rank() over() 不跳过重复，比如 112334#条件相同排名不相同，不间断连续 row_num() over() 依次相加，比如 123456select id,number,dense_rank() over(order by number desc) as 'rank' from passing_number #以grade排序，number按顺序以及grade分组累加#grade number#A 2#D 1#C 2#B 2select grade,sum(number) over(order by grade) t_rank from class_grade#时间##当前时间current_date/curdate() now()current_timestamp, current_timestamp()#格式转换 字符串转时间str_to_date('08/09/2008', '%m/%d/%Y'); -- 2008-08-09str_to_date('08/09/08' , '%m/%d/%y'); -- 2008-08-09str_to_date('08.09.2008', '%m.%d.%Y'); -- 2008-08-09str_to_date('08:09:30', '%h:%i:%s'); -- 08:09:30str_to_date('08.09.2008 08:09:30', '%m.%d.%Y %h:%i:%s'); -- 2008-08-09 08:09:30#时间增减#条件分支(case when 条件 then 语句 when 条件 then 语句 else 语句 end) as bonusselect e.emp_no,e.first_name,e.last_name,eb.btype,s.salary,(case when eb.btype = 1 then s.salary*0.1 when eb.btype= 2 then s.salary*0.2 else s.salary*0.3 end) as bonus from employees e INNER join emp_bonus eb on e.emp_no=eb.emp_no INNER join salaries s on e.emp_no=s.emp_no where s.to_date='9999-01-01'; 窗口函数 1.窗口函数语法 12&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt; order by &lt;用于排序的列名&gt;) &lt;窗口函数&gt;的位置，可以放以下两种函数： 1） 专用窗口函数，比如 序号函数 rank, dense_rank, row_number、 分布函数：PERCENT_RANK()、CUME_DIST()、PERCENT_RANK() 前后函数：LAG(expr,n)、LEAD(expr,n) 前后函数：LAG(expr,n)、LEAD(expr,n) 其它函数：NTH_VALUE(expr, n)、NTILE(n)、NTH_VALUE(expr,n) 2） 聚合函数，如sum. avg, count, max, min等 2.窗口函数有以下功能： 1）同时具有分组（partition by）和排序（order by）的功能 2）不减少原表的行数，所以经常用来在每组内排名 3.注意事项 窗口函数原则上只能写在select子句中 4.窗口函数使用场景 1）业务需求“在每组内排名” mysql截取字符串函数 1、left(str,length) 从左边截取length 2、right(str,length)从右边截取length 3、substring(str,index)当index&gt;0从左边开始截取直到结束 当index&lt;0从右边开始截取直到结束 当index=0返回空 4、substring(str,index,len) 截取str,从index开始，截取len长度 5、substring_index(str,delim,count)，str是要截取的字符串，delim是截取的字段 count是从哪里开始截取(为0则是左边第0个开始，1位左边开始第一个选取左边的，-1从右边第一个开始选取右边的 6、subdate(date,day)截取时间，时间减去后面的day 7、subtime(expr1,expr2) 时分秒expr1-expr2 8、mysql索引从1开始 全文索引： 是目前搜索引擎使用的一种关键技术。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 查询数据库中表信息 123456SELECT *FROM information_schema.`TABLES`WHERE TABLE_SCHEMA = 'hz_hbzy'; Postgres 字段映射 Postgres与Mysql不同 查询字段备注 12select a.attnum,a.attname,concat_ws('',t.typname,SUBSTRING(format_type(a.atttypid,a.atttypmod) from '\\(.*\\)')) as type,d.description from pg_class c,pg_attribute a,pg_type t,pg_description dwhere c.relname='com_capture' and a.attnum&gt;0 and a.attrelid=c.oid and a.atttypid=t.oid and d.objoid=a.attrelid and d.objsubid=a.attnum; 取geom数据 12select st_astext(geom) from ttt # test形式SELECT st_asgeojson(geom) FROM &quot;ygjcgd&quot; # geojson形式 存geom 12点INSERT into ttt values ('1',ST_GeomFromText('POINT(119.104393005 29.3091735840001)',4490))面INSERT into ttt values ('2',ST_GeomFromText('POLYGON((120.489420304 30.2855081450001,120.48927819 30.2855092200001,120.489034534 30.2855205130001,120.488415416 30.2855540510001,120.488238647 30.285563627,120.487775051 30.2856122420001,120.487543209 30.2856365540001,120.48696129 30.28571928,120.486939727 30.285722842,120.486661766 30.2857687420001,120.48676176 30.2859599880001,120.486786439 30.2860551310001,120.486803289 30.286632664,120.487353662 30.2866268860001,120.487351682 30.2864715210001,120.487351632 30.286467605,120.487350721 30.2863964210001,120.487346937 30.2860998030001,120.487342737 30.285770433,120.487400427 30.2857633680001,120.487405079 30.285760184,120.48740623 30.285961001,120.487409165 30.2864727900001,120.487409564 30.286542108,120.487409728 30.2865708910001,120.487410051 30.2866269530001,120.487415064 30.2866269360001,120.487781689 30.2866291820001,120.488148313 30.286631426,120.488229104 30.286631782,120.488646675 30.2866336280001,120.488672652 30.2866335310001,120.489108909 30.286635501,120.489545164 30.286637469,120.489554908 30.2866377670001,120.489587466 30.2866073650001,120.489631665 30.286574779,120.489633218 30.2865468840001,120.489635952 30.286497766,120.489630193 30.2864750590001,120.489620367 30.286436291,120.489576415 30.2862629480001,120.489538252 30.286111927,120.489518579 30.286034075,120.489490667 30.2859208350001,120.489445946 30.285699532,120.489442796 30.285683949,120.489435137 30.2856060400001,120.489420304 30.2855081450001))',4490)) 添加geom扩展 12345CREATE EXTENSION postgis;CREATE EXTENSION fuzzystrmatch;CREATE EXTENSION postgis_tiger_geocoder;CREATE EXTENSION address_standardizer;CREATE EXTENSION postgis_topology; PostgREST 服务发布 常用函数 图形和地理位置 ST_GeometryType(geometry) —— 返回几何图形的类型 ST_Transform(geometry, srid)——将几何图形投影为地理坐标数据 或 转换为不同srid坐标系统的坐标数据 Geography(geometry)——将基于EPSG:4326(srid=4326)的geometry数据类型转换为geography数据类型 ST_NDims(geometry) —— 返回几何图形的维数 ST_SRID(geometry) —— 返回几何图形的空间参考标识码 srid ST_SetSRID(geometry，SRID）——设置srid sum(expression) ——返回一个计算式/表达式的和 count(expression) ——返回一个表达式中的次数 PS : geometry，是几何类型的列的列名 srid，不同的srid就是不同标准的坐标系 点空间函数： ST_X(geometry) —— 返回X坐标 ST_Y(geometry) —— 返回Y坐标 线串空间函数： ST_Length(geometry) —— 返回线串的长度 ST_StartPoint(geometry) —— 将线串的第一个坐标作为点返回 ST_EndPoint(geometry） —— 将线串的最后一个坐标作为点返回 ST_NPoints(geometry) —— 返回线串的坐标数量 多边形空间函数： ST_Area(geometry) —— 返回多边形的面积 ST_NRings(geometry) —— 返回多边形中环的数量（通常为1个，其他是孔） ST_ExteriorRing(geometry) —— 以线串的形式返回多边形最外面的环 ST_InteriorRingN(geometry, n) —— 以线串形式返回指定的内部环 ST_Perimeter(geometry) —— 返回所有环的长度 集合空间函数(多点、多线、多面、任意图形组合)： ST_NumGeometries(geometry) —— 返回集合中的组成部分的数量 ST_GeometryN(geometry, n) —— 返回集合中指定的组成部分 ST_Area(geometry) —— 返回集合中所有多边形组成部分的总面积 ST_Length(geometry) —— 返回所有线段组成部分的总长度 几何图形输入和输出 在数据库中，几何图形（Geometry）以仅供PostGIS使用的格式存储在磁盘上。为了让外部程序插入和检索有用的几何图形信息，需要将它们转换为其他应用程序可以理解的格式。 ①Well-known text（WKT） ST_GeomFromText(text, srid) —— 返回geometry，除非指定SRID，否则将得到一个包含未知SRID的几何图形 ST_GeographyFromText(text)——返回Geography ST_AsText(geometry) —— 返回text ST_AsEWKT(geometry) —— 返回text ②Well-known binary（WKB） ST_GeomFromWKB(bytea) —— 返回geometry ST_AsBinary(geometry) —— 返回bytea ST_AsEWKB(geometry) —— 返回bytea ③Geographic Mark-up Language（GML） ST_GeomFromGML(text) —— 返回geometry ST_ASGML(geometry) —— 返回text ④Keyhole Mark-up Language（KML） ST_GeomFromKML(text) —— 返回geometry ST_ASKML(geometry) —— 返回text ⑤GeoJson ST_AsGeoJSON(geometry) —— 返回text ⑥Scalable Vector Graphics(SVG） ST_AsSVG(geometry) —— 返回text 以上函数最常见的用法是将几何图形的文本（text）表示形式转换为内部表示形式 请注意，除了具有几何图形表示形式的文本参数外，还可以指定一个提供几何图形SRID的数字参数。 图形关系 ST_Equals(geometry A, geometry B) 用于测试两个图形的空间相等性。 如果两个相同类型的几何图形具有相同的x、y坐标值，即如果第二个图形与第一个图形的坐标信息相等（相同），则ST_Equals()返回TRUE。 ST_Intersects、ST_Disjoint、ST_Crosses和ST_Overlaps ST_Intersects、ST_Crosses和ST_Overlaps测试几何图形是否相交。 如果两个图形有重合的部分，即如果它们的边界或内部相交，则**ST_Intersects(geometry A, geometry B)**返回TRUE ST_Disjoint(geometry A, geometry B)，如果两个几何图形没有重合的部分，则它们不相交，反之亦然。事实上测试&quot;not intersect&quot;通常比测试&quot;disjoint&quot;更有效，因为intersect测试可以使用空间索引 对于multipoint/polygon、multipoint/linestring、linestring/linestring、linestring/polygon和linestring/multipolygon的比较，如果相交生成的几何图形的维度小于两个源几何图形的最大维度，且相交集位于两个源几何图形的内部，则**ST_Crosses(geometry A, geometry B)**将返回TRUE。 ST_Overlaps(geometry A, geometry B)比较两个相同维度的几何图形，如果它们的结果集与两个源几何图形都不同但具有相同维度，则返回TRUE。 ST_Touches() 测试两个几何图形是否在它们的边界上接触，但在它们的内部不相交 如果两个几何图形的边界相交，或者只有一个几何图形的内部与另一个几何图形的边界相交，则**ST_Touches(geometry A, geometry B)**将返回TRUE ST_Within和ST_Contains ST_Within()和ST_Contains()测试一个几何图形是否完全位于另一个几何图形内 如果第一个几何图形完全位于第二个几何图形内，则ST_Within(geometry A, geometry B)返回TRUE，ST_Within()测试的结果与ST_Contains()完全相反 如果第二个几何图形完全包含在第一个几何图形内，则ST_Contains(geometry A, geometry B)返回TRUE ST_Distance和ST_DWithin **ST_Distance(geometry A, geometry B)**计算两个几何图形之间的最短距离，并将其作为浮点数返回。这对于实际报告几何图形之间的距离非常有用 ST_DWithin()，测试两个几何图形之间的距离是否在某个范围之内， geography类型 ST_AsText(geography) returns text ST_GeographyFromText(text) returns geography ST_AsBinary(geography) returns bytea ST_GeogFromWKB(bytea) returns geography ST_AsSVG(geography) returns text ST_AsGML(geography) returns text ST_AsKML(geography) returns text ST_AsGeoJson(geography) returns text ST_Distance(geography, geography) returns double ST_DWithin(geography, geography, float8) returns boolean ST_Area(geography) returns double ST_Length(geography) returns double ST_Covers(geography, geography) returns boolean ST_CoveredBy(geography, geography) returns boolean ST_Intersects(geography, geography) returns boolean ST_Buffer(geography, float8) returns geography[1] ST_Intersection(geography, geography) returns geography[1] geography转换为geometry PostgreSQL的类型转换语法是将 ::typename 附加到希望转换的值的末尾。因此，2::text将数字2转换为文本字符串&quot;2&quot;；'POINT(0 0)' :: geometry将点的文本表示形式转换为geometry点 上好例题 时间格式，截取匹配，分组，排序 1select job,date_format(date,&quot;%Y-%m&quot;) as mon,sum(num) as cnt from resume_info where left(date,4)= '2025' group by job,mon order by mon desc,cnt desc 单输入，多字段查询 123&lt;if test=&quot;text!=null and text!=''&quot;&gt; and CONCAT(t.city,t.bh,t.cun,t.jsztmc,t.xmxmmc) like concat ('%',#{text},'%')&lt;/if&gt; 有没有进行过分库分表操作？分库之后如何保持事务一致？ Java字段映射 全字段详表 数值 参考 类型 大小 范围（有符号） 范围（无符号UNSIGNED） 用途 TINYINT 1 Bytes (-128，127) (0，255) 小整数值 SMALLINT 2 Bytes (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 Bytes (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 Bytes (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 Bytes (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 Bytes (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 DOUBLE 8 Bytes (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 时间 类型 大小 ( bytes) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 '-838:59:59'/'838:59:59' HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串 类型 大小 用途 CHAR 0-255 bytes 定长字符串 VARCHAR 0-65535 bytes 变长字符串 TINYBLOB 0-255 bytes 不超过 255 个字符的二进制字符串 TINYTEXT 0-255 bytes 短文本字符串 BLOB 0-65 535 bytes 二进制形式的长文本数据 TEXT 0-65 535 bytes 长文本数据 MEDIUMBLOB 0-16 777 215 bytes 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215 bytes 中等长度文本数据 LONGBLOB 0-4 294 967 295 bytes 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295 bytes 极大文本数据 1bit 位 1字节=8bit 1k=1024字节 1兆=1024k 1G=1023M 1T=1024G 常用Java映射 Mysql Java 包 字段名 备注 datetime Date 数据库设计 名称 **表是否：**is_xxx 类型：任何字段非负数必须是 unsigned tinyint (1是 0否) POJO中需要从is_xxx 去掉is前缀映射到 xxx **表名、字段名：**全小写【windows不区分大小写，linux区分】、非数字开头、两个下划线中不能只是数字、表名不使用复数。不使用保留字。表名称建议：业务名称_表的作用。 **索引名：**主键索引：pk_xxx、唯一索引名：uk_xxx、普通索引：idx_xxx **小数：**decimal （超过其范围建议拆成整数和小数分开存储），禁用float、double 【精度问题】 **字符串：**数据长度几乎相等用char定长，vachar为可变长字符串，长度超过5000 时 使用text类型独立表存储 **表单必备三字段：**id（bigint unsigned）无符号即正值范围更大、gmt_create / gmt_modified(datetime,前者现在时表示主动式创建，后者过去分词表示被动式更新) **库名：**建议同应用名 **备注：**修改字段含义与状态添加时及时更新备注。 **其他：**字段适当长度冗余，单标数据超过500万行或容量超过2GB才建议分库分表。三年的时间估计。 索引 具有唯一特性的字段，即使是组合字段，也必须建成唯一索引 超过三个表禁止join。需要join的字段，数据类型保持绝对一致；多表关联查询时，保证被关联的字段需要有索引。 在varchar字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度 页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。 如果有order by的场景，请注意利用索引的有序性。order by 最后的字段是组合索引的一部分，并且放在索引组合顺序的最后，避免出现file_sort的情况，影响查询性能。 利用覆盖索引来进行查询操作，避免回表。 利用延迟关联或者子查询优化超多分页场景。 SQL性能优化的目标：至少要达到 range 级别，要求是ref级别，如果可以是consts最好。 建组合索引的时候，区分度最高的在最左边。 防止因字段类型不同造成的隐式转换，导致索引失效。 语句 不要使用count(列名)或count(常量)来替代count()，count()是SQL92定义的标准统计行数的语法，跟数据库无关，跟NULL和非NULL无关。 说明：count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。 】count(distinct col) 计算该列除NULL之外的不重复行数，注意 count(distinct col1, col2) 如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。 当某一列的值全是NULL时，count(col)的返回结果为0，但sum(col)的返回结果为NULL，因此使用sum()时需注意NPE问题。 正例：可以使用如下方式来避免sum的NPE问题：SELECT IFNULL(SUM(column), 0) FROM table; 使用ISNULL()来判断是否为NULL值。 说明：NULL与任何值的直接比较都为NULL。 1） NULL&lt;&gt;NULL的返回结果是NULL，而不是false。 2） NULL=NULL的返回结果是NULL，而不是true。 3） NULL&lt;&gt;1的返回结果是NULL，而不是true。 代码中写分页查询逻辑时，若count为0应直接返回，避免执行后面的分页语句。 不得使用外键与级联，一切外键概念必须在应用层解决。 禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。 数据订正（特别是删除或修改记录操作）时，要先select，避免出现误删除，确认无误才能执行更新语句。 对于数据库中表记录的查询和变更，只要涉及多个表，都需要在列名前加表的别名（或表名）进行限定，否则同名字段存在多表中会报错。别名前加as使别名更容易识别。 n操作能避免则避免，若实在避免不了，需要仔细评估in后边的集合元素数量，控制在1000个之内。 ORM映射 一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。因为会 1）增加查询分析器解析成本。2）增减字段容易与resultMap配置不一致。3）无用字段增加网络消耗，尤其是text类型的字段。 POJO类的布尔属性不能加is，而数据库字段必须加is_，要求在resultMap中进行字段与属性之间的映射。 不要用resultClass当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义；反过来，每一个表也必然有一个与之对应。配置映射关系，使字段与DO类解耦，方便维护。 sql.xml配置参数使用：#{}，#param# 不要使用${} 此种方式容易出现SQL注入 iBATIS自带的queryForList(String statementName,int start,int size)不推荐使用。 不允许直接拿HashMap与Hashtable作为查询结果集的输出。 更新数据表记录时，必须同时更新记录对应的gmt_modified字段值为当前时间。 执行SQL时，不要更新无改动的字段，一是易出错；二是效率低；三是增加binlog存储 @Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。 中的compareValue是与属性值对比的常量，一般是数字，表示相等时带上此条件；表示不为空且不为null时执行；表示不为null值时执行。 主键UUID or INT 自增？ 阿里规范摘要 数据库字段设计及对应关系 数据库三原则 数据库设计工具 如何设计数据库 应用 分库分表Mycat 阿里巴巴规范手册：【推荐】单表行数超过500万行或者单表容量超过2GB，才推荐进行分库分表。 说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表。","link":"/2021/02/24/Draft/2021/MYSQL%E4%BC%98%E5%8C%96/"},{"title":"自媒体","text":"自媒体经验、蓝图 2022-GOALS 【GISFSDE】-GIS Full Stack Developer. GIS开发，JAVA全栈开发，软件工程，项目管理知识分享。 个人博客：gisfsde.com 引流输出 核心产品（笔记等有价值物） 摄影摄像笔记 确定IP 七牛图床 域名确定 博客框架确定（自写还是他人框架） 模仿笔记 2021-GOALS 前端栏目 Java栏目 Python栏目 每日算法栏目 设计模式栏目 网络知识栏目 音乐乐理栏目 职场经验栏目 艺术欣赏栏目 设计栏目 工具栏目 语言栏目 自媒体矩阵与分析 受众、方向、 抖音【艺术，生活，剪辑，Vlog】短视频 VUE【Vlog】长视频 bilibili【合集】长视频 头条【知识，新闻】 知乎【生活、知识】 CSDN【专业知识】 微博【宣传】 简书【文字】 小红书【时尚、生活】 豆瓣【记录艺术】 公众号【输出】 核心方向 技术分享、交流、生活分享 自媒体知识 排版 引流 变现 自媒体经验","link":"/2021/02/09/Draft/2021/%E8%87%AA%E5%AA%92%E4%BD%93/"},{"title":"","text":"title: Draft top: 0 toc: true recommend: 1 keywords: categories-Draft date: 2021-01-20 22:10:43 thumbnail: http://rcy276gfy.hd-bkt.clouddn.com/webp.png tags: Draft categories: [Draft,Draft] #以下为文章加密信息 #encrypt: true #password: 123456 #此处为文章密码 #abstract: 咦，这是一篇加密文章，好像需要输入密码才能查看呢！ #message: 嗨，请准确无误地输入密码查看哟！ #wrong_pass_message: 不好意思，密码没对哦，在检查检查呢！ #wrong_hash_message: 不好意思，信息无法验证！ 介绍","link":"/2022/05/22/Draft/2021/Draft/"},{"title":"MyBatis-Plus","text":"MyBatis-Plus 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度：上篇 【】 快查 引用：","link":"/2021/10/20/Draft/2021/MyBatis-Plus/"},{"title":"思维闪光","text":"闪现的灵感记录。 碎片 一个知识口袋，每天一个问题和答案，并进行相关衍生。 众文 闪光：公众文档，共同编写一个笔记，每人可上交笔记完善，通过投票决定是否修改，以此不再每个人都做相同的笔记，多人完善同一个笔记。 技术： 首页 现在的首页都丑，书签收藏，动态排名，书签分类，TODO组件，极美UI","link":"/2021/07/20/Draft/2021/%E9%97%AA%E5%85%89/"},{"title":"Python 学习","text":"Python3学习 简介 Python is powerful... and fast; plays well with others; runs everywhere; is friendly &amp; easy to learn; is Open. Python 发展历史 Python 特点 Python 应用 工具 开发平台：PYCharm 语法基础 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719'''=============基本语法============='''import keyword# from modname import name1[, name2[, ... nameN]]# Python 提供了一个办法，把这些定义存放在文件中，为一些脚本或者交互式的解释器实例使用，这个文件被称为模块。# 模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py。模块可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 python 标准库的方法。# 从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc# ====__name__属性====# 一个模块被另一个程序第一次引入时，其主程序将运行。如果我们想在模块被引入时，模块中的某一程序块不执行，我们可以用__name__属性来使该程序块仅在该模块自身运行时执行。# #!/usr/bin/python3# # Filename: using_name.py# if __name__ == '__main__':# print('程序自身在运行')# else:# print('我来自另一模块')# ====dir() 函数====# 内置的函数 dir() 可以找到模块内定义的所有名称。以一个字符串列表的形式返回:'''输入和输出'''# Python两种输出值的方式: 表达式语句和 print() 函数。# 第三种方式是使用文件对象的 write() 方法，标准输出文件可以用 sys.stdout 引用。# 如果你希望输出的形式更加多样，可以使用 str.format() 函数来格式化输出值。# 如果你希望将输出的值转成字符串，可以使用 repr() 或 str() 函数来实现。# str()： 函数返回一个用户易读的表达形式。# repr()： 产生一个解释器易读的表达形式。print(&quot;Hello word!&quot;)print('{}网址： &quot;{}!&quot;'.format('菜鸟教程', 'www.runoob.com'))print('{0} 和 {1}'.format('Google', 'Runoob'))print('站点列表 {0}, {1}, 和 {other}。'.format('Google', 'Runoob', other='Taobao'))# 读取键盘输入str = input(&quot;请输入：&quot;);print (&quot;你输入的内容是: &quot;, str)# 读和写文件# open(filename, mode)# filename：包含了你要访问的文件名称的字符串值。# mode：决定了打开文件的模式：只读，写入，追加等。所有可取值见如下的完全列表。这个参数是非强制的，默认文件访问模式为只读(r)。# 模式 描述# r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。# rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。# r+ 打开一个文件用于读写。文件指针将会放在文件的开头。# rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。# w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。# ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。# 打开一个文件f = open(&quot;/tmp/foo.txt&quot;, &quot;w&quot;)f.write( &quot;Python 是一个非常好的语言。\\n是的，的确非常好!!\\n&quot; )# f.read(size), 这将读取一定数目的数据, 然后作为字符串或字节对象返回。size 是一个可选的数字类型的参数。 当 size 被忽略了或者为负, 那么该文件的所有内容都将被读取并且返回。print(f.read())# 返回一个空字符串, 说明已经已经读取到最后一行。print(f.readline())# 返回所有行。print(f.readlines())# 将 string 写入到文件中, 然后返回写入的字符数。f.write(&quot;string&quot;)# 返回文件对象当前所处的位置, 它是从文件开头开始算起的字节数。f.tell()# 改变文件当前的位置, 可以使用 f.seek(offset, from_what) 函数。# from_what 的值, 如果是 0 表示开头, 如果是 1 表示当前位置, 2 表示文件的结尾，例如：# seek(x,0) ： 从起始位置即文件首行首字符开始移动 x 个字符# seek(x,1) ： 表示从当前位置往后移动x个字符# seek(-x,2)：表示从文件的结尾往前移动x个字符# from_what 值为默认为0，即文件开头# 关闭打开的文件f.close()# pickle 模块 实现了基本的数据序列和反序列化。import pickle# 使用pickle模块将数据对象保存到文件data1 = {'a': [1, 2.0, 3, 4+6j], 'b': ('string', u'Unicode string'), 'c': None}selfref_list = [1, 2, 3]selfref_list.append(selfref_list)output = open('data.pkl', 'wb')# Pickle dictionary using protocol 0.pickle.dump(data1, output)# Pickle the list using the highest protocol available.pickle.dump(selfref_list, output, -1)output.close()''' 注释 '''&quot;&quot;&quot;注释&quot;&quot;&quot;# 保留字 关键字end可以用于将结果输出到同一行，或者在输出的末尾添加不同的字符print(b, end=',')print(&quot;保留字&quot;, keyword.kwlist)# 缩进的空格数是可变的，但是同一个代码块的语句必须包含相同的缩进空格数if True: print(&quot;ture&quot;)else: print('false')# 多行语句，语句很长，我们可以使用反斜杠(\\)来实现多行语句，在 [], {}, 或 () 中的多行语句，不需要使用反斜杠(\\)total = 'item_one' + \\ 'item_two' + \\ 'item_three'total1 = ['item_one', 'item_two', 'item_three', 'item_four', 'item_five']print(total)print(total1)''' ================基本数据类型===================Python3 中有六个标准的数据类型：Number（数字）：int、float、bool、complex（复数）String（字符串）List（列表）Tuple（元组）Set（集合）Dictionary（字典）Python3 的六个标准数据类型中：不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 type()查看数据类型，isinstance(a, int)进行判断'''# Number（数字）------------------------------------------------------------------------# int(x) 将x转换为一个整数。# float(x) 将x转换到一个浮点数。# complex(x) 将x转换到一个复数，实数部分为 x，虚数部分为 0。# complex(x, y) 将 x 和 y 转换到一个复数，实数部分为 x，虚数部分为 y。x 和 y 是数字表达式。# ==数学函数==# 函数 返回值 ( 描述 )# abs(x) 返回数字的绝对值，如abs(-10) 返回 10# ceil(x) 返回数字的上入整数，如math.ceil(4.1) 返回 5# cmp(x, y) 如果 x &lt; y 返回 -1, 如果 x == y 返回 0, 如果 x &gt; y 返回 1。 Python 3 已废弃，使用 (x&gt;y)-(x&lt;y) 替换。# exp(x) 返回e的x次幂(ex),如math.exp(1) 返回2.718281828459045# fabs(x) 返回数字的绝对值，如math.fabs(-10) 返回10.0# floor(x) 返回数字的下舍整数，如math.floor(4.9)返回 4# log(x) 如math.log(math.e)返回1.0,math.log(100,10)返回2.0# log10(x) 返回以10为基数的x的对数，如math.log10(100)返回 2.0# max(x1, x2,...) 返回给定参数的最大值，参数可以为序列。# min(x1, x2,...) 返回给定参数的最小值，参数可以为序列。# modf(x) 返回x的整数部分与小数部分，两部分的数值符号与x相同，整数部分以浮点型表示。# pow(x, y) x**y 运算后的值。# round(x [,n]) 返回浮点数 x 的四舍五入值，如给出 n 值，则代表舍入到小数点后的位数。其实准确的说是保留值将保留到离上一位更近的一端。# sqrt(x) 返回数字x的平方根。# ==随机数函数==# choice(seq) 从序列的元素中随机挑选一个元素，比如random.choice(range(10))，从0到9中随机挑选一个整数。# randrange ([start,] stop [,step]) 从指定范围内，按指定基数递增的集合中获取一个随机数，基数默认值为 1# random() 随机生成下一个实数，它在[0,1)范围内。# seed([x]) 改变随机数生成器的种子seed。如果你不了解其原理，你不必特别去设定seed，Python会帮你选择seed。# shuffle(lst) 将序列的所有元素随机排序# uniform(x, y) 随机生成下一个实数，它在[x,y]范围内# ==三角函数==# Python包括以下三角函数：# 函数 描述# acos(x) 返回x的反余弦弧度值。# asin(x) 返回x的反正弦弧度值。# atan(x) 返回x的反正切弧度值。# atan2(y, x) 返回给定的 X 及 Y 坐标值的反正切值。# cos(x) 返回x的弧度的余弦值。# hypot(x, y) 返回欧几里德范数 sqrt(x*x + y*y)。# sin(x) 返回的x弧度的正弦值。# tan(x) 返回x弧度的正切值。# degrees(x) 将弧度转换为角度,如degrees(math.pi/2) ， 返回90.0# radians(x) 将角度转换为弧度# ==数学常量==# 常量 描述# pi 数学常量 pi（圆周率，一般以π来表示）# e 数学常量 e，e即自然常数（自然常数）。a = b = c = counter = 100 # 同时为多个变量赋值整型变量d, e, f = 1, 2, &quot;runoob&quot; # 同时为多个变量赋值不同的值miles = 1000.0 # 浮点型变量name = &quot;runoob&quot; # 字符串# del语句删除一些对象引用del a# 数值运算Division = 2 / 4 # 除法，得到一个浮点数Division2 = 2 // 4 # 除法，得到一个整数remainder = 17 % 3 # 取余involution = 2 ** 5 # 乘方print(Division, 'Division')print(Division2, 'Division2')# String（字符串）------------------------------------------------------------------------------# 从后面索引01234567# 从前面索引-8-7-6-5-4-3-2-1# Good boy# 从前面截取:123456:# 从后面截取:-6-5-4-3-2-1:# 反斜杠(\\)可以作为续行符，表示下一行是上一行的延续。也可以使用 &quot;&quot;&quot;...&quot;&quot;&quot; 或者 '''...''' 跨越多行# 用+运算符连接在一起，用*运算符重复# ==转义字符==# \\(在行尾时) 续行符# &gt;&gt;&gt; print(&quot;line1 \\# ... line2 \\# ... line3&quot;)# line1 line2 line3# \\\\ 反斜杠符号# &gt;&gt;&gt; print(&quot;\\\\&quot;)# \\# \\' 单引号# &gt;&gt;&gt; print('\\'')# '# \\&quot; 双引号# &gt;&gt;&gt; print(&quot;\\&quot;&quot;)# &quot;# \\a 响铃# &gt;&gt;&gt; print(&quot;\\a&quot;)执行后电脑有响声。# \\b 退格(Backspace)# &gt;&gt;&gt; print(&quot;Hello \\b World!&quot;)# Hello World!# \\000 空# &gt;&gt;&gt; print(&quot;\\000&quot;)## &gt;&gt;&gt;# \\n 换行# &gt;&gt;&gt; print(&quot;\\n&quot;)## &gt;&gt;&gt;# \\v 纵向制表符# &gt;&gt;&gt; print(&quot;Hello \\v World!&quot;)# Hello# World!# &gt;&gt;&gt;# \\t 横向制表符# &gt;&gt;&gt; print(&quot;Hello \\t World!&quot;)# Hello World!# &gt;&gt;&gt;# \\r 回车，将 \\r 后面的内容移到字符串开头，并逐一替换开头部分的字符，直至将 \\r 后面的内容完全替换完成。# &gt;&gt;&gt; print(&quot;Hello\\rWorld!&quot;)# World!# &gt;&gt;&gt; print('google runoob taobao\\r123456')# 123456 runoob taobao# \\f 换页# &gt;&gt;&gt; print(&quot;Hello \\f World!&quot;)# Hello# World!# &gt;&gt;&gt;# \\yyy 八进制数，y 代表 0~7 的字符，例如：\\012 代表换行。# &gt;&gt;&gt; print(&quot;\\110\\145\\154\\154\\157\\40\\127\\157\\162\\154\\144\\41&quot;)# Hello World!# \\xyy 十六进制数，以 \\x 开头，y 代表的字符，例如：\\x0a 代表换行# &gt;&gt;&gt; print(&quot;\\x48\\x65\\x6c\\x6c\\x6f\\x20\\x57\\x6f\\x72\\x6c\\x64\\x21&quot;)# Hello World!# \\other 其它的字符以普通格式输出# ==字符串运算符==# + 字符串连接 a + b 输出结果： HelloPython# * 重复输出字符串 a*2 输出结果：HelloHello# [] 通过索引获取字符串中字符 a[1] 输出结果 e# [ : ] 截取字符串中的一部分，遵循左闭右开原则，str[0:2] 是不包含第 3 个字符的。 a[1:4] 输出结果 ell# in 成员运算符 - 如果字符串中包含给定的字符返回 True 'H' in a 输出结果 True# not in 成员运算符 - 如果字符串中不包含给定的字符返回 True 'M' not in a 输出结果 True# r/R 原始字符串 - 原始字符串：所有的字符串都是直接按照字面的意思来使用，没有转义特殊或不能打印的字符。 原始字符串除在字符串的第一个引号前加上字母 r（可以大小写）以外，与普通字符串有着几乎完全相同的语法。 print( r'\\n' ) print( R'\\n' )# % 格式字符串# ==字符串格式化==# % c 格式化字符及其ASCII码# % s 格式化字符串# % d 格式化整数# % u 格式化无符号整型# % o 格式化无符号八进制数# % x 格式化无符号十六进制数# % X 格式化无符号十六进制数（大写）# % f 格式化浮点数字，可指定小数点后的精度# % e 用科学计数法格式化浮点数# % E 作用同 % e，用科学计数法格式化浮点数# % g % f和 % e的简写# % G % f 和 % E 的简写# % p 用十六进制数格式化变量的地址# 格式化操作符辅助指令:# 符号 功能# * 定义宽度或者小数点精度# - 用做左对齐# + 在正数前面显示加号( + )# &lt;sp&gt; 在正数前面显示空格# # 在八进制数前面显示零('0')，在十六进制前面显示'0x'或者'0X'(取决于用的是'x'还是'X')# 0 显示的数字前面填充'0'而不是默认的空格# % '%%'输出一个单一的'%'# (var) 映射变量(字典参数)# m.n. m 是显示的最小总宽度,n 是小数点后的位数(如果可用的话)# ==f-string==name = 'Runoob'f'Hello {name}' # 替换变量'Hello Runoob'f'{1 + 2}' # 使用表达式'3'w = {'name': 'Runoob', 'url': 'www.runoob.com'}f'{w[&quot;name&quot;]}: {w[&quot;url&quot;]}''Runoob: www.runoob.com'para_str = &quot;&quot;&quot;这是一个多行字符串的实例多行字符串可以使用制表符TAB ( \\t )。也可以使用换行符 [ \\n ]。&quot;&quot;&quot;# ==字符串内建函数==# capitalize()# 将字符串的第一个字符转换为大写# center(width, fillchar)# 返回一个指定的宽度 width 居中的字符串，fillchar 为填充的字符，默认为空格。# count(str, beg= 0,end=len(string))# 返回 str 在 string 里面出现的次数，如果 beg 或者 end 指定则返回指定范围内 str 出现的次数# bytes.decode(encoding=&quot;utf-8&quot;, errors=&quot;strict&quot;)# Python3 中没有 decode 方法，但我们可以使用 bytes 对象的 decode() 方法来解码给定的 bytes 对象，这个 bytes 对象可以由 str.encode() 来编码返回。# encode(encoding='UTF-8',errors='strict')# 以 encoding 指定的编码格式编码字符串，如果出错默认报一个ValueError 的异常，除非 errors 指定的是'ignore'或者'replace'# endswith(suffix, beg=0, end=len(string))# 检查字符串是否以 obj 结束，如果beg 或者 end 指定则检查指定的范围内是否以 obj 结束，如果是，返回 True,否则返回 False.# expandtabs(tabsize=8)# 把字符串 string 中的 tab 符号转为空格，tab 符号默认的空格数是 8 。# find(str, beg=0, end=len(string))# 检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含返回开始的索引值，否则返回-1# index(str, beg=0, end=len(string))# 跟find()方法一样，只不过如果str不在字符串中会报一个异常。# isalnum()# 如果字符串至少有一个字符并且所有字符都是字母或数字则返 回 True，否则返回 False# isalpha()# 如果字符串至少有一个字符并且所有字符都是字母或中文字则返回 True, 否则返回 False# isdigit()# 如果字符串只包含数字则返回 True 否则返回 False..# islower()# 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True，否则返回 False# isnumeric()# 如果字符串中只包含数字字符，则返回 True，否则返回 False# isspace()# 如果字符串中只包含空白，则返回 True，否则返回 False.# istitle()# 如果字符串是标题化的(见 title())则返回 True，否则返回 False# isupper()# 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True，否则返回 False# join(seq)# 以指定字符串作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串# len(string)# 返回字符串长度# ljust(width[, fillchar])# 返回一个原字符串左对齐,并使用 fillchar 填充至长度 width 的新字符串，fillchar 默认为空格。# lower()# 转换字符串中所有大写字符为小写.# lstrip()# 截掉字符串左边的空格或指定字符。# maketrans()# 创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。# max(str)# 返回字符串 str 中最大的字母。# min(str)# 返回字符串 str 中最小的字母。# replace(old, new [, max])# 把 将字符串中的 old 替换成 new,如果 max 指定，则替换不超过 max 次。# rfind(str, beg=0,end=len(string))# 类似于 find()函数，不过是从右边开始查找.# rindex( str, beg=0, end=len(string))# 类似于 index()，不过是从右边开始.# rjust(width,[, fillchar])# 返回一个原字符串右对齐,并使用fillchar(默认空格）填充至长度 width 的新字符串# rstrip()# 删除字符串末尾的空格或指定字符。# split(str=&quot;&quot;, num=string.count(str))# 以 str 为分隔符截取字符串，如果 num 有指定值，则仅截取 num+1 个子字符串# splitlines([keepends])# 按照行('\\r', '\\r\\n', \\n')分隔，返回一个包含各行作为元素的列表，如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。# startswith(substr, beg=0,end=len(string))# 检查字符串是否是以指定子字符串 substr 开头，是则返回 True，否则返回 False。如果beg 和 end 指定值，则在指定范围内检查。# strip([chars])# 在字符串上执行 lstrip()和 rstrip()# swapcase()# 将字符串中大写转换为小写，小写转换为大写# title()# 返回&quot;标题化&quot;的字符串,就是说所有单词都是以大写开始，其余字母均为小写(见 istitle())# translate(table, deletechars=&quot;&quot;)# 根据 str 给出的表(包含 256 个字符)转换 string 的字符, 要过滤掉的字符放到 deletechars 参数中# upper()# 转换字符串中的小写字母为大写# zfill (width)# 返回长度为 width 的字符串，原字符串右对齐，前面填充0# isdecimal()# 检查字符串是否只包含十进制字符，如果是返回 true，否则返回 false。str = 'Lxl is a good boy'print(str) # 输出字符串print(str[0:-1]) # 输出第一个到倒数第二个的所有字符print(str[0]) # 输出字符串第一个字符print(str[2:5]) # 输出从第三个开始到第五个的字符print(str[2:]) # 输出从第三个开始的后的所有字符print(str * 2) # 输出字符串两次，也可以写成 print (2 * str)print(str + &quot;TEST&quot;) # 连接字符串# 转义print('Ru\\noob')# 不转义字符串前加rprint(2 * r'Ru\\noob')# List（列表）------------------------------------------------------------------------------# 创建列表只需要方括号括起来，里面数据类型可不一致，索引正0反-1，# 列表中的元素是可以改变的# append() 追加 del 删除# ==列表脚本操作符==# Python 表达式 结果 描述# len([1, 2, 3]) 3 长度# [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6] 组合# ['Hi!'] * 4 ['Hi!', 'Hi!', 'Hi!', 'Hi!'] 重复# 3 in [1, 2, 3] True 元素是否存在于列表中# for x in [1, 2, 3]: print(x, end=&quot; &quot;) 1 2 3 迭代# ==函数&amp;方法==# len(list) 列表元素个数# max(list) 返回列表元素最大值# min(list) 返回列表元素最小值# list(seq) 将元组转换为列表# 1 list.append(obj) 在列表末尾添加新的对象# 2 list.count(obj) 统计某个元素在列表中出现的次数# 3 list.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）# 4 list.index(obj) 从列表中找出某个值第一个匹配项的索引位置# 5 list.insert(index, obj) 将对象插入列表# 6 list.pop([index=-1]) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值# 7 list.remove(obj) 移除列表中某个值的第一个匹配项# 8 list.reverse() 反向列表中元素# 9 list.sort( key=None, reverse=False) 对原列表进行排序# 10 list.clear() 清空列表# 11 list.copy() 复制列表lists = ['1', '2', '3', '4', '5']print(lists[:])print(lists[:3])print(lists[1:3])print(lists[4:])print(lists[-1:])print(lists[:-2])print(lists * 3) # 输出多次列表print(lists + lists) # 连接列表for i, v in enumerate(['tic', 'tac', 'toe']): print(i, v)def reverseWords(input): # 通过空格将字符串分隔符，把各个单词分隔为列表 inputWords = input.split(&quot; &quot;) # 翻转字符串 # 假设列表 list = [1,2,3,4], # list[0]=1, list[1]=2 ，而 -1 表示最后一个元素 list[-1]=4 ( 与 list[3]=4 一样) # inputWords[-1::-1] 有三个参数 # 第一个参数 -1 表示最后一个元素 # 第二个参数为空，表示移动到列表末尾 # 第三个参数为步长，-1 表示逆向 inputWords = inputWords[-1::-1] # 重新组合字符串 output = ' '.join(inputWords) return outputif __name__ == &quot;__main__&quot;: input = 'I like runoob' rw = reverseWords(input) print(rw)# Tuple（元组）--------------------------------------------------------------------# 元组（tuple）与列表类似，不同之处在于元组的元素不能修改，其使用小括号（）或不要括号，# 但它可以包含可变的对象，可以连接组合成新元祖，元素不允许删除但可以使用del删除整个元祖。# string、list 和 tuple 都属于 sequence（序列）。# 当元祖只有一个元素时，后面添加逗号，否则括号会被当做运算符使用，tup1 = (50)为整型，tup1 = (50，)为元祖# ==元祖运算符==# Python 表达式 结果 描述# len((1, 2, 3)) 3 计算元素个数# (1, 2, 3) + (4, 5, 6) (1, 2, 3, 4, 5, 6) 连接# ('Hi!',) * 4 ('Hi!', 'Hi!', 'Hi!', 'Hi!') 复制# 3 in (1, 2, 3) True 元素是否存在# for x in (1, 2, 3): print (x,) 1 2 3 迭代# ==内置函数==# len(tuple) 计算元组元素个数。# max(tuple) 返回元组中元素最大值。# min(tuple) 返回元组中元素最小值。# tuple(iterable) 将可迭代系列转换为元组。tuple = ('abcd', 786, 2.23, 'runoob', 70.2)tinytuple = (123, 'runoob')print(tuple) # 输出完整元组print(tuple[0]) # 输出元组的第一个元素print(tuple[1:3]) # 输出从第二个元素开始到第三个元素print(tuple[2:]) # 输出从第三个元素开始的所有元素print(tinytuple * 2) # 输出两次元组print(tuple + tinytuple) # 连接元组# Set（集合）----------------------------------------------------------------------# 创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典sites = {'Google', 'Taobao', 'Runoob', 'Facebook', 'Zhihu', 'Baidu'}sites1 = {}sites2 = ()set()# # 添加元素，重复不操作# sites1.add(&quot;1&quot;)# # 可以添加列表，元祖字典等# sites1.update(&quot;1&quot;)# print(sites) # 输出集合，重复的元素被自动去掉# # 移除，不存在报错# sites1.remove( x )# # 移除，不存在不报错# sites1.discard( x )# # 随机删除# sites1.pop()# # 长度# len(sites1)# # 清空# sites1.clear()# # 判断是否存在# x in s# ==内置方法完整列表==# add() 为集合添加元素# clear() 移除集合中的所有元素# copy() 拷贝一个集合# difference() 返回多个集合的差集# difference_update() 移除集合中的元素，该元素在指定的集合也存在。# discard() 删除集合中指定的元素# intersection() 返回集合的交集# intersection_update() 返回集合的交集。# isdisjoint() 判断两个集合是否包含相同的元素，如果没有返回 True，否则返回 False。# issubset() 判断指定集合是否为该方法参数集合的子集。# issuperset() 判断该方法的参数集合是否为指定集合的子集# pop() 随机移除元素# remove() 移除指定元素# symmetric_difference() 返回两个集合中不重复的元素集合。# symmetric_difference_update() 移除当前集合中在另外一个指定集合相同的元素，并将另外一个指定集合中不同的元素插入到当前集合中。# union() 返回两个集合的并集# update() 给集合添加元素# 成员测试if 'Runoob' in sites: print('Runoob 在集合中')else: print('Runoob 不在集合中')# set可以进行集合运算a = set('abracadabra')b = set('alacazam')print(a)print(a - b) # a 和 b 的差集print(a | b) # a 和 b 的并集print(a &amp; b) # a 和 b 的交集print(a ^ b) # a 和 b 中不同时存在的元素# Dictionary（字典）------------------------------------------------------------------# 字典（dictionary）是Python中另一个非常有用的内置数据类型。# 列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。# 字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。# 键(key)可以是任意类型，但必须使用不可变类型。# 在同一个字典中，键(key)必须是唯一的。# ==内置函数==# len(dict) 计算字典元素个数，即键的总数。# str(dict) 输出字典，以可打印的字符串表示。# type(variable) 返回输入的变量类型，如果变量是字典就返回字典类型。# ==内置方法==# 1 radiansdict.clear() 删除字典内所有元素# 2 radiansdict.copy() 返回一个字典的浅复制# 3 radiansdict.fromkeys() 创建一个新字典，以序列seq中元素做字典的键，val为字典所有键对应的初始值# 4 radiansdict.get(key, default=None) 返回指定键的值，如果键不在字典中返回 default 设置的默认值# 5 key in dict 如果键在字典dict里返回true，否则返回false# 6 radiansdict.items() 以列表返回一个视图对象# 7 radiansdict.keys() 返回一个视图对象# 8 radiansdict.setdefault(key, default=None) 和get()类似, 但如果键不存在于字典中，将会添加键并将值设为default# 9 radiansdict.update(dict2) 把字典dict2的键/值对更新到dict里# 10 radiansdict.values() 返回一个视图对象# 11 pop(key[,default]) 删除字典给定键 key 所对应的值，返回值为被删除的值。key值必须给出。 否则，返回default值。# 12 popitem() 随机返回并删除字典中的最后一对键和值dict = {}dict['one'] = &quot;1 - 菜鸟教程&quot;dict[2] = &quot;2 - 菜鸟工具&quot;tinydict = {'name': 'runoob', 'code': 1, 'site': 'www.runoob.com'}for k, v in knights.items(): print(k, v)print(dict['one']) # 输出键为 'one' 的值print(dict[2]) # 输出键为 2 的值print(tinydict) # 输出完整的字典print(tinydict.keys()) # 输出所有键print(tinydict.values()) # 输出所有值del tinydict['name'] # 删除键 'name'dict.clear() # 清空字典del tinydict # 删除字典# 构造函数 dict() 可以直接从键值对序列中构建字典如下'''============数据类型转换==============='''# int(x [,projects]) 将x转换为一个整数# float(x)将x转换到一个浮点数# complex(real [,imag])创建一个复数# str(x)将对象 x 转换为字符串# repr(x)将对象 x 转换为表达式字符串# eval(str)用来计算在字符串中的有效Python表达式,并返回一个对象# tuple(s)将序列 s 转换为一个元组# list(s)将序列 s 转换为一个列表# set(s)转换为可变集合# dict(d)创建一个字典。d 必须是一个 (key, value)元组序列。# frozenset(s)转换为不可变集合# chr(x)将一个整数转换为一个字符# ord(x)将一个字符转换为它的整数值# hex(x)将一个整数转换为一个十六进制字符串# oct(x)将一个整数转换为一个八进制字符串'''============Python3运算符==============='''# 算术运算符【a为10，b为21】----------------------------------# + 加 - 两个对象相加 a + b 输出结果 31# - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -11# * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 210# / 除 - x 除以 y b / a 输出结果 2.1# % 取模 - 返回除法的余数 b % a 输出结果 1# ** 幂 - 返回x的y次幂 a**b 为10的21次方# // 取整除 - 向下取接近商的整数，除后取小 9//2=4 -9//2=-5# 比较（关系）运算符【a为10，b为20】-----------------------------# == 等于 - 比较对象是否相等 (a == b) 返回 False。# != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True。# &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。# &lt; 小于 - 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价。注意，这些变量名的大写。 (a &lt; b) 返回 True。# &gt;= 大于等于 - 返回x是否大于等于y。 (a &gt;= b) 返回 False。# &lt;= 小于等于 - 返回x是否小于等于y。 (a &lt;= b) 返回 True。# 赋值运算符【a为10，b为20】----------------------------------# = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c# += 加法赋值运算符 c += a 等效于 c = c + a# -= 减法赋值运算符 c -= a 等效于 c = c - a# *= 乘法赋值运算符 c *= a 等效于 c = c * a# /= 除法赋值运算符 c /= a 等效于 c = c / a# %= 取模赋值运算符 c %= a 等效于 c = c % a# **= 幂赋值运算符 c **= a 等效于 c = c ** a# //= 取整除赋值运算符 c //= a 等效于 c = c // a# := 海象运算符，可在表达式内部为变量赋值。Python3.8 版本新增运算符。# 在这个示例中，赋值表达式可以避免调用 len() 两次:# if (n := len(a)) &gt; 10:# print(f&quot;List is too long ({n} elements, expected &lt;= 10)&quot;)# 逻辑运算符-----------------------------------------------------# and x and y 布尔&quot;与&quot; - 如果 x 为 False，x and y 返回 x 的值，否则返回 y 的计算值。 (a and b) 返回 20。# or x or y 布尔&quot;或&quot; - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10。# not not x 布尔&quot;非&quot; - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False# 位运算符-------------------------------------------------------# &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100# | 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 (a | b) 输出结果 61 ，二进制解释： 0011 1101# ^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001# ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1。~x 类似于 -x-1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。# &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由&quot;&lt;&lt;&quot;右边的数指定移动的位数，高位丢弃，低位补0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000# &gt;&gt; 右移动运算符：把&quot;&gt;&gt;&quot;左边的运算数的各二进位全部右移若干位，&quot;&gt;&gt;&quot;右边的数指定移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111# 成员运算符----------------------------------------------------------# in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。# not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。# 身份运算符------------------------------------------------------------# is is 是判断两个标识符是不是引用自一个对象 x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False# is not is not 是判断两个标识符是不是引用自不同对象 x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。# 运算符优先级# ** 指数 (最高优先级)# ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@)# * / % // 乘，除，求余数和取整除# + - 加法减法# &gt;&gt; &lt;&lt; 右移，左移运算符# &amp; 位 'AND'# ^ | 位运算符# &lt;= &lt; &gt; &gt;= 比较运算符# == != 等于运算符# = %= /= //= -= += *= **= 赋值运算符# is is not 身份运算符# in not in 成员运算符# not and or 逻辑运算符 流程控制 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485'''流程控制'''# ifif condition_1: statement_block_1elif condition_2: statement_block_2else: statement_block_3# while whlie后面为false时则执行else语句块while counter &lt;= n: sum = sum + counter counter += 1else: additional_statement(s)# for range()函数。它会生成数列# break 语句可以跳出 for 和 while 的循环体。如果你从 for 或 while 循环中终止，任何对应的循环 else 块将不执行。# continue 语句被用来告诉 Python 跳过当前循环块中的剩余语句，然后继续进行下一轮循环。for i in range(5,9) : print(i)# pass 不做任何事情，一般用做占位语句for letter in 'Runoob': if letter == 'o': pass print('执行 pass 块') print('当前字母 :', letter)print(&quot;Good bye!&quot;)'''迭代器与生成器 iter()创建迭代器对象 和 next()输出迭代器下一个元素 Python 的构造函数为 __init__(),__iter__() 方法返回一个特殊的迭代器对象， 这个迭代器对象实现了 __next__() 方法并通过 StopIteration 异常标识迭代的完成。__next__() 方法（Python 2 里是 next()）会返回下一个迭代器对象。'''# StopIteration 异常用于标识迭代的完成，防止出现无限循环的情况，在 __next__() 方法中我们可以设置在完成指定循环次数后触发 StopIteration 异常来结束迭代。class MyNumbers: def __iter__(self): self.a = 1 return self def __next__(self): if self.a &lt;= 20: x = self.a self.a += 1 return x else: raise StopIterationmyclass = MyNumbers()myiter = iter(myclass)for x in myiter: print(x)# 使用了 yield 的函数被称为生成器（generator）。# 跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。## 在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。## 调用一个生成器函数，返回的是一个迭代器对象。import sysdef fibonacci(n): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b counter += 1f = fibonacci(10) # f 是一个迭代器，由生成器返回生成while True: try: print(next(f), end=&quot; &quot;) except StopIteration: sys.exit() 时间 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&quot;&quot;&quot;时间&quot;&quot;&quot;# Python 提供了一个 time 和 calendar 模块可以用于格式化日期和时间。# 时间间隔是以秒为单位的浮点小数。# 每个时间戳都以自从 1970 年 1 月 1 日午夜（历元）经过了多长时间来表示。import time # 引入time模块ticks = time.time()print (&quot;当前时间戳为:&quot;, ticks)# 时间元组# 很多Python函数用一个元组装起来的9组数字处理时间:## 序号 字段 值# 0 4位数年 2008# 1 月 1 到 12# 2 日 1到31# 3 小时 0到23# 4 分钟 0到59# 5 秒 0到61 (60或61 是闰秒)# 6 一周的第几日 0到6 (0是周一)# 7 一年的第几日 1到366 (儒略历)# 8 夏令时 -1, 0, 1, -1是决定是否为夏令时的旗帜# 上述也就是struct_time元组。这种结构具有如下属性：# 序号 属性 值# 0 tm_year 2008# 1 tm_mon 1 到 12# 2 tm_mday 1 到 31# 3 tm_hour 0 到 23# 4 tm_min 0 到 59# 5 tm_sec 0 到 61 (60或61 是闰秒)# 6 tm_wday 0到6 (0是周一)# 7 tm_yday 一年中的第几天，1 到 366# 8 tm_isdst 是否为夏令时，值有：1(夏令时)、0(不是夏令时)、-1(未知)，默认 -1# 获取当前时间localtime = time.asctime( time.localtime(time.time()) )print (&quot;本地时间为 :&quot;, localtime)# 格式化# %y 两位数的年份表示（00-99）# %Y 四位数的年份表示（000-9999）# %m 月份（01-12）# %d 月内中的一天（0-31）# %H 24小时制小时数（0-23）# %I 12小时制小时数（01-12）# %M 分钟数（00=59）# %S 秒（00-59）# %a 本地简化星期名称# %A 本地完整星期名称# %b 本地简化的月份名称# %B 本地完整的月份名称# %c 本地相应的日期表示和时间表示# %j 年内的一天（001-366）# %p 本地A.M.或P.M.的等价符# %U 一年中的星期数（00-53）星期天为星期的开始# %w 星期（0-6），星期天为星期的开始# %W 一年中的星期数（00-53）星期一为星期的开始# %x 本地相应的日期表示# %X 本地相应的时间表示# %Z 当前时区的名称# %% %号本身print(time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()))# 格式化成Sat Mar 28 22:24:24 2016形式print(time.strftime(&quot;%a %b %d %H:%M:%S %Y&quot;, time.localtime()))# 将格式字符串转换为时间戳a = &quot;Sat Mar 28 22:24:24 2016&quot;print(time.mktime(time.strptime(a, &quot;%a %b %d %H:%M:%S %Y&quot;)))# 获取某月日历import calendarcal = calendar.month(2016, 1)print (&quot;以下输出2016年1月份的日历:&quot;)print (cal) JSON 123456789101112131415161718192021222324252627282930313233343536373839404142434445&quot;&quot;&quot;JSON&quot;&quot;&quot;# json.dumps(): ==编码==# dict object# list, tuple array# str string# int, float, int- &amp; float-derived Enums number# True true# False false# None null# json.loads(): ==解码==# object dict# array list# string str# number (int) int# number (real) float# true True# false False# null Noneimport json# Python 字典类型转换为 JSON 对象data = { 'no': 1, 'name': 'Runoob', 'url': 'http://www.runoob.com'}json_str = json.dumps(data)print(&quot;Python 原始数据：&quot;, repr(data))print(&quot;JSON 对象：&quot;, json_str)# 将 JSON 对象转换为 Python 字典data2 = json.loads(json_str)print (&quot;data2['name']: &quot;, data2['name'])print (&quot;data2['url']: &quot;, data2['url'])# 文件处理# 写入 JSON 数据with open('data.json', 'w') as f: json.dump(data, f)# 读取数据with open('data.json', 'r') as f: data = json.load(f) 标准库概览 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&quot;&quot;&quot;标准库概览&quot;&quot;&quot;# 操作系统接口-----------------------------import osos.getcwd() # 返回当前的工作目录os.chdir('/server/accesslogs') # 修改当前的工作目录os.system('mkdir today') # 执行系统命令 mkdirimport shutilshutil.copyfile('data.db', 'archive.db')shutil.move('/build/executables', 'installdir')# 文件通配符 ---------------------------------------------# 用于从目录通配符搜索中生成文件列表import globglob.glob('*.py')# 命令行参数------------------------------------import sysprint(sys.argv)# 错误输出重定向和程序终止-------------------------------# sys 还有 stdin，stdout 和 stderr 属性，即使在 stdout 被重定向时，后者也可以用于显示警告和错误信息。sys.stderr.write('Warning, log file not found starting a new one\\n')# 字符串正则匹配-----------------------------------------------import rere.findall(r'\\bf[a-z]*', 'which foot or hand fell fastest')re.sub(r'(\\b[a-z]+) \\1', r'\\1', 'cat in the the hat')# 数学-----------------------------------------------import mathmath.cos(math.pi / 4)math.log(1024, 2)import randomrandom.choice(['apple', 'pear', 'banana'])random.sample(range(100), 10) # sampling without replacementrandom.random() # random floatrandom.randrange(6)# 互联网-----------------------------------------------from urllib.request import urlopenfor line in urlopen('http://tycho.usno.navy.mil/cgi-bin/timer.pl'): line = line.decode('utf-8') # Decoding the binary data to text. if 'EST' in line or 'EDT' in line: # look for Eastern Time print(line)import smtplibserver = smtplib.SMTP('localhost')server.sendmail('soothsayer@example.org', 'jcaesar@example.org',&quot;&quot;&quot;To: jcaesar@example.org...From: soothsayer@example.org...... Beware the Ides of March.... &quot;&quot;&quot;)server.quit()# 日期和时间-----------------------------------------------from datetime import datenow = date.today()print(now)now.strftime(&quot;%m-%d-%y. %d %b %Y is a %A on the %d day of %B.&quot;)# dates support calendar arithmeticbirthday = date(1964, 7, 31)age = now - birthdayage.days# 数据压缩import zlibs = b'witch which has which witches wrist watch'len(s)t = zlib.compress(s)len(t)zlib.decompress(t)zlib.crc32(s)# 性能度量from timeit import TimerTimer('t=a; a=b; b=t', 'a=1; b=2').timeit()Timer('a,b = b,a', 'a=1; b=2').timeit()# 测试模块def average(values): &quot;&quot;&quot;Computes the arithmetic mean of a list of numbers. &gt;&gt;&gt; print(average([20, 30, 70])) 40.0 &quot;&quot;&quot; return sum(values) / len(values)import doctestdoctest.testmod() # 自动验证嵌入测试# unittest模块不像 doctest模块那么容易使用，不过它可以在一个独立的文件里提供一个更全面的测试集:import unittestclass TestStatisticalFunctions(unittest.TestCase): def test_average(self): self.assertEqual(average([20, 30, 70]), 40.0) self.assertEqual(round(average([1, 5, 7]), 1), 4.3) self.assertRaises(ZeroDivisionError, average, []) self.assertRaises(TypeError, average, 20, 30, 70)unittest.main() # Calling from the command line invokes all tests OOP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129&quot;&quot;&quot;OOP&quot;&quot;&quot;# ===类===# class ClassName:# &lt;statement-1&gt;# .# .# &lt;statement-N&gt;# ===类对象===# 属性引用和实例化class MyClass: &quot;&quot;&quot;一个简单的类实例&quot;&quot;&quot; i = 12345 def f(self): return 'hello world'# ===实例化类===x = MyClass()# ===访问类的属性和方法===print(&quot;MyClass 类的属性 i 为：&quot;, x.i)print(&quot;MyClass 类的方法 f 输出为：&quot;, x.f())# 类有一个名为 __init__() 的特殊方法（构造方法），该方法在类实例化时会自动调用，像下面这样：# __init__() 方法可以有参数，参数通过 __init__() 传递到类的实例化操作上，self代表类的实例，而非类def __init__(self): self.data = []class Test: def prt(self): print(self) print(self.__class__)t = Test()t.prt()# ===方法===# def定义方法,与一般函数定义不同，类方法必须包含self参数且作为第一个参数，self代表类的实例# 类定义class people: # 定义基本属性 name = '' age = 0 # 定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 # 定义构造方法 def __init__(self, n, a, w): self.name = n self.age = a self.__weight = w def speak(self): print(&quot;%s 说: 我 %d 岁。&quot; % (self.name, self.age))# 实例化类p = people('runoob', 10, 30)p.speak()# ===继承===# 子类（派生类 DerivedClassName）会继承父类（基类 BaseClassName）的属性和方法。# class DerivedClassName(BaseClassName):# &lt;statement-1&gt;# .# .# .# &lt;statement-N&gt;# ===多继承===# class DerivedClassName(Base1, Base2, Base3):# &lt;statement-1&gt;# .# .# .# &lt;statement-N&gt;# 若是父类中有相同的方法名，而在子类使用时未指定，python从左至右搜索 即方法在子类中未找到时，从左到右查找父类中是否包含方法# ===方法重写===class Parent: # 定义父类 def myMethod(self): print('调用父类方法')class Child(Parent): # 定义子类 def myMethod(self): print('调用子类方法')c = Child() # 子类实例c.myMethod() # 子类调用重写方法super(Child, c).myMethod() # 用子类对象调用父类已被覆盖的方法# ===类属性与方法===# 类的私有属性# __private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。## 类的方法# 在类的内部，使用 def 关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数 self，且为第一个参数，self 代表的是类的实例。# self 的名字并不是规定死的，也可以使用 this，但是最好还是按照约定使用 self。## 类的私有方法# __private_method：两个下划线开头，声明该方法为私有方法，只能在类的内部调用 ，不能在类的外部调用。self.__private_methods。# ==重载==class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self, other): return Vector(self.a + other.a, self.b + other.b)v1 = Vector(2, 10)v2 = Vector(5, -2)print(v1 + v2) 函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859'''函数'''# 你可以定义一个由自己想要功能的函数，以下是简单的规则：## 函数代码块以 def 关键词开头，后接函数标识符名称和圆括号 ()。# 任何传入参数和自变量必须放在圆括号中间，圆括号之间可以用于定义参数。# 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。# 函数内容以冒号 : 起始，并且缩进。# return [表达式] 结束函数，选择性地返回一个值给调用方，不带表达式的 return 相当于返回 None。def max(a, b): if a &gt; b: return a else: return b# python 函数的参数传递：# 不可变类型：类似 C++ 的值传递，如整数、字符串、元组。如 fun(a)，传递的只是 a 的值，没有影响 a 对象本身。如果在 fun(a) 内部修改 a 的值，则是新生成一个 a 的对象。# 可变类型：类似 C++ 的引用传递，如 列表，字典。如 fun(la)，则是将 la 真正的传过去，修改后 fun 外部的 la 也会受影响# python 中一切都是对象，严格意义我们不能说值传递还是引用传递，我们应该说传不可变对象和传可变对象。# 可通过id()函数查看对象内存地址# 参数# 以下是调用函数时可使用的正式参数类型：# 必需参数【调用时的数量必须和声明时的一样】 def printme( str ):# 关键字参数【允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值】def printinfo( name, age ):# 默认参数【如果没有传递参数，则会使用默认参数】def printinfo( name, age = 35 ):# 不定长参数 【加了星号 * 的参数会以元组(tuple)的形式导入，存放所有未命名的变量参数，没有指定参数，它就是一个空元组。我们也可以不向函数传递未命名的变量，加了两个星号 ** 的参数会以字典的形式导入，如果单独出现星号 * 后的参数必须用关键字传入】def functionname([formal_args,] *var_args_tuple ):'''匿名函数'''# lambda [arg1 [,arg2,.....argn]]:expression# 可写函数说明sum = lambda arg1, arg2: arg1 + arg2# 调用sum函数print(&quot;相加后的值为 : &quot;, sum(10, 20))print(&quot;相加后的值为 : &quot;, sum(20, 20))'''强制位置参数'''# 在以下的例子中，形参 a 和 b 必须使用指定位置参数，c 或 d 可以是位置形参或关键字形参，而 e 或 f 要求为关键字形参:def f(a, b, /, c, d, *, e, f): print(a, b, c, d, e, f)# 以下使用方法是正确的:f(10, 20, 30, d=40, e=50, f=60)# 以下使用方法会发生错误:f(10, b=20, c=30, d=40, e=50, f=60) # b 不能使用关键字参数的形式f(10, 20, 30, 40, 50, f=60) # e 必须使用关键字参数的形式 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&quot;&quot;&quot;文件&quot;&quot;&quot;# 打开文件返回文件对象# file: 必需，文件路径（相对或者绝对路径）。# mode: 可选，文件打开模式# buffering: 设置缓冲# encoding: 一般使用utf8# errors: 报错级别# newline: 区分换行符# closefd: 传入的file参数类型# opener: 设置自定义开启器，开启器的返回值必须是一个打开的文件描述符。# mode 参数有：# 模式 描述# t 文本模式 (默认)。# x 写模式，新建一个文件，如果该文件已存在则会报错。# b 二进制模式。# + 打开一个文件进行更新(可读可写)。# U 通用换行模式（Python 3 不支持）。# r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。# rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。# r+ 打开一个文件用于读写。文件指针将会放在文件的开头。# rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。# w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。# w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。# a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。# ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。file = open('filetest.txt', mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)# file 对象常用的函数：# file.close() 关闭文件。关闭后文件不能再进行读写操作。# file.flush() 刷新文件内部缓冲，直接把内部缓冲区的数据立刻写入文件, 而不是被动的等待输出缓冲区写入。# file.fileno() 返回一个整型的文件描述符(file descriptor FD 整型), 可以用在如os模块的read方法等一些底层操作上。# file.isatty() 如果文件连接到一个终端设备返回 True，否则返回 False。# file.next() Python 3 中的 File 对象不支持 next() 方法。 返回文件下一行。# file.read([size]) 从文件读取指定的字节数，如果未给定或为负则读取所有。# file.readline([size]) 读取整行，包括 &quot;\\n&quot; 字符。# file.readlines([sizeint]) 读取所有行并返回列表，若给定sizeint&gt;0，返回总和大约为sizeint字节的行, 实际读取值可能比 sizeint 较大, 因为需要填充缓冲区。# file.seek(offset[, whence]) 移动文件读取指针到指定位置# file.tell() 返回文件当前位置。# file.truncate([size]) 从文件的首行首字符开始截断，截断文件为 size 个字符，无 size 表示从当前位置截断；截断之后后面的所有字符被删除，其中 windows 系统下的换行代表2个字符大小。# file.write(str) 将字符串写入文件，返回的是写入的字符长度。# file.writelines(sequence) 向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符。&quot;&quot;&quot;文件和目录&quot;&quot;&quot;# os 模块提供了非常丰富的方法用来处理文件和目录。常用的方法如下表所示：## 序号 方法及描述# os.access(path, mode) 检验权限模式# os.chdir(path) 改变当前工作目录# os.chflags(path, flags) 设置路径的标记为数字标记。# os.chmod(path, mode) 更改权限# os.chown(path, uid, gid) 更改文件所有者# os.chroot(path) 改变当前进程的根目录# os.close(fd) 关闭文件描述符 fd# os.closerange(fd_low, fd_high) 关闭所有文件描述符，从 fd_low (包含) 到 fd_high (不包含), 错误会忽略# os.dup(fd) 复制文件描述符 fd# os.dup2(fd, fd2) 将一个文件描述符 fd 复制到另一个 fd2# os.fchdir(fd) 通过文件描述符改变当前工作目录# os.fchmod(fd, mode) 改变一个文件的访问权限，该文件由参数fd指定，参数mode是Unix下的文件访问权限。# os.fchown(fd, uid, gid) 修改一个文件的所有权，这个函数修改一个文件的用户ID和用户组ID，该文件由文件描述符fd指定。# os.fdatasync(fd) 强制将文件写入磁盘，该文件由文件描述符fd指定，但是不强制更新文件的状态信息。# os.fdopen(fd[, mode[, bufsize]]) 通过文件描述符 fd 创建一个文件对象，并返回这个文件对象# os.fpathconf(fd, name) 返回一个打开的文件的系统配置信息。name为检索的系统配置的值，它也许是一个定义系统值的字符串，这些名字在很多标准中指定（POSIX.1, Unix 95, Unix 98, 和其它）。# os.fstat(fd) 返回文件描述符fd的状态，像stat()。# os.fstatvfs(fd) 返回包含文件描述符fd的文件的文件系统的信息，Python 3.3 相等于 statvfs()。# os.fsync(fd) 强制将文件描述符为fd的文件写入硬盘。# os.ftruncate(fd, length) 裁剪文件描述符fd对应的文件, 所以它最大不能超过文件大小。# os.getcwd() 返回当前工作目录# os.getcwdb() 返回一个当前工作目录的Unicode对象# os.isatty(fd) 如果文件描述符fd是打开的，同时与tty(-like)设备相连，则返回true, 否则False。# os.lchflags(path, flags) 设置路径的标记为数字标记，类似 chflags()，但是没有软链接# os.lchmod(path, mode) 修改连接文件权限# os.lchown(path, uid, gid) 更改文件所有者，类似 chown，但是不追踪链接。# os.link(src, dst) 创建硬链接，名为参数 dst，指向参数 src# os.listdir(path) 返回path指定的文件夹包含的文件或文件夹的名字的列表。# os.lseek(fd, pos, how) 设置文件描述符 fd当前位置为pos, how方式修改: SEEK_SET 或者 0 设置从文件开始的计算的pos; SEEK_CUR或者 1 则从当前位置计算; os.SEEK_END或者2则从文件尾部开始. 在unix，Windows中有效# os.lstat(path) 像stat(),但是没有软链接# os.major(device) 从原始的设备号中提取设备major号码 (使用stat中的st_dev或者st_rdev field)。# os.makedev(major, minor) 以major和minor设备号组成一个原始设备号# os.makedirs(path[, mode]) 递归文件夹创建函数。像mkdir(), 但创建的所有intermediate-level文件夹需要包含子文件夹。# os.minor(device) 从原始的设备号中提取设备minor号码 (使用stat中的st_dev或者st_rdev field )。# os.mkdir(path[, mode]) 以数字mode的mode创建一个名为path的文件夹.默认的 mode 是 0777 (八进制)。# os.mkfifo(path[, mode]) 创建命名管道，mode 为数字，默认为 0666 (八进制)# os.mknod(filename[, mode=0600, device]) 创建一个名为filename文件系统节点（文件，设备特别文件或者命名pipe）。# os.open(file, flags[, mode]) 打开一个文件，并且设置需要的打开选项，mode参数是可选的# os.openpty() 打开一个新的伪终端对。返回 pty 和 tty的文件描述符。# os.pathconf(path, name) 返回相关文件的系统配置信息。# os.pipe() 创建一个管道. 返回一对文件描述符(r, w) 分别为读和写# os.popen(command[, mode[, bufsize]]) 从一个 command 打开一个管道# os.read(fd, n) 从文件描述符 fd 中读取最多 n 个字节，返回包含读取字节的字符串，文件描述符 fd对应文件已达到结尾, 返回一个空字符串。# os.readlink(path) 返回软链接所指向的文件# os.remove(path) 删除路径为path的文件。如果path 是一个文件夹，将抛出OSError; 查看下面的rmdir()删除一个 directory。# os.removedirs(path) 递归删除目录。# os.rename(src, dst) 重命名文件或目录，从 src 到 dst# os.renames(old, new) 递归地对目录进行更名，也可以对文件进行更名。# os.rmdir(path) 删除path指定的空目录，如果目录非空，则抛出一个OSError异常。# os.stat(path) 获取path指定的路径的信息，功能等同于C API中的stat()系统调用。# os.stat_float_times([newvalue]) 决定stat_result是否以float对象显示时间戳# os.statvfs(path) 获取指定路径的文件系统统计信息# os.symlink(src, dst) 创建一个软链接# os.tcgetpgrp(fd) 返回与终端fd（一个由os.open()返回的打开的文件描述符）关联的进程组# os.tcsetpgrp(fd, pg) 设置与终端fd（一个由os.open()返回的打开的文件描述符）关联的进程组为pg。# os.tempnam([dir[, prefix]]) Python3 中已删除。返回唯一的路径名用于创建临时文件。# os.tmpfile() Python3 中已删除。返回一个打开的模式为(w+b)的文件对象 .这文件对象没有文件夹入口，没有文件描述符，将会自动删除。# os.tmpnam() Python3 中已删除。为创建一个临时文件返回一个唯一的路径# os.ttyname(fd) 返回一个字符串，它表示与文件描述符fd 关联的终端设备。如果fd 没有与终端设备关联，则引发一个异常。# os.unlink(path) 删除文件路径# os.utime(path, times) 返回指定的path文件的访问和修改的时间。# os.walk(top[, topdown=True[, onerror=None[, followlinks=False]]]) 输出在文件夹中的文件名通过在树中游走，向上或者向下。# os.write(fd, str) 写入字符串到文件描述符 fd中. 返回实际写入的字符串长度# os.path 模块 获取文件的属性信息。# os.pardir() 获取当前目录的父目录，以字符串形式显示目录名。 语法错误和异常 1234567891011121314151617181920212223242526&quot;&quot;&quot;语法错误和异常&quot;&quot;&quot;# 语法错误 SyntaxError# 异常 ZeroDivisionError，NameError 和 TypeError# 异常处理# ---try/except---# 一个except子句可以同时处理多个异常，这些异常将被放在一个括号里成为一个元组，例如:# except (RuntimeError, TypeError, NameError):while True: try: x = int(input(&quot;请输入一个数字: &quot;)) break except ValueError: print(&quot;您输入的不是数字，请再次尝试输入！&quot;)# ----try/except...else---else 子句将在 try 子句没有发生任何异常的时候执行# ---try-finally----# 抛出异常 raise [Exception [, args [, traceback]]]x = 10if x &gt; 5: raise Exception('x 不能大于 5。x 的值为: {}'.format(x))# 用户自定义异常 通过创建一个新的异常类来拥有自己的异常。异常类继承自 Exception 类，可以直接继承，或者间接继承# with 语句就可以保证诸如文件之类的对象在使用完之后一定会正确的执行他的清理方法with open(&quot;myfile.txt&quot;) as f: for line in f: print(line, end=&quot;&quot;) Mysql 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import mysql.connector# python -m pip install mysql-connector 安装连接驱动# python -m pip install mysql-connector-python# pip uninstall mysql-connector 卸载mydb = mysql.connector.connect( host=&quot;localhost&quot;, # 数据库主机地址 user=&quot;root&quot;, # 数据库用户名 passwd=&quot;root&quot; # 数据库密码 # auth_plugin='mysql_native_password' # database=&quot;runoob_db&quot; #有则直接连接数据库)print(mydb)# 创建数据库mycursor = mydb.cursor()mycursor.execute(&quot;CREATE DATABASE runoob_db&quot;)# 输出所有数据库列表mycursor.execute(&quot;SHOW DATABASES&quot;)for x in mycursor: print(x)# 创建表mycursor.execute(&quot;CREATE TABLE sites (name VARCHAR(255), url VARCHAR(255))&quot;)# mycursor.execute(sql语句)# 增sql = &quot;INSERT INTO sites (name, url) VALUES (%s, %s)&quot;val = (&quot;RUNOOB&quot;, &quot;https://www.runoob.com&quot;)mycursor.execute(sql, val)mydb.commit() # 数据表内容有更新，必须使用到该语句print(mycursor.rowcount, &quot;记录插入成功。&quot;)# 批量增sql = &quot;INSERT INTO sites (name, url) VALUES (%s, %s)&quot;val = [ ('Google', 'https://www.google.com'), ('Github', 'https://www.github.com'), ('Taobao', 'https://www.taobao.com'), ('stackoverflow', 'https://www.stackoverflow.com/')]mycursor.executemany(sql, val)mydb.commit() # 数据表内容有更新，必须使用到该语句print(mycursor.rowcount, &quot;记录插入成功, ID:&quot;, mycursor.lastrowid)# 查mycursor.execute(&quot;SELECT * FROM sites&quot;)myresult = mycursor.fetchall() # fetchall() 获取所有记录myresult = mycursor.fetchone() #获取一条for x in myresult: print(x)# 防sql注入sql = &quot;SELECT * FROM sites WHERE name = %s&quot;na = (&quot;RUNOOB&quot;,)mycursor.execute(sql, na)myresult = mycursor.fetchall()for x in myresult: print(x)#删sql = &quot;DELETE FROM sites WHERE name = 'stackoverflow'&quot;mycursor.execute(sql)mydb.commit()print(mycursor.rowcount, &quot; 条记录删除&quot;)# 改sql = &quot;UPDATE sites SET name = 'ZH' WHERE name = 'Zhihu'&quot;mycursor.execute(sql)mydb.commit()print(mycursor.rowcount, &quot; 条记录被修改&quot;) 网络编程 123456789101112131415161718192021222324252627282930313233343536373839&quot;&quot;&quot;网络编程&quot;&quot;&quot;# 低级别的网络服务支持基本的 Socket，它提供了标准的 BSD Sockets API，可以访问底层操作系统 Socket 接口的全部方法。# 高级别的网络服务模块 SocketServer， 它提供了服务器中心类，可以简化网络服务器的开发。# socket()# socket.socket([family[, type[, proto]]])# family: 套接字家族可以使 AF_UNIX 或者 AF_INET。# type:套接字类型可以根据是面向连接的还是非连接分为SOCK_STREAM或SOCK_DGRAM。# protocol: 一般不填默认为 0。# Socket 对象(内建)方法# 函数 描述# 服务器端套接字-----------# s.bind() 绑定地址（host,port）到套接字， 在 AF_INET下，以元组（host,port）的形式表示地址。# s.listen() 开始 TCP 监听。backlog 指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为 1，大部分应用程序设为 5 就可以了。# s.accept() 被动接受TCP客户端连接,(阻塞式)等待连接的到来# 客户端套接字-----------# s.connect() 主动初始化TCP服务器连接，。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。# s.connect_ex() connect()函数的扩展版本,出错时返回出错码,而不是抛出异常# 公共用途的套接字函数----------# s.recv() 接收 TCP 数据，数据以字符串形式返回，bufsize 指定要接收的最大数据量。flag 提供有关消息的其他信息，通常可以忽略。# s.send() 发送 TCP 数据，将 string 中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于 string 的字节大小。# s.sendall() 完整发送 TCP 数据。将 string 中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回 None，失败则抛出异常。# s.recvfrom() 接收 UDP 数据，与 recv() 类似，但返回值是（data,address）。其中 data 是包含接收数据的字符串，address 是发送数据的套接字地址。# s.sendto() 发送 UDP 数据，将数据发送到套接字，address 是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。# s.close() 关闭套接字# s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。# s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port)# s.setsockopt(level,optname,value) 设置给定套接字选项的值。# s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。# s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒。值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()）# s.gettimeout() 返回当前超时期的值，单位是秒，如果没有设置超时期，则返回None。# s.fileno() 返回套接字的文件描述符。# s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。# s.makefile() 创建一个与该套接字相关连的文件 123456789101112131415161718192021import socketimport sys# 创建 socket 对象s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()# 设置端口号port = 9999# 连接服务，指定主机和端口s.connect((host, port))# 接收小于 1024 字节的数据msg = s.recv(1024)s.close()print (msg.decode('utf-8')) 123456789101112131415161718192021222324252627import socketimport sys# 创建 socket 对象serversocket = socket.socket( socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()port = 9999# 绑定端口号serversocket.bind((host, port))# 设置最大连接数，超过后排队serversocket.listen(5)while True: # 建立客户端连接 clientsocket, addr = serversocket.accept() print(&quot;连接地址: %s&quot; % str(addr)) msg = '欢迎访问菜鸟教程！' + &quot;\\r\\n&quot; clientsocket.send(msg.encode('utf-8')) clientsocket.close() SMTP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101&quot;&quot;&quot;SMTP&quot;&quot;&quot;# SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议,它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。# python的smtplib提供了一种很方便的途径发送电子邮件。它对smtp协议进行了简单的封装。# Python创建 SMTP 对象语法如下：# import smtplib# smtpObj = smtplib.SMTP( [host [, port [, local_hostname]]] )# 参数说明：# host: SMTP 服务器主机。 你可以指定主机的ip地址或者域名如:runoob.com，这个是可选参数。# port: 如果你提供了 host 参数, 你需要指定 SMTP 服务使用的端口号，一般情况下SMTP端口号为25。# local_hostname: 如果SMTP在你的本机上，你只需要指定服务器地址为 localhost 即可。# Python SMTP对象使用sendmail方法发送邮件，语法如下：# SMTP.sendmail(from_addr, to_addrs, msg[, mail_options, rcpt_options]# 参数说明：# from_addr: 邮件发送者地址。# to_addrs: 字符串列表，邮件发送地址。# msg: 发送消息# 这里要注意一下第三个参数，msg是字符串，表示邮件。我们知道邮件一般由标题，发信人，收件人，邮件内容，附件等构成，发送邮件的时候，要注意msg的格式。这个格式就是smtp协议中定义的格式。import smtplibfrom email.mime.image import MIMEImagefrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextfrom email.header import Headermy_sender='714416426@qq.com' # 发件人邮箱账号my_pass = '11111111' # 发件人邮箱密码my_user='714416426@qq.com' # 收件人邮箱账号，我这边发送给自己sender = 'from@runoob.com'receivers = ['714416426@qq.com'] # 接收邮件，可设置为你的QQ邮箱或者其他邮箱msgRoot = MIMEMultipart('related')msgRoot['From'] = Header(&quot;菜鸟教程&quot;, 'utf-8')msgRoot['To'] = Header(&quot;测试&quot;, 'utf-8')subject = 'Python SMTP 邮件测试'msgRoot['Subject'] = Header(subject, 'utf-8')# 发送html内容mail_msg = &quot;&quot;&quot;&lt;p&gt;Python 邮件发送测试...&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.runoob.com&quot;&gt;这是一个链接&lt;/a&gt;&lt;/p&gt;&quot;&quot;&quot;message = MIMEText(mail_msg, 'html', 'utf-8')# 三个参数：第一个为文本内容，第二个 plain 设置文本格式，第三个 utf-8 设置编码message = MIMEText('Python 邮件发送测试...', 'plain', 'utf-8')message['From'] = Header(&quot;菜鸟教程&quot;, 'utf-8') # 发送者message['To'] = Header(&quot;测试&quot;, 'utf-8') # 接收者subject = 'Python SMTP 邮件测试'message['Subject'] = Header(subject, 'utf-8')# 发送带附件内容# # 构造附件1，传送当前目录下的 test.txt 文件# att1 = MIMEText(open('test.txt', 'rb').read(), 'base64', 'utf-8')# att1[&quot;Content-Type&quot;] = 'application/octet-stream'# # 这里的filename可以任意写，写什么名字，邮件中显示什么名字# att1[&quot;Content-Disposition&quot;] = 'attachment; filename=&quot;test.txt&quot;'# message.attach(att1)## # 构造附件2，传送当前目录下的 runoob.txt 文件# att2 = MIMEText(open('runoob.txt', 'rb').read(), 'base64', 'utf-8')# att2[&quot;Content-Type&quot;] = 'application/octet-stream'# att2[&quot;Content-Disposition&quot;] = 'attachment; filename=&quot;runoob.txt&quot;'# message.attach(att2)# 文本中添加图片# msgAlternative = MIMEMultipart('alternative')# msgRoot.attach(msgAlternative)# mail_msg = &quot;&quot;&quot;# &lt;p&gt;Python 邮件发送测试...&lt;/p&gt;# &lt;p&gt;&lt;a href=&quot;http://www.runoob.com&quot;&gt;菜鸟教程链接&lt;/a&gt;&lt;/p&gt;# &lt;p&gt;图片演示：&lt;/p&gt;# &lt;p&gt;&lt;img src=&quot;cid:image1&quot;&gt;&lt;/p&gt;# &quot;&quot;&quot;# msgAlternative.attach(MIMEText(mail_msg, 'html', 'utf-8'))## # 指定图片为当前目录# fp = open('test.png', 'rb')# msgImage = MIMEImage(fp.read())# fp.close()## # 定义图片 ID，在 HTML 文本中引用# msgImage.add_header('Content-ID', '&lt;image1&gt;')# msgRoot.attach(msgImage)try: smtpObj = smtplib.SMTP('localhost') smtpObj.sendmail(my_sender, receivers, message.as_string()) print(&quot;邮件发送成功&quot;)except smtplib.SMTPException: print(&quot;Error: 无法发送邮件&quot;) 多线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101&quot;&quot;&quot;多线程：函数或者用类来包装线程对象&quot;&quot;&quot;# 函数式：调用thread模块中的start_new_thread()函数来产生新线程。语法如下:# thread.start_new_thread ( function, args[, kwargs] )# 参数说明:# function - 线程函数。# args - 传递给线程函数的参数,他必须是个tuple类型。# kwargs - 可选参数。# 两个模块# _thread# threading(推荐使用)import _threadimport time# 为线程定义一个函数def print_time( threadName, delay): count = 0 while count &lt; 5: time.sleep(delay) count += 1 print (&quot;%s: %s&quot; % ( threadName, time.ctime(time.time()) ))# 创建两个线程try: _thread.start_new_thread( print_time, (&quot;Thread-1&quot;, 2, ) ) _thread.start_new_thread( print_time, (&quot;Thread-2&quot;, 4, ) )except: print (&quot;Error: 无法启动线程&quot;)while 1: pass# _thread 提供了低级别的、原始的线程以及一个简单的锁，它相比于 threading 模块的功能还是比较有限的。## threading 模块除了包含 _thread 模块中的所有方法外，还提供的其他方法：## threading.currentThread(): 返回当前的线程变量。# threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。# threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。# 除了使用方法外，线程模块同样提供了Thread类来处理线程，Thread类提供了以下方法:## run(): 用以表示线程活动的方法。# start():启动线程活动。# join([time]): 等待至线程中止。这阻塞调用线程直至线程的join() 方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生。# isAlive(): 返回线程是否活动的。# getName(): 返回线程名。# setName(): 设置线程名。import threadingexitFlag = 0class myThread (threading.Thread): def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): print (&quot;开始线程：&quot; + self.name) print_time(self.name, self.counter, 5) print (&quot;退出线程：&quot; + self.name)def print_time(threadName, delay, counter): while counter: if exitFlag: threadName.exit() time.sleep(delay) print (&quot;%s: %s&quot; % (threadName, time.ctime(time.time()))) counter -= 1# 创建新线程thread1 = myThread(1, &quot;Thread-1&quot;, 1)thread2 = myThread(2, &quot;Thread-2&quot;, 2)# 开启新线程thread1.start()thread2.start()thread1.join()thread2.join()print (&quot;退出主线程&quot;)# 线程同步# 使用 Thread 对象的 Lock 和 Rlock 可以实现简单的线程同步，这两个对象都有 acquire 方法和 release 方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到 acquire 和 release 方法之间# 线程优先级队列（ Queue）# Python 的 Queue 模块中提供了同步的、线程安全的队列类，包括FIFO（先入先出)队列Queue，LIFO（后入先出）队列LifoQueue，和优先级队列 PriorityQueue。# 这些队列都实现了锁原语，能够在多线程中直接使用，可以使用队列来实现线程间的同步。# Queue 模块中的常用方法:# Queue.qsize() 返回队列的大小# Queue.empty() 如果队列为空，返回True,反之False# Queue.full() 如果队列满了，返回True,反之False# Queue.full 与 maxsize 大小对应# Queue.get([block[, timeout]])获取队列，timeout等待时间# Queue.get_nowait() 相当Queue.get(False)# Queue.put(item) 写入队列，timeout等待时间# Queue.put_nowait(item) 相当Queue.put(item, False)# Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号# Queue.join() 实际上意味着等到队列为空，再执行别的操作 urllib 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&quot;&quot;&quot;urllib 库用于操作网页 URL，并对网页的内容进行抓取处理。&quot;&quot;&quot;# urllib.request - 打开和读取 URL。# urllib.error - 包含 urllib.request 抛出的异常。# urllib.parse - 解析 URL。# urllib.robotparser - 解析 robots.txt 文件# urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)# url：url 地址。# data：发送到服务器的其他数据对象，默认为 None。# timeout：设置访问超时时间。# cafile 和 capath：cafile 为 CA 证书， capath 为 CA 证书的路径，使用 HTTPS 需要用到。# cadefault：已经被弃用。# context：ssl.SSLContext类型，用来指定 SSL 设置。from urllib.request import urlopenmyURL = urlopen(&quot;https://www.runoob.com/&quot;)print(myURL.read())print(myURL.read(600))print(myURL.readline())lines = myURL.readlines()for line in lines: print(line)print(myURL.getcode())f = open(&quot;runoob_urllib_test.html&quot;, &quot;wb&quot;)content = myURL.read() # 读取网页内容f.write(content)f.close()import urllib.requestencode_url = urllib.request.quote(&quot;https://www.runoob.com/&quot;) # 编码print(encode_url)unencode_url = urllib.request.unquote(encode_url) # 解码print(unencode_url)# 模拟头部信息# class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)# url：url 地址。# data：发送到服务器的其他数据对象，默认为 None。# headers：HTTP 请求的头部信息，字典格式。# origin_req_host：请求的主机地址，IP 或域名。# unverifiable：很少用整个参数，用于设置网页是否需要验证，默认是False。。# method：请求方法， 如 GET、POST、DELETE、PUT等。import urllib.parseurl = 'https://www.runoob.com/?s=' # 菜鸟教程搜索页面keyword = 'Python 教程'key_code = urllib.request.quote(keyword) # 对请求进行编码url_all = url+key_codeheader = { 'User-Agent':'Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'} #头部信息request = urllib.request.Request(url_all,headers=header)reponse = urllib.request.urlopen(request).read()fh = open(&quot;./urllib_test_runoob_search.html&quot;,&quot;wb&quot;) # 将文件写入到当前目录中fh.write(reponse)fh.close() XML解析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143&quot;&quot;&quot;XML解析&quot;&quot;&quot;# SAX 是一种基于事件驱动的API。# 利用 SAX 解析 XML 文档牵涉到两个部分: 解析器和事件处理器。# 解析器负责读取 XML 文档，并向事件处理器发送事件，如元素开始跟元素结束事件。# 而事件处理器则负责对事件作出响应，对传递的 XML 数据进行处理。# 1、对大型文件进行处理；# 2、只需要文件的部分内容，或者只需从文件中得到特定信息。# 3、想建立自己的对象模型的时候。# 在 Python 中使用 sax 方式处理 xml 要先引入 xml.sax 中的 parse 函数，还有 xml.sax.handler 中的 ContentHandler。# ContentHandler 类方法介绍# characters(content) 方法# 调用时机：# 从行开始，遇到标签之前，存在字符，content 的值为这些字符串。# 从一个标签，遇到下一个标签之前， 存在字符，content 的值为这些字符串。# 从一个标签，遇到行结束符之前，存在字符，content 的值为这些字符串。# 标签可以是开始标签，也可以是结束标签。# startDocument() 方法# 文档启动的时候调用。# endDocument() 方法# 解析器到达文档结尾时调用。# startElement(name, attrs) 方法# 遇到XML开始标签时调用，name 是标签的名字，attrs 是标签的属性值字典。# endElement(name) 方法# 遇到XML结束标签时调用。# make_parser 方法# 以下方法创建一个新的解析器对象并返回。# xml.sax.make_parser( [parser_list] )# 参数说明:# parser_list - 可选参数，解析器列表# parser 方法# 以下方法创建一个 SAX 解析器并解析xml文档：# xml.sax.parse( xmlfile, contenthandler[, errorhandler])# 参数说明:# xmlfile - xml文件名# contenthandler - 必须是一个 ContentHandler 的对象# errorhandler - 如果指定该参数，errorhandler 必须是一个 SAX ErrorHandler 对象# parseString 方法# parseString 方法创建一个 XML 解析器并解析 xml 字符串：# xml.sax.parseString(xmlstring, contenthandler[, errorhandler])# 参数说明:# xmlstring - xml字符串# contenthandler - 必须是一个 ContentHandler 的对象# errorhandler - 如果指定该参数，errorhandler 必须是一个 SAX ErrorHandler对象import xml.sax# from xml.dom.minidom import parse# import xml.dom.minidom# 使用xml.dom解析xml# 文件对象模型（Document Object Model，简称DOM），是W3C组织推荐的处理可扩展置标语言的标准编程接口。# 一个 DOM 的解析器在解析一个 XML 文档时，一次性读取整个文档，把文档中所有元素保存在内存中的一个树结构里，之后你可以利用DOM 提供的不同的函数来读取或修改文档的内容和结构，也可以把修改过的内容写入xml文件。# python中用xml.dom.minidom来解析xml文件# 使用minidom解析器打开 XML 文档# DOMTree = xml.dom.minidom.parse(&quot;movies.xml&quot;)# collection = DOMTree.documentElement# if collection.hasAttribute(&quot;shelf&quot;):# print (&quot;Root element : %s&quot; % collection.getAttribute(&quot;shelf&quot;))## # 在集合中获取所有电影# movies = collection.getElementsByTagName(&quot;movie&quot;)## # 打印每部电影的详细信息# for movie in movies:# print (&quot;*****Movie*****&quot;)# if movie.hasAttribute(&quot;title&quot;):# print (&quot;Title: %s&quot; % movie.getAttribute(&quot;title&quot;))## type = movie.getElementsByTagName('type')[0]# print (&quot;Type: %s&quot; % type.childNodes[0].data)# format = movie.getElementsByTagName('format')[0]# print (&quot;Format: %s&quot; % format.childNodes[0].data)# rating = movie.getElementsByTagName('rating')[0]# print (&quot;Rating: %s&quot; % rating.childNodes[0].data)# description = movie.getElementsByTagName('description')[0]# print (&quot;Description: %s&quot; % description.childNodes[0].data)class MovieHandler(xml.sax.ContentHandler): def __init__(self): self.CurrentData = &quot;&quot; self.type = &quot;&quot; self.format = &quot;&quot; self.year = &quot;&quot; self.rating = &quot;&quot; self.stars = &quot;&quot; self.description = &quot;&quot; # 元素开始调用 def startElement(self, tag, attributes): self.CurrentData = tag if tag == &quot;movie&quot;: print(&quot;*****Movie*****&quot;) title = attributes[&quot;title&quot;] print(&quot;Title:&quot;, title) # 元素结束调用 def endElement(self, tag): if self.CurrentData == &quot;type&quot;: print(&quot;Type:&quot;, self.type) elif self.CurrentData == &quot;format&quot;: print(&quot;Format:&quot;, self.format) elif self.CurrentData == &quot;year&quot;: print(&quot;Year:&quot;, self.year) elif self.CurrentData == &quot;rating&quot;: print(&quot;Rating:&quot;, self.rating) elif self.CurrentData == &quot;stars&quot;: print(&quot;Stars:&quot;, self.stars) elif self.CurrentData == &quot;description&quot;: print(&quot;Description:&quot;, self.description) self.CurrentData = &quot;&quot; # 读取字符时调用 def characters(self, content): if self.CurrentData == &quot;type&quot;: self.type = content elif self.CurrentData == &quot;format&quot;: self.format = content elif self.CurrentData == &quot;year&quot;: self.year = content elif self.CurrentData == &quot;rating&quot;: self.rating = content elif self.CurrentData == &quot;stars&quot;: self.stars = content elif self.CurrentData == &quot;description&quot;: self.description = contentif (__name__ == &quot;__main__&quot;): # 创建一个 XMLReader parser = xml.sax.make_parser() # 关闭命名空间 parser.setFeature(xml.sax.handler.feature_namespaces, 0) # 重写 ContextHandler Handler = MovieHandler() parser.setContentHandler(Handler) parser.parse(&quot;movies.xml&quot;) 应用领域 Web and Internet Development Database Access Desktop GUIs Scientific &amp; Numeric Education Network Programming Software &amp; Game Development 爬虫 WEB开发 图形处理 深度学习 数据分析","link":"/2021/05/31/Draft/2021/Python3%E5%AD%A6%E4%B9%A0/"},{"title":"开发修电脑","text":"介绍 工欲善其事，必先利其器。---《论语·卫灵公》 硬软件各种知识 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度：上篇 【】 快查 一、硬件 1.主板 参数： 2.显卡 3.内存 4.硬盘 5. 二、系统 三、工具 CPU - Z 查看CPU等相关信息 AID64 查看全部电脑硬件信息 XTU 测试稳定性 四、服务器 五、 引用：","link":"/2021/05/20/Draft/2021/%E5%BC%80%E5%8F%91%E4%BF%AE%E7%94%B5%E8%84%91/"},{"title":"信息系统项目管理师论文模板","text":"架构 一、项目背景(项目基本情况介绍、项目特点、要论述的论点) 二、提出论点(描述对××管理的重要性的认识) ​ 2020年6月，我参加了╳╳市资源局发起的“城市大脑”一地一码协同平台系统的建设工作，担任承建方项目经理，该信息综合管理平台系统的主要模块包括平台门户、“多规合一”业务协同系统优化、行政审批业务整合、土地码建设、数字驾驶舱规划和自然资源系统、驾驶舱指标库建设等9个业务管理模块，建设费用780万人民币，该项目于2021年6月通过了业主方的验收上线运行后,赢得了用户的一致好评，使项目获得了圆满成功。本文结合我的实际经验,以该项目为例,讨论了信息系统项目建设过程中的XX管理,主要从以下几个方面进行了阐述:XXXXX等过程内容，有效提高了XX管理水平，满足了项目干系人需求和期望。 ​ 2018年12月，“多规合一”业务协同系统中“多规共享”、“项目生成”等模块为各部门提供了非常便捷和快速的服务，取得了良好的效益。随着系统应用的不断深入，用户对“项目生成”等应用场景提出了更高的使用要求，如：项目类型、业务类型细化不足，导致在实际工作过程中存在盲区，影响了部分项目的推进等。另一方面，部门平台各自为政、系统间的数据存在壁垒，导致无法有效的进行数据共享，无法实现项目审批减材料、减时间、减环节，急需对原系统的各应用场景进行优化。杭州市XX资源局成立后，为尽快落实机构改革要求，释放改革红利，以自然资源“两统一”和业务需求为根本出发点，聚焦中心工作，立足已有基础，统筹整合现有资源，积极开展立足“一块土地一件事”全服务工作，由此一地一码协同平台应运而生。该项目采用了公开招标的方式，我公司参与了投标并顺利中标，于2020年6月签订了合同，随后成立了专门项目小组，任命我为项目经理。我公司的组织方式为项目型，项目成员直接归属项目经理领导，我组核心成员共15人。考虑到项目的复杂性、各区县地理位置不集中等原因，系统采用JAVA语言开发，项目组决定此项目采用SSM的三层b/s架构模式，地图基于leaflet的WebGIS可视化，数据库使用Postgres，这样的设计有利于后期的维护及扩展。项目共分为数字驾驶舱规划和自然资源系统、“一地一码”协同服务平台等子系统。经过全组成员的共同努力，项目于2021年6月顺利通过验收，目前运行情况良好。 ​ 由于该系统具有建设规模大（涵盖物价部门几乎所有业务），建设时间紧（建设期限为一年），涉及的干系人多（内部干系人包括项目组各类成员、公司领导和相关部门人员，外部干系人不仅包括市局各科室，也包括12个区县物价局的相关人员、运营商等），参与的项目成员多（高峰时达到60人，我将小组成员分为硬件集成部署组、数据中心组、软件开发组、平台测试组（四个小组），功能复杂等特点，为了保证项目圆满完成，我组建了强矩阵的项目组织结构，通过有效的项目管理，特别是出色的XX管理，带领项目团队全体成员经过奋战获得了良好的绩效，取得了项目的成功。本文将围绕该项目的XX管理进行重点讨论。 三、按照所涉及管理领域的过程分段进行论述(先给出每个过程的定义、输入、输出、工具与方法，再结合做过的项目描述项目中是如何做的) 一、 二、 三、 四、 五、 四、小结(对上面的论述进行一下小结) 五、分析项目存在的问题及改进的建议(写够三条就可以了) 六、总结(主要是写项目体会，比如通过项目实施，自己自觉应用项目管理知识、工具、技术，对成功实施项目所起的积极作用等)。 ​ 经过我们团队不懈的努力，历时1 年，本项目终于于2021年6月，通过了业主方组织的验收，实现从规划编制和实施、资源保护和利用、确权登记、地理资源等多方面进行业务和数据的融合，涵盖了规资局统一行使全民所有自然资源资产所有者职责，统一行使所有国土空间用途管制和生态保护修复职责。通过门户串联“多规合一”业务协同系统、行政审批系统、不动产登记系统等三大系统，以模块化形式，搭建多规共享、编制统筹、项目生成、实施监测、立项用地规划许可、工程许可、施工许可、竣工验收、确权登记等9个应用场景。本项目的成功在某些方面得益于我成功的xx 管理，在此我总结一下几条管理经验：（1）重视项目的调研，充分了解项目需求与范围；（2）树立正确的思想，采用适当的方法、遵循一定的流程，严格按照进度管理的要求做好活动定义、活动排序、活动的资源估算、活动的历时估算、编制进度计划、进度控制工作；（3）建立问题跟踪机制，对每个阶段的问题进行记录和跟踪，将每个问题落实到具体负责人。 ​ 当然，在本项目中，还有一些不足之处，比如：在项目的实施过程中，由于项目组2 名成员因为自身原因突然离职，导致项目的团队建设出现一些小问题，还有，曾经由于疫情原因导致一位技术人员居家隔离在家办公影响了些项目进度，不过，经过我后期的纠偏，并没有对项目产生什么影响。在后续的学习和工作中，我将不断的充电学习，和同行进行交流，提升自己的业务和管理水平，力争为我国信息化建设做出自己的努力。","link":"/2021/03/29/Draft/2021/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/"},{"title":"信息系统项目管理师","text":"新学四问 **WHY【与前代优化了什么，弥补了什么空白】：**管理知识、职称福利 **WHAT【框架，思维导图，主题框架】：**教材准备:视频一套，历年真题 HOW【如何记忆，学习资源】：平板做真题，查阅记录陌生概念，电脑Markdown记下重要笔记 **LEVEL【不是每个都学精】：**应试后实践 项目示例：追肖小姐的工程 准备：江山老师（视频），十大管理（打印），历年真题答案（） 技巧：多口诀，&quot;不择手段记忆&quot;，论文字要练，错不涂，十大管理要滚瓜烂熟，速看视频，适当笔记记忆，做题对应细化，论文提前准备 考试计划： 时间段 8.17-11.06（82）周末两天【专业知识，软考】 工作日【中午一个半小时专业知识，下班7-9两个小时以上软考】 阶段 详细步骤 完成情况 用时 8.17-9.15 完成所有视频并笔记，背诵基础知识【选择，案例，计算，论文】专项研究 17【2】18【1】19【】20【1】21【】22【】23【1】24【1】25【2】26【2】27【】28【1】29【】30【3】31【】1【2】2【2】3【2】4【】5【】6【3】7【2基础十大管理完成，案例两节，计算两节】8【3】9【3】案例完成，计算四节10、13【4计算完】 完成 9.15-9.30 16-18年6套真题精做并分类总结 （一） 做真题套题平板或打印，对应基础速览，重点Anki记忆，错题归结，论文课开始。闲时软件刷题。构建知识框架。16年之前20套的直接看解析，陌生记录 三天一套，知识点Anki一遍并添加，狠补基础 16【2005上】26【2005下】 9.30-10.15 19年2套真题精做并分类总结 （二） 同上，完成（一）未完成的 真题外，论文框架准备并背诵 完成 10.15-10.30 20、21年三套真题定时并分类总结 （一） 总结，所有错题排查，查漏补缺 五天周末一次真实模拟，论文写后总结，练练字 完成 10.30-11.05 论文框架梳理，模拟练字速度 考前准备（工具，考场，是否留宿） 加油！ 倒计时：3 考试介绍，学习方法，重难点，10大知识域概述 考试介绍 通过考试相当于拿到高级职称，难度上午大于论文大于案例。 作用 方法 联想记忆，知识术语哪个板块，板块下那些过程，输入输出工具技术，计算相关，陌生词汇 重视基础（上午），考哪里学哪里，重视真题，费曼学习法，艾宾浩斯记忆，利用碎片时间，考上决心，2/8法则，PDCA循环，人机料法环，无事不项目，错题，知识点ANKI记录记忆 狗（沟通）子（质量）整（整体）范（范围）进（进度），成（成本）人（人力资源，干系人）风（风险）采（采购） 佂帆进城志，力够风采干。 掌管质检边界，鬼激钉娃缺恐，鬼火牌子痴恐，鬼故狱恐，鬼食恐，鬼祖见冠，鬼棺孔，鬼时醒两嘤孔，鬼识孔姐，识鬼棺孔 整 章 管 指 监变 结 范 规集定W 确控 进 规活排资持制 控 成 规估预 控 质 规 实 控 力 规 组建管 沟 规 管 控 风 规识性量应 控 采 规实 控 ···结 人 识 规 管 控 所有管理的综合性管理 考点题型，先做题再对应看书细化 重视周末大段时间 三从 从过程想结果，从结果知输入，从输入选工具 四得 一得文件计划；二得成果数据；三得变更请求；四的因素资产 共性总结 1、上一个过程的输出大部分是下一个过程的输入 2、计划和文件是不一样的（每个输入都有计划和文件） 3、被批准的变更请求约等于计划 4、在执行和监控过程产生新的变更请求（变更请求包括变什么和怎么变，这是变更请求和纠正、预防、缺陷修复的关系） 5、执行过程产生工作绩效数据---数据+背景在监控过程组成为了工作绩效信息然后输出工作绩效报告注意工作绩效数据是执行过程的输出→那么就是监控的输入→监控的输出就成了工作绩效信息 6、通过过程的含义记忆每个过程组最主要的成果（输出）！ 7、监控过程组每个过程都有计划+工作绩效报告要记住我说的监控的那些原理 跟踪进展→拿着计划的这把尺子测量实际的工作绩效报告→偏差计算→偏差评估、分析是否变更→预测→趋势分析 8、项目管理计划和其它子计划（范围管理计划、进度管理计划、质量管理计划等）区别和联系下面以项目管理计划和其中一个子计划为例子总结规律如下：了解后对其它过程帮助甚多 （1）当子计划或基准是主要的输入时→专门列出（如定义范围、收集需求等过程） （2）当子计划或基准是首次输出时→专门列出（如范围管理计划），以后的输出都以“项目管理计划更新”的形式出现 （3）子计划或基准是作为输出时候→项目管理计划将作为输入（如规划范围管理） （4）对控制过程组来说，输入和输出都是项目管理计划而不是具体的子计划 重点，十大管理 1.论文写作基本介绍 ​ 二选一，两小时，注意写字整洁。2500左右 摘要300 正文2000。 ​ 学好理论，写字速度整洁，多看范文，选定素材，别写具体项目名称。 2.信息化与信息系统【23】 网络 物数网传会表应 OSI/RM（Open System Interconnection/Reference Model，开放系统互连参考模型） 物理层：比特，串行传输。数据链路层：帧。网络层：分组，处理与寻址和传输有关的管理问题，提供点对点的连接。传输层：报文，建立、维护和撤销传输连接（端对端的连接），并进行流量控制和差错控制。 测试，软件需求，生命周期，面向对象，中间件，新网络技术 UML，测试 重点 新一代信息技术（英语单词），物联网，大数据，云计算，移动互联网，智慧城市，互联网+（互联网+各种传统行业），智能制造2025，AR，VR，AI，区块链，特别联网，5G，华为鸿蒙麒麟，一带一路，两化融合 信息安全技术 3.信息系统项目管理基础 项目特点：（1）临时性：有明确的开始和结束时间。 （2）独特性：世上没有两个完全相同的项目。 （3）渐进明细性：前期只能粗略定义，然后逐渐明朗、完善和精确，这也就意味着变更不可避免，所以要控制变更。 **项目的组织方式：**职能型、项目型、矩阵型 4.立项管理 2 （一个项目从提出申请到批准立项）招投标，合同，采购 项目建议书（立项申请)主要内容：项目的必要性、项目的市场预测、产品方案或服务的市场预测、项目建设必需的条件等 计算: 利率 利率有单利和复利，单利息=本金×利率×期限，F=P×（1+i）n F：复利终值 P：本金 i：利率 N：利率获取时间的整数倍 净现值（Net Present Value，NPV） 净现值大于零则方案可行，且净现值越大，方案越优，投资效益越好。 NPV 的计算步骤如下： （1）根据项目的资本结构设定项目的折现率。 （2）计算每年项目现金流量的净值。 （3）根据设定的折现率计算每年的净现值。 （4）将净现值累加起来。 净现值率 净现值率（NPVR）=项目的净现值（NPV）/原始投资的现值合计 投资回收期 招标 招标（招标公告，招标文件，） 中标通知书发出之日起三十日内应当签订合同，截止时间15日前进行必要的澄清或者修改，招标人应当确定投标人编制投标文件所需要的合理时间。但是，依法必须进行招标的项目，自招标文件开始发出之日起至投标人提交投标文件截止之日止，最短不得少于20日。 投标 投标人少于三个的，招标人应当重新招标。在招标文件要求提交投标文件的截止时间后送达的投标文件，招标人应当拒收。 开标 开标应当在招标文件确定的提交投标文件截止时间的同一时间公开进行。开标地点应当为招标文件中预先确定的地点。开标由招标人主持，邀请所有投标人参加。 评标 评标委员会由招标人的代表和有关技术、经济等方面的专家组成，成员人数为5人以上单数，其中技术、经济等方面的专家不得少于成员总数的三分之二。 中标 中标通知书发出后，招标人改变中标结果的，或者中标人放弃中标项目的，应当依法承担法律责任。 合同 分类： 范围：项目总承包合同、项目单项承包合同、项目分包合同。 付款方式：项目总价合同、项目单价合同、项目成本加酬金合同 合同的主要内容包括：项目名称；标的内容和范围；项目的质量要求：通常情况下采用技术指标限定等各种方式来描述信息系统工程的整体质量标准以及各部分质量标准，它是判断整个工程项目成败的重要依据；项目的计划、进度、地点、地域和方式；项目建设过程中的各种期限；技术情报和资料的保密；风险责任的承担；技术成果的归属；验收的标准和方法；价款、报酬（或使用费）及其支付方式；违约金或者损失赔偿的计算方法；解决争议的方法：该条款中应尽可能地明确在出现争议与纠纷时采取何种方式来协商解决；名词术语解释等 8.整体管理 ITO ITO 名称及定义 输入 工具技术 输出 项目章程 正式批准一个项目的文档，制定项目的头（项目经理） 项目工作说明书商业论证协议事业环境因素组织过程资产 专家判断引导技术 项目章程 制定项目管理计划 包括定义、准备和协调所有构成计划，形成项目管理计划所必要的行动 项目章程其他过程输入【项目初步范围说明书，管理过程，预测，工作绩效信息】事业环境因素组织过程资产 专家判断引导技术 项目管理计划 指导与管理项目工作 提意见，记考核，做东西 项目管理计划，批准的变更请求，事业环境因素，组织过程资产 专家判断项目管理信息系统会议 可交付成果工作绩效数据变更请求项目管理计划更新项目文件更新 监控项目工作 从项目开始到结束，收集，测量发布绩效信息，及评估会影响过程改进的度量项和趋势 项目管理计划事业环境因素组织过程资产进度预测成本预测确认的变更工作绩效信息 专家判断分析技术项目管理信息系统【PMIS】会议 变更请求工作绩效报告项目管理计划更新项目文件更新 实施整体变更控制 审批变更，项目经理付最终责任 变更请求，工作绩效报告，变更请求，项目管理计划，事业环境因素，组织过程资产 专家判断会议变更控制工具 批准的变更请求变更日志项目管理计划更新项目文件更新 项目收尾 打包交货 项目管理计划验收的可交付成果组织过程资产 专家判断分析技术会议 最终产品、服务或成果移交组织过程资产更新 9.范围管理 ITO ITO 名称及定义 输入 工具技术 输出 10.进度管理 ITO ITO 名称及定义 输入 工具技术 输出 11.成本管理 ITO ITO 名称及定义 输入 工具技术 输出 挣值分析计算 12.质量管理 ITO ITO 名称及定义 输入 工具技术 输出 13.人力资源管理 ITO ITO 名称及定义 输入 工具技术 输出 14.干系人管理 ITO ITO 名称及定义 输入 工具技术 输出 15.沟通管理 ITO ITO 名称及定义 输入 工具技术 输出 16.风险管理 ITO ITO 名称及定义 输入 工具技术 输出 17.采购管理 ITO ITO 名称及定义 输入 工具技术 输出 PPT加星看了就可以了 18.合同管理 上午一分 合同的分类 19.信息文档和配置管理 上午2分 文档分类（开发，产品，管理） 上午一般三分 20.20-28章 知识管理 显性知识，隐性知识,隐性知识分享途经，知识产权保护，软件著作权 战略管理 组织战略因素组成，战略实施，类型层次，平衡计分卡 组织级项目管理 流程管理 项目集，项目组合管理 信息安全管理 综合测试管理、量化项目管理，成熟度模型 CMMI（能力成熟度模型）、 法律法规 合同法 1 合同内容，要约，标的，格式（非格式【手写】）条款，违心合同， 招标法 1 必须进行招标的项目、标底（必须保密） 著作权法 政府采购法 2 常用技术标准 2-3 软件工程国家标准GB/T11457-2006（审计-代码审记-配置审计-认证-走查-鉴定-基线-配置控制委员会-配置状态报告-设计评审-桌面检查-评价-故障-功能配置审计） 软件生存周期的过程，软件生命周期各阶段与软件文档编制工作的关系，各类人员与软件文档的使用关系，软件产品质量，6个质量特性21个质量子特性 案例分析 五种题型 问答题，计算题，分析题，理论题，填空，选择，判断题 分析题 回答简练，文字工整清晰，答题有序，多写不扣分，专业化 万金油： 技术出身：开发和管理所需技能不同，需要培训 身兼数职：导致没时间去学习管理知识，工作负荷过载，身心疲惫，全局影响 新技术：风险，需培训，学习，监控技术风险，找到合适的人，实在不行外包 有人对项目不满意：简历有效沟通机制方式发法，缺乏有效的项目绩效管理机制，需加强沟通 变更：书面申请，审批确认，跟踪变更缺一不可 客户验收不通过：说明验收标准没有得到认可确认，没有验收测试规范和方法 愚人有关问题：沟通不到位 过一段时间才发现问题：监控不力 里程碑，时间紧促：没有冗余考虑风险的想法 外部因素导致延工：没有考虑外在因素影响，变更5个理由 争执：沟通问题，计划不够周明 多头汇报：项目章程，多头汇报导致信息沟通不畅通或产生冲突 知识点 如何缩短活动的工期，成本控制，质量新老7工具，提升项目质量， 答题关键词 新人：培训 并行工作：有风险 客户要求都答应： 口头：书面 备忘录：配置管理 部分人：全员参与 开会很久：效率低下 当场修改，能改就改：变更监控分析记录 文档配置管理 人员离职：AB思想，风险 总价合同：乙方风险太大 前期需求调研不充分： 计算 进度类 EV、AC、PV需验算确保后面不错。 管理储备不计入挣值和基准，但是总预算一部分。 注意审题，时间和单位。 总时差 是指在不延误项目完成日期或违反进度因素的前提下，某活动可以推迟的时间。 总时差=LS-ES=LF-EF 自由时差 是指在不影响紧后活动最早开始的情况下，当前活动可以推迟的时间。 自由时差=(后一活动)ES-(前一活动的)EF 所以总时差影响总工期，自由时差影响紧后活动。 （1）总时差（TF）：当一项活动的最早开始时间和最迟开始时间不相同时，它们之间的差值是该工作的总时差。计算公式是：TF=LS-ES。 （2）自由时差（FF）：在不影响紧后活动完成时间的条件下，一项活动可能被延迟的时间是该项活动的自由时差，它由该项活动的最早完成时间EF和它的紧后活动的最早开始时间决定的。计算公式是：FF=min{紧后活动的ES}-EF。 （3）关键路径。项目的关键路径是指能够决定项目最早完成时间的一系列活动。它是网络图中的最长路径，具有最少的时差。在实际求关键路径时，一般的方法是看哪些活动的总时差为0，总时差为0的活动称为关键活动，关键活动组成的路径称为关键路径。 尽管关键路径是最长的路径，但它代表了完成项目所需的最短时间。因此，关键路径上各活动持续时间（历时）的和就是项目的计算工期。 最早开始时间（ES）： 一项活动的最早开始时间取决于它的所有紧前活动的完成时间。通过计算到该活动路径上所有活动的完成时间的和，可得到指定活动的ES。如果有多条路径指向此活动，则计算需要时间最长的那条路径，即ES=max{紧前活动的EF}。 最早结束时间（EF）： 一项活动的最早完成时间取决于该工作的最早开始时间和它的持续时间（D），即EF=ES+D。 最晚结束时间（LF）： 在不影响项目完成时间的条件下，一项活动可能完成的最迟时间。计算公式是：LF=min{紧后活动的LS}。 最晚开始时间（LS）： 在不影响项目完成时间的条件下，一项活动可能开始的最晚时间。计算公式是：LS=LF-D。 前推法来计算最早时间 某一活动的最早开始时间（ES）=指向它的所有紧前活动的最早结束时间的最大值。 某一活动的最早结束时间（EF）=ES+T（作业时间） 逆推法来计算最迟时间 某一活动的最迟结束时间（LF）=指向它的所有紧后活动的最迟开始时间的最小值。 某一活动的最迟开始时间（LS）=LF-T（作业时间） 计算关键路径的步骤 **1. 用有方向的线段标出各结点的紧前活动和紧后活动的关系，使之成为一个有方向的网络图（PDM） \\2. 用正推和逆推法计算出各个活动的ES,LS, EF, LF，并计算出各个活动的自由时差。找出所有总时差为零或为负的活动，就是关键活动 \\3. 关键路径上的活动持续时间决定了项目的工期，总和就是项目工期。 **自由时差 ** 计算公式： 自由时差=所有紧后工作中最早开始时间最小值－ 最早结束时间 **总时差 ** 计算公式： 总时差=最迟开始时间-最早开始时间=最迟结束时间-最早结束时间 单代号网络图 双代号网络图 双代号时标网络图 全为实线的为关键路径， 成本类 挣值分析 PV [Planned Value]计划值：应该完成多少工作？ 要干的活 EV [Earned Value]挣值：完成了多少预算工作？ 干完的活 AC [Actual Cost]实际成本：完成工作的实际成本是多少？ 实际花费 BAC [Budget cost at completion] 基线预算成本：全部工作的预算是多少？不改变成本基准，BAC就不会发生变化 CV [Cost Variance] 成本偏差 CV＝EV－AC，CV&gt;0，成本节约，CV&lt;0，成本超支。 SV [Schedule Variance] 进度偏差 SV＝EV－PV，SV&gt;0，进度超前，SV&lt;0，进度落后。 CPI [Cost Performance Index] 成本执行指数CPI＝EV/AC，CPI＝1，资金使用效率一般；CPI&gt;1成本节约，资金使用效率高；CPI&lt;1，成本超支，资金使用效率低。 SPI [Schedule Performance Index] 进度执行指数 SPI＝EV/PV，SPI＝1，进度与计划相符，SPI&gt;1，进度超前，SPI&lt;1，进度落后。 ETC [Estimate (or Estimated) To Complete] 完工时尚需成本估算：到完成时，剩余工作量还需要多少成本,ETC也就是估计完成项目的剩余工作成本 BAC [Budget cost at completion] 完工预算：全部工作的预算是多少？不改变成本基准，BAC就不会发生变化 EAC [Estimate at completion] 完成预估：全部工作的成本是多少？是根据项目的绩效和风险量化对项目最可能的总成本所做的一种预测。 完工工期估算=预算工期/SPI 预测EAC 与 ETC： 由于存在成本偏差情况，所以在典型偏差与非典型偏差时，计算顺序不一样，如下: 典型偏差：未来项目的CPI、SPI会保持不变，此时预测项目完成时的总成本和预计完成时间，应该用典型偏差公式。 EAC= AC+(BAC-EV) =BAC/CPI ETC=EAC-AC 非典型偏差：非典型偏差的含义是项目未来的工作绩效与当前无关，和原计划保持一样，即项目未来的成本绩效指数和进度绩效指数都是“1”）需要纠偏 ETC=BAC-EV 基线总成本-已挣得部份 EAC=ETC+AC 上午小计算 盈亏平衡点 决策树计算 资源平衡问题 统计抽样问题 自制\\外购分析 沟通渠道数 [n(n-1)]/2 风险曝光度 现值 净现值，投资回收期 运筹学 计算量大 求最短和最长路径 线性规划问题 投资收益最大问题 更换设备问题 车床铣床问题 流量问题 人员分配问题【矩阵法（行列分别减一次最小值，根据行零个数从少到多分配）】 伏格尔方法解传输问题 论文 提前准备，摘要300内，正文不少于2000，字迹清晰，无需写题目 准备一个项目（投标书，方案书） 准备框架 自己写一个","link":"/2021/03/29/Draft/2021/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88/"},{"title":"RabbitMQ","text":"RabbitMQ简介 MQ全称为Message Queue，消息队列是应用程序和应用程序之间的通信方法。 RabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。RabbitMQ官方地址：http://www.rabbitmq.com/ AMQP 一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。 AMQP是一个二进制协议，拥有一些现代化特点：多信道、协商式，异步，安全，扩平台，中立，高效。 RabbitMQ是AMQP协议的Erlang的实现。 概念 说明 连接Connection 一个网络连接，比如TCP/IP套接字连接。 会话Session 端点之间的命名对话。在一个会话上下文中，保证“恰好传递一次”。 信道Channel 多路复用连接中的一条独立的双向数据流通道。为会话提供物理传输介质。 客户端Client AMQP连接或者会话的发起者。AMQP是非对称的，客户端生产和消费消息，服务器存储和路由这些消息。 服务节点Broker 消息中间件的服务节点；一般情况下可以将一个RabbitMQ Broker看作一台RabbitMQ 服务器。 端点 AMQP对话的任意一方。一个AMQP连接包括两个端点（一个是客户端，一个是服务器）。 消费者Consumer 一个从消息队列里请求消息的客户端程序。 生产者Producer 一个向交换机发布消息的客户端应用程序。 6种工作模式： 简单模式，work模式，Publish/Subscribe发布与订阅模式，Routing路由模式，Topics主题模式，RPC远程调用模式（远程调用，不太算MQ；暂不作介绍）； 官网对应模式介绍：https://www.rabbitmq.com/getstarted.html 优势： 应用解耦，提升容错性和可维护性 异步提速，提升用户体验，系统吞吐量 削峰填谷，提高系统稳定性 劣势： 系统可用性降低（需保证MQ高可用） 系统复杂度提高（不被重复消费，保证消息顺序） 数据一致性问题 使用条件: 生产者不需要从消费者处获得反馈 容许短暂的不一致性 确实用了有效果 ，好处大于复杂性等劣势 常见MQ产品 JMS JMS即Java消息服务（JavaMessage Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 AMQP 与 JMS 区别 JMS是定义了统一的接口，来对消息操作进行统一；AMQP是通过规定协议来统一数据交互的格式 JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的。 JMS规定了两种消息模式；而AMQP的消息模式更加丰富 RabbitMQ 安装配置 MQ版本选择和erlang相关 使用资料里提供的CentOS-7-x86_64-DVD-1810.iso 安装虚拟机. 1. 安装依赖环境 在线安装依赖环境： 1yum install build-essential openssl openssl-devel unixODBC unixODBC-devel make gcc gcc-c++ kernel-devel m4 ncurses-devel tk tc xz 2. 安装Erlang 上传 alt+p进入sftp界面 erlang-18.3-1.el7.centos.x86_64.rpm socat-1.7.3.2-5.el7.lux.x86_64.rpm rabbitmq-server-3.6.5-1.noarch.rpm 12# 安装rpm -ivh erlang-18.3-1.el7.centos.x86_64.rpm 如果出现如下错误 说明gblic 版本太低。我们可以查看当前机器的gblic 版本 1strings /lib64/libc.so.6 | grep GLIBC 当前最高版本2.12，需要2.15.所以需要升级glibc 使用yum更新安装依赖 1sudo yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make -y 下载rpm包 1234567wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-utils-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-static-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-common-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-devel-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-headers-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/nscd-2.17-55.el6.x86_64.rpm &amp; 安装rpm包 1sudo rpm -Uvh *-2.17-55.el6.x86_64.rpm --force --nodeps 安装完毕后再查看glibc版本,发现glibc版本已经到2.17了 1strings /lib64/libc.so.6 | grep GLIBC 3. 安装RabbitMQ 123456# 安装rpm -ivh socat-1.7.3.2-5.el7.lux.x86_64.rpm# 安装rpm -ivh rabbitmq-server-3.6.5-1.noarch.rpm 12345678910111213[root@softbank126011009150 rabbitmq]# lserlang-18.3-1.el7.centos.x86_64.rpm rabbitmq-server-3.6.5-1.noarch.rpm socat-1.7.3.2-1.1.el7.x86_64.rpm[root@softbank126011009150 rabbitmq]# rpm -ivh erlang-18.3-1.el7.centos.x86_64.rpm 准备中... ################################# [100%] 软件包 erlang-18.3-1.el7.centos.x86_64 已经安装[root@softbank126011009150 rabbitmq]# rpm -ivh socat-1.7.3.2-1.1.el7.x86_64.rpm 警告：socat-1.7.3.2-1.1.el7.x86_64.rpm: 头V4 RSA/SHA1 Signature, 密钥 ID 87e360b8: NOKEY准备中... ################################# [100%] 软件包 socat-1.7.3.2-1.1.el7.x86_64 已经安装[root@softbank126011009150 rabbitmq]# rpm -ivh rabbitmq-server-3.6.5-1.noarch.rpm 警告：rabbitmq-server-3.6.5-1.noarch.rpm: 头V4 RSA/SHA1 Signature, 密钥 ID 6026dfca: NOKEY准备中... ################################# [100%] 软件包 rabbitmq-server-3.6.5-1.noarch 已经安装 4. 开启管界面及配置 123456# 开启管理界面rabbitmq-plugins enable rabbitmq_management# 修改默认配置信息vim /usr/lib/rabbitmq/lib/rabbitmq_server-3.6.5/ebin/rabbit.app # 比如修改密码、配置等等，例如：loopback_users 中的 &lt;&lt;&quot;guest&quot;&gt;&gt;,只保留guesthttp://ip:15672 访问 5. 启动 123456service rabbitmq-server start # 启动服务service rabbitmq-server stop # 停止服务service rabbitmq-server restart # 重启服务service iptables stop#关闭防火墙systemctl stop firewalld #conos7关闭防火墙方法 设置配置文件 1234cd /usr/share/doc/rabbitmq-server-3.6.5/cp rabbitmq.config.example /etc/rabbitmq/rabbitmq.config 6. 配置虚拟主机及用户 6.1. 用户角色 RabbitMQ在安装好后，可以访问http://ip地址:15672 ；其自带了guest/guest的用户名和密码；如果需要创建自定义用户；那么也可以登录管理界面后，如下操作： 角色说明： 1、 超级管理员(administrator) 可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 2、 监控者(monitoring) 可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 3、 策略制定者(policymaker) 可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 4、 普通管理者(management) 仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。 5、 其他 无法登陆管理控制台，通常就是普通的生产者和消费者。 6.2. Virtual Hosts配置 像mysql拥有数据库的概念并且可以指定用户对库和表等操作的权限。RabbitMQ也有类似的权限管理；在RabbitMQ中可以虚拟消息服务器Virtual Host，每个Virtual Hosts相当于一个相对独立的RabbitMQ服务器，每个VirtualHost之间是相互隔离的。exchange、queue、message不能互通。 相当于mysql的db。Virtual Name一般以/开头。 6.2.1. 创建Virtual Hosts 6.2.2. 设置Virtual Hosts权限 解决配置文件未找到 1234567891011121314151617181920212223242526272829[root@softbank126011009150 rabbitmq]# cd /usr/share/doc/rabbitmq-server-3.6.5/[root@softbank126011009150 rabbitmq-server-3.6.5]# lsLICENSE LICENSE-APL2-Stomp-Websocket LICENSE-EPL-OTP LICENSE-MIT-jQuery164 LICENSE-MIT-Sammy060 LICENSE-MPL-RabbitMQ set_rabbitmq_policy.sh.exampleLICENSE-APACHE2-ExplorerCanvas LICENSE-BSD-base64js LICENSE-MIT-EJS10 LICENSE-MIT-Mochi LICENSE-MIT-SockJS rabbitmq.config.exampleLICENSE-APL2-Rebar LICENSE-BSD-glMatrix LICENSE-MIT-Flot LICENSE-MIT-Mochiweb LICENSE-MPL2 README[root@softbank126011009150 rabbitmq-server-3.6.5]# ll总用量 200-rw-r--r--. 1 root root 28945 8月 5 2016 LICENSE-rw-r--r--. 1 root root 11358 8月 5 2016 LICENSE-APACHE2-ExplorerCanvas-rw-r--r--. 1 root root 10175 8月 5 2016 LICENSE-APL2-Rebar-rw-r--r--. 1 root root 10851 8月 5 2016 LICENSE-APL2-Stomp-Websocket-rw-r--r--. 1 root root 1206 8月 5 2016 LICENSE-BSD-base64js-rw-r--r--. 1 root root 1304 8月 5 2016 LICENSE-BSD-glMatrix-rw-r--r--. 1 root root 14041 8月 5 2016 LICENSE-EPL-OTP-rw-r--r--. 1 root root 1087 8月 5 2016 LICENSE-MIT-EJS10-rw-r--r--. 1 root root 1069 8月 5 2016 LICENSE-MIT-Flot-rw-r--r--. 1 root root 1075 8月 5 2016 LICENSE-MIT-jQuery164-rw-r--r--. 1 root root 1087 3月 31 2016 LICENSE-MIT-Mochi-rw-r--r--. 1 root root 1087 8月 5 2016 LICENSE-MIT-Mochiweb-rw-r--r--. 1 root root 1076 8月 5 2016 LICENSE-MIT-Sammy060-rw-r--r--. 1 root root 1056 8月 5 2016 LICENSE-MIT-SockJS-rw-r--r--. 1 root root 16726 8月 5 2016 LICENSE-MPL2-rw-r--r--. 1 root root 24897 8月 5 2016 LICENSE-MPL-RabbitMQ-rw-r--r--. 1 root root 21023 4月 11 2016 rabbitmq.config.example-rw-r--r--. 1 root root 943 3月 31 2016 README-rw-r--r--. 1 root root 277 3月 31 2016 set_rabbitmq_policy.sh.example[root@softbank126011009150 rabbitmq-server-3.6.5]# cp ./rabbitmq.config.example /etc/rabbitmq/rabbitmq.config[root@softbank126011009150 rabbitmq-server-3.6.5]# service rabbitmq-server restartRestarting rabbitmq-server (via systemctl): [ 确定 ] 工作模式 简单模式 生产者 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.lxl.producer;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Producer_HelloWord { public static void main(String[] args) throws IOException, TimeoutException { //1.创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); //2. 设置参数 factory.setHost(&quot;126.11.41.128&quot;);//ip 默认值 localhost factory.setPort(5672); //端口 默认值 5672 factory.setVirtualHost(&quot;/lxlv&quot;);//虚拟机 默认值/ factory.setUsername(&quot;lxl&quot;);//用户名 默认 guest factory.setPassword(&quot;lxl&quot;);//密码 默认值 guest //3. 创建连接 Connection Connection connection = factory.newConnection(); //4. 创建Channel Channel channel = connection.createChannel(); //5. 创建队列Queue /* queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) 参数： 1. queue：队列名称 2. durable:是否持久化，当mq重启之后，还在 3. exclusive： * 是否独占。只能有一个消费者监听这队列 * 当Connection关闭时，是否删除队列 * 4. autoDelete:是否自动删除。当没有Consumer时，自动删除掉 5. arguments：参数。 */ //如果没有一个名字叫hello_world的队列，则会创建该队列，如果有则不会创建 channel.queueDeclare(&quot;hello_world&quot;,true,false,false,null); /* basicPublish(String exchange, String routingKey, BasicProperties props, byte[] body) 参数： 1. exchange：交换机名称。简单模式下交换机会使用默认的 &quot;&quot; 2. routingKey：路由名称 3. props：配置信息 4. body：发送消息数据 */ String body = &quot;hello rabbitmq~~~&quot;; //6. 发送消息 channel.basicPublish(&quot;&quot;,&quot;hello_world&quot;,null,body.getBytes()); //7.释放资源// channel.close();// connection.close(); }} 消费者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.lxl.consumer;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer_HelloWorld { public static void main(String[] args) throws IOException, TimeoutException { //1.创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); //2. 设置参数 factory.setHost(&quot;126.11.41.128&quot;);//ip 默认值 localhost factory.setPort(5672); //端口 默认值 5672 factory.setVirtualHost(&quot;/lxlv&quot;);//虚拟机 默认值/ factory.setUsername(&quot;lxl&quot;);//用户名 默认 guest factory.setPassword(&quot;lxl&quot;);//密码 默认值 guest //3. 创建连接 Connection Connection connection = factory.newConnection(); //4. 创建Channel Channel channel = connection.createChannel(); //5. 创建队列Queue /* queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) 参数： 1. queue：队列名称 2. durable:是否持久化，当mq重启之后，还在 3. exclusive： * 是否独占。只能有一个消费者监听这队列 * 当Connection关闭时，是否删除队列 * 4. autoDelete:是否自动删除。当没有Consumer时，自动删除掉 5. arguments：参数。 */ //如果没有一个名字叫hello_world的队列，则会创建该队列，如果有则不会创建 channel.queueDeclare(&quot;hello_world&quot;,true,false,false,null); /* basicConsume(String queue, boolean autoAck, Consumer callback) 参数： 1. queue：队列名称 2. autoAck：是否自动确认 3. callback：回调对象 */ // 接收消息 Consumer consumer = new DefaultConsumer(channel){ /* 回调方法，当收到消息后，会自动执行该方法 1. consumerTag：标识 2. envelope：获取一些信息，交换机，路由key... 3. properties:配置信息 4. body：数据 */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(&quot;consumerTag：&quot;+consumerTag); System.out.println(&quot;Exchange：&quot;+envelope.getExchange()); System.out.println(&quot;RoutingKey：&quot;+envelope.getRoutingKey()); System.out.println(&quot;properties：&quot;+properties); System.out.println(&quot;body：&quot;+new String(body)); } }; channel.basicConsume(&quot;hello_world&quot;,true,consumer); //关闭资源？不要 }} RabbitMQ运转流程 在入门案例中： 生产者发送消息 生产者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker； 声明队列并设置属性；是否排它，是否持久化，是否自动删除； 将路由键（空字符串）与队列绑定起来； 发送消息至RabbitMQ Broker； 关闭信道； 关闭连接； 消费者接收消息 消费者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker 向Broker 请求消费相应队列中的消息，设置相应的回调函数； 等待Broker回应闭关投递响应队列中的消息，消费者接收消息； 确认（ack，自动确认）接收到的消息； RabbitMQ从队列中删除相应已经被确认的消息； 关闭信道； 关闭连接； 生产者流转过程说明 客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQPO-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start/.Start-OK 、Connection.Tune/.Tune-Ok ，Connection.Open/ .Open-Ok 这6 个命令的交互。 客户端调用connection.createChannel方法。此方法开启信道，其包装的channel.open命令发送给Broker,等待channel.basicPublish方法，对应的AMQP命令为Basic.Publish,这个命令包含了content Header 和content Body()。content Header 包含了消息体的属性，例如:投递模式，优先级等，content Body 包含了消息体本身。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 消费者流转过程说明 消费者客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQPO-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start/.Start-OK 、Connection.Tune/.Tune-Ok ，Connection.Open/ .Open-Ok 这6 个命令的交互。 消费者客户端调用connection.createChannel方法。和生产者客户端一样，协议涉及Channel . Open/Open-Ok命令。 在真正消费之前，消费者客户端需要向Broker 发送Basic.Consume 命令(即调用channel.basicConsume 方法〉将Channel 置为接收模式，之后Broker 回执Basic . Consume - Ok 以告诉消费者客户端准备好消费消息。 Broker 向消费者客户端推送(Push) 消息，即Basic.Deliver 命令，这个命令和Basic.Publish 命令一样会携带Content Header 和Content Body。 消费者接收到消息并正确消费之后，向Broker 发送确认，即Basic.Ack 命令。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 工作队列模式 模式说明（只有一盒糖你们几个抢） Work Queues与入门程序的简单模式相比，多了一个或一些消费端，多个消费端共同消费同一个队列中的消息。 应用场景：对于 任务过重或任务较多情况使用工作队列可以提高任务处理的速度。 代码 Work Queues与入门程序的简单模式的代码是几乎一样的；可以完全复制，并复制多一个消费者进行多个消费者同时消费消息的测试。 生产者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.itheima.rabbitmq.work;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class Producer { static final String QUEUE_NAME = &quot;work_queue&quot;; public static void main(String[] args) throws Exception { //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(QUEUE_NAME, true, false, false, null); for (int i = 1; i &lt;= 30; i++) { // 发送信息 String message = &quot;你好；小兔子！work模式--&quot; + i; /** * 参数1：交换机名称，如果没有指定则使用默认Default Exchage * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot;已发送消息：&quot; + message); } // 关闭资源 channel.close(); connection.close(); }} 消费者1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.itheima.rabbitmq.work;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.*;import java.io.IOException;public class Consumer1 { public static void main(String[] args) throws Exception { Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //一次只能接收并处理一个消息 channel.basicQos(1); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel){ @Override /** * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { try { //路由key System.out.println(&quot;路由key为：&quot; + envelope.getRoutingKey()); //交换机 System.out.println(&quot;交换机为：&quot; + envelope.getExchange()); //消息id System.out.println(&quot;消息id为：&quot; + envelope.getDeliveryTag()); //收到的消息 System.out.println(&quot;消费者1-接收到的消息为：&quot; + new String(body, &quot;utf-8&quot;)); Thread.sleep(1000); //确认消息 channel.basicAck(envelope.getDeliveryTag(), false); } catch (InterruptedException e) { e.printStackTrace(); } } }; //监听消息 /** * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, false, consumer); }} 消费者2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.itheima.rabbitmq.work;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.*;import java.io.IOException;public class Consumer2 { public static void main(String[] args) throws Exception { Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //一次只能接收并处理一个消息 channel.basicQos(1); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel){ @Override /** * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { try { //路由key System.out.println(&quot;路由key为：&quot; + envelope.getRoutingKey()); //交换机 System.out.println(&quot;交换机为：&quot; + envelope.getExchange()); //消息id System.out.println(&quot;消息id为：&quot; + envelope.getDeliveryTag()); //收到的消息 System.out.println(&quot;消费者2-接收到的消息为：&quot; + new String(body, &quot;utf-8&quot;)); Thread.sleep(1000); //确认消息 channel.basicAck(envelope.getDeliveryTag(), false); } catch (InterruptedException e) { e.printStackTrace(); } } }; //监听消息 /** * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, false, consumer); }} 测试 启动两个消费者，然后再启动生产者发送消息；到IDEA的两个消费者对应的控制台查看是否竞争性的接收到消息。 小结 在一个队列中如果有多个消费者，那么消费者之间对于同一个消息的关系是竞争的关系。 Publish/Subscribe发布与订阅模式 模式说明（给喜欢的人一样的糖）【FANOUT】 在订阅模型中，多了一个 Exchange 角色，而且过程略有变化： P：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机） 123456789//1.创建连接工厂//2. 设置参数//3. 创建连接 Connection//4. 创建Channel//5. 创建交换机//6. 创建队列//7. 绑定队列和交换机//8. 发送消息//9. 释放资源 C：消费者，消息的接收者，会一直等待消息到来 Queue：消息队列，接收消息、缓存消息 Exchange：交换机（X）。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有常见以下3种类型： Fanout：广播，将消息交给所有绑定到交换机的队列 Direct：定向，把消息交给符合指定routing key 的队列 Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与 Exchange 绑定，或者没有符合路由规则的队列，那么消息会丢失！ Routing路由工作模式 模式说明：给更爱的人更甜的糖【DIRECT】 队列与交换机的绑定，不能是任意绑定了，而是要指定一个 RoutingKey（路由key） l消息的发送方在向 Exchange 发送消息时，也必须指定消息的 RoutingKey lExchange 不再把消息交给每一个绑定的队列，而是根据消息的 Routing Key 进行判断，只有队列的Routingkey 与消息的 Routing key 完全一致，才会接收到消息 lP：生产者，向 Exchange 发送消息，发送消息时，会指定一个routing key lX：Exchange（交换机），接收生产者的消息，然后把消息递交给与 routing key 完全匹配的队列 lC1：消费者，其所在队列指定了需要 routing key 为 error 的消息 lC2：消费者，其所在队列指定了需要 routing key 为 info、error、warning 的消息 Routing 模式要求队列在绑定交换机时要指定 routing key，消息会转发到符合 routing key 的队列 Topics通配符工作模式 模式说明：给姓李的人类棉花糖，姓梁的人类鸡屎麻糖【TOPIC】 Topic类型与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！ Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert 通配符规则： #：匹配一个或多个词 *：匹配不多不少恰好1个词 举例： item.#：能够匹配item.insert.abc 或者 item.insert item.*：只能匹配item.insert 图解： 红色Queue：绑定的是usa.# ，因此凡是以 usa.开头的routing key 都会被匹配到 黄色Queue：绑定的是#.news ，因此凡是以 .news结尾的 routing key 都会被匹配 Spring-RabbitMQ 详情见代码 SpringBoot-RabbitMQ RabbitMQ高级特性 发、收、限、时、达、 1.1消息可靠投递 【寄的糖你收到了吗】 confirm确认模式 【消息从 producer 到 exchange 无论如何会返回一个confirmCallback】 return退回模式 【消息从producer到 queue 失败则会返回一个returnCallback】 rabbitmq整各消息投递的路径为： profucer-----》rabbitmq broker----》exchange-----》queue----》consumer 设置ConnectionFactory的publisher-confirm=&quot;true&quot; 开启确认模式 使用rabbitTemplate.setConfirmCallback设置回调函数。当消息发送到exchange后回调confirm方法。在方法中判断true，则发送成功，反之失败，需要处理 设置ConnectionFactory的publisher-returns=&quot;true&quot; 开启退回模式 使用rabbitTemplate.setReturnCallback设置回调函数。当消息从exchange路由到queue失败后，如果设置了rabbitTemplate.setMandatory(true)参数，则会将消息退回给producer。并执行回调函数returnedMessage。 在RabbitMQ中也提供了事物机制，但是性能较差。 1.2Consumer ACK 【收到糖给你怎么回复】 ack指acknowledge，确认。表示消费端收到消息后的确认方式 三种确认方式： ​ 自动确认：acknowledge=&quot;none&quot;【业务出问题也会回复收到】 ​ 手动确认：acknowledge=&quot;manual&quot;【等业务处理没问题后回复收到，有问题可处理】 ​ 根据异常情况确认：acknowledge=&quot;auto&quot; ​ 自动确认消息被consumer接收到，则自动确认，并将响应message从缓存中移除，实际业务中小细节受到，业务出现异常，那么消息会丢失，如果是设置的手动确认方式，则需要业务处理成功后，调用channel.basicAck(),手动签收，如果出现异常，则调用channel.basicNack（）方法，让其自动重发消息。 1234* 1. 设置手动签收。acknowledge=&quot;manual&quot;* 2. 让监听器类实现ChannelAwareMessageListener接口* 3. 如果消息成功处理，则调用channel的 basicAck()签收* 4. 如果消息处理失败，则调用channel的basicNack()拒绝签收，broker重新发送给consumer 1.3消费端限流qos 【我每次只收1000颗糖】 12341. 确保ack机制为手动确认。2. listener-container配置属性 perfetch = 1,表示消费端每次从mq拉去一条消息来消费，直到手动确认消费完毕后，才会继续拉去下一条消息。 &lt;rabbit:listener-container connection-factory=&quot;connectionFactory&quot; acknowledge=&quot;manual&quot; prefetch=&quot;1&quot; &gt; 1.4TTL 【糖放快递站十年就不能吃了】 1234567* TTL:过期时间* 1. 队列统一过期* 2. 消息单独过期** 如果设置了消息的过期时间，也设置了队列的过期时间，它以时间短的为准。* 队列过期后，会将队列所有消息全部移除。* 消息过期后，只有消息在队列顶端，才会判断其是否过期(移除掉) 1.5死信队列 【不好吃的糖都给其他人吧】 死信队列（DXL）。Dead Letter Exchange（死信队列交换机），当消息成为Dead Message后，可以被重新发送到另外一个交换机，这个交换机就是DEX。死信队列和死信交换机和普通的没有区别。 问题： 1.消息什么时候成为死信? 消息成为死信的三种情况： 1.队列消息的长度达到限制 2.消费者拒收消费消息，basicNack/basicReject，并且不把消息重新放入原目标队列，requeue=false 3.原队列存在消息过期设置，消息达超时间未被消费； 2.队列如何与DEX绑定？ 1.6延迟队列 【我妈叫我三十分钟后才能吃这颗糖】 消息进入队列后不会被立即被消费，只有达到指定时间后，才会被消费。 需求： ​ 1.下单后三十分钟未支付，取消订单，回滚库存 ​ 2.新用户注册7天后，发送短信问候 实现方式 ​ 1.定时器（有延迟，对数据库有影响） ​ 2.延迟队列（MQ中未直接提供延迟队列，DLX+TTL实现） 1.7日志与监控 RabbitMQ默认日志存放路径： /var/log/rabbitmq/rabbit@xxx.log 日志包含了RabbitMQ的版本号、Erlang的版本号、RabbitMQ服务节点名称、cookie的hash值、RabbitMQ配置文件地址、内存限制、磁盘限制、默认账户guest的创建以及权限配置等等。 123456789101112131415161718查看队列# rabbitmqctl list_queues查看exchanges# rabbitmqctl list_exchanges查看用户# rabbitmqctl list_users查看连接# rabbitmqctl list_connections查看消费者信息# rabbitmqctl list_consumers查看环境变量# rabbitmqctl environment查看未被确认的队列# rabbitmqctl list_queues name messages_unacknowledged查看单个队列的内存使用# rabbitmqctl list_queues name memory查看准备就绪的队列# rabbitmqctl list_queues name messages_ready 1.8 消息追踪 在使用任何消息中间件的过程中，难免会出现某条消息异常丢失的情况。对于RabbitMQ而言，可能是因为生产者或消费者与RabbitMQ断开了连接，而它们与RabbitMQ又采用了不同的确认机制；也有可能是因为交换器与队列之间不同的转发策略；甚至是交换器并没有与任何队列进行绑定，生产者又不感知或者没有采取相应的措施；另外RabbitMQ本身的集群策略也可能导致消息的丢失。这个时候就需要有一个较好的机制跟踪记录消息的投递过程，以此协助开发和运维人员进行问题的定位。 在RabbitMQ中可以使用Firehose和rabbitmq_tracing插件功能来实现消息追踪。 Firehose firehose的机制是将生产者投递给rabbitmq的消息，rabbitmq投递给消费者的消息按照指定的格式发送到默认的exchange上。这个默认的exchange的名称为amq.rabbitmq.trace，它是一个topic类型的exchange。发送到这个exchange上的消息的routing key为 publish.exchangename 和 deliver.queuename。其中exchangename和queuename为实际exchange和queue的名称，分别对应生产者投递到exchange的消息，和消费者从queue上获取的消息。 注意：打开 trace 会影响消息写入功能，适当打开后请关闭。 rabbitmqctl trace_on：开启Firehose命令 rabbitmqctl trace_off：关闭Firehose命令 rabbitmq_tracing rabbitmq_tracing和Firehose在实现上如出一辙，只不过rabbitmq_tracing的方式比Firehose多了一层GUI的包装，更容易使用和管理。 启用插件：rabbitmq-plugins enable rabbitmq_tracing 应用问题 1.消息可靠性保障------消息补偿 2.消息幂等性保障-----乐观锁机制 幂等性指一次和多次请求某一个资源，对于资源本身应该具有同样的结果。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。 在MQ中指，消费多条相同的消息，得到与消费该消息一次相同的结果。 3.RabbitMQ集群搭建 摘要：实际生产应用中都会采用消息队列的集群方案，如果选择RabbitMQ那么有必要了解下它的集群方案原理 一般来说，如果只是为了学习RabbitMQ或者验证业务工程的正确性那么在本地环境或者测试环境上使用其单实例部署就可以了，但是出于MQ中间件本身的可靠性、并发性、吞吐量和消息堆积能力等问题的考虑，在生产环境上一般都会考虑使用RabbitMQ的集群方案。 3.1 集群方案的原理 RabbitMQ这款消息队列中间件产品本身是基于Erlang编写，Erlang语言天生具备分布式特性（通过同步Erlang集群各节点的magic cookie来实现）。因此，RabbitMQ天然支持Clustering。这使得RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。集群是保证可靠性的一种方式，同时可以通过水平扩展以达到增加消息吞吐量能力的目的。 3.2 单机多实例部署 由于某些因素的限制，有时候你不得不在一台机器上去搭建一个rabbitmq集群，这个有点类似zookeeper的单机版。真实生成环境还是要配成多机集群的。有关怎么配置多机集群的可以参考其他的资料，这里主要论述如何在单机中配置多个rabbitmq实例。 主要参考官方文档：https://www.rabbitmq.com/clustering.html 首先确保RabbitMQ运行没有问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@super ~]# rabbitmqctl statusStatus of node rabbit@super ...[{pid,10232}, {running_applications, [{rabbitmq_management,&quot;RabbitMQ Management Console&quot;,&quot;3.6.5&quot;}, {rabbitmq_web_dispatch,&quot;RabbitMQ Web Dispatcher&quot;,&quot;3.6.5&quot;}, {webmachine,&quot;webmachine&quot;,&quot;1.10.3&quot;}, {mochiweb,&quot;MochiMedia Web Server&quot;,&quot;2.13.1&quot;}, {rabbitmq_management_agent,&quot;RabbitMQ Management Agent&quot;,&quot;3.6.5&quot;}, {rabbit,&quot;RabbitMQ&quot;,&quot;3.6.5&quot;}, {os_mon,&quot;CPO CXC 138 46&quot;,&quot;2.4&quot;}, {syntax_tools,&quot;Syntax tools&quot;,&quot;1.7&quot;}, {inets,&quot;INETS CXC 138 49&quot;,&quot;6.2&quot;}, {amqp_client,&quot;RabbitMQ AMQP Client&quot;,&quot;3.6.5&quot;}, {rabbit_common,[],&quot;3.6.5&quot;}, {ssl,&quot;Erlang/OTP SSL application&quot;,&quot;7.3&quot;}, {public_key,&quot;Public key infrastructure&quot;,&quot;1.1.1&quot;}, {asn1,&quot;The Erlang ASN1 compiler version 4.0.2&quot;,&quot;4.0.2&quot;}, {ranch,&quot;Socket acceptor pool for TCP protocols.&quot;,&quot;1.2.1&quot;}, {mnesia,&quot;MNESIA CXC 138 12&quot;,&quot;4.13.3&quot;}, {compiler,&quot;ERTS CXC 138 10&quot;,&quot;6.0.3&quot;}, {crypto,&quot;CRYPTO&quot;,&quot;3.6.3&quot;}, {xmerl,&quot;XML parser&quot;,&quot;1.3.10&quot;}, {sasl,&quot;SASL CXC 138 11&quot;,&quot;2.7&quot;}, {stdlib,&quot;ERTS CXC 138 10&quot;,&quot;2.8&quot;}, {kernel,&quot;ERTS CXC 138 10&quot;,&quot;4.2&quot;}]}, {os,{unix,linux}}, {erlang_version, &quot;Erlang/OTP 18 [erts-7.3] [source] [64-bit] [async-threads:64] [hipe] [kernel-poll:true]\\n&quot;}, {memory, [{total,56066752}, {connection_readers,0}, {connection_writers,0}, {connection_channels,0}, {connection_other,2680}, {queue_procs,268248}, {queue_slave_procs,0}, {plugins,1131936}, {other_proc,18144280}, {mnesia,125304}, {mgmt_db,921312}, {msg_index,69440}, {other_ets,1413664}, {binary,755736}, {code,27824046}, {atom,1000601}, {other_system,4409505}]}, {alarms,[]}, {listeners,[{clustering,25672,&quot;::&quot;},{amqp,5672,&quot;::&quot;}]}, {vm_memory_high_watermark,0.4}, {vm_memory_limit,411294105}, {disk_free_limit,50000000}, {disk_free,13270233088}, {file_descriptors, [{total_limit,924},{total_used,6},{sockets_limit,829},{sockets_used,0}]}, {processes,[{limit,1048576},{used,262}]}, {run_queue,0}, {uptime,43651}, {kernel,{net_ticktime,60}}] 停止rabbitmq服务 123[root@super sbin]# service rabbitmq-server stopStopping rabbitmq-server: rabbitmq-server. 启动第一个节点： 12345678910[root@super sbin]# RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit1 rabbitmq-server start RabbitMQ 3.6.5. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit1.log ###### ## /var/log/rabbitmq/rabbit1-sasl.log ########## Starting broker... completed with 6 plugins. 启动第二个节点： web管理插件端口占用,所以还要指定其web插件占用的端口号。 1234567891011[root@super ~]# RABBITMQ_NODE_PORT=5674 RABBITMQ_SERVER_START_ARGS=&quot;-rabbitmq_management listener [{port,15674}]&quot; RABBITMQ_NODENAME=rabbit2 rabbitmq-server start RabbitMQ 3.6.5. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit2.log ###### ## /var/log/rabbitmq/rabbit2-sasl.log ########## Starting broker... completed with 6 plugins. 结束命令： 12rabbitmqctl -n rabbit1 stoprabbitmqctl -n rabbit2 stop rabbit1操作作为主节点： 1234567[root@super ~]# rabbitmqctl -n rabbit1 stop_app Stopping node rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit1 reset Resetting node rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit1 start_appStarting node rabbit1@super ...[root@super ~]# rabbit2操作为从节点： 123456789[root@super ~]# rabbitmqctl -n rabbit2 stop_appStopping node rabbit2@super ...[root@super ~]# rabbitmqctl -n rabbit2 resetResetting node rabbit2@super ...[root@super ~]# rabbitmqctl -n rabbit2 join_cluster rabbit1@'super' ###''内是主机名换成自己的Clustering node rabbit2@super with rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit2 start_appStarting node rabbit2@super ... 查看集群状态： 1234567[root@super ~]# rabbitmqctl cluster_status -n rabbit1Cluster status of node rabbit1@super ...[{nodes,[{disc,[rabbit1@super,rabbit2@super]}]}, {running_nodes,[rabbit2@super,rabbit1@super]}, {cluster_name,&lt;&lt;&quot;rabbit1@super&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit2@super,[]},{rabbit1@super,[]}]}] web监控： 3.3 集群管理 rabbitmqctl join_cluster {cluster_node} [–ram] 将节点加入指定集群中。在这个命令执行前需要停止RabbitMQ应用并重置节点。 rabbitmqctl cluster_status 显示集群的状态。 rabbitmqctl change_cluster_node_type {disc|ram} 修改集群节点的类型。在这个命令执行前需要停止RabbitMQ应用。 rabbitmqctl forget_cluster_node [–offline] 将节点从集群中删除，允许离线执行。 rabbitmqctl update_cluster_nodes {clusternode} 在集群中的节点应用启动前咨询clusternode节点的最新信息，并更新相应的集群信息。这个和join_cluster不同，它不加入集群。考虑这样一种情况，节点A和节点B都在集群中，当节点A离线了，节点C又和节点B组成了一个集群，然后节点B又离开了集群，当A醒来的时候，它会尝试联系节点B，但是这样会失败，因为节点B已经不在集群中了。 rabbitmqctl cancel_sync_queue [-p vhost] {queue} 取消队列queue同步镜像的操作。 rabbitmqctl set_cluster_name {name} 设置集群名称。集群名称在客户端连接时会通报给客户端。Federation和Shovel插件也会有用到集群名称的地方。集群名称默认是集群中第一个节点的名称，通过这个命令可以重新设置。 3.4 RabbitMQ镜像集群配置 上面已经完成RabbitMQ默认集群模式，但并不保证队列的高可用性，尽管交换机、绑定这些可以复制到集群里的任何一个节点，但是队列内容不会复制。虽然该模式解决一项目组节点压力，但队列节点宕机直接导致该队列无法应用，只能等待重启，所以要想在队列节点宕机或故障也能正常应用，就要复制队列内容到集群里的每个节点，必须要创建镜像队列。 镜像队列是基于普通的集群模式的，然后再添加一些策略，所以你还是得先配置普通集群，然后才能设置镜像队列，我们就以上面的集群接着做。 设置的镜像队列可以通过开启的网页的管理端Admin-&gt;Policies，也可以通过命令。 rabbitmqctl set_policy my_ha &quot;^&quot; '{&quot;ha-mode&quot;:&quot;all&quot;}' Name:策略名称 Pattern：匹配的规则，如果是匹配所有的队列，是^. Definition:使用ha-mode模式中的all，也就是同步所有匹配的队列。问号链接帮助文档。 3.5 负载均衡-HAProxy HAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案,包括Twitter，Reddit，StackOverflow，GitHub在内的多家知名互联网公司在使用。HAProxy实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。 3.5.1 安装HAProxy 12345678910111213141516//下载依赖包yum install gcc vim wget//上传haproxy源码包//解压tar -zxvf haproxy-1.6.5.tar.gz -C /usr/local//进入目录、进行编译、安装cd /usr/local/haproxy-1.6.5make TARGET=linux31 PREFIX=/usr/local/haproxymake install PREFIX=/usr/local/haproxymkdir /etc/haproxy//赋权groupadd -r -g 149 haproxyuseradd -g haproxy -r -s /sbin/nologin -u 149 haproxy//创建haproxy配置文件mkdir /etc/haproxyvim /etc/haproxy/haproxy.cfg 3.5.2 配置HAProxy 配置文件路径：/etc/haproxy/haproxy.cfg 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#logging optionsglobal log 127.0.0.1 local0 info maxconn 5120 chroot /usr/local/haproxy uid 99 gid 99 daemon quiet nbproc 20 pidfile /var/run/haproxy.piddefaults log global mode tcp option tcplog option dontlognull retries 3 option redispatch maxconn 2000 contimeout 5s clitimeout 60s srvtimeout 15s #front-end IP for consumers and producterslisten rabbitmq_cluster bind 0.0.0.0:5672 mode tcp #balance url_param userid #balance url_param session_id check_post 64 #balance hdr(User-Agent) #balance hdr(host) #balance hdr(Host) use_domain_only #balance rdp-cookie #balance leastconn #balance source //ip balance roundrobin server node1 127.0.0.1:5673 check inter 5000 rise 2 fall 2 server node2 127.0.0.1:5674 check inter 5000 rise 2 fall 2listen stats bind 172.16.98.133:8100 mode http option httplog stats enable stats uri /rabbitmq-stats stats refresh 5s 启动HAproxy负载 123456/usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg//查看haproxy进程状态ps -ef | grep haproxy访问如下地址对mq节点进行监控http://172.16.98.133:8100/rabbitmq-stats 代码中访问mq集群地址，则变为访问haproxy地址:5672 参考文献：黑马视频相关笔记","link":"/2021/03/24/Draft/2021/RabbitMQ/"},{"title":"魑魅先生 | 每日面题","text":"保持基础知识的敏感度，以至于平时的不断应用。 Java刷题 主要分类： JVM，网络，Java基础，算法数据结构，数据库高级知识，框架原理，设计模式，分布式 Java基础 查看JAVA基础项目并补充，包含java基础， JVM 网络 算法数据结构 数据库高级知识 框架原理 设计模式 分布式","link":"/2021/03/01/Draft/2021/%E6%AF%8F%E6%97%A5%E9%9D%A2%E9%A2%98/"},{"title":"魑魅先生 | 服务器","text":"Linux常用知识、服务器选择、搭建个人网盘、JAVA开发相关软件配置、博客搭建、域名配置、相关软件推荐。 常用命令 文件 本地文件上传下载 方式一：lrzsz 1234567#安装yum -y install lrzsz#上传下载的目录SecureCRT可配置#上传【弹出窗口选择文件】rz#下载sz 文件名 方式二：sftp SecureCRT 快捷键alt+p 12get -r linux绝对路径文件 #Linux文件上传到Windowsput -r Windows绝对路径文件 #Windows文件上传到Linux 远程文件下载 12345678#下载到当前目录wget https://download.redis.io/releases/redis-6.0.8.tar.gz#下载到指定目录wget -P /usr/software https://download.redis.io/releases/redis-6.0.8.tar.gz#下载并更换文件名wget -O redis.tar.gz https://download.redis.io/releases/redis-6.0.8.tar.gz#断点续传【断网等中断下载情况可借此无需从头下载】wget -c https://download.redis.io/releases/redis-6.0.8.tar.gz 其他图形化方式 XFTP、SecureFX、FileZilla等 解压缩 1.压缩文件 tar -czf test.tar.gz /test1 /test2 2.列出压缩文件列表 tar -tzf test.tar.gz 3.解压文件 tar -xvzf test.tar.gz 新建 touch 文件名 删除 rm -rf 文件名 修改 打开：vi/vim 文件名 i:在光标所在字符前开始插入 a:在光标所在字符后开始插入 o:在光标所在行的下面另起一新行插入 保存文件： 第一步：ESC 进入命令行模式 第二步：: 进入底行模式 第三步：wq 保存并退出编辑 取消编辑： 第一步：ESC 进入命令行模式 第二步：: 进入底行模式 第三步：q! 撤销本次修改并退出编辑 1) 命令行模式command mode） 控制屏幕光标的移动，字符、字或行的删除，查找，移动复制某区段及进入Insert mode下，或者到 last line mode。 命令行模式下的常用命令： 【1】控制光标移动：↑，↓，j 【2】删除当前行：dd 【3】查找：/字符 【4】进入编辑模式：i o a 【5】进入底行模式：: 2) 编辑模式（Insert mode） 只有在Insert mode下，才可以做文字输入，按「ESC」键可回到命令行模式。 编辑模式下常用命令： 【1】ESC 退出编辑模式到命令行模式； 3) 底行模式（last line mode） 将文件保存或退出vi，也可以设置编辑环境，如寻找字符串、列出行号……等。 底行模式下常用命令： 【1】退出编辑： :q 【2】强制退出： :q! 【3】保存并退出： :wq 移动 mv /temp/movefile /targetFolder 复制 cp source dest 复制文件 cp -r sourceFolder targetFolder 递归复制整个文件夹 scp sourecFile romoteUserName@remoteIp:remoteAddr 远程拷贝 查看内容 12345678910111213141516171819202122232425262728cat：看最后一屏#示例：使用cat查看/etc/sudo.conf文件，只能显示最后一屏内容#cat sudo.confmore：百分比显示#示例：使用more查看/etc/sudo.conf文件，可以显示百分比，回车可以向下一行，空格可以向下一页，q可以退出查看#more sudo.confless：翻页查看#示例：使用less查看/etc/sudo.conf文件，可以使用键盘上的PgUp和PgDn向上 和向下翻页，q结束查看#less sudo.conftail：指定行数或者动态查看#示例：使用tail -10 查看/etc/sudo.conf文件的后10行，Ctrl+C结束 #tail -10 sudo.conf#查看文件头10行head -n 10 example.txt#查看文件尾10行tail -n 10 example.txt#查看日志类型文件tail -f exmaple.log //这个命令会自动显示新增内容，屏幕只显示10行内容的（可设置）。 查找 find / -name filename.txt 根据名称查找/目录下的filename.txt文件。 find . -name &quot;*.xml&quot; 递归查找所有的xml文件 find . -name &quot;*.xml&quot; |xargs grep &quot;hello world&quot; 递归查找所有文件内容中包含hello world的xml文件 grep -H 'spring' *.xml 查找所以有的包含spring的xml文件 find ./ -size 0 | xargs rm -f &amp; 删除文件大小为零的文件 ls -l | grep '.jar' 查找当前目录中的所有jar文件 grep 'test' d* 显示所有以d开头的文件中包含test的行。 grep 'test' aa bb cc 显示在aa，bb，cc文件中匹配test的行。 grep '[a-z]{5}' aa 显示所有包含每个字符串至少有5个连续小写字符的字符串的行。 重命名 mv oldNameFile newNameFile 权限 chmod 777 file.java //file.java的权限-rwxrwxrwx，r表示读、w表示写、x表示可执行 更改文件属性 目录 1.查看文件列表，包含隐藏文件 ls -al 2.当前工作目录 pwd 3.创建目录 mkdir newfolder 4.删除目录 rmdir deleteEmptyFolder 删除空目录 rm -rf deleteFile 递归删除目录中所有内容 5.以树状图列出目录的内容 tree a ps: Mac下使用tree命令 查找 权限 1.切换用户 su -username 2.使用超级管理员身份执行命令 sudo 命令 进程 1.查看一个程序是否运行 12345678#查看所有有关tomcat的进程ps –ef|grep tomcat #高亮要查询的关键字ps -ef|grep --color java #查看java进程ps aux|grep java#查看所有进程ps aux 2.终止线程 kill -9 19979 终止线程号位19979的进程 3.端口 12345678#查看目前所有运行端口netstat -tunlp#查看端口8080的使用情况netstat -tln | grep 8080 #查看端口属于哪个程序lsof -i :8080 系统 1234567891011121314151617181920212223242526272829303132333435363738#关机 shutdown -h now #立刻关机 shutdown -h 5 # 5分钟后关机 poweroff # 立刻关机#重启 shutdown -r now #立刻重启 shutdown -r 5 #5分钟后重启 reboot #立刻重启#--help命令 shutdown --help： ifconfig --help：#查看网卡信息#man命令（命令说明书） man shutdown #注意：man shutdown打开命令说明书之后，使用按键q退出 #远程登录ssh userName@ip#####查看系统硬件信息######查看操作系统的版本cat /etc/redhat-release #查看系统内核uname –a#CPUlscpucat /proc/cpuinfo |grep &quot;processor&quot;|sort -u|wc -l #逻辑cpu数量grep &quot;physical id&quot; /proc/cpuinfo|sort -u|wc -l #物理cpu的数量grep &quot;cpu cores&quot; /proc/cpuinfo|uniq #每个cpu内核数#内存free -h #查看内存占用情况cat /proc/meminfo#查看内存详细情况#硬盘fdisk -l #硬盘分区情况df -h #磁盘使用情况du -sh #查看当前目录总共占的容量。而不单独列出各子项占用的容量。du -lh --max-depth=1 #查看当前目录下一级子文件和子目录占用的磁盘容量。du -cks * | sort -rn | head -n 10 #查看某个目录下占用空间最多的文件或目录。 网络 1234#ip相关信息查询ip addr#网络联通ping www.just-ping.com 部署 JAVA常用 打印信息 echo $JAVA_HOME 打印java home环境变量的值 java 常用命令 java javac jps , jstat , jmap, jstack 应用场景 离线/在线 安装各种包 jdk安装配置 \\1. 创建 /usr/java 文件夹，将安装包放入解压 解压命令：tar zxvf 压缩包名称 （例如：tar zxvf jdk-8u152-linux-x64.tar.gz） 删除命令：rm -f 压缩包名称 （例如 rm -f jdk-8u152-linux-x64.tar.gz） \\2. 修改配置文件 vi /etc/profile export JAVA_HOME=/usr/java/jdk1.8.0_221 export JRE_HOME=/usr/java/jdk1.8.0_221/jre export CLASSPATH=.😒{JAVA_HOME}/jre/lib/rt.jar:${JAVA_HOME}/lib/dt.jar:${JAVA_HOME}/lib/tools.jar export PATH=$PATH:${JAVA_HOME}/bin 配置文件生效 source /etc/profile 修改默认jdk cd /usr/bin ln -s -f /usr/java/jdk1.8.0_221/jre/bin/java ln -s -f /usr/java/jdk1.8.0_221/bin/javac java -version redis安装 1）连接服务器，将redis安装包放入服务器 2）解压安装包 tar -zxvf redis-5.0.5.tar.gz 3）解压后目录为 4）进入 redis-5.0.5 目录，然后执行make命令 5）出现下面标识说明成功 6）执行命令 make install 7）安装完毕，修改配置 Vi redis.conf（或者直接将文件拿出来修改） 修改 daemonize no 为yes (功能，设置redis后台运行) 设置 其他机器访问 bind 127.0.0.1 改成 bind 0.0.0.0 关闭持久化操作：728行appendfsync 设置为no ，216行 save &quot;&quot; 注释其他3个save 启动redis命令：redis-server redis.conf ps -ef |grep redis 查看redis进程是否启动成功 redis-cli -p 6379 使用redis 如果设置了密码，则先登录：auth *** 输入命令ping 返回pong成功 退出命令 卸载redis ps -ef |grep redis 查看redis进程 kill -9 **** 停止redis服务 执行 ll /usr/local/bin 执行删除命令 rm -f /usr/local/bin/redis* 查看文件是否已经删除执行 ll /usr/local/bin 查到源文件所在的目录 执行删除命令 rm -rf redis-4.0.6/ 卸载完成 Redis设置密码 需要永久配置密码的话就去redis.conf的配置文件中找到requirepass这个参数，如下配置： 修改redis.conf配置文件 # requirepass foobared requirepass 123 指定密码123 保存后重启redis就可以了 Tomcat安装 \\1. 将安装包放入服务器中 \\2. 命令解压 tar -xzvf apache-tomcat-8.5.51.tar.gz \\3. 进入bin目录执行命令将文件的权限设置低 chmod -R 777 startup.sh shutdown.sh catalina.sh \\4. 启动tomcat \\5. 是否启动成功 ps -ef|grep tomcat \\6. 浏览器访问查看能否访问 访问失败执行 firewall-cmd --zone=public --add-port=8080/tcp --permanent 将8080端口加入防火墙success成功 firewall-cmd --state 防火墙状态 firewall-cmd --reload 重启防火墙 success成功 firewall-cmd --zone=public --query-port=8080/tcp 查询8080端口是加入成功 yes成功 \\7. 开机自启 vi /etc/rc.d/rc.local 输入i进入编辑模式 加入代码 source /etc/profile sh /opt/Tomcat/apache-tomcat-8.5.51/bin/startup.sh esc—&gt;退出编辑模式 :wq 保存退出 8．第二种开机自启 每次开机都要启动tomcat，非常麻烦：通过直接修改系统文件，实现tomcat自启动： \\1. 修改脚本文件rc.local：vim /etc/rc.d/rc.local 这个脚本是使用者自定的开机启动程序，可以在里面添加想在系统启动之后执行的脚本或者脚本执行命令 \\2. 添加如下内容： export JAVA_HOME=/usr/java/jdk1.8.0_221 /opt/tomcat/apache-tomcat-8.5.51/bin/startup.sh start \\3. esc 退出编辑，:wq 保存修改 \\4. 将rc.local修改为可执行 chmod 777 /etc/rc.d/rc.local 卸载tomcat 卸载: 找到tomcat的安装目录 rm -rf /usr/java/tomcat/apache-tomcat-8.5.31 (安装目录) 安装 Tomcat管理页面设置密码 Tomcat-user.xml修改 ​ &lt;user username**=&quot;tomcat&quot;** password=&quot;123&quot;** roles=&quot;manager-gui&quot;/&gt;** 1234Manager ``下面的META-INF配置 &lt;Context antiResourceLocking=&quot;false&quot; privileged=&quot;true&quot; &gt; &lt;Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot; allow=&quot;^.*$&quot; /&gt; Manager App 在 tomcat/bin/Catalina.sh文件中 找到 -Djava.security.manager \\ -Djava.security.policy==&quot;&quot;$CATALINA_BASE/conf/catalina.policy&quot;&quot;\\ -Dcatalina.base=&quot;&quot;$CATALINA_BASE&quot;&quot; \\ -Dcatalina.home=&quot;&quot;$CATALINA_HOME&quot;&quot; \\ -Djava.io.tmpdir=&quot;&quot;$CATALINA_TMPDIR&quot;&quot;\\ 在每段后追加一句 -Djava.awt.headless=true \\ 该步骤解决使用中无法选择数据路径的问题 Nginx安装 \\1. 将安装包放入服务器，解压 tar xf nginx-1.8.1.tar.gz \\2. 安装依赖 yum -y install gcc gcc-c++ automake pcre-devel zlib zlib-devel open openssl-devel \\3. 进入目录编译安装 ./configure --prefix=/opt/nginx \\4. 安装 make &amp;&amp; make install \\5. 查看/opt/nginx目录 \\6. 进入/opt/nginx/sbin目录 ./nginx -v 查看nginx进程 ps aux | grep nginx \\7. 启动nginx ./nginx \\8. 将端口加入防火墙 firewall-cmd --zone=public --add-port=80/tcp 卸载nginx: ./nginx –s stop 停止nginx服务 find / -name nginx 查找包含nginx的文件 执行删除命令 rm –rf * yum remove nginx 查看nginx是否还存在 Which nginx Nginx配置SSL报错 nginx: [emerg] the &quot;ssl&quot; parameter requires ngx_http_ssl_module in /opt/nginx/conf/nginx.conf:130 https://blog.csdn.net/m0_37711292/article/details/85163834 Mysql安装 \\1. 上传安装包到服务器并解压 tar -xvf MySQL-5.6.40-1.el7.x86_64.rpm-bundle.tar -C /opt/mysql \\2. 查看是否有安装mariadb rpm -qa | grep -i mariadb 卸载 yum remove mariadb-server-5.5.65-1.el7.x86_64 \\3. 安装文件 rpm -ivh MySQL-shared-compat-5.6.40-1.el7.x86_64.rpm --nodeps –force rpm -ivh MySQL-server-5.6.40-1.el7.x86_64.rpm --nodeps –force rpm -ivh MySQL-client-5.6.40-1.el7.x86_64.rpm --nodeps –force rpm -ivh MySQL-devel-5.6.40-1.el7.x86_64.rpm --nodeps –force rpm -ivh MySQL-embedded-5.6.40-1.el7.x86_64.rpm --nodeps –force rpm -ivh MySQL-shared-5.6.40-1.el7.x86_64.rpm --nodeps –force 获取安装状态 rpm -qa | grep -i mysql \\4. 获取随机生成密码 cat /root/.mysql_secret 启动服务 service mysql start 登录 mysql -u root –p 输入刚才查询的密码 登录成功，修改密码 set password for 'root'@'localhost' = password('root'); 设置远程连接 grant all privileges on . to 'root'@'%' identified by 'root' with grant option; 3306端口加入防火墙 firewall-cmd --add-port=3306/tcp --permanent 重启防火墙 firewall-cmd –reload Geoserver安装 \\1. 上传安装包，解压 unzip geoserver-2.15.3-war.zip -d geoserver \\2. 将war包放入tomcat的webapps里面 \\3. 修改tomcat/bin/catalina.sh增加启动参数 Xml代码 CATALINA_OPTS=&quot;-Djava.awt.headless=true&quot; 启动Tomcat(tomcat/bin/startup.sh) 验证：登陆 http://10.32.250.178:8080/ geoserver ，看到GeoServer页面，账号：admin，密码：geoserver 服务报错： org.springframework.web.util.NestedServletException: Handler dispatch failed; nested exception is java.lang.NoClassDefFoundError: Could not initialize class sun.awt.X11GraphicsEnvironment 修改tomcat/bin/catalina.sh 在所有-Djava.io.tmpdir…后面增加一个 -Djava.awt.headless=true \\ Openfile安装 https://blog.csdn.net/github_38924695/article/details/89470960 通过xftp上传到linux中。我的目录在/opt/openoffice中 解压文件：tar -zxvf Apache_OpenOffice_4.1.6_Linux_x86-64_install-rpm_zh-CN.tar.gz 解压后进入zh-CN目录中： cd RPMS/ 里面都是rpm文件，我们需要安装这些文件 rpm -ivh *.rpm 进入desktop-integration/目录：cd desktop-integration/ 安装openoffice:rpm -ivh openoffice4.1.6-redhat-menus-4.1.6-9790.noarch.rpm Postgres安装 \\1. 下载安装包 下载离线 rpm 包：https://yum.postgresql.org/rpmchart/ https://www.postgresql.org/download/ 下载4个安装包 \\2. 上传到服务器 \\3. 按照顺序安装 rpm -ivh postgresql11-libs-11.12-1PGDG.rhel7.x86_64.rpm rpm -ivh postgresql11-11.12-1PGDG.rhel7.x86_64.rpm rpm -ivh postgresql11-server-11.12-1PGDG.rhel7.x86_64.rpm rpm -ivh postgresql11-contrib-11.12-1PGDG.rhel7.x86_64.rpm 如果报错则在语句后面增加 --force –nodeps 例：rpm -ivh postgresql11-libs-11.12-1PGDG.rhel7.x86_64.rpm --force –nodeps 安装error 解决： 下载安装： http://mirror.centos.org/centos/7/os/x86_64/Packages/libicu-50.2-4.el7_7.x86_64.rpm rpm -ivh libicu-50.2-4.el7_7.x86_64.rpm 下载安装： http://mirror.centos.org/centos/7/os/x86_64/Packages/libxslt-1.1.28-6.el7.x86_64.rpm rpm -ivh libxslt-1.1.28-6.el7.x86_64.rpm 卸载： #ps:如果要卸载的话, 先停止postges服务，然后执行下面的命令即可 rpm -qe postgresql11-libs-11.1-1PGDG.rhel7.x86_64 rpm -qe postgresql11-11.1-1PGDG.rhel7.x86_64 rpm -qe postgresql11-server-11.1-1PGDG.rhel7.x86_64 \\4. 初始化 /usr/pgsql-11/bin/postgresql-11-setup initdb \\5. 启动服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374systemctl enable postgresql-11.servicesystemctl start postgresql-11.servicevi /var/lib/pgsql/11/data/postgresql.conf按需修改以下两行：# *表示监听所有的ip信息，也可以使用localhost、127.0.0.1等listen_addresses = '*' # 默认的监听端口为5432，也可以换为其它的port = 5432修改同目录下的pg_hba.conf文件，设置允许访问的ip，这里我因为是测试服务器，所以设置所有：host all all 0.0.0.0/0 trust重启：systemctl restart postgresql-11.service # 状态查看service postgresql-11 status # 停止service postgresql-11 stop# 启动service postgresql-11 start https://blog.csdn.net/justlpf/article/details/847698135. 移动数据库到指定目录5.1 移动目录#须先执行命令:# 1. mkdir -p /data/pgsql/ # 2. chown postgres:postgres -R /data/pgsql[root@VMTest ~]# mv /var/lib/pgsql/11/* /data/pgsql/[root@VMTest ~]# chown -R postgres:postgres /data/pgsql/[root@VMTest ~]# chmod 700 /data/pgsql/data -R5.2 修改配置文件#a.修改指定的数据目录[root@VMTest ~]# vi /usr/lib/systemd/system/postgresql-11.service # 执行前须执行命令: # 1. mkdir -p /data/pgsql/ # 2. chown postgres:postgres -R /data/pgsql# 3. chmod 700 /data/pgsql/data -R#修改Environment=PGDATA=/var/lib/pgsql/11/data/为Environment=PGDATA=/data/pgsql/data/ #b.修改数据目录[root@VMTest ~]# vi /data/pgsql/data/postgresql.conf#执行前须执行命令: # 1. mkdir -p /data/pgsql/ # 2. chown postgres:postgres -R /data/pgsql# 3. chmod 700 /data/pgsql/data -R#修改data_directory: data_directory = '/data/pgsql/data' 其它配置修改:log_directory = 'log' max_connections = 100 5.3 重新加载配置文件，重启数据库[root@VMTest ~]# systemctl daemon-reload[root@VMTest ~]# systemctl restart postgresql-11[root@VMTest ~]# ps -ef | grep postgres #确认启动成功6. 修改密码[root@VMTest ~]# su postgres[postgres@VMTest root]$ psqlcould not change directory to &quot;/root&quot;: Permission deniedpsql (11.1)Type &quot;help&quot; for help. postgres=# #------------------------------------------------------#执行命令postgres=# ALTER ROLE postgres WITH PASSWORD '123abc'; postgres卸载yum remove postgresql*验证是都卸载完毕rpm -qa | grep postgresql Postges安装 https://blog.csdn.net/qq_33554285/article/details/111135786 上传tar.gz包 tar –zxvf **** cd postgresql-11.5/ ./configure 失败，提示 查看readline是否安装 rpm -qa | grep readline ./configure --without-readline Zlib 下载 http://mirror.centos.org/centos/7/os/x86_64/Packages/zlib-devel-1.2.7-18.el7.x86_64.rpm rpm -ivh zlib-devel-1.2.7-18.el7.x86_64.rpm make make install vi /etc/profile export PG_HOME=/usr/local/pgsql export PATH=$PG_HOME/bin:$PATH export LD_LIBRARY_PATH=/usr/local/pgsql/lib source /etc/profile 查看版本 psql --version mkdir data sudo groupadd postgres sudo useradd -gpostgres postgres chown postgres:postgres /opt/postgres/data/ 创建数据库集簇 su postgres -c '/usr/local/pgsql/bin/initdb -D /opt/postgres/data/’ 启动数据库的进程服务 /usr/local/pgsql/bin/pg_ctl -D /opt/postgres/data/ -l logfile start 切换到root对postgres的/usr/local/pgsql/bin与/opt/postgres/进行赋权 chown -R postgres:postgres ./ chmod -R 700 ./data/ 停止pg_ctl bin/pg_ctl -D /opt/postgres/data/ stop /usr/local/pgsql/bin/pg_ctl -D /opt/postgres/data/ -m fast start启动数据库 查看数据库端口并连接 netstat -nptl psql -p 5432b -U postgres 修改data里面的配置文件 postgresql.conf： listen_address = ‘*’ port = 5432 注释取消 在pg_hba.conf第一行添加host all all 0.0.0.0/0 md5 (md5:需要密码,trust：无需密码) 先设置trust修改密码： alter user postgres with password ‘postgres’; 在改为MD5,重启服务 su postgres /usr/local/pgsql/bin/pg_ctl -D /opt/postgres/data/ stop /usr/local/pgsql/bin/pg_ctl -D /opt/postgres/data/ start 查看防火墙状态，判断是否需要放开端口 firewall-cmd –state 如果running firewall-cmd --zone=public --add-port=5432/tcp --permanent firewall-cmd --reload firewall-cmd --zone=public --query-port=5432/tcp postgis安装 gdal tar -zxvf gdal-2.4.1.tar.gz cd gdal-2.4.1 ./configure --prefix=/opt/gdal-2.4.1 make make install 安装proj报错 Sqllite下载地址 https://www.sqlite.org/chronology.html https://www.sqlite.org/index.html https://www.sqlite.org/2015/... https://blog.csdn.net/dikcychen2011/article/details/21969357 升级sqlite3版本： https://blog.csdn.net/weixin_39559523/article/details/116814527 防火墙 一、iptables防火墙 1、基本操作 # 查看防火墙状态 service iptables status # 停止防火墙 service iptables stop # 启动防火墙 service iptables start # 重启防火墙 service iptables restart # 永久关闭防火墙 chkconfig iptables off # 永久关闭后重启 chkconfig iptables on 2、开启80端口 vim /etc/sysconfig/iptables # 加入如下代码 -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT 保存退出后重启防火墙 service iptables restart 二、firewall防火墙 1、查看firewall服务状态 systemctl status firewalld 出现Active: active (running)切高亮显示则表示是启动状态。 出现 Active: inactive (dead)灰色表示停止，看单词也行。 2、查看firewall的状态 firewall-cmd --state 3、开启、重启、关闭、firewalld.service服务 # 开启 service firewalld start # 重启 service firewalld restart # 关闭 service firewalld stop 4、查看防火墙规则 firewall-cmd --list-all 5、查询、开放、关闭端口 # 查询端口是否开放 firewall-cmd --query-port=8080/tcp # 开放80端口 firewall-cmd --permanent --add-port=80/tcp # 移除端口 firewall-cmd --permanent --remove-port=8080/tcp #重启防火墙(修改配置后要重启防火墙) firewall-cmd --reload # 参数解释 1、firwall-cmd：是Linux提供的操作firewall的一个工具； 2、--permanent：表示设置为持久； 3、--add-port：标识添加的端口； 原文：https://blog.csdn.net/bbwangj/article/details/74502967 程序部署 文件操作 清理速度优化 进程管理 权限管理 查看服务器配置 Linux简介 Linux，全称GNU/Linux，是一种免费使用和自由传播的类UNIX操作系统 应用 Linux工具 SecureCRT 7.3 rz上传文件到当前目录 sz 文件名下载 yum -y install bash-completion自动补全 tab 启动过程 内核的引导。 电源 BIOS开机自检，按照BIOS中设置的启动设备（通常是硬盘）来启动 操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 运行 init。 init程序的类型 SysV: init, CentOS 5之前, 配置文件： /etc/inittab。 Upstart: init,CentOS 6, 配置文件： /etc/inittab, /etc/init/*.conf。 Systemd： systemd, CentOS 7,配置文件： /usr/lib/systemd/system、 /etc/systemd/system。 init 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。 init 程序首先是需要读取配置文件 /etc/inittab。 运行级别 程序需要开机启动。它们在Windows叫做&quot;服务&quot;（service），在Linux就叫做&quot;守护进程&quot;（daemon）。 init进程的一大任务，就是去运行这些开机启动的程序。 但是，不同的场合需要启动不同的程序，比如用作服务器时，需要启动Apache，用作桌面就不需要。 Linux允许为不同的场合，分配不同的开机启动程序，这就叫做&quot;运行级别&quot;（runlevel）。也就是说，启动时根据&quot;运行级别&quot;，确定要运行哪些程序。 - 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 系统初始化。 si::sysinit:/etc/rc.d/rc.sysinit 它调用执行了/etc/rc.d/rc.sysinit，而rc.sysinit是一个bash shell的脚本，它主要是完成一些系统初始化的工作，rc.sysinit是每一个运行级别都要首先运行的重要脚本。 它主要完成的工作有：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务。 l5:5:wait:/etc/rc.d/rc 5 这一行表示以5为参数运行/etc/rc.d/rc，/etc/rc.d/rc是一个Shell脚本，它接受5作为参数，去执行/etc/rc.d/rc5.d/目录下的所有的rc启动脚本，/etc/rc.d/rc5.d/目录中的这些启动脚本实际上都是一些连接文件，而不是真正的rc启动脚本，真正的rc启动脚本实际上都是放在/etc/rc.d/init.d/目录下。 而这些rc启动脚本有着类似的用法，它们一般能接受start、stop、restart、status等参数。 /etc/rc.d/rc5.d/中的rc启动脚本通常是K或S开头的连接文件，对于以 S 开头的启动脚本，将以start参数来运行。 而如果发现存在相应的脚本也存在K打头的连接，而且已经处于运行态了(以/var/lock/subsys/下的文件作为标志)，则将首先以stop为参数停止这些已经启动了的守护进程，然后再重新运行。 这样做是为了保证是当init改变运行级别时，所有相关的守护进程都将重启。 至于在每个运行级中将运行哪些守护进程，用户可以通过chkconfig或setup中的&quot;System Services&quot;来自行设定。 建立终端 。 rc执行完毕后，返回init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。 init接下来会打开6个终端，以便用户登录系统。在inittab中的以下6行就是定义了6个终端： 1:2345:respawn:/sbin/mingetty tty1 2:2345:respawn:/sbin/mingetty tty2 3:2345:respawn:/sbin/mingetty tty3 4:2345:respawn:/sbin/mingetty tty4 5:2345:respawn:/sbin/mingetty tty5 6:2345:respawn:/sbin/mingetty tty6 从上面可以看出在2、3、4、5的运行级别中都将以respawn方式运行mingetty程序，mingetty程序能打开终端、设置模式。 同时它会显示一个文本登录界面，这个界面就是我们经常看到的登录界面，在这个登录界面中会提示用户输入用户名，而用户输入的用户将作为参数传给login程序来验证用户的身份。 用户登录系统。 一般来说，用户的登录方式有三种： （1）命令行登录 （2）ssh登录 （3）图形界面登录 对于运行级别为5的图形方式用户来说，他们的登录是通过一个图形化的登录界面。登录成功后可以直接进入 KDE、Gnome 等窗口管理器。 而本文主要讲的还是文本方式登录的情况：当我们看到mingetty的登录界面时，我们就可以输入用户名和密码来登录系统了。 Linux 的账号验证程序是 login，login 会接收 mingetty 传来的用户名作为用户名参数。 然后 login 会对用户名进行分析：如果用户名不是 root，且存在 /etc/nologin 文件，login 将输出 nologin 文件的内容，然后退出。 这通常用来系统维护时防止非root用户登录。只有/etc/securetty中登记了的终端才允许 root 用户登录，如果不存在这个文件，则 root 用户可以在任何终端上登录。 /etc/usertty文件用于对用户作出附加访问限制，如果不存在这个文件，则没有其他限制。 图形模式与文字模式的切换方式 Linux预设提供了六个命令窗口终端机让我们来登录。 默认我们登录的就是第一个窗口，也就是tty1，这个六个窗口分别为tty1,tty2 … tty6，你可以按下Ctrl + Alt + F1 ~ F6 来切换它们。 如果你安装了图形界面，默认情况下是进入图形界面的，此时你就可以按Ctrl + Alt + F1 ~ F6来进入其中一个命令窗口界面。 当你进入命令窗口界面后再返回图形界面只要按下Ctrl + Alt + F7 就回来了。 如果你用的vmware 虚拟机，命令窗口切换的快捷键为 Alt + Space + F1~F6. 如果你在图形界面下请按Alt + Shift + Ctrl + F1~F6 切换至命令窗口。 Linux 关机 在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机。 正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 关机指令为：shutdown ，你可以man shutdown 来看一下帮助文档。 例如你可以运行如下命令关机： sync 将数据由内存同步到硬盘中。 shutdown 关机指令，你可以man shutdown 来看一下帮助文档。例如你可以运行如下命令关机： shutdown –h 10 ‘This server will shutdown after 10 mins’ 这个命令告诉大家，计算机将在10分钟后关机，并且会显示在登陆用户的当前屏幕中。 shutdown –h now 立马关机 shutdown –h 20:25 系统会在今天20:25关机 shutdown –h +10 十分钟后关机 shutdown –r now 系统立马重启 shutdown –r +10 系统十分钟后重启 reboot 就是重启，等同于 shutdown –r now halt 关闭系统，等同于shutdown –h now 和 poweroff 最后总结一下，不管是重启系统还是关闭系统，首先要运行 sync 命令，把内存中的数据写到磁盘中。 关机的命令有 shutdown –h now halt poweroff 和 init 0 , 重启系统的命令有 shutdown –r now reboot init 6。 注意事项 严格区分大小写 所有内容以文件形式保存，包括硬件 硬件/dev/sd[a-p] 光盘文件/dec/sr0等 不靠扩展名区分文件类型 有扩展都是给管理员看的 所有存储设备都需要挂载之后才能使用 windows程序不能直接在linux安装运行 服务器不允许关机，只能重启 重启时应该关闭服务 服务器访问高峰不要使用高负载命令 远程配置防火墙时不要把自己踢出服务器 指定合理密码规范并定期更新 合理分配权限 定期备份重要数据和日志 系统目录结构 ls / 查看目录 /bin： bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。 /boot： 这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ： dev 是 Device(设备) 的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。 /etc： etc 是 Etcetera(等等) 的缩写,这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home： 用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的，如上图中的 alice、bob 和 eve。 /lib： lib 是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found： 这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media： linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux 会把识别的设备挂载到这个目录下。 /mnt： 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在 /mnt/ 上，然后进入该目录就可以查看光驱里的内容了。 /opt： opt 是 optional(可选) 的缩写，这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc： proc 是 Processes(进程) 的缩写，/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器： echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root： 该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin： s 就是 Super User 的意思，是 Superuser Binaries (超级用户的二进制文件) 的缩写，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是 Linux2.6 内核的一个很大的变化。该目录下安装了 2.6 内核中新出现的一个文件系统 sysfs 。 sysfs 文件系统集成了下面3种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的 devpts 文件系统。 该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp： tmp 是 temporary(临时) 的缩写这个目录是用来存放一些临时文件的。 /usr： usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。 /usr/bin： 系统用户使用的应用程序。 /usr/sbin： 超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src： 内核源代码默认的放置目录。 /var： var 是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run： 是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。 /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在 /bin/ls 目录下的。 值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给 root 使用的指令。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。 忘记密码解决方法https://www.runoob.com/linux/linux-forget-password.html 重启linux系统 3 秒之内要按一下回车 输入e 第二行最后边输入 single，有一个空格。具体方法为按向下尖头移动到第二行，按&quot;e&quot;进入编辑模式 在后边加上single 回车 最后按&quot;b&quot;启动，启动后就进入了单用户模式了 更改root密码了。更密码的命令为 passwd 光盘启动，按F5 进入rescue模式 输入linux rescue 回车 选择英语 选择us 键盘 问你是否启动网络，有时候可能会联网调试。我们选no 这里告诉我们，接下来会把系统挂载在/mnt/sysimage 中。 其中有三个选项: Continue 就是挂载后继续下一步。 Read-Only 挂载成只读，这样更安全，有时文件系统损坏时，只读模式会防止文件系统近一步损坏。 Skip就是不挂载，进入一个命令窗口模式。 这里我们选择Continue。 - 至此，系统已经挂载到了/mnt/sysimage中。接下来回车，输入chroot /mnt/sysimage 进入管理员环境。 远程登录 Linux 一般作为服务器使用，而服务器一般放在机房，你不可能在机房操作你的 Linux 服务器。这时我们就需要远程登录到Linux服务器来管理维护系统。 Linux 系统中是通过 ssh 服务实现的远程登录功能，默认 ssh 服务端口号为 22。 Window 系统上 Linux 远程登录客户端有 SecureCRT, Putty, SSH Secure Shell 等，本文以 Putty 为例来登录远程服务器。 Putty 下载地址：https://www.putty.org/ 在Host Name( or IP address) 下面的框中输入你要登录的远程服务器IP(可以通过ifconfig命令查看服务器ip)，然后回车。 输入要登录的用户名。 再输入密码，就能登录到远程的linux系统了 使用密钥认证机制远程登录linux SSH 为 Secure Shell 的缩写，由 IETF 的网络工作小组（Network Working Group）所制定。 SSH 为建立在应用层和传输层基础上的安全协议。 首先使用工具 PUTTYGEN.EXE 生成密钥对。打开工具 PUTTYGEN.EXE 后如下图所示： 该工具可以生成三种格式的key ：SSH-1(RSA) SSH-2(RSA) SSH-2(DSA) ，我们采用默认的格式即 SSH-2(RSA)。Number of bits in a generated key 这个是指生成的key的大小，这个数值越大，生成的key就越复杂，安全性就越高。这里我们写 2048。 Generate 过程中鼠标要来回的动，否则这个进度条是不会动的。 可以给你的密钥输入一个密码，（在Key Passphrase那里）也可以留空。然后点 Save public key 保存公钥，点 Save private Key 保存私钥。笔者建议你放到一个比较安全的地方，一来防止别人偷窥，二来防止误删除。接下来就该到远程 linux 主机上设置了。 1）创建目录 /root/.ssh 并设置权限 [root@localhost ~]# mkdir /root/.ssh mkdir 命令用来创建目录，以后会详细介绍，暂时只了解即可。 [root@localhost ~]# chmod 700 /root/.ssh chmod 命令是用来修改文件属性权限的，以后会详细介绍。 2）创建文件 / root/.ssh/authorized_keys [root@localhost ~]# vim /root/.ssh/authorized_keys vim 命令是编辑一个文本文件的命令，同样在后续章节详细介绍。 3）打开刚才生成的public key 文件，建议使用写字板打开，这样看着舒服一些，复制从AAAA开头至 &quot;---- END SSH2 PUBLIC KEY ----&quot; 该行上的所有内容，粘贴到/root/.ssh/authorized_keys 文件中，要保证所有字符在一行。（可以先把复制的内容拷贝至记事本，然后编辑成一行载粘贴到该文件中）。 在这里要简单介绍一下，如何粘贴，用vim打开那个文件后，该文件不存在，所以vim会自动创建。按一下字母&quot;i&quot;然后同时按shift + Insert 进行粘贴（或者单击鼠标右键即可），前提是已经复制到剪切板中了。粘贴好后，然后把光标移动到该行最前面输入 ssh-rsa ，然后按空格。再按ESC，然后输入冒号wq 即 :wq 就保存了。格式如下图： - 再设置putty选项，点窗口左侧的SSh –&gt; Auth ，单击窗口右侧的Browse… 选择刚刚生成的私钥， 再点Open ，此时输入root，就不用输入密码就能登录了。如果在前面你设置了Key Passphrase ，那么此时就会提示你输入密码的。为了更加安全建议大家要设置一个Key Passphrase。 文件基本属性 Linux 系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。 为了保护系统的安全性，Linux 系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。 在 Linux 中我们通常使用以下两个命令来修改文件或目录的所属用户与权限： chown (change ownerp) ： 修改所属用户与组。 chmod (change mode) ： 修改用户的权限。 使用 ll 或者 ls –l 命令来显示一个文件的属性以及文件所属的用户和组，如： [root@www /]# ls -l total 64 dr-xr-xr-x 2 root root 4096 Dec 14 2012 bin dr-xr-xr-x 4 root root 4096 Apr 19 2012 boot …… - 实例中，bin 文件的第一个属性用 d 表示。d 在 Linux 中代表该文件是一个目录文件。 在 Linux 中第一个字符代表这个文件是目录、文件或链接文件等等。 当为 d 则是目录 当为 - 则是文件； 若是 l 则表示为链接文档(link file)； 若是 b 则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是 c 则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。 - 接下来的字符中，以三个为一组，且均为 rwx 的三个参数的组合。其中， r 代表可读(read)、 w 代表可写(write)、 x 代表可执行(execute)。 要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号 - 而已。 - 每个文件的属性由左边第一部分的 10 个字符来确定（如图）。 363003_1227493859FdXT 从左至右用 0-9 这些数字来表示。 第 0 位确定文件类型，第 1-3 位确定属主（该文件的所有者）拥有该文件的权限。 第4-6位确定属组（所有者的同组用户）拥有该文件的权限，第7-9位确定其他用户拥有该文件的权限。 其中，第 1、4、7 位表示读权限，如果用 r 字符表示，则有读权限，如果用 - 字符表示，则没有读权限； 第 2、5、8 位表示写权限，如果用 w 字符表示，则有写权限，如果用 - 字符表示没有写权限；第 3、6、9 位表示可执行权限，如果用 x 字符表示，则有执行权限，如果用 - 字符表示，则没有执行权限。 Linux文件属主和属组 [root@www /]# ls -l total 64 drwxr-xr-x 2 root root 4096 Feb 15 14:46 cron drwxr-xr-x 3 mysql mysql 4096 Apr 21 2014 mysql …… 对于文件来说，它都有一个特定的所有者，也就是对该文件具有所有权的用户。 同时，在Linux系统中，用户是按组分类的，一个用户属于一个或多个组。 文件所有者以外的用户又可以分为文件所有者的同组用户和其他用户。 因此，Linux系统按文件所有者、文件所有者同组用户和其他用户来规定了不同的文件访问权限。 在以上实例中，mysql 文件是一个目录文件，属主和属组都为 mysql，属主有可读、可写、可执行的权限；与属主同组的其他用户有可读和可执行的权限；其他用户也有可读和可执行的权限。 对于 root 用户来说，一般情况下，文件的权限对其不起作用。 更改文件属性 chgrp：更改文件属组 语法：chgrp [-R] 属组名 文件名 参数选项 -R：递归更改文件属组，就是在更改某个目录文件的属组时，如果加上-R的参数，那么该目录下的所有文件的属组都会更改。 chown：更改文件属主，也可以同时更改文件属组 语法： chown [–R] 属主名 文件名 chown [-R] 属主名：属组名 文件名 进入 /root 目录（~）将install.log的拥有者改为bin这个账号： [root@www ~] cd ~ [root@www ~]# chown bin install.log [root@www ~]# ls -l -rw-r--r-- 1 bin users 68495 Jun 25 08:53 install.log 将install.log的拥有者与群组改回为root： [root@www ~]# chown root:root install.log [root@www ~]# ls -l -rw-r--r-- 1 root root 68495 Jun 25 08:53 install.log chmod：更改文件9个属性 Linux文件属性有两种设置方法，一种是数字，一种是符号。 Linux 文件的基本权限就有九个，分别是 owner/group/others(拥有者/组/其他) 三种身份各有自己的 read/write/execute 权限。 先复习一下刚刚上面提到的数据：文件的权限字符为： -rwxrwxrwx ， 这九个权限是三个三个一组的！其中，我们可以使用数字来代表各个权限，各权限的分数对照表如下： r:4 w:2 x:1 每种身份(owner/group/others)各自的三个权限(r/w/x)分数是需要累加的，例如当权限为： -rwxrwx--- 分数则是： owner = rwx = 4+2+1 = 7 group = rwx = 4+2+1 = 7 others= --- = 0+0+0 = 0 所以等一下我们设定权限的变更时，该文件的权限数字就是 770。变更权限的指令 chmod 的语法是这样的： chmod [-R] xyz 文件或目录 选项与参数： xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加。 -R : 进行递归(recursive)的持续变更，亦即连同次目录下的所有文件都会变更 举例来说，如果要将 .bashrc 这个文件所有的权限都设定启用，那么命令如下： [root@www ~]# ls -al .bashrc -rw-r--r-- 1 root root 395 Jul 4 11:45 .bashrc [root@www ~]# chmod 777 .bashrc [root@www ~]# ls -al .bashrc -rwxrwxrwx 1 root root 395 Jul 4 11:45 .bashrc 那如果要将权限变成 -rwxr-xr-- 呢？那么权限的分数就成为 [4+2+1][4+0+1][4+0+0]=754。 符号类型改变文件权限 还有一个改变权限的方法，从之前的介绍中我们可以发现，基本上就九个权限分别是： user：用户 group：组 others：其他 那么我们就可以使用 u, g, o 来代表三种身份的权限。 此外， a 则代表 all，即全部的身份。读写的权限可以写成 r, w, x，也就是可以使用下表的方式来看： chmod u g o a +(加入) -(除去) =(设定) r w x 文件或目录 如果我们需要将文件权限设置为 -rwxr-xr-- ，可以使用 chmod u=rwx,g=rx,o=r 文件名 来设定: touch test1 // 创建 test1 文件 ls -al test1 // 查看 test1 默认权限 -rw-r--r-- 1 root root 0 Nov 15 10:32 test1 chmod u=rwx,g=rx,o=r test1 // 修改 test1 权限 ls -al test1 -rwxr-xr-- 1 root root 0 Nov 15 10:32 test1 而如果是要将权限去掉而不改变其他已存在的权限呢？例如要拿掉全部人的可执行权限，则： chmod a-x test1 ls -al test1 -rw-r--r-- 1 root root 0 Nov 15 10:32 test1 文件与目录管理 Linux的目录结构为树状结构，最顶级的目录为根目录 /。 其他目录通过挂载可以将它们添加到树中，通过解除挂载可以移除它们。 在开始本教程前我们需要先知道什么是绝对路径与相对路径。 绝对路径： 路径的写法，由根目录 / 写起，例如： /usr/share/doc 这个目录。 相对路径： 路径的写法，不是由 / 写起，例如由 /usr/share/doc 要到 /usr/share/man 底下时，可以写成： cd ../man 这就是相对路径的写法。 处理目录的常用命令 ls（英文全拼：list files）: 列出目录及文件名 cd（英文全拼：change directory）：切换目录 pwd（英文全拼：print work directory）：显示目前的目录 mkdir（英文全拼：make directory）：创建一个新的目录 rmdir（英文全拼：remove directory）：删除一个空的目录 cp（英文全拼：copy file）: 复制文件或目录 rm（英文全拼：remove）: 移除文件或目录 mv（英文全拼：move file）: 移动文件与目录，或修改文件与目录的名称 你可以使用 man [命令] 来查看各个命令的使用文档，如 ：man cp。 ls (列出目录) 在Linux系统当中， ls 命令可能是最常被运行的。 语法： [root@www ~]# ls [-aAdfFhilnrRSt] 目录名称 [root@www ~]# ls [--color={never,auto,always}] 目录名称 [root@www ~]# ls [--full-time] 目录名称 选项与参数： -a ：全部的文件，连同隐藏文件( 开头为 . 的文件) 一起列出来(常用) -d ：仅列出目录本身，而不是列出目录内的文件数据(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用) 将家目录下的所有文件列出来(含属性与隐藏档) [root@www ~]# ls -al ~ cd (切换目录) cd是Change Directory的缩写，这是用来变换工作目录的命令。 语法： cd [相对路径或绝对路径] #使用 mkdir 命令创建 runoob 目录 [root@www ~]# mkdir runoob #使用绝对路径切换到 runoob 目录 [root@www ~]# cd /root/runoob/ #使用相对路径切换到 runoob 目录 [root@www ~]# cd ./runoob/ 表示回到自己的家目录，亦即是 /root 这个目录 [root@www runoob]# cd ~ 表示去到目前的上一级目录，亦即是 /root 的上一级目录的意思； [root@www ~]# cd .. 接下来大家多操作几次应该就可以很好的理解 cd 命令的。 - pwd (显示目前所在的目录) pwd 是 Print Working Directory 的缩写，也就是显示目前所在目录的命令。 [root@www ~]# pwd [-P] 选项与参数： -P ：显示出确实的路径，而非使用连结 (link) 路径。 实例：单纯显示出目前的工作目录： [root@www ~]# pwd /root &lt;== 显示出目录啦～ 实例显示出实际的工作目录，而非连结档本身的目录名而已。 [root@www ~]# cd /var/mail &lt;==注意，/var/mail是一个连结档 [root@www mail]# pwd /var/mail &lt;==列出目前的工作目录 [root@www mail]# pwd -P /var/spool/mail &lt;==怎么回事？有没有加 -P 差很多～ [root@www mail]# ls -ld /var/mail lrwxrwxrwx 1 root root 10 Sep 4 17:54 /var/mail -&gt; spool/mail 看到这里应该知道为啥了吧？因为 /var/mail 是连结档，连结到 /var/spool/mail 所以，加上 pwd -P 的选项后，会不以连结档的数据显示，而是显示正确的完整路径啊！ - mkdir (创建新目录) 如果想要创建新的目录的话，那么就使用mkdir (make directory)吧。 语法： mkdir [-mp] 目录名称 选项与参数： -m ：配置文件的权限喔！直接配置，不需要看默认权限 (umask) 的脸色～ -p ：帮助你直接将所需要的目录(包含上一级目录)递归创建起来！ 实例：请到/tmp底下尝试创建数个新目录看看： [root@www ~]# cd /tmp [root@www tmp]# mkdir test &lt;创建一名为 test 的新目录 [root@www tmp]# mkdir test1/test2/test3/test4 mkdir: cannot create directory `test1/test2/test3/test4': No such file or directory &lt; 没办法直接创建此目录啊！ [root@www tmp]# mkdir -p test1/test2/test3/test4 加了这个 -p 的选项，可以自行帮你创建多层目录！ 实例：创建权限为 rwx--x--x 的目录。 [root@www tmp]# mkdir -m 711 test2 [root@www tmp]# ls -l drwxr-xr-x 3 root root 4096 Jul 18 12:50 test drwxr-xr-x 3 root root 4096 Jul 18 12:53 test1 drwx--x--x 2 root root 4096 Jul 18 12:54 test2 上面的权限部分，如果没有加上 -m 来强制配置属性，系统会使用默认属性。 如果我们使用 -m ，如上例我们给予 -m 711 来给予新的目录 drwx--x--x 的权限。 - rmdir (删除空的目录) 语法： rmdir [-p] 目录名称 选项与参数： -p ：连同上一级『空的』目录也一起删除 删除 runoob 目录 [root@www tmp]# rmdir runoob/ 将 mkdir 实例中创建的目录(/tmp 底下)删除掉！ [root@www tmp]# ls -l &lt;==看看有多少目录存在？ drwxr-xr-x 3 root root 4096 Jul 18 12:50 test drwxr-xr-x 3 root root 4096 Jul 18 12:53 test1 drwx--x--x 2 root root 4096 Jul 18 12:54 test2 [root@www tmp]# rmdir test &lt;==可直接删除掉，没问题 [root@www tmp]# rmdir test1 &lt;==因为尚有内容，所以无法删除！ rmdir: `test1': Directory not empty [root@www tmp]# rmdir -p test1/test2/test3/test4 [root@www tmp]# ls -l &lt;==您看看，底下的输出中test与test1不见了！ drwx--x--x 2 root root 4096 Jul 18 12:54 test2 利用 -p 这个选项，立刻就可以将 test1/test2/test3/test4 一次删除。 不过要注意的是，这个 rmdir 仅能删除空的目录，你可以使用 rm 命令来删除非空目录。 - cp (复制文件或目录) cp 即拷贝文件和目录。 语法: [root@www ~]# cp [-adfilprsu] 来源档(source) 目标档(destination) [root@www ~]# cp [options] source1 source2 source3 .... directory 选项与参数： -a：相当於 -pdr 的意思，至於 pdr 请参考下列说明；(常用) -d：若来源档为连结档的属性(link file)，则复制连结档属性而非文件本身； -f：为强制(force)的意思，若目标文件已经存在且无法开启，则移除后再尝试一次； -i：若目标档(destination)已经存在时，在覆盖时会先询问动作的进行(常用) -l：进行硬式连结(hard link)的连结档创建，而非复制文件本身； -p：连同文件的属性一起复制过去，而非使用默认属性(备份常用)； -r：递归持续复制，用於目录的复制行为；(常用) -s：复制成为符号连结档 (symbolic link)，亦即『捷径』文件； -u：若 destination 比 source 旧才升级 destination ！ 用 root 身份，将 root 目录下的 .bashrc 复制到 /tmp 下，并命名为 bashrc [root@www ~]# cp ~/.bashrc /tmp/bashrc [root@www ~]# cp -i ~/.bashrc /tmp/bashrc cp: overwrite `/tmp/bashrc'? n &lt;==n不覆盖，y为覆盖 - rm (移除文件或目录) 语法： rm [-fir] 文件或目录 选项与参数： -f ：就是 force 的意思，忽略不存在的文件，不会出现警告信息； -i ：互动模式，在删除前会询问使用者是否动作 -r ：递归删除啊！最常用在目录的删除了！这是非常危险的选项！！！ 将刚刚在 cp 的实例中创建的 bashrc 删除掉！ [root@www tmp]# rm -i bashrc rm: remove regular file `bashrc'? y 如果加上 -i 的选项就会主动询问喔，避免你删除到错误的档名！ - mv (移动文件与目录，或修改名称) 语法： [root@www ~]# mv [-fiu] source destination [root@www ~]# mv [options] source1 source2 source3 .... directory 选项与参数： -f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖； -i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！ -u ：若目标文件已经存在，且 source 比较新，才会升级 (update) 复制一文件，创建一目录，将文件移动到目录中 [root@www ~]# cd /tmp [root@www tmp]# cp ~/.bashrc bashrc [root@www tmp]# mkdir mvtest [root@www tmp]# mv bashrc mvtest 将某个文件移动到某个目录去，就是这样做！ 将刚刚的目录名称更名为 mvtest2 [root@www tmp]# mv mvtest mvtest2 - Linux 文件内容查看 Linux系统中使用以下命令来查看文件的内容： cat 由第一行开始显示文件内容 tac 从最后一行开始显示，可以看出 tac 是 cat 的倒着写！ nl 显示的时候，顺道输出行号！ more 一页一页的显示文件内容 less 与 more 类似，但是比 more 更好的是，他可以往前翻页！ head 只看头几行 tail 只看尾巴几行 你可以使用 man [命令]来查看各个命令的使用文档，如 ：man cp。 - cat 由第一行开始显示文件内容 语法： cat [-AbEnTv] 选项与参数： -A ：相当於 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号！ -E ：将结尾的断行字节 $ 显示出来； -n ：列印出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符 检看 /etc/issue 这个文件的内容： [root@www ~]# cat /etc/issue CentOS release 6.4 (Final) Kernel \\r on an \\m - tac tac与cat命令刚好相反，文件内容从最后一行开始显示，可以看出 tac 是 cat 的倒着写！如： [root@www ~]# tac /etc/issue Kernel \\r on an \\m CentOS release 6.4 (Final) - nl 显示行号 语法： nl [-bnw] 文件 选项与参数： -b ：指定行号指定的方式，主要有两种： -b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)； -b t ：如果有空行，空的那一行不要列出行号(默认值)； -n ：列出行号表示的方法，主要有三种： -n ln ：行号在荧幕的最左方显示； -n rn ：行号在自己栏位的最右方显示，且不加 0 ； -n rz ：行号在自己栏位的最右方显示，且加 0 ； -w ：行号栏位的占用的位数。 实例一：用 nl 列出 /etc/issue 的内容 [root@www ~]# nl /etc/issue 1 CentOS release 6.4 (Final) 2 Kernel \\r on an \\m - more 一页一页翻动 [root@www ~]# more /etc/man_db.config Generated automatically from man.conf.in by the configure script. man.conf from man-1.6d ....(中间省略).... --More--(28%) &lt;== 重点在这一行喔！你的光标也会在这里等待你的命令 在 more 这个程序的运行过程中，你有几个按键可以按的： 空白键 (space)：代表向下翻一页； Enter ：代表向下翻『一行』； /字串 ：代表在这个显示的内容当中，向下搜寻『字串』这个关键字； :f ：立刻显示出档名以及目前显示的行数； q ：代表立刻离开 more ，不再显示该文件内容。 b 或 [ctrl]-b ：代表往回翻页，不过这动作只对文件有用，对管线无用。 - less 一页一页翻动，以下实例输出/etc/man.config文件的内容： [root@www ~]# less /etc/man.config Generated automatically from man.conf.in by the configure script. man.conf from man-1.6d ....(中间省略).... &lt;== 这里可以等待你输入命令！ less运行时可以输入的命令有： 空白键 ：向下翻动一页； [pagedown]：向下翻动一页； [pageup] ：向上翻动一页； /字串 ：向下搜寻『字串』的功能； ?字串 ：向上搜寻『字串』的功能； n ：重复前一个搜寻 (与 / 或 ? 有关！) N ：反向的重复前一个搜寻 (与 / 或 ? 有关！) q ：离开 less 这个程序； - head 取出文件前面几行 语法： head [-n number] 文件 选项与参数： -n ：后面接数字，代表显示几行的意思 [root@www ~]# head /etc/man.config 默认的情况中，显示前面 10 行！若要显示前 20 行，就得要这样： [root@www ~]# head -n 20 /etc/man.config - tail 取出文件后面几行 语法： tail [-n number] 文件 选项与参数： -n ：后面接数字，代表显示几行的意思 -f ：表示持续侦测后面所接的档名，要等到按下[ctrl]-c才会结束tail的侦测 [root@www ~]# tail /etc/man.config 默认的情况中，显示最后的十行！若要显示最后的 20 行，就得要这样： [root@www ~]# tail -n 20 /etc/man.config 用户和用户组管理 Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 每个用户账号都拥有一个唯一的用户名和各自的口令。 用户在登录时键入正确的用户名和口令后，就能够进入系统和自己的主目录。 实现用户账号的管理，要完成的工作主要有如下几个方面： 用户账号的添加、删除与修改。 用户口令的管理。 用户组的管理。 一、Linux系统用户账号的管理 用户账号的管理工作主要涉及到用户账号的添加、修改和删除。 添加用户账号就是在系统中创建一个新账号，然后为新账号分配用户号、用户组、主目录和登录Shell等资源。刚添加的账号是被锁定的，无法使用。 1、添加新的用户账号使用useradd命令，其语法如下： useradd 选项 用户名 参数说明： 选项: -c comment 指定一段注释性描述。 -d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，可以创建主目录。 -g 用户组 指定用户所属的用户组。 -G 用户组，用户组 指定用户所属的附加组。 -s Shell文件 指定用户的登录Shell。 -u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号。 用户名: 指定新账号的登录名。 实例1 useradd –d /home/sam -m sam 此命令创建了一个用户sam，其中-d和-m选项用来为登录名sam产生一个主目录 /home/sam（/home为默认的用户主目录所在的父目录）。 实例2 useradd -s /bin/sh -g group –G adm,root gem 此命令新建了一个用户gem，该用户的登录Shell是 /bin/sh，它属于group用户组，同时又属于adm和root用户组，其中group用户组是其主组。 这里可能新建组：#groupadd group及groupadd adm 增加用户账号就是在/etc/passwd文件中为新用户增加一条记录，同时更新其他系统文件如/etc/shadow, /etc/group等。 Linux提供了集成的系统管理工具userconf，它可以用来对用户账号进行统一管理。 2、删除帐号 如果一个用户的账号不再使用，可以从系统中删除。删除用户账号就是要将/etc/passwd等系统文件中的该用户记录删除，必要时还删除用户的主目录。 删除一个已有的用户账号使用userdel命令，其格式如下： userdel 选项 用户名 常用的选项是 -r，它的作用是把用户的主目录一起删除。 例如： userdel -r sam 此命令删除用户sam在系统文件中（主要是/etc/passwd, /etc/shadow, /etc/group等）的记录，同时删除用户的主目录。 3、修改帐号 修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录Shell等。 修改已有用户的信息使用usermod命令，其格式如下： usermod 选项 用户名 常用的选项包括-c, -d, -m, -g, -G, -s, -u以及-o等，这些选项的意义与useradd命令中的选项一样，可以为用户指定新的资源值。 另外，有些系统可以使用选项：-l 新用户名 这个选项指定一个新的账号，即将原来的用户名改为新的用户名。 例如： usermod -s /bin/ksh -d /home/z –g developer sam 此命令将用户sam的登录Shell修改为ksh，主目录改为/home/z，用户组改为developer。 4、用户口令的管理 用户管理的一项重要内容是用户口令的管理。用户账号刚创建时没有口令，但是被系统锁定，无法使用，必须为其指定口令后才可以使用，即使是指定空口令。 指定和修改用户口令的Shell命令是passwd。超级用户可以为自己和其他用户指定口令，普通用户只能用它修改自己的口令。命令的格式为： passwd 选项 用户名 可使用的选项： -l 锁定口令，即禁用账号。 -u 口令解锁。 -d 使账号无口令。 -f 强迫用户下次登录时修改口令。 如果默认用户名，则修改当前用户的口令。 例如，假设当前用户是sam，则下面的命令修改该用户自己的口令： $ passwd Old password:****** New password:******* Re-enter new password:******* 如果是超级用户，可以用下列形式指定任何用户的口令： passwd sam New password:******* Re-enter new password:******* 普通用户修改自己的口令时，passwd命令会先询问原口令，验证后再要求用户输入两遍新口令，如果两次输入的口令一致，则将这个口令指定给用户；而超级用户为用户指定口令时，就不需要知道原口令。 为了系统安全起见，用户应该选择比较复杂的口令，例如最好使用8位长的口令，口令中包含有大写、小写字母和数字，并且应该与姓名、生日等不相同。 为用户指定空口令时，执行下列形式的命令： passwd -d sam 此命令将用户 sam 的口令删除，这样用户 sam 下一次登录时，系统就不再允许该用户登录了。 passwd 命令还可以用 -l(lock) 选项锁定某一用户，使其不能登录，例如： passwd -l sam 二、Linux系统用户组的管理 每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1、增加一个新的用户组使用groupadd命令。其格式如下： groupadd 选项 用户组 可以使用的选项有： -g GID 指定新用户组的组标识号（GID）。 -o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。 实例1： groupadd group1 此命令向系统中增加了一个新组group1，新组的组标识号是在当前已有的最大组标识号的基础上加1。 实例2： groupadd -g 101 group2 此命令向系统中增加了一个新组group2，同时指定新组的组标识号是101。 2、如果要删除一个已有的用户组，使用groupdel命令，其格式如下： groupdel 用户组 例如： groupdel group1 此命令从系统中删除组group1。 3、修改用户组的属性使用groupmod命令。其语法如下： groupmod 选项 用户组 常用的选项有： -g GID 为用户组指定新的组标识号。 -o 与-g选项同时使用，用户组的新GID可以与系统已有用户组的GID相同。 -n新用户组 将用户组的名字改为新名字 实例1： groupmod -g 102 group2 此命令将组group2的组标识号修改为102。 实例2： groupmod –g 10000 -n group3 group2 此命令将组group2的标识号改为10000，组名修改为group3。 4、如果一个用户同时属于多个用户组，那么用户可以在用户组之间切换，以便具有其他用户组的权限。 用户可以在登录后，使用命令newgrp切换到其他用户组，这个命令的参数就是目的用户组。例如： $ newgrp root 这条命令将当前用户切换到root用户组，前提条件是root用户组确实是该用户的主组或附加组。类似于用户账号的管理，用户组的管理也可以通过集成的系统管理工具来完成。 三、与用户账号有关的系统文件 完成用户管理的工作有许多种方法，但是每一种方法实际上都是对有关的系统文件进行修改。 与用户和用户组相关的信息都存放在一些系统文件中，这些文件包括/etc/passwd, /etc/shadow, /etc/group等。 下面分别介绍这些文件的内容。 1、/etc/passwd文件是用户管理工作涉及的最重要的一个文件。 Linux系统中的每个用户都在/etc/passwd文件中有一个对应的记录行，它记录了这个用户的一些基本属性。 这个文件对所有用户都是可读的。它的内容类似下面的例子： ＃ cat /etc/passwd root❌0:0:Superuser:/: daemon❌1:1:System daemons:/etc: bin❌2:2:Owner of system commands:/bin: sys❌3:3:Owner of system files:/usr/sys: adm❌4:4:System accounting:/usr/adm: uucp❌5:5:UUCP administrator:/usr/lib/uucp: auth❌7:21:Authentication administrator:/tcb/files/auth: cron❌9:16:Cron daemon:/usr/spool/cron: listen❌37:4:Network daemon:/usr/net/nls: lp❌71:18:Printer administrator:/usr/spool/lp: sam❌200:50:Sam san:/home/sam:/bin/sh 从上面的例子我们可以看到，/etc/passwd中一行记录对应着一个用户，每行记录又被冒号(:)分隔为7个字段，其格式和具体含义如下： 用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell 1）&quot;用户名&quot;是代表用户账号的字符串。 通常长度不超过8个字符，并且由大小写字母和/或数字组成。登录名中不能有冒号(😃，因为冒号在这里是分隔符。 为了兼容起见，登录名中最好不要包含点字符(.)，并且不使用连字符(-)和加号(+)打头。 2）“口令”一些系统中，存放着加密后的用户口令字。 虽然这个字段存放的只是用户口令的加密串，不是明文，但是由于/etc/passwd文件对所有用户都可读，所以这仍是一个安全隐患。因此，现在许多Linux 系统（如SVR4）都使用了shadow技术，把真正的加密后的用户口令字存放到/etc/shadow文件中，而在/etc/passwd文件的口令字段中只存放一个特殊的字符，例如“x”或者“*”。 3）“用户标识号”是一个整数，系统内部用它来标识用户。 一般情况下它与用户名是一一对应的。如果几个用户名对应的用户标识号是一样的，系统内部将把它们视为同一个用户，但是它们可以有不同的口令、不同的主目录以及不同的登录Shell等。 通常用户标识号的取值范围是0～65 535。0是超级用户root的标识号，1～99由系统保留，作为管理账号，普通用户的标识号从100开始。在Linux系统中，这个界限是500。 4）“组标识号”字段记录的是用户所属的用户组。 它对应着/etc/group文件中的一条记录。 5)“注释性描述”字段记录着用户的一些个人情况。 例如用户的真实姓名、电话、地址等，这个字段并没有什么实际的用途。在不同的Linux 系统中，这个字段的格式并没有统一。在许多Linux系统中，这个字段存放的是一段任意的注释性描述文字，用做finger命令的输出。 6)“主目录”，也就是用户的起始工作目录。 它是用户在登录到系统之后所处的目录。在大多数系统中，各用户的主目录都被组织在同一个特定的目录下，而用户主目录的名称就是该用户的登录名。各用户对自己的主目录有读、写、执行（搜索）权限，其他用户对此目录的访问权限则根据具体情况设置。 7)用户登录后，要启动一个进程，负责将用户的操作传给内核，这个进程是用户登录到系统后运行的命令解释器或某个特定的程序，即Shell。 Shell是用户与Linux系统之间的接口。Linux的Shell有许多种，每种都有不同的特点。常用的有sh(Bourne Shell), csh(C Shell), ksh(Korn Shell), tcsh(TENEX/TOPS-20 type C Shell), bash(Bourne Again Shell)等。 系统管理员可以根据系统情况和用户习惯为用户指定某个Shell。如果不指定Shell，那么系统使用sh为默认的登录Shell，即这个字段的值为/bin/sh。 用户的登录Shell也可以指定为某个特定的程序（此程序不是一个命令解释器）。 利用这一特点，我们可以限制用户只能运行指定的应用程序，在该应用程序运行结束后，用户就自动退出了系统。有些Linux 系统要求只有那些在系统中登记了的程序才能出现在这个字段中。 8)系统中有一类用户称为伪用户（pseudo users）。 这些用户在/etc/passwd文件中也占有一条记录，但是不能登录，因为它们的登录Shell为空。它们的存在主要是方便系统管理，满足相应的系统进程对文件属主的要求。 常见的伪用户如下所示： 伪 用 户 含 义 bin 拥有可执行的用户命令文件 sys 拥有系统文件 adm 拥有帐户文件 uucp UUCP使用 lp lp或lpd子系统使用 nobody NFS使用 拥有帐户文件 1、除了上面列出的伪用户外，还有许多标准的伪用户，例如：audit, cron, mail, usenet等，它们也都各自为相关的进程和文件所需要。 由于/etc/passwd文件是所有用户都可读的，如果用户的密码太简单或规律比较明显的话，一台普通的计算机就能够很容易地将它破解，因此对安全性要求较高的Linux系统都把加密后的口令字分离出来，单独存放在一个文件中，这个文件是/etc/shadow文件。 有超级用户才拥有该文件读权限，这就保证了用户密码的安全性。 2、/etc/shadow中的记录行与/etc/passwd中的一一对应，它由pwconv命令根据/etc/passwd中的数据自动产生 它的文件格式与/etc/passwd类似，由若干个字段组成，字段之间用&quot;:&quot;隔开。这些字段是： 登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志 &quot;登录名&quot;是与/etc/passwd文件中的登录名相一致的用户账号 &quot;口令&quot;字段存放的是加密后的用户口令字，长度为13个字符。如果为空，则对应用户没有口令，登录时不需要口令；如果含有不属于集合 { ./0-9A-Za-z }中的字符，则对应的用户不能登录。 &quot;最后一次修改时间&quot;表示的是从某个时刻起，到用户最后一次修改口令时的天数。时间起点对不同的系统可能不一样。例如在SCO Linux 中，这个时间起点是1970年1月1日。 &quot;最小时间间隔&quot;指的是两次修改口令之间所需的最小天数。 &quot;最大时间间隔&quot;指的是口令保持有效的最大天数。 &quot;警告时间&quot;字段表示的是从系统开始警告用户到用户密码正式失效之间的天数。 &quot;不活动时间&quot;表示的是用户没有登录活动但账号仍能保持有效的最大天数。 &quot;失效时间&quot;字段给出的是一个绝对的天数，如果使用了这个字段，那么就给出相应账号的生存期。期满后，该账号就不再是一个合法的账号，也就不能再用来登录了。 下面是/etc/shadow的一个例子： ＃ cat /etc/shadow root:Dnakfw28zf38w:8764:0:168:7::: daemon:::0:0:::: bin:::0:0:::: sys:::0:0:::: adm:::0:0:::: uucp:::0:0:::: nuucp:::0:0:::: auth:::0:0:::: cron:::0:0:::: listen:::0:0:::: lp:::0:0:::: sam:EkdiSECLWPdSa:9740:0:0:::: 3、用户组的所有信息都存放在/etc/group文件中。 将用户分组是Linux 系统中对用户进行管理及控制访问权限的一种手段。 每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不同的组。 当一个用户同时是多个组中的成员时，在/etc/passwd文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。 用户要访问属于附加组的文件时，必须首先使用newgrp命令使自己成为所要访问的组中的成员。 用户组的所有信息都存放在/etc/group文件中。此文件的格式也类似于/etc/passwd文件，由冒号(:)隔开若干个字段，这些字段有： 组名:口令:组标识号:组内用户列表 &quot;组名&quot;是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。 &quot;口令&quot;字段存放的是用户组加密后的口令字。一般Linux 系统的用户组都没有口令，即这个字段一般为空，或者是*。 &quot;组标识号&quot;与用户标识号类似，也是一个整数，被系统内部用来标识组。 &quot;组内用户列表&quot;是属于这个组的所有用户的列表/b]，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。 /etc/group文件的一个例子如下： root::0:root bin::2:root,bin sys::3:root,uucp adm::4:root,adm daemon::5:root,daemon lp::7:root,lp users::20:root,sam 四、添加批量用户 添加和删除用户对每位Linux系统管理员都是轻而易举的事，比较棘手的是如果要添加几十个、上百个甚至上千个用户时，我们不太可能还使用useradd一个一个地添加，必然要找一种简便的创建大量用户的方法。Linux系统提供了创建大量用户的工具，可以让您立即创建大量用户，方法如下： （1）先编辑一个文本用户文件。 每一列按照/etc/passwd密码文件的格式书写，要注意每个用户的用户名、UID、宿主目录都不可以相同，其中密码栏可以留做空白或输入x号。一个范例文件user.txt内容如下： user001::600💯user:/home/user001:/bin/bash user002::601💯user:/home/user002:/bin/bash user003::602💯user:/home/user003:/bin/bash user004::603💯user:/home/user004:/bin/bash user005::604💯user:/home/user005:/bin/bash user006::605💯user:/home/user006:/bin/bash （2）以root身份执行命令 /usr/sbin/newusers，从刚创建的用户文件user.txt中导入数据，创建用户： newusers &lt; user.txt 然后可以执行命令 vipw 或 vi /etc/passwd 检查 /etc/passwd 文件是否已经出现这些用户的数据，并且用户的宿主目录是否已经创建。 （3）执行命令/usr/sbin/pwunconv。 将 /etc/shadow 产生的 shadow 密码解码，然后回写到 /etc/passwd 中，并将/etc/shadow的shadow密码栏删掉。这是为了方便下一步的密码转换工作，即先取消 shadow password 功能。 pwunconv （4）编辑每个用户的密码对照文件。 格式为： 用户名:密码 实例文件 passwd.txt 内容如下： user001:123456 user002:123456 user003:123456 user004:123456 user005:123456 user006:123456 （5）以 root 身份执行命令 /usr/sbin/chpasswd。 创建用户密码，chpasswd 会将经过 /usr/bin/passwd 命令编码过的密码写入 /etc/passwd 的密码栏。 chpasswd &lt; passwd.txt （6）确定密码经编码写入/etc/passwd的密码栏后。 执行命令 /usr/sbin/pwconv 将密码编码为 shadow password，并将结果写入 /etc/shadow。 pwconv 这样就完成了大量用户的创建了，之后您可以到/home下检查这些用户宿主目录的权限设置是否都正确，并登录验证用户密码是否正确。 Linux磁盘管理 Linux磁盘管理好坏直接关系到整个系统的性能问题。 Linux磁盘管理常用三个命令为df，du和fdisk。 df：列出文件系统的整体磁盘使用量 du：检查磁盘空间使用量 fdisk：用于磁盘分区 df df命令参数功能：检查文件系统的磁盘空间占用情况。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 语法： df [ -ahikHTm ] [目录或文件名] 选项与参数： -a：列出所有的文件系统，包括系统特有的/ proc等文件系统； -k：以KBytes的容量显示各文件系统； -m：以MBytes的容量显示各文件系统； -h：以人们较易阅读的GB，MB，KB等格式自行显示； -H：以M = 1000K取代M = 1024K的进位方式； -T：显示文件系统类型，并合并该分区的文件系统名称（例如ext3）也列出； -i：不用硬盘容量，而以inode的数量来显示 实例1 将系统内所有的文件系统列出来！ [根@ WWW 〜]＃DF 文件系统1K -块 用于推介使用％安装上 / dev的/ HDC2 9920624 3823112 5585444 41 ％/的/ dev / hdc3上 4956316 141376 4559108 4 ％/家 / dev的/ hdc1分区 101086 11126 84741 12 ％/引导 tmpfs 371332 0 371332 0 ％/ dev / shm 在Linux底下如果df没有加任何选项，那么替换重定向系统内部的所有（排除特殊内存内部的文件系统与swap）都以1 KB的容量来列出来！ 实例2 将容量结果以易读的容量格式显示出来 [根@ WWW 〜]＃DF - ħ 文件系统大小用于库存状况使用％安装上 / dev的/ HDC2 9.5克3.7G 5.4G 41 ％/的/ dev / hdc3上 4.8G 139M 4.4G 4 ％/家 / dev的/ hdc1分区 99M 11M 83M 12 ％/开机 tmpfs 363M 0 363M 0 ％/ dev / shm 实例3 将系统内的所有特殊文件格式及名称都列出来 [根@ WWW 〜]＃DF -在 文件系统类型1K -块 用于推介使用％安装上 / dev的/ HDC2 EXT3 9920624 3823112 5585444 41 ％/ PROC PROC 0 0 0 - / PROC sysfs中的sysfs 0 0 0 - / SYS devpts devpts 0 0 0 -的/ dev / PTS / dev的/位于hdc3 EXT3 4956316 141376 4559108 4 ％/家 / dev的/ hdc1分区EXT3 101086 11126 84741 12 ％/引导 tmpfs tmpfs 371332 0 371332 0 ％/ dev / shm 没有binfmt_misc 0 0 0 -的/ proc / SYS / FS / binfmt_misc sunrpc rpc_pipefs 0 0 0 -在/ var / lib中/ NFS / rpc_pipefs 实例4 将/ etc底下的可用的磁盘容量以易读的容量格式显示 [根@ WWW 〜]＃DF - ħ /等 文件系统大小用于库存状况使用％安装上 / dev的/ HDC2 9.5克3.7G 5.4G 41 ％/ du Linux du命令也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的，这里介绍Linux du命令。 语法： du [ -ahskm ]文件或目录名称 选项与参数： -a：列出所有的文件与目录容量，因为唯一仅统计目录底下的文件量而已。 -h：以人们较易读的容量格式（G / M）显示； -s：列出总数而已，而不列出每个各别的目录占用容量； -S：不包括子目录下的总计，与-s有点区别。 -k：以KBytes列出容量显示； -m：以MBytes列出容量显示； 实例1 只列出当前目录下的所有文件夹容量（包括隐藏文件夹）： [根@ WWW 〜]＃杜 8 ./ TEST4 &lt;==每个目录都会列出来8 ./ TEST2 ....中间省略.... 12 ./。gconfd &lt;==包括隐藏文件的目录220 。&lt;==这个目录（。）所占用的大量 直接输入du没有加任何选项时，则du会分析当前所在目录的文件与目录所占用的硬盘空间。 实例2 将文件的容量也列出来 [根@ WWW 〜]＃杜-一个 12 ./安装。日志。syslog &lt;==有文件的列表了8 ./。bash_logout 8 ./ test4 8 ./ test2 ....中间省略.... 12 ./。gconfd 220 。 实例3 检查根目录底下每个目录所占用的容量 [ root @ www〜]＃du - sm / * 7 /箱 6 /启动 .....中间省略.... 0 /进程 .....中间省略.... 1 /转 3859 / usr &lt;==系统初期最大就是他了啦！ 77 / var 通配符*来代表每个目录。 与df不一样的是，du这个命令实际上会直接到文件系统内去搜寻所有的文件数据。 fdisk fdisk是Linux的磁盘分区表操作工具。 语法： fdisk [ -l ]装置名称 选项与参数： -l：输出后面接的装置所有的分区内容。若仅有fdisk -l时，则系统将会把整个系统内部能够搜寻到的装置的分区均列出来。 实例1 列出所有分区信息 [ root @ AY120919111755c246621 tmp ]＃fdisk - l 磁盘/ dev / xvda ：21.5 GB ，21474836480字节 255磁头，63扇区/磁道，2610个柱面 单位=柱面16065 * 512 = 8225280字节 扇区大小（逻辑/物理）：512字节/ 512字节 I / O大小（最小/最佳）：512字节/ 512字节 磁盘标识符：0x00000000 设备引导启动结束块ID系统/ dev / xvda1 * 1 2550 20480000 83 Linux / dev / xvda2 2550 2611 490496 82 Linux swap / Solaris ​ 磁盘/ dev / xvdb ：21.5 GB ，21474836480字节 255磁头，63扇区/磁道，2610个柱面 单位=柱面16065 * 512 = 8225280字节 扇区大小（逻辑/物理）：512字节/ 512字节 I / O大小（最小/最佳）：512字节/ 512字节 磁盘标识符：0x56f40944 设备启动开始端块ID系统/ dev / xvdb2 1 2610 20964793 + 83 Linux 实例2 找出您系统中的根目录所在磁盘，并查阅该硬盘内的相关信息 [根@ WWW 〜]＃DF / &lt;==注意：重点在找出磁盘文件名而已文件系统1K -块 用于推介使用％安装上 / dev的/ HDC2 9920624 3823168 5585388 41 ％/ [根@ WWW 〜]＃的fdisk /开发/ HDC &lt;==仔细看，不要加上数字喔！的汽缸数为这盘被设置到5005有是没有错与那个，但是这是大于1024 ，并且可以在某些设置会导致问题与：1 ）软件，在系统启动时运行（ē 。摹。老版本的LILO ）2 ）引导和分区软件从其它操作系统（É 。克。，DOS FDISK ，OS / 2 FDISK ） 命令（m为帮助）：&lt;==等待你的输入！ 输入m后，就会看到底下这些命令介绍 命令（m为帮助）：m &lt;==输入m后，就会看到底下这些命令介绍Command action 切换可启动标志 b编辑bsd disklabel c切换dos兼容性标志 d 删除分区 &lt;==删除一个分区 l列出已知的分区类型 m 打印此菜单 n添加一个新分区 &lt;==添加一个分区 o创建一个新的空DOS分区表 p 打印分区表 &lt;==在屏幕上显示分区表 q退出而不保存更改&lt;==不保存 离开fdisk程序 创建一个新的空Sun disklabel t更改分区的系统ID u更改显示/输入单位 v验证分区表 w将表写入磁盘并退出&lt;==将刚刚的动作写入分割表 x额外功能（仅限专家） 离开fdisk时点击q，那么所有的动作都不会生效！相反的，按下w就是动作生效的意思。 命令（m为帮助）：p &lt;==这里可以输出当前磁盘的状态 磁盘/ dev / hdc ：41.1 GB ，41174138880字节 &lt;==这个磁盘的文件名与容量255个磁头，63个扇区/磁道，5005个磁柱 &lt;==磁头，增大与磁柱大小单位= 16065 * 512磁柱= 8225280字节&lt;==每个磁柱的大小 ​ 设备引导启动结束块ID系统/ dev / hdc1 * 1 13 104391 83 Linux / dev / hdc2 14 1288 10241437 + 83 Linux / dev / hdc3 1289 1925 5116702 + 83 Linux / dev / hdc4 1926 5005 24740100 5扩展/ dev / hdc5 1926 2052 1020096 82 Linux swap / Solaris ＃装置文件名启动区否开始磁柱结束磁柱1K大小容量磁盘分区槽内的系统 命令（m表示帮助）：q 想要不储存离开吗？点击q就对了！不要随便按w啊！ 使用p可以列出目前这颗磁盘的分割表信息，这个信息的上半部在显示整体磁盘的状态。 磁盘格式化 磁盘分割完成后自然就是要进行文件系统的格式化，格式化的命令非常的简单，使用mkfs（使文件系统）命令。 语法： mkfs [ -t文件系统格式]装置文件名 选项与参数： -t：可以接文件系统格式，例如ext3，ext2，vfat等（系统有支持才会生效） 实例1 查看mkfs支持的文件格式 [ root @ www〜] ＃mkfs [标签] [标签] mkfs mkfs 。cramfs mkfs 。ext2 mkfs 。ext3 mkfs 。msdos mkfs 。胖子 按下两个[tab]，会发现mkfs支持的文件格式如上所示。 实例2 将分区/ dev / hdc6（可指定你自己的分区）格式化为ext3文件系统： [ root @ www〜]＃mkfs - t ext3 / dev / hdc6 mke2fs 1.39 （29 - May - 2006 ）文件系统标签= &lt;==这里指的是分割槽的名称（label ） 操作系统类型：Linux块大小= 4096 （log = 2 ）&lt;==块的大小配置为4K片段大小= 4096 （日志= 2 ）251392个i节点，502023块 &lt;==由此配置决定的索引节点/块数量 25101块（5.00 ％）保留用于该超级用户 首先数据块= 0最大文件系统块= 515899392个16块组 32768个每块组，32768个每片段组15712个每索引节点组的超级块存储在块的备份：32768 ，98304 ，163840 ，229376 ，294912 ​ 编写inode表：完成创建日志（8192个块）：完成&lt;==有日志记录编写超级块和文件系统记帐信息：完成 该文件系统将每34坐骑或180天自动检查一次，以先到者为准。使用tune2fs - ç或-我要重写。＃这样就创建了我们所需要的Ext3文件系统了！简单明了！ 磁盘检验 fsck（文件系统检查）用来检查和维护二者的文件系统。 若系统掉电或磁盘发生问题，可利用fsck命令对文件系统进行检查。 语法： fsck [ -t文件系统] [ -ACay ]装置名称 选项与参数： -t：给定档案系统的型式，若在/ etc / fstab中已有定义或kernel本身已支持的则不需加上此参数 -s：依序一个一个地执行fsck的指令来检查 -A：对/ etc / fstab中所有列出来的分区（partition）做检查 -C：显示完整的检查进度 -d：打印出e2fsck的调试结果 -p：同时有-A条件时，同时有多个fsck的检查一起执行 -R：同时有-A条件时，省略/不检查 -V：详细显示模式 -a：如果检查有错则自动修复 -r：如果检查有错则由使用者回答是否恢复 -y：选项指定检测每个文件是自动输入yes，在不确定那些是不正常的时候，可以执行＃fsck -y全部检查修复。 实例1 查看系统有多少文件系统支持的fsck命令： [ root @ www〜] ＃fsck [标签] [标签] fsck fsck 。cramfs fsck 。ext2 fsck 。ext3 fsck 。msdos fsck 。胖子 实例2 强制检测/ dev / hdc6分区： [ root @ www〜]＃fsck - C - f - t ext3 / dev / hdc6 fsck的1.39 （29 -月- 2006年） 用e2fsck 1.39 （29 -月- 2006年）通1 ：检查索引节点，块，和尺寸 通行证2 ：检查目录结构 通行证3 ：检查目录的连接 通4 ：检查引用计数 传递5 ：检查组 摘要信息 vbird_logical ：11 /二十五万一千九百六十八文件（9.1 ％非-连续的），36926 /一百万四千零四十六块 如果没有加上-f的选项，则由于该文件系统不曾出现问题，检查的经过非常快速！若加上-f强制检查，只会有一项的显示过程。 磁盘挂载与卸除 Linux的磁盘挂载使用mount命令，卸载使用umount命令。 磁盘挂载语法： mount [ -t文件系统] [ -L标签名] [ -o其他选项] [ -n ]装置文件名挂载点 实例1 用最小的方式，将刚刚创建的/ dev / hdc6挂载到/ mnt / hdc6上面！ [根@ WWW 〜]＃MKDIR / MNT / hdc6 [根@ WWW 〜]＃安装/ dev的/ hdc6 / MNT / hdc6 [根@ WWW 〜]＃DF 文件系统1K -块 用于推介使用％安装上 .... 。中间省略..... / dev / hdc6 1976312 42072 1833836 3 ％/ mnt / hdc6 磁盘卸载命令umount语法： 卸除[ - FN ]装置文件名或挂载点 选项与参数： -f：强制卸除！可用在类似网络文件系统（NFS）无法读取到的情况下； -n：不升级/ etc / mtab情况下卸除。 卸载/ dev / hdc6 [根@ WWW 〜]＃卸除/ dev的/ hdc6 vi/vim 所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。但是目前我们使用比较多的是 vim 编辑器。vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。 什么是 vim？ Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说， vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。 vim 则可以说是程序开发者的一项很好用的工具。 连 vim 的官方网站 (http://www.vim.org) 自己也说 vim 是一个程序开发工具而不是文字处理软件。 vim 键盘图： vi/vim 的使用 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 这三种模式的作用分别是： 命令模式： 用户刚刚启动 vi/vim，便进入了命令模式。 此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。 以下是常用的几个命令： i 切换到输入模式，以输入字符。 x 删除当前光标所在处的字符。 : 切换到底线命令模式，以在最底一行输入命令。 若想要编辑文本：启动Vim，进入了命令模式，按下i，切换到输入模式。 命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。 输入模式 在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键： 字符按键以及Shift组合，输入字符 ENTER，回车键，换行 BACK SPACE，退格键，删除光标前一个字符 DEL，删除键，删除光标后一个字符 方向键，在文本中移动光标 HOME/END，移动光标到行首/行尾 Page Up/Page Down，上/下翻页 Insert，切换光标为输入/替换模式，光标将变成竖线/下划线 ESC，退出输入模式，切换到命令模式 底线命令模式 在命令模式下按下:（英文冒号）就进入了底线命令模式。 底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。 在底线命令模式中，基本的命令有（已经省略了冒号）： q 退出程序 w 保存文件 按ESC键可随时退出底线命令模式。 简单的说，我们可以将这三个模式想成底下的图标来表示 vi/vim 使用实例 使用 vi/vim 进入一般模式 如果你想要使用 vi 来建立一个名为 runoob.txt 的文件时，你可以这样做： $ vim runoob.txt 直接输入 vi 文件名 就能够进入 vi 的一般模式了。请注意，记得 vi 后面一定要加文件名，不管该文件存在与否！ - 按下 i 进入输入模式(也称为编辑模式)，开始编辑文字 在一般模式之中，只要按下 i, o, a 等字符就可以进入输入模式了！ 在编辑模式当中，你可以发现在左下角状态栏中会出现 –INSERT- 的字样，那就是可以输入任意字符的提示。 这个时候，键盘上除了 Esc 这个按键之外，其他的按键都可以视作为一般的输入按钮了，所以你可以进行任何的编辑。 - 按下 ESC 按钮回到一般模式 好了，假设我已经按照上面的样式给他编辑完毕了，那么应该要如何退出呢？是的！没错！就是给他按下 Esc 这个按钮即可！马上你就会发现画面左下角的 – INSERT – 不见了！ 在一般模式中按下 :wq 储存后离开 vi OK，我们要存档了，存盘并离开的指令很简单，输入 :wq 即可保存离开！ - Linux vi_vim _ 菜鸟教程.pdf Linux yum命令 yum（Yellow dog Updater，Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端连接管理器。 基于RPM包管理，能够从指定的服务器自动下载RPM包和安装，可以自动处理相对关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载，安装。 yum提供了查找，安装，删除某人，多个甚至全部完全的命令，而且命令简洁而又好记。 yum语法 yum [选项] [命令] [包...] 选项：任选，选项包括-h（帮助），-y（当安装过程提示选择全部为“ yes”），-q（不显示安装的过程）等等。 命令：要进行的操作。 package：安装的包名。 yum常用命令 1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install &lt;程序包名称&gt; 4.仅更新指定的软件命令：yum update &lt;程序包名称&gt; 5.列出所有可安装的软件清单命令：百胜名单 6.删除删除命令：yum remove &lt;package_name&gt; 7.查找普通命令：yum search &lt;关键字&gt; 8.清除缓存命令： 百胜清洁包：清除缓存目录下的双重 yum clean headers：清除缓存目录下的headers yum clean oldheaders：清除缓存目录下旧的标题 yum clean，yum clean all（= yum clean package; yum clean oldheaders）：清除缓存目录下的副本及旧的标题 实例1 安装pam-devel [根@ WWW 〜]＃百胜安装PAM - devel的 设置了安装过程解析包安装参数 解析依赖性&lt;==先检查软件的属性相依问题- &gt;运行的事务检查 ---&gt;封装PAM - devel的。i386 0 ：0.99 。6.2 - 4.el5集被更新 处理相关：PAM = 0.99 。6.2 - 4.el5的包：PAM - devel的 运行交易检查 ---&gt;包装PAM 。i386 0 ：0.99 。6.2 - 4.el5集进行更新 文件列表。xml 。gz 100 ％| ======================== | 1.6 MB 00 ：05 的文件列表。xml 。gz 100 ％| ======================== | 138 kB 00 : 00- &gt;完成的依赖性解析……（省略） 实例2 可拆卸pam-devel [根@ WWW 〜]＃荫删除PAM - devel的 设置了删除过程解决依赖&lt;==同样的，先解决属性相依的问题- &gt;运行的事务检查 ---&gt;封装PAM - devel的。i386 0 ：0.99 。6.2 - 4.el5集被擦除 成品依赖决议 解决依赖性 ================================================== =软件包Arch版本库大小 ================================================== =删除： pam - devel i386 0.99 。6.2 - 4.el5 安装 495 ķ 交易摘要================================ =============================安装0包（小号）更新0包（小号）中取出1包（小号）&lt;==还好，并没有属性相依的问题，单纯可移除一个软件 是该行[ ÿ / Ñ ]：Ÿ 下载软件包：运行rpm_check_debug 运行交易测试已完成的交易测试的交易测试成功运行交易擦除：PAM - devel的 ################### ###### [1/1] 删除：pam - devel 。i386 0 ：0.99 。6.2 - 4.el5完成！ 实例3 利用yum的功能，发现以pam为开头的软件名称有什么？ [ root @ www〜] ＃yum list pam *已安装的软件包 pam 。i386 0.99 。6.2 - 3.27 。已安装el5 pam_ccreds 。I386 3 - 5 安装 pam_krb5 。i386 2.2 。14 - 1 安装 pam_passwdqc 。i386 1.0 。2 - 1.2 。2 安装 pam_pkcs11 。i386 0.5 。3 - 23 安装 pam_smb 。i386 1.1 。7 - 7.2 。1个 已安装的 可用软件包&lt;==底下则是『可升级』的或『未安装』的 pam 。i386 0.99 。6.2 - 4.el5基 PAM - devel的。i386 0.99 。6.2 - 4.el5基 pam_krb5的。i386 2.2 。14 - 10基 国内yum源 网易（163）yum源是国内最好的yum源之一，无论是速度还是软件版本，都非常的不错。 将yum源设置为163 yum，可以提高扩展安装和更新的速度，同时避免一些常见软件版本无法找到。 安装步骤 首先备份/etc/yum.repos.d/CentOS-Base.repo mv / etc / yum 。回购。d / CentOS的-基础。回购/ etc / yum 。回购。d / CentOS的-基础。回购。后备 下载对应版本repo文件，加入/etc/yum.repos.d/（操作前请做好相应备份） CentOS5：http：//mirrors.163.com/.help/CentOS5-Base-163.repo CentOS6：http：//mirrors.163.com/.help/CentOS6-Base-163.repo CentOS7：http：//mirrors.163.com/.help/CentOS7-Base-163.repo wget的HTTP ：//mirrors.163.com/.help/CentOS6-Base-163.repo MV CentOS6 -基地- 163.repo的CentOS -基地。回购 运行以下命令生成缓存 百胜清理所有 yum makecache 除了网易之外，国内还有其他不错的yum源，某些中科大和搜狐。 中科大的yum源，安装方法查看：https 😕/lug.ustc.edu.cn/wiki/mirrors/help/centos sohu的yum源安装方法查看：http : //mirrors.sohu.com/help/centos.html Linux apt 命令 apt（Advanced Packaging Tool）是一个在 Debian 和 Ubuntu 中的 Shell 前端软件包管理器。 apt 命令提供了查找、安装、升级、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 apt 命令执行需要超级管理员权限(root)。 apt 语法 apt [options] [command] [package ...] options：可选，选项包括 -h（帮助），-y（当安装过程提示选择全部为&quot;yes&quot;），-q（不显示安装的过程）等等。 command：要进行的操作。 package：安装的包名。 apt 常用命令 列出所有可更新的软件清单命令：sudo apt update 升级软件包：sudo apt upgrade 列出可更新的软件包及版本信息：apt list --upgradeable 升级软件包，升级前先删除需要更新软件包：sudo apt full-upgrade 安装指定的软件命令：sudo apt install &lt;package_name&gt; 安装多个软件包：sudo apt install &lt;package_1&gt; &lt;package_2&gt; &lt;package_3&gt; 更新指定的软件命令：sudo apt update &lt;package_name&gt; 显示软件包具体信息,例如：版本号，安装大小，依赖关系等等：sudo apt show &lt;package_name&gt; 删除软件包命令：sudo apt remove &lt;package_name&gt; 清理不再使用的依赖和库文件: sudo apt autoremove 移除软件包及配置文件: sudo apt purge &lt;package_name&gt; 查找软件包命令： sudo apt search 列出所有已安装的包：apt list --installed 列出所有已安装的包的版本信息：apt list --all-versions 实例 查看一些可更新的包： sudo apt update 升级安装包： sudo apt upgrade 在以上交互式输入字母 Y 即可开始升级。 可以将以下两个命令组合起来，一键升级： sudo apt update &amp;&amp; sudo apt upgrade -y 安装 mplayer 包： sudo apt install mplayer 如过不太记得完整的包名，我们可以只输入前半部分的包名，然后按下 Tab 键，会列出相关的包名： 以上实例我们输入来 reds，然后按下 Tab 键，输出来四个相关的包。 如果我们想安装一个软件包，但如果软件包已经存在，则不要升级它，可以使用 –no-upgrade 选项: sudo apt install &lt;package_name&gt; --no-upgrade 安装 mplayer 如果存在则不要升级： sudo apt install mplayer --no-upgrade 如果只想升级，不要安装可以使用 --only-upgrade 参数： sudo apt install &lt;package_name&gt; --only-upgrade 只升级 mplayer，如果不存在就不要安装它： sudo apt install mplayer --only-upgrade 如果需要设置指定版本，语法格式如下： sudo apt install &lt;package_name&gt;=&lt;version_number&gt; package_name 为包名，version_number 为版本号。 移除包可以使用 remove 命令： sudo apt remove mplayer 查找名为 libimobile 的相关包： apt search libimobile 查看 pinta 包的相关信息： apt show pinta 列出可更新的软件包： apt list --upgradeable 清理不再使用的依赖和库文件： sudo apt autoremove 在以上交互式输入字母 Y 即可开始清理。","link":"/2021/03/01/Draft/2021/Linux/"},{"title":"NO GAME NO LIFE","text":"游戏人生 🎈🎈🎈😎 吉他扒谱--数绘--英语--摄影 😎🎈🎈🎈 目前学习路线内容 优先等级 吉他乐理，扒谱 😎😎😎😎😎 数绘 😎😎😎😎😎 英语 😎😎😎😎 摄影 😎😎😎😎 运动 😎😎😎😎 自媒体 😎😎😎😎 平面设计 😎😎😎😎 目前学习路线内容 优先等级 音乐 吉他，钢琴，尤克里里，拇指琴，箫，口琴，演唱，词曲 绘画 数绘 设计 平面 运动 篮球，网球 语言 英语，日语 摄影 人像，景色 自媒体 剪辑，特效 修图 办公效率工具高端操作 OFFICE","link":"/2021/02/25/Draft/2021/NO%20GAME%20NO%20LIFE/"},{"title":"Software Engineer","text":"高级全栈软件工程师养成记 GIS研发工程师，全栈Java开发工程师 🎈🎈🎈😎 TODO 😎🎈🎈🎈 技术 设计模式 Redis 算法 目题(Java基础深入，其他面试题) Springcloud 目前学习路线内容 优先等级 设计模式 😎😎😎😎😎 Redis 😎😎😎😎😎 RabbitMQ 😎😎😎😎 数据库优化 😎😎😎😎 网络 😎😎😎😎 算法 😎😎😎😎 SpringCloud 😎😎😎😎 GIS 😎😎😎😎 Linux 😎😎😎 JVM 😎😎 Dubbo 😎😎 ElasticSearch 😎😎 Node.js 😎😎 ES6 😎😎 Docker 😎😎 分布式 😎😎 高并发 😎😎 Python 😎😎 学习内容 项目名称 详细内容 进度记录 软件工程 （微服务，分布式，高并发，多线程，性能调优，缓存，消息，搜索）其他应用服务器了解。 项目 个人博客 个人博客完成 前端 VUE，Bootstrap，JS，Html，JQuery，Ajax，Thymeleaf,Axios 后端 基础源码深入，代码多写，JVM 框架 框架扎实深入，SpringCloud尝试了解 数据库 Oracle基础使用，MySQL,索引，触发器，存储过程，优化（文件化，从设计到e-r图到创建），Redis,MangoDB 算法 算法机试题 设计模式 Mybatis（ 1、Builder模式5、组合模式9、迭代器模式2、工厂模式3、单例模式4、代理6、模板方法模式7、适配器模式8、装饰者模式）Spring（1.简单工厂2.工厂方法3.单例模式4.适配器模式5.装饰器模式6.代理模式7.观察者模式8.策略模式9.模版方法模式） 网络 协议 系统 Linux常用命令 工具 Git，SVN高级操作，Pageoffice 实操 机试题 中间件 RabbitMQ ElasticSearch Docker Dubbo 测试 单元测试 部署 服务器Jboss 维护 其他 Python【熟悉基础】安卓【有时间了解】 公司 springboot+mybatis+gis+vue+redis+框架+基础前后端数据交互+springcloud【有时间了解】","link":"/2021/02/25/Draft/2021/Software%20Engineer/"},{"title":"魑魅先生 | 程序员英语","text":"新学四问 WHY【与前代优化了什么，弥补了什么空白】学习交流娱乐 WHAT【框架，思维导图，主题框架】程序员专业词汇，日常英语 HOW【如何记忆，学习资源】:Google日常化、英文文档日常化、词汇记忆 LEVEL【不是每个都学精】熟练使用 日常英语单词图示 电影、书籍、技术英文文档 通俗易懂的英语语法 针对计算机英语 口语练习记录 英文文档记录 JAVA Spring全家桶 计算机单词","link":"/2021/02/25/Draft/2021/%E6%AF%8F%E6%97%A5%E8%8B%B1%E8%AF%AD/"},{"title":"魑魅先生 | 音乐","text":"乐理、吉他、尤克里里、钢琴、拇指琴 乐理指南 吉他指南","link":"/2021/02/25/Draft/2021/%E9%9F%B3%E4%B9%90/"},{"title":"SpringCloud","text":"新学四问 WHY【与前代优化了什么，弥补了什么空白】微服务，主流 WHAT【框架，思维导图，主题框架】eureka注册中心，Gateway网关，Ribbon负载均衡，Feign服务调用，Hystrix熔断器等，springcloudalibaba HOW【如何记忆，学习资源】:bilibili，官网 LEVEL【不是每个都学精】当前阶段熟练运用 Spring Cloud 【Hoxton】 简介 1. 系统架构演变 随着互联网的发展，网站应用的规模不断扩大。需求的激增，带来的是技术上的压力。系统架构也因此也不断的演 进、升级、迭代。从单一应用，到垂直拆分，到分布式服务，到SOA，以及现在火热的微服务架构，还有在Google 带领下来势汹涌的Service Mesh。 1.1. 集中式架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。 优点： 系统开发速度快 维护成本低 适用于并发要求较低的系统 缺点： 代码耦合度高，后期维护困难 无法针对不同模块进行针对性优化 无法水平扩展 单点容错率低，并发能力差 1.2. 垂直拆分 ​ 当访问量逐渐增大，单一应用无法满足需求，此时为了应对更高的并发和业务需求，我们根据业务功能对系统进行拆 分： 优点： 系统拆分实现了流量分担，解决了并发问题 可以针对不同模块进行优化 方便水平扩展，负载均衡，容错率提高 缺点： 系统间相互独立，会有很多重复开发工作，影响开发效率 1.3. 分布式服务 ​ 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。 优点： 将基础服务进行了抽取，系统间相互调用，提高了代码复用和开发效率 缺点： 系统间耦合度变高，调用关系错综复杂，难以维护 1.4. 面向服务架构（SOA） ​ SOA（Service Oriented Architecture）面向服务的架构：它是一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。一个服务 通常以独立的形式存在与操作系统进程中。各个服务之间 通过网络调用。SOA结构图： ESB（企业服务总线），简单 来说 ESB 就是一根管道，用来连接各个服务节点。为了集 成不同系统，不同协议的服务，ESB 做了消息的转化解释和路由工作，让不同的服务互联互通。 SOA缺点：每个供应商提供的ESB产品有偏差，自身实现较为复杂；应用服务粒度较大，ESB集成整合所有服务和协 议、数据转换使得运维、测试部署困难。所有服务都通过一个通路通信，直接降低了通信速度。 1.5. 微服务架构 ​ 微服务架构是使用一套小服务来开发单个应用的方式或途径，每个服务基于单一业务能力构建，运行在自己的进程中，并使用轻量级机制通信，通常是HTTP API，并能够通过自动化部署机制来独立部署。这些服务可以使用不同的编程语言实现，以及不同数据存储技术，并保持最低限度的集中式管理。微服务结构图： API Gateway网关是一个服务器，是系统的唯一入口。为每个客户端提供一个定制的API。API网关核心是，所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有的非业务功能。如它还可以具有其它职责，如身份验证、监控、负载均衡、缓存、请求分片与管理、静态响应处理。通常，网关提供RESTful/HTTP的方式访问服务。而服务端通过服务注册中心进行服务注册和管理。 微服务的特点： 单一职责：微服务中每一个服务都对应唯一的业务能力，做到单一职责 微：微服务的服务拆分粒度很小，例如一个用户管理就可以作为一个服务。每个服务虽小，但“五脏俱全”。 面向服务：面向服务是说每个服务都要对外暴露Rest风格服务接口API。并不关心服务的技术实现，做到与平台和语言无关，也不限定用什么技术实现，只要提供Rest的接口即可。 自治：自治是说服务间互相独立，互不干扰 团队独立：每个服务都是一个独立的开发团队，人数不能过多。 技术独立：因为是面向服务，提供Rest接口，使用什么技术没有别人干涉 前后端分离：采用前后端分离开发，提供统一Rest接口，后端不用再为PC、移动端开发不同接口 数据库分离：每个服务都使用自己的数据源 部署独立：服务间虽然有调用，但要做到服务重启不影响其它服务。有利于持续集成和持续交付。每个服务都是独立的组件，可复用，可替换，降低耦合，易维护 微服务架构与SOA都是对系统进行拆分；微服务架构基于SOA思想，可以把微服务当做去除了ESB的SOA。ESB是SOA架构中的中心总线，设计图形应该是星形的，而微服务是去中心化的分布式软件架构。两者比较类似，但其实也有一些差别： 2. 服务调用方式 2.1. RPC和HTTP 无论是微服务还是SOA，都面临着服务间的远程调用。那么服务间的远程调用方式有哪些呢？ 常见的远程调用方式有以下2种： RPC：Remote Produce Call远程过程调用，RPC基于Socket，工作在会话层。自定义数据格式，速度快，效率高。早期的webservice，现在热门的dubbo，都是RPC的典型代表 Http：http其实是一种网络传输协议，基于TCP，工作在应用层，规定了数据传输的格式。现在客户端浏览器与服务端通信基本都是采用Http协议，也可以用来进行远程服务调用。缺点是消息封装臃肿，优势是对服务的提供和调用方没有任何技术限定，自由灵活，更符合微服务理念。 现在热门的Rest风格，就可以通过http协议来实现。 区别：RPC的机制是根据语言的API（language API）来定义的，而不是根据基于网络的应用来定义的。 如果你们公司全部采用Java技术栈，那么使用Dubbo作为微服务架构是一个不错的选择。 相反，如果公司的技术栈多样化，而且你更青睐Spring家族，那么Spring Cloud搭建微服务是不二之选。在我们的项目中，会选择Spring Cloud套件，因此会使用Http方式来实现服务间调用。 2.2. Http客户端工具 既然微服务选择了Http，那么我们就需要考虑自己来实现对请求和响应的处理。不过开源世界已经有很多的http客户端工具，能够帮助我们做这些事情，例如： HttpClient OKHttp URLConnection 不过这些不同的客户端，API各不相同。而Spring也有对http的客户端进行封装，提供了工具类叫RestTemplate。 2.3. Spring的RestTemplate Spring提供了一个RestTemplate模板工具类，对基于Http的客户端进行了封装，并且实现了对象与json的序列化和 反序列化，非常方便。RestTemplate并没有限定Http的客户端类型，而是进行了抽象，目前常用的3种都有支持： HttpClient OkHttp JDK原生的URLConnection（默认的） 3.Springcloud综述 版本命名：伦敦地铁站字母顺序 实验环境 停更、升级、替换 父工程创建 创建工程前准备 约定》配置》编码 编码UTF-8全套 作用 注解生效激活 作用 JAVA编译版本 文件过滤 作用，关闭对应文件idea可见 Eureka【大楼老板】 Spring Cloud [Finchley] ​ 一系列框架有序集合，封装后屏蔽了复杂的配置和实现原理。Spring Cloud为开发人员提供了工具，以快速构建分布式系统中的一些常见模式（例如，配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁，领导选举，分布式会话，群集状态）。分布式系统的协调导致样板式样，并且使用Spring Cloud开发人员可以快速站起来实现这些样板的服务和应用程序。它们将在任何分布式环境中都能很好地工作，包括开发人员自己的笔记本电脑，裸机数据中心以及诸如Cloud Foundry之类的托管平台。 1.系统架构演变 graph LR; 1[集中式架构] --> 2[垂直拆分] 2 --> 3[分布式服务] 3 --> 4[SOA面向服务架构] 4 --> 5[微服务架构] 微服务架构 一套使用小服务或者单一业务来开发单个应用的方式或途径。 微服务架构特点： 单一职责 服务粒度小 面向服务（对外暴露REST api） 服务之间相互独立 与使用ESB的SOA架构的区别： 微服务架构没有使用ESB，有服务治理注册中心；业务粒度小。 2.服务调用方式 RPC：基于socket，速度快，效率高；webservice、dubbo HTTP：基于TCP，封装比较臃肿；对服务和调用方没有任何技术、语言的限定，自由灵活；RESTful，Spring Cloud 一般情况下有如下三种http客户端工具类包都可以方便的进行http服务调用： httpClient okHttp JDK原生URLConnection spring 提供了RestTemplate的工具类对上述的3种http客户端工具类进行了封装，可在spring项目中使用RestTemplate进行服务调用。 小结： 12345678910111213141516@RunWith(SpringRunner.class)@SpringBootTestpublic class RestTemplateTest { @Autowired private RestTemplate restTemplate; @Test public void test(){ String url = &quot;http://localhost/user/8&quot;; //restTemplate可以对json格式字符串进行反序列化 User user = restTemplate.getForObject(url, User.class); System.out.println(user); }} 3.SpringCloud概述 整合的组件可以有很多组件；常见的组件有：eureka注册中心，Gateway网关，Ribbon负载均衡，Feign服务调用，Hystrix熔断器。在有需要的时候项目添加对于的启动器依赖即可。 版本特征：以英文单词命名（伦敦地铁站名） 4.创建微服务工程 父工程springcloud：添加spring boot父坐标和管理其它组件的依赖 用户服务工程user-service：整合mybatis查询数据库中用户数据；提供查询用户服务 服务消费工程consumer-demo：利用查询用户服务获取用户数据并输出到浏览器 5.搭建配置Service工程 添加启动器依赖（web、通用Mapper）； 创建启动引导类和配置文件； 修改配置文件中的参数； 编写测试代码（UserMapper，UserService，UserController）； 测试 6.搭建配置Client工程 添加启动器依赖； 创建启动引导类（注册RestTemplate）和配置文件； 编写测试代码（ConsumerController中使用restTemplate访问服务获取数据） 测试 7.问题 服务管理 如何自动注册和发现 如何实现状态监管 如何实现动态路由 服务如何实现负载均衡 服务如何解决容灾问题 服务如何实现统一配置 上述问题通过springcloud各种组件解决 8.Eureka Eureka的主要功能是进行服务管理，定期检查服务状态，返回服务地址列表。 8.1Eureka-server Eureka是服务注册中心，只做服务注册；自身并不提供服务也不消费服务。可以搭建web工程使用Eureka，可以使用Spring Boot方式搭建。 搭建步骤： 创建工程； 添加启动器依赖； 编写启动引导类（添加Eureka的服务注解）和配置文件； 修改配置文件（端口，应用名称...）； 启动测试 8.2服务注册与发现 服务注册：在服务提供工程user-service上添加Eureka客户端依赖；自动将服务注册到EurekaServer服务地址列表。 添加依赖； 改造启动引导类；添加开启Eureka客户端发现的注解； 修改配置文件；设置Eureka 服务地址 服务发现：在服务消费工程consumer-demo上添加Eureka客户端依赖；可以使用工具类根据服务名称获取对应的服务地址列表。 添加依赖； 改造启动引导类；添加开启Eureka客户端发现的注解； 修改配置文件；设置Eureka 服务地址； 改造处理器类Controller，可以使用工具类DiscoveryClient根据服务名称获取对应服务地址列表。 8.3高可用配置 ​ 将Eureka Server作为一个服务注册到其它Eureka Server，这样多个Eureka Server之间就能够互相发现对方，同步服务，实现Eureka Server集群。 9.负载均衡Ribbon Ribbon提供了轮询、随机两种负载均衡算法（默认是轮询）可以实现从地址列表中使用负载均衡算法获取地址进行服务调用。 9.1Ribbon应用 在实例化RestTemplate的时候使用@LoadBalanced，服务地址直接可以使用服务名。 10.熔断器Hystrix（豪猪） Hystrix是一个延迟和容错库，用于隔离访问远程服务，防止出现级联失败。 10.1线程隔离&amp;服务降级 Hystrix解决雪崩效应： 线程隔离：用户请求不直接访问服务，而是使用线程池中空闲的线程访问服务，加速失败判断时间。 服务降级：及时返回服务调用失败的结果，让线程不因为等待服务而阻塞。 小结： 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 开启熔断 降级逻辑 12345678910111213141516171819202122232425262728293031323334353637@RestController@RequestMapping(&quot;/consumer&quot;)@Slf4j@DefaultProperties(defaultFallback = &quot;defaultFallback&quot;)public class ConsumerController { @Autowired private RestTemplate restTemplate; @Autowired private DiscoveryClient discoveryClient; @GetMapping(&quot;/{id}&quot;) //@HystrixCommand(fallbackMethod = &quot;queryByIdFallback&quot;) @HystrixCommand public String queryById(@PathVariable Long id){ /*String url = &quot;http://localhost:9091/user/&quot;+id; //获取eureka中注册的user-service的实例 List&lt;ServiceInstance&gt; serviceInstances = discoveryClient.getInstances(&quot;user-service&quot;); ServiceInstance serviceInstance = serviceInstances.get(0); url = &quot;http://&quot; + serviceInstance.getHost() + &quot;:&quot; + serviceInstance.getPort() + &quot;/user/&quot; + id;*/ String url = &quot;http://user-service/user/&quot; + id; return restTemplate.getForObject(url, String.class); } public String queryByIdFallback(Long id){ log.error(&quot;查询用户信息失败。id：{}&quot;, id); return &quot;对不起，网络太拥挤了！&quot;; } public String defaultFallback(){ return &quot;默认提示：对不起，网络太拥挤了！&quot;; }} 修改超时配置 1234567hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 2000 10.2服务熔断 ​ 在服务熔断中，使用的熔断器，也叫断路器，其英文单词为：Circuit Breaker ​ 熔断机制与家里使用的电路熔断原理类似；当如果电路发生短路的时候能立刻熔断电路，避免发生灾难。在分布式系 统中应用服务熔断后；服务调用方可以自己进行判断哪些服务反应慢或存在大量超时，可以针对这些服务进行主动熔 断，防止整个系统被拖垮。 ​ Hystrix的服务熔断机制，可以实现弹性容错；当服务请求情况好转之后，可以自动重连。通过断路的方式，将后续 请求直接拒绝，一段时间（默认5秒）之后允许部分请求通过，如果调用成功则回到断路器关闭状态，否则继续打 开，拒绝请求的服务。 Hystrix的熔断状态机模型： 状态机有3个状态： Closed：关闭状态（断路器关闭），所有请求都正常访问。 Open：打开状态（断路器打开），所有请求都会被降级。Hystrix会对请求情况计数，当一定时间内失败请求百 分比达到阈值，则触发熔断，断路器会完全打开。默认失败比例的阈值是50%，请求次数最少不低于20次。 Half Open：半开状态，不是永久的，断路器打开后会进入休眠时间（默认是5S）。随后断路器会自动进入半开 状态。此时会释放部分请求通过，若这些请求都是健康的，则会关闭断路器，否则继续保持打开，再次进行休 眠计时。 11.服务消费者（Feign） 11.1简介 ​ Feign是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用Feign 注解和JAX-RS注解。Feign支持可插拔的编码器和解码器。Feign默认集成了Ribbon，并和Eureka结合，默认实现了负载均衡的效果。 简而言之： Feign 采用的是基于接口的注解 Feign 整合了ribbon，具有负载均衡的能力 整合了Hystrix，具有熔断的能力 12.断路器（Hystrix） ​ 在微服务架构中，根据业务来拆分成一个个的服务，服务与服务之间可以相互调用（RPC），在Spring Cloud可以用RestTemplate+Ribbon和Feign来调用。为了保证其高可用，单个服务通常会集群部署。由于网络原因或者自身的原因，服务并不能保证100%可用，如果单个服务出现问题，调用这个服务就会出现线程阻塞，此时若有大量的请求涌入，Servlet容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的“雪崩”效应。 为了解决这个问题，业界提出了断路器模型。 12.1断路器简介 ​ Netflix has created a library called Hystrix that implements the circuit breaker pattern. In a microservice architecture it is common to have multiple layers of service calls.Netflix. 开源了Hystrix组件，实现了断路器模式，SpringCloud对这一组件进行了整合。 在微服务架构中，一个请求需要调用多个服务是非常常见的。较底层的服务如果出现故障，会导致连锁故障。当对特定的服务的调用的不可用达到一个阀值（Hystric 是5秒20次） 断路器将会被打开。断路打开后，可用避免连锁故障，fallback方法可以直接返回一个固定值。 13.路由网关(zuul) ​ Zuul的主要功能是路由转发和过滤器。路由功能是微服务的一部分，比如／api/user转发到到user服务，/api/shop转发到到shop服务。zuul默认和Ribbon结合实现了负载均衡的功能。 zuul有以下功能： Authentication Insights Stress Testing Canary Testing Dynamic Routing Service Migration Load Shedding Security Static Response handling Active/Active traffic management 服务过滤 filterType：返回一个字符串代表过滤器的类型，在zuul中定义了四种不同生命周期的过滤器类型，具体如下： pre：路由之前 routing：路由之时 post： 路由之后 error：发送错误调用 filterOrder：过滤的顺序 shouldFilter：这里可以写逻辑判断，是否要过滤，本文true,永远过滤。 run：过滤器的具体逻辑。可用很复杂，包括查sql，nosql去判断该请求到底有没有权限访问。 14.分布式配置中心(Spring Cloud Config) ​ 在分布式系统中，由于服务数量巨多，为了方便服务配置文件统一管理，实时更新，所以需要分布式配置中心组件。在Spring Cloud中，有分布式配置中心组件spring cloud config ，它支持配置服务放在配置服务的内存中（即本地），也支持放在远程Git仓库中。在spring cloud config 组件中，分两个角色，一是config server，二是config client。 15. 高可用的分布式配置中心(Spring Cloud Config) ​ 当服务实例很多时，都从配置中心读取文件，这时可以考虑将配置中心做成一个微服务，将其集群化，从而达到高可用 16.消息总线(Spring Cloud Bus) ​ Spring Cloud Bus 将分布式的节点用轻量的消息代理连接起来。它可以用于广播配置文件的更改或者服务之间的通讯，也可以用于监控。本文要讲述的是用Spring Cloud Bus实现通知微服务架构的配置文件的更改。【修改配置文件，无需重启】 准备：安装rabbitMq 17.服务链路追踪(Spring Cloud Sleuth) ​ 微服务架构上通过业务来划分服务的，通过REST调用，对外暴露的一个接口，可能需要很多个服务协同才能完成这个接口功能，如果链路上任何一个服务出现问题或者网络超时，都会形成导致接口调用失败。随着业务的不断扩张，服务之间互相调用会越来越复杂。 术语 Span：基本工作单元，例如，在一个新建的span中发送一个RPC等同于发送一个回应请求给RPC，span通过一个64位ID唯一标识，trace以另一个64位ID表示，span还有其他数据信息，比如摘要、时间戳事件、关键值注释(tags)、span的ID、以及进度ID(通常是IP地址) span在不断的启动和停止，同时记录了时间信息，当你创建了一个span，你必须在未来的某个时刻停止它。 Trace：一系列spans组成的一个树状结构，例如，如果你正在跑一个分布式大数据工程，你可能需要创建一个trace。 Annotation：用来及时记录一个事件的存在，一些核心annotations用来定义一个请求的开始和结束 cs - Client Sent -客户端发起一个请求，这个annotion描述了这个span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳便可得到网络延迟 ss - Server Sent -注解表明请求处理的完成(当请求返回客户端)，如果ss减去sr时间戳便可得到服务端需要的处理请求时间 cr - Client Received -表明span的结束，客户端成功接收到服务端的回复，如果cr减去cs时间戳便可得到客户端从服务端获取回复的所有所需时间 将Span和Trace在一个系统中使用Zipkin注解的过程图形化： 18.高可用的服务注册中心 ​ 服务注册中心Eureka Server，是一个实例，当成千上万个服务向它注册的时候，它的负载是非常高的，这在生产环境上是不太合适的，这篇文章主要介绍怎么将Eureka Server集群化。Eureka通过运行多个实例，使其更具有高可用性。事实上，这是它默认的熟性，你需要做的就是给对等的实例一个合法的关联serviceurl。 SpringCloud Alibaba 参考文献：黑马课程笔记，方志朋的博客","link":"/2021/02/25/Draft/2021/SpringCloud/"},{"title":"魑魅先生 | 微信小程序","text":"微信小程序指南 常用函数 跳转 123wx.navigateTo({ url: '../component/L-component' }) 简介 前端，后端任意，上线需配置域名。 WeixinJSBridge对内---》 JS-SDK，前者包装，对内转对外---》 小程序，逻辑层和渲染层是分开的，逻辑层运行在 JSCore 中，并没有一个完整浏览器对象，因而缺少相关的DOM API和BOM API。导致无法运行常用前端库jQuery、 Zepto 等， JSCore 的环境同 NodeJS 环境也是不尽相同，所以一些 NPM 的包在小程序中也是无法运行的。 **初始过程：**申请小程序账号(管理小程序，获取AppID)，安装小程序开发工具、配置项目 结构 app.json 全局配置，包括了小程序的所有页面路径、界面表现、网络超时时间、底部 tab 1234567891011121314151617{ // 描述当前小程序所有页面路径，这是为了让微信客户端知道当前你的小程序页面定义在哪个目录。 &quot;pages&quot;:[ &quot;pages/index/index&quot;, &quot;pages/logs/logs&quot; ], // 定义小程序所有页面的顶部背景颜色，文字颜色定义等。 &quot;window&quot;:{ &quot;backgroundTextStyle&quot;:&quot;light&quot;, &quot;navigationBarBackgroundColor&quot;: &quot;#fff&quot;, &quot;navigationBarTitleText&quot;: &quot;Weixin&quot;, &quot;navigationBarTextStyle&quot;:&quot;black&quot; }, &quot;style&quot;: &quot;v2&quot;, &quot;sitemapLocation&quot;: &quot;sitemap.json&quot;} project.config.json 小程序开发者工具个性化配置，界面颜色、编译配置等等 page.json 表示 pages/logs 目录下的 logs.json 这类和小程序页面相关的配置。独立定义每个页面的一些属性 页面中配置项会覆盖 app.json 的 window 中相同的配置项 index.wxml WXML（WeiXin Markup Language）， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748数据绑定&lt;!--wxml--&gt;&lt;view&gt; {{message}} &lt;/view&gt;// page.jsPage({ data: { message: 'Hello MINA!' }})列表渲染&lt;!--wxml--&gt;&lt;view wx:for=&quot;{{array}}&quot;&gt; {{item}} &lt;/view&gt;// page.jsPage({ data: { array: [1, 2, 3, 4, 5] }})条件渲染&lt;!--wxml--&gt;&lt;view wx:if=&quot;{{view == 'WEBVIEW'}}&quot;&gt; WEBVIEW &lt;/view&gt;&lt;view wx:elif=&quot;{{view == 'APP'}}&quot;&gt; APP &lt;/view&gt;&lt;view wx:else=&quot;{{view == 'MINA'}}&quot;&gt; MINA &lt;/view&gt;// page.jsPage({ data: { view: 'MINA' }})模板&lt;!--wxml--&gt;&lt;template name=&quot;staffName&quot;&gt; &lt;view&gt; FirstName: {{firstName}}, LastName: {{lastName}} &lt;/view&gt;&lt;/template&gt;&lt;template is=&quot;staffName&quot; data=&quot;{{...staffA}}&quot;&gt;&lt;/template&gt;&lt;template is=&quot;staffName&quot; data=&quot;{{...staffB}}&quot;&gt;&lt;/template&gt;&lt;template is=&quot;staffName&quot; data=&quot;{{...staffC}}&quot;&gt;&lt;/template&gt;// page.jsPage({ data: { staffA: {firstName: 'Hulk', lastName: 'Hu'}, staffB: {firstName: 'Shang', lastName: 'You'}, staffC: {firstName: 'Gideon', lastName: 'Lin'} }}) index.wxss CSS,仅支持部分 CSS 选择器,新增了尺寸单位,提供了全局的样式[app.wxss]和局部样式 .js 逻辑层 除此之外，只有后缀名在白名单内的文件可以被上传，不在白名单列表内文件在开发工具能被访问到，但无法被上传。具体白名单列表如下： wxs png jpg jpeg gif svg json cer mp3 aac m4a mp4 wav ogg silk wasm br 基础 场景值： 描述用户进入小程序的路径，目前还无法获取到按 Home 键退出到桌面，然后从桌面再次进小程序的场景值，对于这种情况，会保留上一次的场景值。 获取方式：onLaunch 和 onShow，wx.getLaunchOptionsSync 场景值 场景 appId含义 1020 公众号 profile 页相关小程序列表 来源公众号 1035 公众号自定义菜单 来源公众号 1036 App 分享消息卡片 来源App 1037 小程序打开小程序 来源小程序 1038 从另一个小程序返回 来源小程序 1043 公众号模板消息 来源公众号 逻辑层App Service JavaScript 的基础上，增加一些功能，由于并非运行在浏览器中，window，document等一些web能力无法使用： 增加 App 和 Page 方法，进行程序注册和页面注册。 增加 getApp 和 getCurrentPages 方法，分别用来获取 App 实例和当前页面栈。 提供丰富的 API，如微信用户数据，扫一扫，支付等微信特有能力。 提供模块化能力，每个页面有独立的作用域。 注册小程序 app.js 中调用 App 方法注册小程序实例，绑定生命周期回调函数、错误监听和页面不存在监听函数等。 12345整个小程序只有一个 App 实例，是全部页面共享的。开发者可以通过 getApp 方法获取到全局唯一的 App 实例，获取App上的数据或调用开发者注册在 App 上的函数。// xxx.jsconst appInstance = getApp()console.log(appInstance.globalData) // I am global data 注册页面 Page() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Page({ data: { text: &quot;This is page data.&quot; }, onLoad: function(options) { // 页面创建时执行 }, onShow: function() { // 页面出现在前台时执行 }, onReady: function() { // 页面首次渲染完毕时执行 }, onHide: function() { // 页面从前台变为后台时执行 }, onUnload: function() { // 页面销毁时执行 }, onPullDownRefresh: function() { // 触发下拉刷新时执行 }, onReachBottom: function() { // 页面触底时执行 }, onShareAppMessage: function () { // 页面被用户分享时执行 }, onPageScroll: function() { // 页面滚动时执行 }, onResize: function() { // 页面尺寸变化时执行 }, onTabItemTap(item) { // tab 点击时执行 console.log(item.index) console.log(item.pagePath) console.log(item.text) }, // 事件响应函数 viewTap: function() { this.setData({ text: 'Set some data for updating view.' }, function() { // this is setData callback }) }, // 自由数据 customData: { hi: 'MINA' }}) behaviors 2.9.2 开始支持,低版本需做兼容处理。 behaviors 可以用来让多个页面有相同的数据字段和方法。 12345678910111213141516171819// my-behavior.jsmodule.exports = Behavior({ data: { sharedText: 'This is a piece of data shared between pages.' }, methods: { sharedMethod: function() { this.data.sharedText === 'This is a piece of data shared between pages.' } }})// page-a.jsvar myBehavior = require('./my-behavior.js')Page({ behaviors: [myBehavior], onLoad: function() { this.data.sharedText === 'This is a piece of data shared between pages.' }}) Component () 基础库 1.6.3 开始支持，低版本需做兼容处理。方法需要放在 methods: { } 里面。类似自定义组件，适合复杂页面 1234567891011121314151617Component({ data: { text: &quot;This is page data.&quot; }, methods: { onLoad: function(options) { // 页面创建时执行 }, onPullDownRefresh: function() { // 下拉刷新时执行 }, // 事件响应函数 viewTap: function() { // ... } }}) 页面生命周期 页面路由 在小程序中所有页面的路由全部由框架进行管理。 页面栈 框架以栈的形式维护了当前的所有页面。 当发生路由切换的时候，页面栈的表现如下： 路由方式 页面栈表现 初始化 新页面入栈 打开新页面 新页面入栈 页面重定向 当前页面出栈，新页面入栈 页面返回 页面不断出栈，直到目标返回页 Tab 切换 页面全部出栈，只留下新的 Tab 页面 重加载 页面全部出栈，只留下新的页面 开发者可以使用 getCurrentPages() 函数获取当前页面栈。 路由方式 对于路由的触发方式以及页面生命周期函数如下： 路由方式 触发时机 路由前页面 路由后页面 初始化 小程序打开的第一个页面 onLoad, onShow 打开新页面 调用 API wx.navigateTo 使用组件 `` onHide onLoad, onShow 页面重定向 调用 API wx.redirectTo 使用组件 `` onUnload onLoad, onShow 页面返回 调用 API wx.navigateBack 使用组件`` 用户按左上角返回按钮 onUnload onShow Tab 切换 调用 API wx.switchTab 使用组件 `` 用户切换 Tab 各种情况请参考下表 重启动 调用 API wx.reLaunch 使用组件 `` onUnload onLoad, onShow Tab 切换对应的生命周期（以 A、B 页面为 Tabbar 页面，C 是从 A 页面打开的页面，D 页面是从 C 页面打开的页面为例）： 当前页面 路由后页面 触发的生命周期（按顺序） A A Nothing happend A B A.onHide(), B.onLoad(), B.onShow() A B（再次打开） A.onHide(), B.onShow() C A C.onUnload(), A.onShow() C B C.onUnload(), B.onLoad(), B.onShow() D B D.onUnload(), C.onUnload(), B.onLoad(), B.onShow() D（从转发进入） A D.onUnload(), A.onLoad(), A.onShow() D（从转发进入） B D.onUnload(), B.onLoad(), B.onShow() 注意事项 navigateTo, redirectTo 只能打开非 tabBar 页面。 switchTab 只能打开 tabBar 页面。 reLaunch 可以打开任意页面。 页面底部的 tabBar 由页面决定，即只要是定义为 tabBar 的页面，底部都有 tabBar。 调用页面路由带的参数可以在目标页面的onLoad中获取。 模块化 公共的代码抽离成为一个单独的 js 文件，作为一个模块。模块只有通过 module.exports 或者 exports【exports 是 module.exports 的一个引用，因此在模块里边随意更改 exports 的指向会造成未知的错误。】 才能对外暴露接口。不支持直接引入 node_modules , 开发者需要使用到 node_modules 时候建议拷贝出相关的代码到小程序的目录中，或者使用小程序支持的 npm 功能 123456789101112131415161718192021// common.jsfunction sayHello(name) { console.log(`Hello ${name} !`)}function sayGoodbye(name) { console.log(`Goodbye ${name} !`)}module.exports.sayHello = sayHelloexports.sayGoodbye = sayGoodbye​在需要使用这些模块的文件中，使用 require 将公共代码引入var common = require('common.js')Page({ helloMINA: function() { common.sayHello('MINA') }, goodbyeMINA: function() { common.sayGoodbye('MINA') }}) 文件作用域 在 JavaScript 文件中声明的变量和函数只在该文件中有效；不同的文件中可以声明相同名字的变量和函数，不会互相影响。 通过全局函数 getApp 可以获取全局的应用实例，如果需要全局的数据可以在 App() 中设置 1234567891011121314151617// app.jsApp({ globalData: 1})// a.js// The localValue can only be used in file a.js.var localValue = 'a'// Get the app instance.var app = getApp()// Get the global data and change it.app.globalData++// b.js// You can redefine localValue in file b.js, without interference with the localValue in a.js.var localValue = 'b'// If a.js it run before b.js, now the globalData shoule be 2.console.log(getApp().globalData) API 事件监听 API on 开头,监听某个事件是否触发,如：wx.onSocketOpen，wx.onCompassChange 等。 同步 API Sync 结尾,如 wx.setStorageSync，wx.getSystemInfoSync 等 异步 API 如 wx.request，wx.login 等 云开发 API 视图层 View WXML WXSS (WeiXin Style Sheets) @import语句可以导入外联样式表 内联样式 框架组件上支持使用 style、class 属性来控制组件的样式。 选择器 目前支持的选择器有： 选择器 样例 样例描述 .class .intro 选择所有拥有 class=&quot;intro&quot; 的组件 #id #firstname 选择拥有 id=&quot;firstname&quot; 的组件 element view 选择所有 view 组件 element, element view, checkbox 选择所有文档的 view 组件和所有的 checkbox 组件 ::after view::after 在 view 组件后边插入内容 ::before view::before 在 view 组件前边插入内容 WXS WXS（WeiXin Script）是小程序的一套脚本语言 WXS 不依赖于运行时的基础库版本，可以在所有版本的小程序中运行。 WXS 与 JavaScript 是不同的语言，有自己的语法，并不和 JavaScript 一致。 WXS 的运行环境和其他 JavaScript 代码是隔离的，WXS 中不能调用其他 JavaScript 文件中定义的函数，也不能调用小程序提供的API。 WXS 函数不能作为组件的事件回调。 由于运行环境的差异，在 iOS 设备上小程序内的 WXS 会比 JavaScript 代码快 2 ~ 20 倍。在 android 设备上二者运行效率无差异。 1234567&lt;wxs module=&quot;m1&quot;&gt;var msg = &quot;hello world&quot;;module.exports.message = msg;&lt;/wxs&gt;&lt;view&gt; {{m1.message}} &lt;/view&gt; 事件系统 简意双向绑定 只能是一个单一字段的绑定 12&lt;input model:value=&quot;值为 {{value}}&quot; /&gt;&lt;input model:value=&quot;{{ a + b }}&quot; /&gt; 都是非法的； 目前，尚不能 data 路径 1&lt;input model:value=&quot;{{ a.b }}&quot; /&gt; 组件中传递双向绑定并触发更新 12345678910111213Component({ properties: { myValue: String }, methods: { update: function() { // 更新 myValue this.setData({ myValue: 'leaf' }) }})&lt;input model:value=&quot;{{myValue}}&quot; /&gt; 基础组件 获取界面上的节点信息 WXML节点信息：获取节点属性、样式、在界面上的位置等信息 WXML节点布局相交状态： 参照节点：监听的参照节点，取它的布局区域作为参照区域。如果有多个参照节点，则会取它们布局区域的 交集 作为参照区域。页面显示区域也可作为参照区域之一。 目标节点：监听的目标，默认只能是一个节点（使用 selectAll 选项时，可以同时监听多个节点）。 相交区域：目标节点的布局区域与参照区域的相交区域。 相交比例：相交区域占参照区域的比例。 阈值：相交比例如果达到阈值，则会触发监听器的回调函数。阈值可以有多个。 1234567891011121314Page({ onLoad: function(){ wx.createIntersectionObserver().relativeToViewport().observe('.target-class', (res) =&gt; { res.id // 目标节点 id res.dataset // 目标节点 dataset res.intersectionRatio // 相交区域占目标节点的布局区域的比例 res.intersectionRect // 相交区域 res.intersectionRect.left // 相交区域的左边界坐标 res.intersectionRect.top // 相交区域的上边界坐标 res.intersectionRect.width // 相交区域的宽度 res.intersectionRect.height // 相交区域的高度 }) }}) 响应显示区域变化 在手机上启用屏幕旋转支持 1app.json` 的 `window` 段中设置 `&quot;pageOrientation&quot;: &quot;auto&quot;` ，或在页面 json 文件中配置 `&quot;pageOrientation&quot;: &quot;auto&quot;, 2.5.0 开始， pageOrientation 还可以被设置为 landscape ，表示固定为横屏显示 在 iPad 上启用屏幕旋转支持 1app.json` 中添加 `&quot;resizable&quot;: true，不能单独配置某个页面是否支持屏幕旋转 Media Query 对于不同尺寸的显示区域，页面的布局会有所差异。此时可以使用 media query 来解决大多数问题。 代码示例： 12345678910.my-class { width: 40px;}@media (min-width: 480px) { /* 仅在 480px 或更宽的屏幕上生效的样式规则 */ .my-class { width: 200px; }} 在 WXML 中，可以使用 match-media 组件来根据 media query 匹配状态展示、隐藏节点。 此外，可以在页面或者自定义组件 JS 中使用 this.createMediaQueryObserver() 方法来创建一个 MediaQueryObserver 对象，用于监听指定的 media query 的匹配状态。 分栏模式 启用 3.3版本以上在pc等大屏幕上支持分栏模式， app.json 中同时添加 &quot;resizable&quot;: true 和 &quot;frameset&quot;: true 分栏占位图片 当某一栏没有展示任何页面时，会展示一张图片在此栏正中央。 如果代码包中的 frameset/placeholder.png 文件存在，这张图片将作为此时展示的图片。 动画 初始渲染缓存 自定义组件 介绍 创建自定义组件 类似于页面，一个自定义组件由 json wxml wxss js 4个文件组成。要编写一个自定义组件，首先需要在组件的 json 文件中进行自定义组件声明： 123{ &quot;component&quot;: true} 同时，还要在 wxml 文件中编写组件模板，在 wxss 文件中加入组件样式，它们的写法与页面的写法类似。 123456789&lt;!-- 这是自定义组件的内部WXML结构 --&gt;&lt;view class=&quot;inner&quot;&gt; {{innerText}}&lt;/view&gt;&lt;slot&gt;&lt;/slot&gt;/* 这里的样式只应用于这个自定义组件 */.inner { color: red;} 组件wxss中不应使用ID选择器、属性选择器和标签名选择器。 在自定义组件的 js 文件中，需要使用 Component() 来注册组件，并提供组件的属性定义、内部数据和自定义方法。 组件的属性值和内部数据将被用于组件 wxml 的渲染，其中，属性值是可由组件外部传入的。 1234567891011121314151617Component({ properties: { // 这里定义了innerText属性，属性值可以在组件使用时指定 innerText: { type: String, value: 'default value', } }, data: { // 这里是一些组件内部数据 someData: {} }, methods: { // 这里是一个自定义方法 customMethod: function(){} }}) 使用自定义组件 使用已注册的自定义组件前，首先要在用组件的页面的 json 文件中进行引用声明。此时需要提供每个自定义组件的标签名和对应的自定义组件文件路径： 12345{ &quot;usingComponents&quot;: { &quot;component-tag-name&quot;: &quot;path/to/the/custom/component&quot;//同目录下直接写文件名 }} 这样，在页面的 wxml 中就可以像使用基础组件一样使用自定义组件。节点名即自定义组件的标签名，节点属性即传递给组件的属性值。 1234&lt;view&gt; &lt;!-- 以下是对一个自定义组件的引用 --&gt; &lt;component-tag-name inner-text=&quot;Some text&quot;&gt;&lt;/component-tag-name&gt;&lt;/view&gt; 组件模板和样式 组件模板 组件模板的写法与页面模板相同。组件模板与组件数据结合后生成的节点树，将被插入到组件的引用位置上。 在组件模板中可以提供一个 &lt;slot&gt; 节点，用于承载组件引用时提供的子节点。 代码示例： 在开发者工具中预览效果 123456789101112&lt;!-- 组件模板 --&gt;&lt;view class=&quot;wrapper&quot;&gt; &lt;view&gt;这里是组件的内部节点&lt;/view&gt; &lt;slot&gt;&lt;/slot&gt;&lt;/view&gt;&lt;!-- 引用组件的页面模板 --&gt;&lt;view&gt; &lt;component-tag-name&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot&gt; 的位置上 --&gt; &lt;view&gt;这里是插入到组件slot中的内容&lt;/view&gt; &lt;/component-tag-name&gt;&lt;/view&gt; 注意，在模板中引用到的自定义组件及其对应的节点名需要在 json 文件中显式定义，否则会被当作一个无意义的节点。除此以外，节点名也可以被声明为抽象节点 模板数据绑定 与普通的 WXML 模板类似，可以使用数据绑定，这样就可以向子组件的属性传递动态数据。 代码示例： 在开发者工具中预览效果 1234567&lt;!-- 引用组件的页面模板 --&gt;&lt;view&gt; &lt;component-tag-name prop-a=&quot;{{dataFieldA}}&quot; prop-b=&quot;{{dataFieldB}}&quot;&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot&gt; 的位置上 --&gt; &lt;view&gt;这里是插入到组件slot中的内容&lt;/view&gt; &lt;/component-tag-name&gt;&lt;/view&gt; 在以上例子中，组件的属性 propA 和 propB 将收到页面传递的数据。页面可以通过 setData 来改变绑定的数据字段。 注意：这样的数据绑定只能传递 JSON 兼容数据。自基础库版本 2.0.9 开始，还可以在数据中包含函数（但这些函数不能在 WXML 中直接调用，只能传递给子组件）。 组件 wxml 的 slot 在组件的 wxml 中可以包含 slot 节点，用于承载组件使用者提供的 wxml 结构。 默认情况下，一个组件的 wxml 中只能有一个 slot 。需要使用多 slot 时，可以在组件 js 中声明启用。 1234567Component({ options: { multipleSlots: true // 在组件定义时的选项中启用多slot支持 }, properties: { /* ... */ }, methods: { /* ... */ }}) 此时，可以在这个组件的 wxml 中使用多个 slot ，以不同的 name 来区分。 123456&lt;!-- 组件模板 --&gt;&lt;view class=&quot;wrapper&quot;&gt; &lt;slot name=&quot;before&quot;&gt;&lt;/slot&gt; &lt;view&gt;这里是组件的内部细节&lt;/view&gt; &lt;slot name=&quot;after&quot;&gt;&lt;/slot&gt;&lt;/view&gt; 使用时，用 slot 属性来将节点插入到不同的 slot 上。 123456789&lt;!-- 引用组件的页面模板 --&gt;&lt;view&gt; &lt;component-tag-name&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot name=&quot;before&quot;&gt; 的位置上 --&gt; &lt;view slot=&quot;before&quot;&gt;这里是插入到组件slot name=&quot;before&quot;中的内容&lt;/view&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot name=&quot;after&quot;&gt; 的位置上 --&gt; &lt;view slot=&quot;after&quot;&gt;这里是插入到组件slot name=&quot;after&quot;中的内容&lt;/view&gt; &lt;/component-tag-name&gt;&lt;/view&gt; 组件样式 组件对应 wxss 文件的样式，只对组件wxml内的节点生效。编写组件样式时，需要注意以下几点： 组件和引用组件的页面不能使用id选择器（#a）、属性选择器（[a]）和标签名选择器，请改用class选择器。 组件和引用组件的页面中使用后代选择器（.a .b）在一些极端情况下会有非预期的表现，如遇，请避免使用。 子元素选择器（.a&gt;.b）只能用于 view 组件与其子节点之间，用于其他组件可能导致非预期的情况。 继承样式，如 font 、 color ，会从组件外继承到组件内。 除继承样式外， app.wxss 中的样式、组件所在页面的的样式对自定义组件无效（除非更改组件样式隔离选项）。 1234#a { } /* 在组件中不能使用 */[a] { } /* 在组件中不能使用 */button { } /* 在组件中不能使用 */.a &gt; .b { } /* 除非 .a 是 view 组件节点，否则不一定会生效 */ 除此以外，组件可以指定它所在节点的默认样式，使用 :host 选择器（需要包含基础库 1.7.2 或更高版本的开发者工具支持）。 代码示例： 123456/* 组件 custom-component.wxss */:host { color: yellow;}&lt;!-- 页面的 WXML --&gt;&lt;custom-component&gt;这段文本是黄色的&lt;/custom-component&gt; 组件样式隔离 默认情况下，自定义组件的样式只受到自定义组件 wxss 的影响。除非以下两种情况： app.wxss 或页面的 wxss 中使用了标签名选择器（或一些其他特殊选择器）来直接指定样式，这些选择器会影响到页面和全部组件。通常情况下这是不推荐的做法。 指定特殊的样式隔离选项 styleIsolation 。 12345Component({ options: { styleIsolation: 'isolated' }}) 在开发者工具中预览效果 styleIsolation 选项从基础库版本 2.6.5 开始支持。它支持以下取值： isolated 表示启用样式隔离，在自定义组件内外，使用 class 指定的样式将不会相互影响（一般情况下的默认值）； apply-shared 表示页面 wxss 样式将影响到自定义组件，但自定义组件 wxss 中指定的样式不会影响页面； shared 表示页面 wxss 样式将影响到自定义组件，自定义组件 wxss 中指定的样式也会影响页面和其他设置了 apply-shared 或 shared 的自定义组件。（这个选项在插件中不可用。） 使用后两者时，请务必注意组件间样式的相互影响。 如果这个 Component 构造器用于构造页面 ，则默认值为 shared ，且还有以下几个额外的样式隔离选项可用： page-isolated 表示在这个页面禁用 app.wxss ，同时，页面的 wxss 不会影响到其他自定义组件； page-apply-shared 表示在这个页面禁用 app.wxss ，同时，页面 wxss 样式不会影响到其他自定义组件，但设为 shared 的自定义组件会影响到页面； page-shared 表示在这个页面禁用 app.wxss ，同时，页面 wxss 样式会影响到其他设为 apply-shared 或 shared 的自定义组件，也会受到设为 shared 的自定义组件的影响。 从小程序基础库版本 2.10.1 开始，也可以在页面或自定义组件的 json 文件中配置 styleIsolation （这样就不需在 js 文件的 options 中再配置）。例如： 123{ &quot;styleIsolation&quot;: &quot;isolated&quot;} 此外，小程序基础库版本 2.2.3 以上支持 addGlobalClass 选项，即在 Component 的 options 中设置 addGlobalClass: true 。 这个选项等价于设置 styleIsolation: apply-shared ，但设置了 styleIsolation 选项后这个选项会失效。 代码示例： 在开发者工具中预览效果 123456789101112/* 组件 custom-component.js */Component({ options: { addGlobalClass: true, }})&lt;!-- 组件 custom-component.wxml --&gt;&lt;text class=&quot;red-text&quot;&gt;这段文本的颜色由 `app.wxss` 和页面 `wxss` 中的样式定义来决定&lt;/text&gt;/* app.wxss */.red-text { color: red;} 外部样式类 基础库 1.9.90 开始支持，低版本需做兼容处理。 有时，组件希望接受外部传入的样式类。此时可以在 Component 中用 externalClasses 定义段定义若干个外部样式类。 这个特性可以用于实现类似于 view 组件的 hover-class 属性：页面可以提供一个样式类，赋予 view 的 hover-class ，这个样式类本身写在页面中而非 view 组件的实现中。 注意：在同一个节点上使用普通样式类和外部样式类时，两个类的优先级是未定义的，因此最好避免这种情况。 代码示例： 123456/* 组件 custom-component.js */Component({ externalClasses: ['my-class']})&lt;!-- 组件 custom-component.wxml --&gt;&lt;custom-component class=&quot;my-class&quot;&gt;这段文本的颜色由组件外的 class 决定&lt;/custom-component&gt; 这样，组件的使用者可以指定这个样式类对应的 class ，就像使用普通属性一样。在 2.7.1 之后，可以指定多个对应的 class 。 代码示例： 在开发者工具中预览效果 1234567891011&lt;!-- 页面的 WXML --&gt;&lt;custom-component my-class=&quot;red-text&quot; /&gt;&lt;custom-component my-class=&quot;large-text&quot; /&gt;&lt;!-- 以下写法需要基础库版本 2.7.1 以上 --&gt;&lt;custom-component my-class=&quot;red-text large-text&quot; /&gt;.red-text { color: red;}.large-text { font-size: 1.5em;} 引用页面或父组件的样式 基础库 2.9.2 开始支持，低版本需做兼容处理。 即使启用了样式隔离 isolated ，组件仍然可以在局部引用组件所在页面的样式或父组件的样式。 例如，如果在页面 wxss 中定义了： 123.blue-text { color: blue;} 在这个组件中可以使用 ~ 来引用这个类的样式： 1&lt;view class=&quot;~blue-text&quot;&gt; 这段文本是蓝色的 &lt;/view&gt; 如果在一个组件的父组件 wxss 中定义了： 123.red-text { color: red;} 在这个组件中可以使用 ^ 来引用这个类的样式： 1&lt;view class=&quot;^red-text&quot;&gt; 这段文本是红色的 &lt;/view&gt; 也可以连续使用多个 ^ 来引用祖先组件中的样式。 注意：如果组件是比较独立、通用的组件，请优先使用外部样式类的方式，而非直接引用父组件或页面的样式。 虚拟化组件节点 基础库 2.11.2 开始支持，低版本需做兼容处理。 默认情况下，自定义组件本身的那个节点是一个“普通”的节点，使用时可以在这个节点上设置 class style 、动画、 flex 布局等，就如同普通的 view 组件节点一样。 12345&lt;!-- 页面的 WXML --&gt;&lt;view style=&quot;display: flex&quot;&gt; &lt;!-- 默认情况下，这是一个普通的节点 --&gt; &lt;custom-component style=&quot;color: blue; flex: 1&quot;&gt;蓝色、满宽的&lt;/custom-component&gt;&lt;/view&gt; 但有些时候，自定义组件并不希望这个节点本身可以设置样式、响应 flex 布局等，而是希望自定义组件内部的第一层节点能够响应 flex 布局或者样式由自定义组件本身完全决定。 这种情况下，可以将这个自定义组件设置为“虚拟的”： 1234567891011Component({ options: { virtualHost: true }, properties: { style: { // 定义 style 属性可以拿到 style 属性上设置的值 type: String, } }, externalClasses: ['class'], // 可以将 class 设为 externalClasses}) 这样，可以将 flex 放入自定义组件内： 12345678910&lt;!-- 页面的 WXML --&gt;&lt;view style=&quot;display: flex&quot;&gt; &lt;!-- 如果设置了 virtualHost ，节点上的样式将失效 --&gt; &lt;custom-component style=&quot;color: blue&quot;&gt;不是蓝色的&lt;/custom-component&gt;&lt;/view&gt;&lt;!-- custom-component.wxml --&gt;&lt;view style=&quot;flex: 1&quot;&gt; 满宽的 &lt;slot&gt;&lt;/slot&gt;&lt;/view&gt; 需要注意的是，自定义组件节点上的 class style 和动画将不再生效，但仍可以： 将 style 定义成 properties 属性来获取 style 上设置的值； 将 class 定义成 externalClasses 外部样式类使得自定义组件 wxml 可以使用 class 值。 Component 构造器 Component 构造器可用于定义组件，调用 Component 构造器时可以指定组件的属性、数据、方法等。 详细的参数含义和使用请参考 Component 参考文档。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051Component({ behaviors: [], properties: { myProperty: { // 属性名 type: String, value: '' }, myProperty2: String // 简化的定义方式 }, data: {}, // 私有数据，可用于模板渲染 lifetimes: { // 生命周期函数，可以为函数，或一个在methods段中定义的方法名 attached: function () { }, moved: function () { }, detached: function () { }, }, // 生命周期函数，可以为函数，或一个在methods段中定义的方法名 attached: function () { }, // 此处attached的声明会被lifetimes字段中的声明覆盖 ready: function() { }, pageLifetimes: { // 组件所在页面的生命周期函数 show: function () { }, hide: function () { }, resize: function () { }, }, methods: { onMyButtonTap: function(){ this.setData({ // 更新属性和数据的方法与更新页面数据的方法类似 }) }, // 内部方法建议以下划线开头 _myPrivateMethod: function(){ // 这里将 data.A[0].B 设为 'myPrivateData' this.setData({ 'A[0].B': 'myPrivateData' }) }, _propertyChange: function(newVal, oldVal) { } }}) 使用 Component 构造器构造页面 事实上，小程序的页面也可以视为自定义组件。因而，页面也可以使用 Component 构造器构造，拥有与普通组件一样的定义段与实例方法。但此时要求对应 json 文件中包含 usingComponents 定义段。 此时，组件的属性可以用于接收页面的参数，如访问页面 /pages/index/index?paramA=123&amp;paramB=xyz ，如果声明有属性 paramA 或 paramB ，则它们会被赋值为 123 或 xyz 。 页面的生命周期方法（即 on 开头的方法），应写在 methods 定义段中。 代码示例： 123456789101112131415161718{ &quot;usingComponents&quot;: {}}Component({ properties: { paramA: Number, paramB: String, }, methods: { onLoad: function() { this.data.paramA // 页面参数 paramA 的值 this.data.paramB // 页面参数 paramB 的值 } }}) 使用 Component 构造器来构造页面的一个好处是可以使用 behaviors 来提取所有页面中公用的代码段。 例如，在所有页面被创建和销毁时都要执行同一段代码，就可以把这段代码提取到 behaviors 中。 代码示例： 12345678910111213141516171819202122232425// page-common-behavior.jsmodule.exports = Behavior({ attached: function() { // 页面创建时执行 console.info('Page loaded!') }, detached: function() { // 页面销毁时执行 console.info('Page unloaded!') }})// 页面 Avar pageCommonBehavior = require('./page-common-behavior')Component({ behaviors: [pageCommonBehavior], data: { /* ... */ }, methods: { /* ... */ },})// 页面 Bvar pageCommonBehavior = require('./page-common-behavior')Component({ behaviors: [pageCommonBehavior], data: { /* ... */ }, methods: { /* ... */ },}) 组件间通信与事件 组件间通信 组件间的基本通信方式有以下几种。 WXML 数据绑定：用于父组件向子组件的指定属性设置数据，仅能设置 JSON 兼容数据（自基础库版本 2.0.9 开始，还可以在数据中包含函数）。具体在 组件模板和样式 章节中介绍。 事件：用于子组件向父组件传递数据，可以传递任意数据。 如果以上两种方式不足以满足需要，父组件还可以通过 this.selectComponent 方法获取子组件实例对象，这样就可以直接访问组件的任意数据和方法。 监听事件 事件系统是组件间通信的主要方式之一。自定义组件可以触发任意的事件，引用组件的页面可以监听这些事件。关于事件的基本概念和用法，参见 事件 。 监听自定义组件事件的方法与监听基础组件事件的方法完全一致： 代码示例： 123456789&lt;!-- 当自定义组件触发“myevent”事件时，调用“onMyEvent”方法 --&gt;&lt;component-tag-name bindmyevent=&quot;onMyEvent&quot; /&gt;&lt;!-- 或者可以写成 --&gt;&lt;component-tag-name bind:myevent=&quot;onMyEvent&quot; /&gt;Page({ onMyEvent: function(e){ e.detail // 自定义组件触发事件时提供的detail对象 }}) 触发事件 自定义组件触发事件时，需要使用 triggerEvent 方法，指定事件名、detail对象和事件选项： 代码示例： 在开发者工具中预览效果 123456789101112&lt;!-- 在自定义组件中 --&gt;&lt;button bindtap=&quot;onTap&quot;&gt;点击这个按钮将触发“myevent”事件&lt;/button&gt;Component({ properties: {}, methods: { onTap: function(){ var myEventDetail = {} // detail对象，提供给事件监听函数 var myEventOption = {} // 触发事件的选项 this.triggerEvent('myevent', myEventDetail, myEventOption) } }}) 触发事件的选项包括： 选项名 类型 是否必填 默认值 描述 bubbles Boolean 否 false 事件是否冒泡 composed Boolean 否 false 事件是否可以穿越组件边界，为false时，事件将只能在引用组件的节点树上触发，不进入其他任何组件内部 capturePhase Boolean 否 false 事件是否拥有捕获阶段 关于冒泡和捕获阶段的概念，请阅读 事件 章节中的相关说明。 代码示例： 在开发者工具中预览效果 12345678910111213141516171819202122// 页面 page.wxml&lt;another-component bindcustomevent=&quot;pageEventListener1&quot;&gt; &lt;my-component bindcustomevent=&quot;pageEventListener2&quot;&gt;&lt;/my-component&gt;&lt;/another-component&gt;// 组件 another-component.wxml&lt;view bindcustomevent=&quot;anotherEventListener&quot;&gt; &lt;slot /&gt;&lt;/view&gt;// 组件 my-component.wxml&lt;view bindcustomevent=&quot;myEventListener&quot;&gt; &lt;slot /&gt;&lt;/view&gt;// 组件 my-component.jsComponent({ methods: { onTap: function(){ this.triggerEvent('customevent', {}) // 只会触发 pageEventListener2 this.triggerEvent('customevent', {}, { bubbles: true }) // 会依次触发 pageEventListener2 、 pageEventListener1 this.triggerEvent('customevent', {}, { bubbles: true, composed: true }) // 会依次触发 pageEventListener2 、 anotherEventListener 、 pageEventListener1 } }}) 获取组件实例 可在父组件里调用 this.selectComponent ，获取子组件的实例对象。 调用时需要传入一个匹配选择器 selector，如：this.selectComponent(&quot;.my-component&quot;)。 selector 详细语法可查看 selector 语法参考文档。 代码示例： 在开发者工具中预览效果 12345678// 父组件Page({ data: {}, getChildComponent: function () { const child = this.selectComponent('.my-component'); console.log(child) }}) 在上例中，父组件将会获取 class 为 my-component 的子组件实例对象，即子组件的 this 。 注意 ：默认情况下，小程序与插件之间、不同插件之间的组件将无法通过 selectComponent 得到组件实例（将返回 null）。如果想让一个组件在上述条件下依然能被 selectComponent 返回，可以自定义其返回结果（见下）。 自定义的组件实例获取结果 若需要自定义 selectComponent 返回的数据，可使用内置 behavior: wx://component-export 从基础库版本 2.2.3 开始提供支持。 使用该 behavior 时，自定义组件中的 export 定义段将用于指定组件被 selectComponent 调用时的返回值。 代码示例： 在开发者工具中预览效果 1234567891011// 自定义组件 my-component 内部Component({ behaviors: ['wx://component-export'], export() { return { myField: 'myValue' } }})&lt;!-- 使用自定义组件时 --&gt;&lt;my-component id=&quot;the-id&quot; /&gt;// 父组件调用const child = this.selectComponent('#the-id') // 等于 { myField: 'myValue' } 在上例中，父组件获取 id 为 the-id 的子组件实例的时候，得到的是对象 { myField: 'myValue' } 。 开发经验 准备 需要后端接口使用要开通企业账号，注册费用300元 有后台数据：服务器、域名、端口443、8080、80 开通并且备案。 443端口 业务域名需要，其他端口服务域名需要，开发时可关闭域名校验。 可通过 UniApp ，通过 VUE 方式简介发行小程序。 可通过VSCode打开HBuilder X 运行的项目比较方便 业务域名，服务器域名 注意分包问题，每个包不大于2M. 可以通过代理通过一个域名使用多个代理。 转载整理来源：https://developers.weixin.qq.com/miniprogram/dev/framework/app-service/route.html","link":"/2021/02/25/Draft/2021/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"title":"魑魅先生 | 计算机网络","text":"网络基础 1. 网络层次划分 OSI/RM模型（Open System Interconnection/Reference Model）开放系统互联参考模型 2. 网络模型 OSI七层网络模型 1）物理层（Physical Layer） · 激活、维持、关闭通信端点之间的机械特性、电气特性、功能特性以及过程特性。该层为上层协议提供了一个传输数据的可靠的物理媒体。简单的说，物理层确保原始的数据可在各种物理媒体上传输。两个重要的设备名称，中继器（Repeater，放大器），集线器。 2）数据链路层（Data Link Layer） · 数据链路层在物理层提供的服务的基础上向网络层提供服务，其最基本的服务是将源自网络层来的数据可靠地传输到相邻节点的目标机网络层。为达到这一目的，数据链路必须具备一系列相应的功能，主要有：如何将数据组合成数据块，在数据链路层中称这种数据块为帧（frame），帧是数据链路层的传送单位；如何控制帧在物理信道上的传输，包括如何处理传输差错，如何调节发送速率以使与接收方相匹配；以及在两个网络实体之间提供数据链路通路的建立、维持和释放的管理。数据链路层在不可靠的物理介质上提供可靠的传输。该层的作用包括：物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。 · 有关数据链路层的重要知识点： 1&gt; 数据链路层为网络层提供可靠的数据传输； 2&gt; 基本数据单位为帧； 3&gt; 主要的协议：以太网协议； 4&gt; 两个重要设备名称：网桥和交换机。 3）网络层（Network Layer） · 实现两个端系统之间的数据透明传送，具体功能包括寻址和路由选择、连接的建立、保持和终止等。它提供的服务使传输层不需要了解网络中的数据传输和交换技术。如果您想用尽量少的词来记住网络层，那就是&quot;路径选择、路由及逻辑寻址&quot;。网络层中涉及众多的协议，其中包括最重要的协议，也是TCP/IP的核心协议——IP协议。IP协议非常简单，仅仅提供不可靠、无连接的传送服务。IP协议的主要功能有：无连接数据报传输、数据报路由选择和差错控制。与IP协议配套使用实现其功能的还有地址解析协议ARP、逆地址解析协议RARP、因特网报文协议ICMP、因特网组管理协议IGMP。 · 具体的协议我们会在接下来的部分进行总结，有关网络层的重点为： 1&gt; 网络层负责对子网间的数据包进行路由选择。此外，网络层还可以实现拥塞控制、网际互连等功能； 2&gt; 基本数据单位为IP数据报； 3&gt; 包含的主要协议： IP协议（Internet Protocol，因特网互联协议）; ICMP协议（Internet Control Message Protocol，因特网控制报文协议）; ARP协议（Address Resolution Protocol，地址解析协议）; RARP协议（Reverse Address Resolution Protocol，逆地址解析协议）。 4&gt; 重要的设备：路由器。 4）传输层（Transport Layer） · 第一个端到端，即主机到主机的层次。传输层负责将上层数据分段并提供端到端的、可靠的或不可靠的传输。此外，传输层还要处理端到端的差错控制和流量控制问题。 传输层的任务是根据通信子网的特性，最佳的利用网络资源，为两个端系统的会话层之间，提供建立、维护和取消传输连接的功能，负责端到端的可靠数据传输。在这一层，信息传送的协议数据单元称为段或报文。 网络层只是根据网络地址将源结点发出的数据包传送到目的结点，而传输层则负责将数据可靠地传送到相应的端口。 · 重点： 1&gt; 传输层负责将上层数据分段并提供端到端的、可靠的或不可靠的传输以及端到端的差错控制和流量控制问题； 2&gt; 包含的主要协议：TCP协议（Transmission Control Protocol，传输控制协议）、UDP协议（User Datagram Protocol，用户数据报协议）； 3&gt; 重要设备：网关。 5）会话层 · 会话层管理主机之间的会话进程，即负责建立、管理、终止进程之间的会话。会话层还利用在数据中插入校验点来实现数据的同步。 6）表示层 · 表示层对上层数据或信息进行变换以保证一个主机应用层信息可以被另一个主机的应用程序理解。表示层的数据转换包括数据的加密、压缩、格式转换等。 7）应用层 · 为操作系统或网络应用程序提供访问网络服务的接口。 · 会话层、表示层和应用层重点： · 1&gt; 数据传输基本单位为报文； · 2&gt; 包含的主要协议：FTP（文件传送协议）、Telnet（远程登录协议）、DNS（域名解析协议）、SMTP（邮件传送协议），POP3协议（邮局协议），HTTP协议（Hyper Text Transfer Protocol）。 TCP/IP四 (五)层模型 3. IP地址 1）网络地址 IP地址由网络号（包括子网号）和主机号组成，网络地址的主机号为全0，网络地址代表着整个网络。 2）广播地址 广播地址通常称为直接广播地址，是为了区分受限广播地址。 广播地址与网络地址的主机号正好相反，广播地址中，主机号为全1。当向某个网络的广播地址发送消息时，该网络内的所有主机都能收到该广播消息。 3）组播地址 D类地址就是组播地址。 先回忆下A，B，C，D类地址吧： A类地址以0开头，第一个字节作为网络号，地址范围为：0.0.0.0~127.255.255.255；(modified @2016.05.31) B类地址以10开头，前两个字节作为网络号，地址范围是：128.0.0.0~191.255.255.255; C类地址以110开头，前三个字节作为网络号，地址范围是：192.0.0.0~223.255.255.255。 D类地址以1110开头，地址范围是224.0.0.0~239.255.255.255，D类地址作为组播地址（一对多的通信）； E类地址以1111开头，地址范围是240.0.0.0~255.255.255.255，E类地址为保留地址，供以后使用。 注：只有A,B,C有网络号和主机号之分，D类地址和E类地址没有划分网络号和主机号。 4）255.255.255.255 该IP地址指的是受限的广播地址。受限广播地址与一般广播地址（直接广播地址）的区别在于，受限广播地址只能用于本地网络，路由器不会转发以受限广播地址为目的地址的分组；一般广播地址既可在本地广播，也可跨网段广播。例如：主机192.168.1.1/30上的直接广播数据包后，另外一个网段192.168.1.5/30也能收到该数据报；若发送受限广播数据报，则不能收到。 注：一般的广播地址（直接广播地址）能够通过某些路由器（当然不是所有的路由器），而受限的广播地址不能通过路由器。 5）0.0.0.0 常用于寻找自己的IP地址，例如在我们的RARP，BOOTP和DHCP协议中，若某个未知IP地址的无盘机想要知道自己的IP地址，它就以255.255.255.255为目的地址，向本地范围（具体而言是被各个路由器屏蔽的范围内）的服务器发送IP请求分组。 6）回环地址 127.0.0.0/8被用作回环地址，回环地址表示本机的地址，常用于对本机的测试，用的最多的是127.0.0.1。 7）A、B、C类私有地址 私有地址(private address)也叫专用地址，它们不会在全球使用，只具有本地意义。 A类私有地址：10.0.0.0/8，范围是：10.0.0.0~10.255.255.255 B类私有地址：172.16.0.0/12，范围是：172.16.0.0~172.31.255.255 C类私有地址：192.168.0.0/16，范围是：192.168.0.0~192.168.255.255 4. 子网掩码及网络划分 随着互连网应用的不断扩大，原先的IPv4的弊端也逐渐暴露出来，即网络号占位太多，而主机号位太少，所以其能提供的主机地址也越来越稀缺，目前除了使用NAT在企业内部利用保留地址自行分配以外，通常都对一个高类别的IP地址进行再划分，以形成多个子网，提供给不同规模的用户群使用。这里主要是为了在网络分段情况下有效地利用IP地址，通过对主机号的高位部分取作为子网号，从通常的网络位界限中扩展或压缩子网掩码，用来创建某类地址的更多子网。但创建更多的子网时，在每个子网上的可用主机地址数目会比原先减少。什么是子网掩码？子网掩码是标志两个IP地址是否同属于一个子网的，也是32位二进制地址，其每一个为1代表该位是网络位，为0代表主机位。它和IP地址一样也是使用点式十进制来表示的。如果两个IP地址在子网掩码的按位与的计算下所得结果相同，即表明它们共属于同一子网中。在计算子网掩码时，我们要注意IP地址中的保留地址，即&quot; 0&quot;地址和广播地址，它们是指主机地址或网络地址全为&quot; 0&quot;或&quot; 1&quot;时的IP地址，它们代表着本网络地址和广播地址，一般是不能被计算在内的。子网掩码的计算：对于无须再划分成子网的IP地址来说，其子网掩码非常简单，即按照其定义即可写出：如某B类IP地址为 10.12.3.0，无须再分割子网，则该IP地址的子网掩码255.255.0.0。如果它是一个C类地址，则其子网掩码为 255.255.255.0。其它类推，不再详述。下面我们关键要介绍的是一个IP地址，还需要将其高位主机位再作为划分出的子网网络号，剩下的是每个子网的主机号，这时该如何进行每个子网的掩码计算。 下面总结一下有关子网掩码和网络划分常见的面试考题： 1）利用子网数来计算 · 在求子网掩码之前必须先搞清楚要划分的子网数目，以及每个子网内的所需主机数目。(1) 将子网数目转化为二进制来表示;如欲将B类IP地址168.195.0.0划分成27个子网：27=11011；(2) 取得该二进制的位数，为N；该二进制为五位数，N = 5(3) 取得该IP地址的类子网掩码，将其主机地址部分的的前N位置1即得出该IP地址划分子网的子网掩码。将B类地址的子网掩码255.255.0.0的主机地址前5位置 1，得到 255.255.248.0 2）利用主机数来计算 · 如欲将B类IP地址168.195.0.0划分成若干子网，每个子网内有主机700台：(1) 将主机数目转化为二进制来表示；700=1010111100(2) 如果主机数小于或等于254（注意去掉保留的两个IP地址），则取得该主机的二进制位数，为N，这里肯定 N&lt;8。如果大于254，则 N&gt;8，这就是说主机地址将占据不止8位；该二进制为十位数，N=10；(3) 使用255.255.255.255来将该类IP地址的主机地址位数全部置1，然后从后向前的将N位全部置为 0，即为子网掩码值。将该B类地址的子网掩码255.255.0.0的主机地址全部置1，得到255.255.255.255，然后再从后向前将后 10位置0,即为：11111111.11111111.11111100.00000000，即255.255.252.0。这就是该欲划分成主机为700台的B类IP地址 168.195.0.0的子网掩码。 3）根据每个网络的主机数量进行子网地址的规划和计算子网掩码 · 这也可按上述原则进行计算。比如一个子网有10台主机，那么对于这个子网需要的IP地址是：10＋1＋1＋1＝13注意：加的第一个1是指这个网络连接时所需的网关地址，接着的两个1分别是指网络地址和广播地址。因为13小于16（16等于2的4次方），所以主机位为4位。而256－16＝240，所以该子网掩码为255.255.255.240。如果一个子网有14台主机，不少人常犯的错误是：依然分配具有16个地址空间的子网，而忘记了给网关分配地址。这样就错误了，因为14＋1＋1＋1＝17，17大于16，所以我们只能分配具有32个地址（32等于2的5次方）空间的子网。这时子网掩码为：255.255.255.224。 5. ARP/RARP协议 地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。主机发送信息时将包含目标IP地址的ARP请求广播到网络上的所有主机，并接收返回消息，以此确定目标的物理地址；收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。地址解析协议是建立在网络中各个主机互相信任的基础上的，网络上的主机可以自主发送ARP应答消息，其他主机收到应答报文时不会检测该报文的真实性就会将其记入本机ARP缓存；由此攻击者就可以向某一主机发送伪ARP应答报文，使其发送的信息无法到达预期的主机或到达错误的主机，这就构成了一个ARP欺骗。ARP命令可用于查询本机ARP缓存中IP地址和MAC地址的对应关系、添加或删除静态对应关系等。 ARP工作流程举例： 主机A的IP地址为192.168.1.1，MAC地址为0A-11-22-33-44-01； 主机B的IP地址为192.168.1.2，MAC地址为0A-11-22-33-44-02； 当主机A要与主机B通信时，地址解析协议可以将主机B的IP地址（192.168.1.2）解析成主机B的MAC地址，以下为工作流程： · （1）根据主机A上的路由表内容，IP确定用于访问主机B的转发IP地址是192.168.1.2。然后A主机在自己的本地ARP缓存中检查主机B的匹配MAC地址。 · （2）如果主机A在ARP缓存中没有找到映射，它将询问192.168.1.2的硬件地址，从而将ARP请求帧广播到本地网络上的所有主机。源主机A的IP地址和MAC地址都包括在ARP请求中。本地网络上的每台主机都接收到ARP请求并且检查是否与自己的IP地址匹配。如果主机发现请求的IP地址与自己的IP地址不匹配，它将丢弃ARP请求。 · （3）主机B确定ARP请求中的IP地址与自己的IP地址匹配，则将主机A的IP地址和MAC地址映射添加到本地ARP缓存中。 · （4）主机B将包含其MAC地址的ARP回复消息直接发送回主机A。 · （5）当主机A收到从主机B发来的ARP回复消息时，会用主机B的IP和MAC地址映射更新ARP缓存。本机缓存是有生存期的，生存期结束后，将再次重复上面的过程。主机B的MAC地址一旦确定，主机A就能向主机B发送IP通信了。 逆地址解析协议，即RARP，功能和ARP协议相对，其将局域网中某个主机的物理地址转换为IP地址，比如局域网中有一台主机只知道物理地址而不知道IP地址，那么可以通过RARP协议发出征求自身IP地址的广播请求，然后由RARP服务器负责回答。 RARP协议工作流程： · （1）给主机发送一个本地的RARP广播，在此广播包中，声明自己的MAC地址并且请求任何收到此请求的RARP服务器分配一个IP地址； · （2）本地网段上的RARP服务器收到此请求后，检查其RARP列表，查找该MAC地址对应的IP地址； · （3）如果存在，RARP服务器就给源主机发送一个响应数据包并将此IP地址提供给对方主机使用； · （4）如果不存在，RARP服务器对此不做任何的响应； 6. 路由选择协议 常见的路由选择协议有：RIP协议、OSPF协议。RIP协议 ：底层是贝尔曼福特算法，它选择路由的度量标准（metric)是跳数，最大跳数是15跳，如果大于15跳，它就会丢弃数据包。OSPF协议 ：Open Shortest Path First开放式最短路径优先，底层是迪杰斯特拉算法，是链路状态路由选择协议，它选择路由的度量标准是带宽，延迟。 7. TCP/IP协议 TCP/IP协议是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。通俗而言：TCP负责发现传输的问题，一有问题就发出信号，要求重新传输，直到所有数据安全正确地传输到目的地。而IP是给因特网的每一台联网设备规定一个地址。IP层接收由更低层（网络接口层例如以太网设备驱动程序）发来的数据包，并把该数据包发送到更高层---TCP或UDP层；相反，IP层也把从TCP或UDP层接收来的数据包传送到更低层。IP数据包是不可靠的，因为IP并没有做任何事情来确认数据包是否按顺序发送的或者有没有被破坏，IP数据包中含有发送它的主机的地址（源地址）和接收它的主机的地址（目的地址）。TCP是面向连接的通信协议，通过三次握手建立连接，通讯完成时要拆除连接，由于TCP是面向连接的所以只能用于端到端的通讯。TCP提供的是一种可靠的数据流服务，采用&quot;带重传的肯定确认&quot;技术来实现传输的可靠性。TCP还采用一种称为&quot;滑动窗口&quot;的方式进行流量控制，所谓窗口实际表示接收能力，用以限制发送方的发送速度。 TCP报文首部格式：件）、HTTP协议等。 TCP协议的三次握手和四次挥手： 注：seq:&quot;sequance&quot;序列号；ack:&quot;acknowledge&quot;确认号；SYN:&quot;synchronize&quot;请求同步标志；；ACK:&quot;acknowledge&quot;确认标志&quot;；FIN：&quot;Finally&quot;结束标志。 TCP连接建立过程：首先Client端发送连接请求报文，Server段接受连接后回复ACK报文，并为这次连接分配资源。Client端接收到ACK报文后也向Server段发生ACK报文，并分配资源，这样TCP连接就建立了。TCP连接断开过程：假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说&quot;我Client端没有数据要发给你了&quot;，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，&quot;告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息&quot;。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，&quot;告诉Client端，好了，我这边数据发完了，准备好关闭连接了&quot;。Client端收到FIN报文后，&quot;就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。&quot;，Server端收到ACK后，&quot;就知道可以断开连接了&quot;。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！为什么要三次握手？在只有两次&quot;握手&quot;的情形下，假设Client想跟Server建立连接，但是却因为中途连接请求的数据报丢失了，故Client端不得不重新发送一遍；这个时候Server端仅收到一个连接请求，因此可以正常的建立连接。但是，有时候Client端重新发送请求不是因为数据报丢失了，而是有可能数据传输过程因为网络并发量很大在某结点被阻塞了，这种情形下Server端将先后收到2次请求，并持续等待两个Client请求向他发送数据...问题就在这里，Cient端实际上只有一次请求，而Server端却有2个响应，极端的情况可能由于Client端多次重新发送请求数据而导致Server端最后建立了N多个响应在等待，因而造成极大的资源浪费！所以，&quot;三次握手&quot;很有必要！为什么要四次挥手？试想一下，假如现在你是客户端你想断开跟Server的所有连接该怎么做？第一步，你自己先停止向Server端发送数据，并等待Server的回复。但事情还没有完，虽然你自身不往Server发送数据了，但是因为你们之前已经建立好平等的连接了，所以此时他也有主动权向你发送数据；故Server端还得终止主动向你发送数据，并等待你的确认。其实，说白了就是保证双方的一个合约的完整执行！使用TCP的协议：FTP（文件传输协议）、Telnet（远程登录协议）、SMTP（简单邮件传输协议）、POP3（和SMTP相对，用于接收邮 8. UDP协议 UDP用户数据报协议，是面向无连接的通讯协议，UDP数据包括目的端口号和源端口号信息，由于通讯不需要连接，所以可以实现广播发送。UDP通讯时不需要接收方确认，属于不可靠的传输，可能会出现丢包现象，实际应用中要求程序员编程验证。UDP与TCP位于同一层，但它不管数据包的顺序、错误或重发。因此，UDP不被应用于那些使用虚电路的面向连接的服务，UDP主要用于那些面向查询---应答的服务，例如NFS。相对于FTP或Telnet，这些服务需要交换的信息量较小。每个UDP报文分UDP报头和UDP数据区两部分。报头由四个16位长（2字节）字段组成，分别说明该报文的源端口、目的端口、报文长度以及校验值。UDP报头由4个域组成，其中每个域各占用2个字节，具体如下：（1）源端口号；（2）目标端口号；（3）数据报长度；（4）校验值。使用UDP协议包括：TFTP（简单文件传输协议）、SNMP（简单网络管理协议）、DNS（域名解析协议）、NFS、BOOTP。TCP 与 UDP 的区别：TCP是面向连接的，可靠的字节流服务；UDP是面向无连接的，不可靠的数据报服务。 9. DNS协议 DNS是域名系统(DomainNameSystem)的缩写，该系统用于命名组织到域层次结构中的计算机和网络服务，可以简单地理解为将URL转换为IP地址。域名是由圆点分开一串单词或缩写组成的，每一个域名都对应一个惟一的IP地址，在Internet上域名与IP地址之间是一一对应的，DNS就是进行域名解析的服务器。DNS命名用于Internet等TCP/IP网络中，通过用户友好的名称查找计算机和服务。 10. NAT协议 NAT网络地址转换(Network Address Translation)属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技术，它被广泛应用于各种类型Internet接入方式和各种类型的网络中。原因很简单，NAT不仅完美地解决了lP地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 11. DHCP协议 DHCP动态主机设置协议（Dynamic Host Configuration Protocol）是一个局域网的网络协议，使用UDP协议工作，主要有两个用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 12. HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。 HTTP 协议包括哪些请求？ GET：请求读取由URL所标志的信息。 POST：给服务器添加信息（如注释）。 PUT：在给定的URL下存储一个文档。 DELETE：删除给定的URL所标志的资源。 HTTP 中， POST 与 GET 的区别 1）Get是从服务器上获取数据，Post是向服务器传送数据。 2）Get是把参数数据队列加到提交表单的Action属性所指向的URL中，值和表单内各个字段一一对应，在URL中可以看到。 3）Get传送的数据量小，不能大于2KB；Post传送的数据量较大，一般被默认为不受限制。 4）根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的。 I. 所谓 安全的 意味着该操作用于获取信息而非修改信息。换句话说，GET请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 II. 幂等 的意味着对同一URL的多个请求应该返回同样的结果。 HTTP 协议的 8 种请求类型介绍 HTTP 协议中共定义了八种方法或者叫“动作”来表明对 Request-URI 指定的资源的不同操作方式，具体介绍如下： OPTIONS：返回服务器针对特定资源所支持的HTTP请求方法。也可以利用向Web服务器发送'*'的请求来测试服务器的功能性。 HEAD：向服务器索要与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以在不必传输整个响应内容的情况下，就可以获取包含在响应消息头中的元信息。 GET：向特定的资源发出请求。 POST：向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的创建和/或已有资源的修改。 PUT：向指定资源位置上传其最新内容。 DELETE：请求服务器删除 Request-URI 所标识的资源。 TRACE：回显服务器收到的请求，主要用于测试或诊断。 CONNECT：HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。 虽然 HTTP 的请求方式有 8 种，但是我们在实际应用中常用的也就是 get 和 post，其他请求方式也都可以通过这两种方式间接的来实现。 网络常用参数 Content-Type 格式：Content-Type：type/subtype ;parameter type：主类型，任意的字符串，如text，如果是*号代表所有； subtype：子类型，任意的字符串，如html，如果是*号代表所有，用“/”与主类型隔开； parameter：可选参数，如charset，boundary等。 参考：1、2 常见的媒体格式类型如下： text/html ： HTML格式 text/plain ：纯文本格式 text/xml ： XML格式 image/gif ：gif图片格式 image/jpeg ：jpg图片格式 image/png：png图片格式 以application开头的媒体格式类型： application/xhtml+xml ：XHTML格式 application/xml： XML数据格式 application/atom+xml ：Atom XML聚合格式 application/json： JSON数据格式 application/pdf：pdf格式 application/msword ： Word文档格式 application/octet-stream ： 二进制流数据（如常见的文件下载） application/x-www-form-urlencoded ： &lt;-form encType=&quot;&quot;&gt;中默认的encType，form表单数据被编码为key1=val1&amp;key2=val2格式发送到服务器（表单默认的提交数据的格式） 另外一种常见的媒体格式是上传文件之时使用的： multipart/form-data ： 需要在表单中进行文件上传时，就需要使用该格式 13. 一个举例 在浏览器中输入 http://www.baidu.com/ 后执行的全部过程。现在假设如果我们在客户端（客户端）浏览器中输入 http://www.baidu.com， 而 baidu.com 为要访问的服务器（服务器），下面详细分析客户端为了访问服务器而执行的一系列关于协议的操作：1）客户端浏览器通过DNS解析到www.baidu.com的IP地址220.181.27.48，通过这个IP地址找到客户端到服务器的路径。客户端浏览器发起一个HTTP会话到220.161.27.48，然后通过TCP进行封装数据包，输入到网络层。2）在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。3）客户端的网络层不用关系应用层或者传输层的东西，主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，不作过多的描述，无非就是通过查找路由表决定通过那个路径到达服务器。4）客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换的IP数据包现在就可以传输了，然后发送IP数据包到达服务器的地址。 14.TCP三次握手/四次挥手 TCP TCP头部为20字节 源端口号（16位）和目的端口号（16位）：再加上Ip首部的源IP地址和目的IP地址可以唯一确定一个TCP连接 数据序号（16位）：表示在这个报文段中的第一个数据字节序号 确认序号：仅当ACK标志为1时有效，确认号表示期望收到的下一个字节的序号 偏移：就是头部长度，有4位，跟IP头部一样，以4字节为单位。最大是60个字节 保留位：6位，必须为0 6个标志位：URG-紧急指针有效；ACK-确认序号有效；PSH-接收方应尽快将这个报文交给应用层；RST-连接重置；SYN-同步序号用来发起一个连接；FIN-终止一个连接。 窗口字段：16位，代表的是窗口的字节容量，也就是TCP的标准窗口最大为2^16 - 1 = 65535个字节 校验和：源机器基于数据内容计算一个数值，收信息机要与源机器数值结果完全一样，从而证明数据的有效性。检验和覆盖了整个的TCP报文段：这是一个强制性的字段，一定是由发送端计算和存储，并由接收端进行验证的。 TCP 三次握手 为了保证数据能到达目标，TCP采用三次握手策略。 发送端首先发送一个带SYN（synchronize）标志的数据包给接收方【第一次的seq序列号是随机产生的，这样是为了网络安全，如果不是随机产生初始序列号，黑客将会以很容易的方式获取到你与其他主机之间的初始化序列号，并且伪造序列号进行攻击】 接收端收到后，回传一个带有SYN/ACK（acknowledgement）标志的数据包以示传达确认信息【SYN 是为了告诉发送端，发送方到接收方的通道没问题；ACK 用来验证接收方到发送方的通道没问题】 最后，发送端再回传一个带ACK标志的数据包，代表握手结束 若在握手某个过程中某个阶段莫名中断，TCP协议会再次以相同的顺序发送相同的数据包 Q：为什么要三次握手？ 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的 第一次握手，发送端：什么都确认不了；接收端：对方发送正常，自己接受正常 第二次握手，发送端：对方发送，接受正常，自己发送，接受正常 ；接收端：对方发送正常，自己接受正常 第三次握手，发送端：对方发送，接受正常，自己发送，接受正常；接收端：对方发送，接受正常，自己发送，接受正常 Q：两次握手不行吗？为什么TCP客户端最后还要发送一次确认呢？ 主要防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误。 经典场景：客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了 由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。 此时此前滞留的那一次请求连接，网络通畅了到达服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。 如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。 Q：为什么三次握手，返回时，ack 值是 seq 加 1（ack = x+1） 假设对方接收到数据，比如sequence number = 1000，TCP Payload = 1000，数据第一个字节编号为1000，最后一个为1999，回应一个确认报文，确认号为2000，意味着编号2000前的字节接收完成，准备接收编号为2000及更多的数据 确认收到的序列，并且告诉发送端下一次发送的序列号从哪里开始（便于接收方对数据排序，便于选择重传） Q：SYN洪泛攻击(SYN Flood，半开放攻击)，怎么解决？ 什么是SYN洪范泛攻击？ SYN Flood利用TCP协议缺陷，发送大量伪造的TCP连接请求，常用假冒的IP或IP号段发来海量的请求连接的第一个握手包（SYN包），被攻击服务器回应第二个握手包（SYN+ACK包），因为对方是假冒IP，对方永远收不到包且不会回应第三个握手包。导致被攻击服务器保持大量SYN_RECV状态的“半连接”，并且会重试默认5次回应第二个握手包，大量随机的恶意syn占满了未完成连接队列，导致正常合法的syn排不上队列，让正常的业务请求连接不进来。【服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的，所以服务器容易受到SYN洪泛攻击】 检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击【在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击】 怎么解决？ 缩短超时（SYN Timeout）时间 增加最大半连接数 过滤网关防护 SYN cookies技术： 当服务器接受到 SYN 报文段时，不直接为该 TCP 分配资源，而只是打开一个半开的套接字。接着会使用 SYN 报文段的源 Id，目的 Id，端口号以及只有服务器自己知道的一个秘密函数生成一个 cookie，并把 cookie 作为序列号响应给客户端。 如果客户端是正常建立连接，将会返回一个确认字段为 cookie + 1 的报文段。接下来服务器会根据确认报文的源 Id，目的 Id，端口号以及秘密函数计算出一个结果，如果结果的值 + 1 等于确认字段的值，则证明是刚刚请求连接的客户端，这时候才为该 TCP 分配资源 Q：TCP三次握手中，最后一次回复丢失，会发生什么？ 如果最后一次ACK在网络中丢失，那么Server端（服务端）该TCP连接的状态仍为SYN_RECV，并且根据 TCP的超时重传机制依次等待3秒、6秒、12秒后重新发送 SYN+ACK 包，以便 Client（客户端）重新发送ACK包 如果重发指定次数后，仍然未收到ACK应答，那么一段时间后，Server（服务端）自动关闭这个连接 但是Client（客户端）认为这个连接已经建立，如果Client（客户端）端向Server（服务端）发送数据，Server端（服务端）将以RST包（Reset，标示复位，用于异常的关闭连接）响应，此时，客户端知道第三次握手失败 TCP四次挥手 主动断开方（客户端/服务端）-发送一个 FIN，用来关闭主动断开方（客户端/服务端）到被动断开方（客户端/服务端）的数据传送 被动断开方（客户端/服务端）-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 被动点开方（客户端/服务端）-关闭与主动断开方（客户端/服务端）的连接，发送一个FIN给主动断开方（客户端/服务端） 主动断开方（客户端/服务端）-发回 ACK 报文确认，并将确认序号设置为收到序号加1 Q：为什么连接的时候是三次握手，关闭的时候却是四次握手？ 建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。 关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了,所以服务器可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接。因此，服务器ACK和FIN一般都会分开发送，从而导致多了一次。 Q：为什么TCP挥手每两次中间有一个 FIN-WAIT2等待时间？ 主动关闭的一端调用完close以后（即发FIN给被动关闭的一端， 并且收到其对FIN的确认ACK）则进入FIN_WAIT_2状态。如果这个时候因为网络突然断掉、被动关闭的一段宕机等原因，导致主动关闭的一端不能收到被动关闭的一端发来的FIN（防止对端不发送关闭连接的FIN包给本端），这个时候就需要FIN_WAIT_2定时器， 如果在该定时器超时的时候，还是没收到被动关闭一端发来的FIN，那么直接释放这个链接，进入CLOSE状态 Q：为什么客户端最后还要等待2MSL？为什么还有个TIME-WAIT的时间等待？ 保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，服务器已经发送了FIN+ACK报文，请求断开，客户端却没有回应，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。 防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，这样新的连接中不会出现旧连接的请求报文。 2MSL，最大报文生存时间，一个MSL 30 秒，2MSL = 60s Q：客户端 TIME-WAIT 状态过多会产生什么后果？怎样处理？ 作为服务器，短时间内关闭了大量的Client连接，就会造成服务器上出现大量的TIME_WAIT连接，占据大量的tuple /tApl/ ，严重消耗着服务器的资源，此时部分客户端就会显示连接不上 作为客户端，短时间内大量的短连接，会大量消耗的Client机器的端口，毕竟端口只有65535个，端口被耗尽了，后续就无法在发起新的连接了 在高并发短连接的TCP服务器上，当服务器处理完请求后立刻主动正常关闭连接。这个场景下会出现大量socket处于TIME_WAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上 高并发可以让服务器在短时间范围内同时占用大量端口，而端口有个0~65535的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了 短连接表示“业务处理+传输数据的时间 远远小于 TIMEWAIT超时的时间”的连接 解决方法： 用负载均衡来抗这些高并发的短请求； 服务器可以设置 SO_REUSEADDR 套接字选项来避免 TIME_WAIT状态，TIME_WAIT 状态可以通过优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的 强制关闭，发送 RST 包越过TIMEWAIT状态，直接进入CLOSED状态 Q：服务器出现了大量 CLOSE_WAIT 状态如何解决？ 大量 CLOSE_WAIT 表示程序出现了问题，对方的 socket 已经关闭连接，而我方忙于读或写没有及时关闭连接，需要检查代码，特别是释放资源的代码，或者是处理请求的线程配置。 Q：服务端会有一个TIME_WAIT状态吗？如果是服务端主动断开连接呢？ 发起链接的主动方基本都是客户端，但是断开连接的主动方服务器和客户端都可以充当，也就是说，只要是主动断开连接的，就会有 TIME_WAIT状态 四次挥手是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发 由于TCP连接时全双工的，因此，每个方向的数据传输通道都必须要单独进行关闭。","link":"/2021/02/25/Draft/2021/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"BUG","text":"一次相见，绝不再见 BUG分类解决记录，常见排查方法，常见避免方法总结。 MYSQL Client does not support authentication protocol requested by server; 123mysql -hlocalhost -uroot -pALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root';FLUSH PRIVILEGES; Unknown initial character set index '255' received from server. Initial client character set can be forced via the 'characterEncoding' property. 12String url后添加?useUnicode=true&amp;characterEncoding=utf8 mybatis url后添加?useUnicode=true&amp;characterEncoding=utf8 cannot open git-upload-pack IDEA 设置 请求有参数有特殊字符 post参数写body里面 tomcat connecter加relaxedPathChars=&quot;|{}[]^&quot; relaxedQueryChars=&quot;|{}[]^&quot; 123456789101112131415161718192021222324252627package com.jeethink.framework.config;import org.apache.catalina.connector.Connector;import org.springframework.boot.web.embedded.tomcat.TomcatConnectorCustomizer;import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * 解决高版本Tomcat不支持在URL中传入特殊字符（比如|）的问题 */@Configurationpublic class TomcatServerConfig { @Bean public ConfigurableServletWebServerFactory webServerFactory() { TomcatServletWebServerFactory factory = new TomcatServletWebServerFactory(); factory.addConnectorCustomizers(new TomcatConnectorCustomizer() { @Override public void customize(Connector connector) { //允许特殊字符 connector.setProperty(&quot;relaxedQueryChars&quot;, &quot;|{}[]&quot;); } }); return factory; }} Maven ‘parent.relativePath’ of POM 123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;relativePath /&gt;&lt;!-- 加上这个--&gt; &lt;/parent&gt; JAVA 不编译到target问题 12345678910111213141516171819202122232425262728&lt;!--引入本地资源--&gt; &lt;resources&gt; &lt;!--加载lib文件，特殊情况下会有lib，但大部分都靠依赖下载了--&gt; &lt;resource&gt; &lt;directory&gt;lib&lt;/directory&gt; &lt;targetPath&gt;BOOT-INF/lib/&lt;/targetPath&gt; &lt;includes&gt; &lt;include&gt;**/*.jar&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;!--打jar包--&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;!--将properties和xml文件编译--&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;/resources&gt; (1条消息) SpringBoot项目报错解决：“Error starting ApplicationContext. To display the conditions report re-run ...”_牛·云说的博客-CSDN博客","link":"/2021/02/24/Draft/2021/BUG/"},{"title":"Nginx","text":"Nginx(&quot;engine x&quot;)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 下载 安装部署 ​ 解压后不要直接点击exe，会导致修改配置后重启、停止nginx无效，需要手动关闭任务管理器内的所有nginx进程，再启动才可以。 12345start nginx//启动服务nginx -s reload//重新加载配置并启动nginx -s stop// 快速停止nginx -s quit//完整有序的关闭nginx -t // 检查配置是否正确 配置自定义nginx.conf 指令必须以分号结束 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223#user nobody;#==工作进程数，一般设置为cpu核心数worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events { #==最大连接数，一般设置为cpu*2048 worker_connections 1024;}http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' # '$status $body_bytes_sent &quot;$http_referer&quot; ' # '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; #==客户端链接超时时间 keepalive_timeout 65; #gzip on; #当配置多个server节点时，默认server names的缓存区大小就不够了，需要手动设置大一点 server_names_hash_bucket_size 512; #server表示虚拟主机可以理解为一个站点，可以配置多个server节点搭建多个站点 #每一个请求进来确定使用哪个server由server_name确定 server { #站点监听端口 listen 8800; #站点访问域名 server_name localhost; #编码格式，避免url参数乱码 charset utf-8; #access_log logs/host.access.log main; #location用来匹配同一域名下多个URI的访问规则 #比如动态资源如何跳转，静态资源如何跳转等 #location后面跟着的/代表匹配规则 location / { #站点根目录，可以是相对路径，也可以使绝对路径 root html; #默认主页 index index.html index.htm; #转发后端站点地址，一般用于做软负载，轮询后端服务器 #proxy_pass http://11.11.11.11:8080; #拒绝请求，返回403，一般用于某些目录禁止访问 #deny all; #允许请求 #allow all; add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type'; #重新定义或者添加发往后端服务器的请求头 #给请求头中添加客户请求主机名 proxy_set_header Host $host; #给请求头中添加客户端IP proxy_set_header X-Real-IP $remote_addr; #将$remote_addr变量值添加在客户端“X-Forwarded-For”请求头的后面，并以逗号分隔。 如果客户端请求未携带“X-Forwarded-For”请求头，$proxy_add_x_forwarded_for变量值将与$remote_addr变量相同 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #给请求头中添加客户端的Cookie proxy_set_header Cookie $http_cookie; #将使用代理服务器的主域名和端口号来替换。如果端口是80，可以不加。 proxy_redirect off; #浏览器对 Cookie 有很多限制，如果 Cookie 的 Domain 部分与当前页面的 Domain 不匹配就无法写入。 #所以如果请求 A 域名，服务器 proxy_pass 到 B 域名，然后 B 服务器输出 Domian=B 的 Cookie， #前端的页面依然停留在 A 域名上，于是浏览器就无法将 Cookie 写入。 #不仅是域名，浏览器对 Path 也有限制。我们经常会 proxy_pass 到目标服务器的某个 Path 下， #不把这个 Path 暴露给浏览器。这时候如果目标服务器的 Cookie 写死了 Path 也会出现 Cookie 无法写入的问题。 #设置“Set-Cookie”响应头中的domain属性的替换文本，其值可以为一个字符串、正则表达式的模式或一个引用的变量 #转发后端服务器如果需要Cookie则需要将cookie domain也进行转换，否则前端域名与后端域名不一致cookie就会无法存取 #配置规则：proxy_cookie_domain serverDomain(后端服务器域) nginxDomain(nginx服务器域) proxy_cookie_domain localhost .testcaigou800.com; #取消当前配置级别的所有proxy_cookie_domain指令 #proxy_cookie_domain off; #与后端服务器建立连接的超时时间。一般不可能大于75秒； proxy_connect_timeout 30; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } #当需要对同一端口监听多个域名时，使用如下配置，端口相同域名不同，server_name也可以使用正则进行配置 #但要注意server过多需要手动扩大server_names_hash_bucket_size缓存区大小 server { listen 80; server_name www.abc.com; charset utf-8; location / { proxy_pass http://localhost:10001; } } server { listen 80; server_name aaa.abc.com; charset utf-8; location / { proxy_pass http://localhost:20002; } } #配置 https 跨域解决 server { listen 86 ssl; server_name xxx.xxx.com; ssl_certificate E://soft//nginx-1.20.1//key_pem//7585805__hzsgis.com.pem; ssl_certificate_key E://soft//nginx-1.20.1//key_pem//7585805__hzsgis.com.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 20m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #charset koi8-r; #access_log logs/host.access.log main; root E:\\lxl\\Project\\ghyzt\\dist; location / { try_files $uri $uri/ @router; index index.html index.htm; client_max_body_size 600m; } location @router { rewrite ^.*$ /index.html last; } location ^~/prod-api/ { rewrite ^/prod-api/(.*)$ /$1 break; add_header Cache-Control &quot;no-catche,no-store&quot;; if ($request_method = 'OPTIONS') { add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Methods' 'GET, POST, PATCH, DELETE, PUT, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type, Access-Control-Expose-Headers, Token, Authorization'; add_header 'Access-Control-Max-Age' 1728000; add_header 'Content-Type' 'text/plain charset=UTF-8'; add_header 'Content-Length' 0; return 204; } add_header 'Access-Control-Allow-Origin' '*'; proxy_pass http://172.0.0.1:8086; proxy_buffering off; proxy_request_buffering off; } client_max_body_size 600m; #location / { #root html; #index index.html index.htm; #} #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } 功能 1）正反向代理 正向代理：特定情况下，代理用户访问服务器，需要用户手动的设置代理服务器的ip和端口号。 反向代理：是用来代理服务器，代理用户要访问的目标服务器。代理服务器接受请求，然后将请求转发给内部网络的服务器(服务集群模式)，并将从服务器上得到的结果返回给客户端，此时代理服务器对外就表现为一个服务器。 Nginx在反向代理上，提供灵活的功能，可以根据不同的正则采用不同的转发策略，如图设置好后不同的请求就可以走不同的服务器。 2）负载均衡 负载均衡：多在高并发情况下需要使用。其原理就是将数据流量分摊到多个服务器执行，减轻每台服务器的压力，多台服务器(集群)共同完成工作任务，从而提高了数据的吞吐量。 Nginx可使用的负载均衡策略有：轮询（默认）、权重、ip_hash、url_hash(第三方)、fair(第三方)。 3）动静分离 常用于前后端分离，Nginx提供的动静分离是指把动态请求和静态请求分离开，合适的服务器处理相应的请求，使整个服务器系统的性能、效率更高。 Nginx可以根据配置对不同的请求做不同转发，这是动态分离的基础。静态请求对应的静态资源可以直接放在Nginx上做缓冲，更好的做法是放在相应的缓冲服务器上。动态请求由相应的后端服务器处理。","link":"/2021/02/24/Draft/2021/Nginx/"},{"title":"书影音","text":"记录与回忆才能让那些美好的不美好的存留脑中，阵阵回荡与发酵。 工具：Kindle、阿里云盘 月度书籍：《代码整洁之道》 想看： [ ] 已看： BOOKS--OTHER 《白夜行》（东野奎吾） 《解忧杂货铺》（东野奎吾） 《许三观卖血记》（余华） 《我们仨》（杨绛） 《我的家》（巴金） 《局外人》 《小王子》 《恶意》（东野奎吾） 《摆渡人》克莱尔麦克福尔 《偷影子的人》（马克李维） 《岛上书店》（东野奎吾） 《嫌疑人X的献身》 《活着》（余华） 《穆斯林的葬礼》（霍达） 《乖，摸摸头》（大冰） 《水彩从入门到精通》（飞乐鸟） 《人，诗意的栖居》（海德格尔） 《明朝那些事》当年明月，石锐 《看见》柴静 《混血豹王》沈石溪 《狼王梦》沈石溪 《第七天》余华 《地心游记》 《目送》龙应台 《人间失格》 《别上了摄影的当》 《拍出明星范》 《人像摄影教程》 唐东平 《安徒生童话》 BOOKS--MAJOR 《Effective Java》 《JAVA编程思想》 《图解HTTP》 《深入理解Java虚拟机3》 《计算机网络自顶下下方法》 《程序员的自我修养》 1.实践出真知，通用最宝贵，最新乐于求，沟通不可无，环境影响大，切保身体好，薪酬等量级，单项求发展，声誉建品牌，不断学进步 2.英语很重要，阅读优秀项目， 《阿里巴巴开发手册泰山版》 1.所有数据库都要配置gmy_create(创建时间)、gmt_modified(更新时间)且需要自动化 MUSIC MOVIES 《银河护卫队2》 《新木乃伊》 《加勒比海盗1234》 《这个杀手不太冷》 《肖申克的救赎》 《泰坦尼克号》 《怦判心动》 《星际穿越》 《源代码》 《黑客帝国》 《曾经》 《爱丽丝的梦游仙境》 《冈仁波次》 《天空之城》 《变形金刚5》 《机器管家》 《独立日》 《幽灵行动阿尔法队》 《太空旅客》 《太空一号》 《地心引力》 《暮光之城》 《异形》 《火星救援》 《美丽人生》 《深夜食堂》 《战狼1/2》 《摆渡人》 《安德得游戏》 《十二只猴子》 《小森林》 《逆世界》 《金刚骷髅岛》 《夏洛特烦恼》 《左耳》 《钢铁骑士》 《捉妖记》 《重返20岁》 《机械师》 《攻壳机动队》 《一万公里的约定》 《生化危机全》 《谋杀似水年华》 《第九区》 《与君相恋100次》 《悟空传》 《刺客信条》 《绣春刀，修罗场》 《权利的游戏1234567》 嫌疑人X的献身 寻梦环游记 缝纫机乐队 敦刻尔克 洛丽塔 魁拔123 解忧杂货店 前任3 神奇女侠 芳华 被偷走的那五年 从你的全世界路过 分手合约 夏洛特烦恼 匆匆那年 失恋33天 海上钢琴师 公牛历险记 179小时 月球 记忆大师 黑天鹅 银翼杀手 血战钢锯岭 自杀小队 看不见的客人 二代妖精之今生有幸 比得兔 天空之眼 小萝莉的猴神大叔 弱点 阿甘正传 当幸福来敲门 硅谷123 国王的演讲 我是江小白 影 绿皮书 盗梦空间 调音师 少年派的奇幻漂流 怦然心动 霸王别姬 复仇者联盟四 蜘蛛侠平行宇宙 头号玩家 明日边缘 小时光 疯狂动物城 驯龙高手12","link":"/2021/02/24/Draft/2021/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"title":"健身穿搭","text":"身体是革命的本钱 软件：小红书，记录心得，读书电影笔记，摄影，旅游 第一个小目标：腹肌、胸肌 健身基础知识 有氧无氧？","link":"/2021/02/24/Draft/2021/%E5%81%A5%E8%BA%AB%E7%A9%BF%E6%90%AD/"},{"title":"BUG","text":"项目总结，开发之外技能提升 项目管理流程 资源准备 服务器 【域名、内外网络联通开通、端口开通【443、80、部署端口等】】 整体 范围 进度 成本 质量 人力 沟通 风险 采购 干系人","link":"/2021/02/24/Draft/2021/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"},{"title":"Dubbo","text":"分布式RPC框架Apache Dubbo 1. 软件架构的演进过程 软件架构的发展经历了由单体架构、垂直架构、SOA架构到微服务架构的演进过程，下面我们分别了解一下这几个架构。 1.1 单体架构 架构说明： ​ 全部功能集中在一个项目内（All in one）。 架构优点： ​ 架构简单，前期开发成本低、开发周期短，适合小型项目。 架构缺点： ​ 全部功能集成在一个工程中，对于大型项目不易开发、扩展和维护。 ​ 技术栈受限，只能使用一种语言开发。 ​ 系统性能扩展只能通过扩展集群节点，成本高。 1.2 垂直架构 架构说明： ​ 按照业务进行切割，形成小的单体项目。 架构优点： ​ 技术栈可扩展（不同的系统可以用不同的编程语言编写）。 架构缺点： ​ 功能集中在一个项目中，不利于开发、扩展、维护。 ​ 系统扩张只能通过集群的方式。 ​ 项目之间功能冗余、数据冗余、耦合性强。 1.3 SOA架构 SOA全称为Service-Oriented Architecture，即面向服务的架构。它可以根据需求通过网络对松散耦合的粗粒度应用组件(服务)进行分布式部署、组合和使用。一个服务通常以独立的形式存在于操作系统进程中。 站在功能的角度，把业务逻辑抽象成可复用的服务，通过服务的编排实现业务的快速再生，目的：把原先固有的业务功能转变为通用的业务服务，实现业务逻辑的快速复用。 架构说明： ​ 将重复功能或模块抽取成组件的形式，对外提供服务，在项目与服务之间使用ESB（企业服务总线）的形式作为通信的桥梁。 架构优点： ​ 重复功能或模块抽取为服务，提高开发效率。 ​ 可重用性高。 ​ 可维护性高。 架构缺点： ​ 各系统之间业务不同，很难确认功能或模块是重复的。 ​ 抽取服务的粒度大。 ​ 系统和服务之间耦合度高。 1.4 微服务架构 架构说明： ​ 将系统服务层完全独立出来，抽取为一个一个的微服务。 ​ 抽取的粒度更细，遵循单一原则。 ​ 采用轻量级框架协议传输。 架构优点： ​ 服务拆分粒度更细，有利于提高开发效率。 ​ 可以针对不同服务制定对应的优化方案。 ​ 适用于互联网时代，产品迭代周期更短。 架构缺点： ​ 粒度太细导致服务太多，维护成本高。 ​ 分布式系统开发的技术成本高，对团队的挑战大。 2. Apache Dubbo概述 2.1 Dubbo简介 Apache Dubbo是一款高性能的Java RPC框架。其前身是阿里巴巴公司开源的一个高性能、轻量级的开源Java RPC框架，可以和Spring框架无缝集成。 什么是RPC？ RPC全称为remote procedure call，即远程过程调用。比如两台服务器A和B，A服务器上部署一个应用，B服务器上部署一个应用，A服务器上的应用想调用B服务器上的应用提供的方法，由于两个应用不在一个内存空间，不能直接调用，所以需要通过网络来表达调用的语义和传达调用的数据。 需要注意的是RPC并不是一个具体的技术，而是指整个网络远程调用过程。 RPC是一个泛化的概念，严格来说一切远程过程调用手段都属于RPC范畴。各种开发语言都有自己的RPC框架。Java中的RPC框架比较多，广泛使用的有RMI、Hessian、Dubbo等。 Dubbo官网地址：http://dubbo.apache.org Dubbo提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 2.2 Dubbo架构 Dubbo架构图（Dubbo官方提供）如下： 节点角色说明： 节点 角色名称 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 虚线都是异步访问，实线都是同步访问 蓝色虚线:在启动时完成的功能 红色虚线(实线)都是程序运行过程中执行的功能 调用关系说明: 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 3. 服务注册中心Zookeeper 通过前面的Dubbo架构图可以看到，Registry（服务注册中心）在其中起着至关重要的作用。Dubbo官方推荐使用Zookeeper作为服务注册中心。 3.1 Zookeeper介绍 Zookeeper 是 Apache Hadoop 的子项目，是一个树型的目录服务，支持变更推送，适合作为 Dubbo 服务的注册中心，工业强度较高，可用于生产环境，并推荐使用 。 为了便于理解Zookeeper的树型目录服务，我们先来看一下我们电脑的文件系统(也是一个树型目录结构)： 我的电脑可以分为多个盘符（例如C、D、E等），每个盘符下可以创建多个目录，每个目录下面可以创建文件，也可以创建子目录，最终构成了一个树型结构。通过这种树型结构的目录，我们可以将文件分门别类的进行存放，方便我们后期查找。而且磁盘上的每个文件都有一个唯一的访问路径，例如：C:\\Windows\\itcast\\hello.txt。 Zookeeper树型目录服务： 流程说明： 服务提供者(Provider)启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者(Consumer)启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心(Monitor)启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址 3.2 安装Zookeeper 下载地址：http://archive.apache.org/dist/zookeeper/ 本课程使用的Zookeeper版本为3.4.6，下载完成后可以获得名称为zookeeper-3.4.6.tar.gz的压缩文件。 安装步骤： 第一步：安装 jdk（略） 第二步：把 zookeeper 的压缩包（zookeeper-3.4.6.tar.gz）上传到 linux 系统 第三步：解压缩压缩包 ​ tar -zxvf zookeeper-3.4.6.tar.gz 第四步：进入zookeeper-3.4.6目录，创建data目录 ​ mkdir data 第五步：进入conf目录 ，把zoo_sample.cfg 改名为zoo.cfg ​ cd conf ​ mv zoo_sample.cfg zoo.cfg 第六步：打开zoo.cfg文件, 修改data属性：dataDir=/root/zookeeper-3.4.6/data 3.3 启动、停止Zookeeper 进入Zookeeper的bin目录，启动服务命令 ./zkServer.sh start 停止服务命令 ./zkServer.sh stop 查看服务状态： ./zkServer.sh status 4. Dubbo快速入门 Dubbo作为一个RPC框架，其最核心的功能就是要实现跨网络的远程调用。本小节就是要创建两个应用，一个作为服务的提供方，一个作为服务的消费方。通过Dubbo来实现服务消费方远程调用服务提供方的方法。 4.1 服务提供方开发 开发步骤： （1）创建maven工程（打包方式为war）dubbodemo_provider，在pom.xml文件中导入如下坐标 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- dubbo相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;version&gt;3.12.1.GA&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!-- 指定端口 --&gt; &lt;port&gt;8081&lt;/port&gt; &lt;!-- 请求路径 --&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; （2）配置web.xml文件 1234567891011121314&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext*.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt;&lt;/web-app&gt; （3）创建服务接口 1234package com.itheima.service;public interface HelloService { public String sayHello(String name);} （4）创建服务实现类 12345678910package com.itheima.service.impl;import com.alibaba.dubbo.config.annotation.Service;import com.itheima.service.HelloService;@Servicepublic class HelloServiceImpl implements HelloService { public String sayHello(String name) { return &quot;hello &quot; + name; }} 注意：服务实现类上使用的Service注解是Dubbo提供的，用于对外发布服务 （5）在src/main/resources下创建applicationContext-service.xml 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo_provider&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 注册 协议和port 端口默认是20880 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20881&quot;&gt;&lt;/dubbo:protocol&gt; &lt;!-- 扫描指定包，加入@Service注解的类会被发布为服务 --&gt; &lt;dubbo:annotation package=&quot;com.itheima.service.impl&quot; /&gt;&lt;/beans&gt; （6）启动服务 tomcat7:run 4.2 服务消费方开发 开发步骤： （1）创建maven工程（打包方式为war）dubbodemo_consumer，pom.xml配置和上面服务提供者相同，只需要将Tomcat插件的端口号改为8082即可 （2）配置web.xml文件 1234567891011121314151617181920&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext-web.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; （3）将服务提供者工程中的HelloService接口复制到当前工程 （4）编写Controller 12345678910111213141516171819202122package com.itheima.controller;import com.alibaba.dubbo.config.annotation.Reference;import com.itheima.service.HelloService;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(&quot;/demo&quot;)public class HelloController { @Reference private HelloService helloService; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String getName(String name){ //远程调用 String result = helloService.sayHello(name); System.out.println(result); return result; }} 注意：Controller中注入HelloService使用的是Dubbo提供的@Reference注解 （5）在src/main/resources下创建applicationContext-web.xml 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo-consumer&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 扫描的方式暴露接口 --&gt; &lt;dubbo:annotation package=&quot;com.itheima.controller&quot; /&gt;&lt;/beans&gt; （6）运行测试 tomcat7:run启动 在浏览器输入http://localhost:8082/demo/hello.do?name=Jack，查看浏览器输出结果 **思考一：**上面的Dubbo入门案例中我们是将HelloService接口从服务提供者工程(dubbodemo_provider)复制到服务消费者工程(dubbodemo_consumer)中，这种做法是否合适？还有没有更好的方式？ **答：**这种做法显然是不好的，同一个接口被复制了两份，不利于后期维护。更好的方式是单独创建一个maven工程，将此接口创建在这个maven工程中。需要依赖此接口的工程只需要在自己工程的pom.xml文件中引入maven坐标即可。 **思考二：**在服务消费者工程(dubbodemo_consumer)中只是引用了HelloService接口，并没有提供实现类，Dubbo是如何做到远程调用的？ **答：**Dubbo底层是基于代理技术为HelloService接口创建代理对象，远程调用是通过此代理对象完成的。可以通过开发工具的debug功能查看此代理对象的内部结构。另外，Dubbo实现网络传输底层是基于Netty框架完成的。 **思考三：**上面的Dubbo入门案例中我们使用Zookeeper作为服务注册中心，服务提供者需要将自己的服务信息注册到Zookeeper，服务消费者需要从Zookeeper订阅自己所需要的服务，此时Zookeeper服务就变得非常重要了，那如何防止Zookeeper单点故障呢？ **答：**Zookeeper其实是支持集群模式的，可以配置Zookeeper集群来达到Zookeeper服务的高可用，防止出现单点故障。 5. Dubbo管理控制台 我们在开发时，需要知道Zookeeper注册中心都注册了哪些服务，有哪些消费者来消费这些服务。我们可以通过部署一个管理中心来实现。其实管理中心就是一个web应用，部署到tomcat即可。 5.1 安装 安装步骤： （1）将资料中的dubbo-admin-2.6.0.war文件复制到tomcat的webapps目录下 （2）启动tomcat，此war文件会自动解压 （3）修改WEB-INF下的dubbo.properties文件，注意dubbo.registry.address对应的值需要对应当前使用的Zookeeper的ip地址和端口号 ​ dubbo.registry.address=zookeeper://192.168.134.129:2181 ​ dubbo.admin.root.password=root ​ dubbo.admin.guest.password=guest （4）重启tomcat 5.2 使用 操作步骤： （1）访问http://localhost:8080/dubbo-admin-2.6.0/，输入用户名(root)和密码(root) （2）启动服务提供者工程和服务消费者工程，可以在查看到对应的信息 6. Dubbo相关配置说明 6.1 包扫描 1&lt;dubbo:annotation package=&quot;com.itheima.service&quot; /&gt; 服务提供者和服务消费者都需要配置，表示包扫描，作用是扫描指定包(包括子包)下的类。 如果不使用包扫描，也可以通过如下配置的方式来发布服务： 12&lt;bean id=&quot;helloService&quot; class=&quot;com.itheima.service.impl.HelloServiceImpl&quot; /&gt;&lt;dubbo:service interface=&quot;com.itheima.api.HelloService&quot; ref=&quot;helloService&quot; /&gt; 作为服务消费者，可以通过如下配置来引用服务： 12&lt;!-- 生成远程服务代理，可以和本地bean一样使用helloService --&gt;&lt;dubbo:reference id=&quot;helloService&quot; interface=&quot;com.itheima.api.HelloService&quot; /&gt; 上面这种方式发布和引用服务，一个配置项(dubbo:service、dubbo:reference)只能发布或者引用一个服务，如果有多个服务，这种方式就比较繁琐了。推荐使用包扫描方式。 6.2 协议 1&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot;/&gt; 一般在服务提供者一方配置，可以指定使用的协议名称和端口号。 其中Dubbo支持的协议有：dubbo、rmi、hessian、http、webservice、rest、redis等。 推荐使用的是dubbo协议。 dubbo 协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 也可以在同一个工程中配置多个协议，不同服务可以使用不同的协议，例如： 1234567&lt;!-- 多协议配置 --&gt;&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt;&lt;dubbo:protocol name=&quot;rmi&quot; port=&quot;1099&quot; /&gt;&lt;!-- 使用dubbo协议暴露服务 --&gt;&lt;dubbo:service interface=&quot;com.itheima.api.HelloService&quot; ref=&quot;helloService&quot; protocol=&quot;dubbo&quot; /&gt;&lt;!-- 使用rmi协议暴露服务 --&gt;&lt;dubbo:service interface=&quot;com.itheima.api.DemoService&quot; ref=&quot;demoService&quot; protocol=&quot;rmi&quot; /&gt; 6.3 启动时检查 1&lt;dubbo:consumer check=&quot;false&quot;/&gt; 上面这个配置需要配置在服务消费者一方，如果不配置默认check值为true。Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题。可以通过将check值改为false来关闭检查。 建议在开发阶段将check值设置为false，在生产环境下改为true。 6.4 负载均衡 负载均衡（Load Balance）：其实就是将请求分摊到多个操作单元上进行执行，从而共同完成工作任务。 在集群负载均衡时，Dubbo 提供了多种均衡策略（包括随机、轮询、最少活跃调用数、一致性Hash），缺省为random随机调用。 配置负载均衡策略，既可以在服务提供者一方配置，也可以在服务消费者一方配置，如下： 12345678910111213141516@Controller@RequestMapping(&quot;/demo&quot;)public class HelloController { //在服务消费者一方配置负载均衡策略 @Reference(check = false,loadbalance = &quot;random&quot;) private HelloService helloService; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String getName(String name){ //远程调用 String result = helloService.sayHello(name); System.out.println(result); return result; }} 1234567//在服务提供者一方配置负载均衡@Service(loadbalance = &quot;random&quot;)public class HelloServiceImpl implements HelloService { public String sayHello(String name) { return &quot;hello &quot; + name; }} 可以通过启动多个服务提供者来观察Dubbo负载均衡效果。 注意：因为我们是在一台机器上启动多个服务提供者，所以需要修改tomcat的端口号和Dubbo服务的端口号来防止端口冲突。 在实际生产环境中，多个服务提供者是分别部署在不同的机器上，所以不存在端口冲突问题。 7. 解决Dubbo无法发布被事务代理的Service问题 前面我们已经完成了Dubbo的入门案例，通过入门案例我们可以看到通过Dubbo提供的标签配置就可以进行包扫描，扫描到@Service注解的类就可以被发布为服务。 但是我们如果在服务提供者类上加入@Transactional事务控制注解后，服务就发布不成功了。原因是事务控制的底层原理是为服务提供者类创建代理对象，而默认情况下Spring是基于JDK动态代理方式创建代理对象，而此代理对象的完整类名为com.sun.proxy.$Proxy42（最后两位数字不是固定的），导致Dubbo在发布服务前进行包匹配时无法完成匹配，进而没有进行服务的发布。 7.1 问题展示 在入门案例的服务提供者dubbodemo_provider工程基础上进行展示 操作步骤： （1）在pom.xml文件中增加maven坐标 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; （2）在applicationContext-service.xml配置文件中加入数据源、事务管理器、开启事务注解的相关配置 1234567891011121314&lt;!--数据源--&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot; /&gt;&lt;/bean&gt;&lt;!-- 事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt;&lt;!--开启事务控制的注解支持--&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; 上面连接的数据库可以自行创建 （3）在HelloServiceImpl类上加入@Transactional注解 （4）启动服务提供者和服务消费者，并访问 上面的错误为没有可用的服务提供者 查看dubbo管理控制台发现服务并没有发布，如下： 可以通过断点调试的方式查看Dubbo执行过程，Dubbo通过AnnotationBean的postProcessAfterInitialization方法进行处理 7.2 解决方案 通过上面的断点调试可以看到，在HelloServiceImpl类上加入事务注解后，Spring会为此类基于JDK动态代理技术创建代理对象，创建的代理对象完整类名为com.sun.proxy.$Proxy35，导致Dubbo在进行包匹配时没有成功（因为我们在发布服务时扫描的包为com.itheima.service），所以后面真正发布服务的代码没有执行。 解决方式操作步骤： （1）修改applicationContext-service.xml配置文件，开启事务控制注解支持时指定proxy-target-class属性，值为true。其作用是使用cglib代理方式为Service类创建代理对象 12&lt;!--开启事务控制的注解支持--&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot; proxy-target-class=&quot;true&quot;/&gt; （2）修改HelloServiceImpl类，在Service注解中加入interfaceClass属性，值为HelloService.class，作用是指定服务的接口类型 1234567@Service(interfaceClass = HelloService.class)@Transactionalpublic class HelloServiceImpl implements HelloService { public String sayHello(String name) { return &quot;hello &quot; + name; }} 此处也是必须要修改的，否则会导致发布的服务接口为SpringProxy，而不是HelloService接口，如下：","link":"/2021/02/24/Draft/2021/Dubbo/"},{"title":"Spring","text":"Spring5 轻量级的开源的 JavaEE 框架 进度【55，Webflux】 源码解析笔记加入 进度【】 综合问题 [IOC?DI?AOP?](#IOC【Inversion of Control】) BeanFactory 和 ApplicationContext分别是什么，有什么区别？ 什么是bean？作用域？生命周期？是否线程安全？ SprintBoot(Sprint)中Controller、Service、DAO线程安全问题_zzhongcy的博客-CSDN博客_service线程安全问题 Spring支持的事务管理类型？传播行为？事务隔离？相关注解？ AOP名词有哪些？动态代理？ 常用注解？ Spring中设计模式（工厂，代理） Spring5框架概述 1、Spring 是轻量级的开源的 JavaEE 框架 2、Spring 可以解决企业应用开发的复杂性 3、Spring 有两个核心部分：IOC 和AOP （1） IOC【Inversion of Control】：控制反转，把创建对象过程交给 Spring 进行管理 （2） Aop【Aspect Oriented Programming】：面向切面，不修改源代码进行功能增强 4、Spring 特点 （1） 方便解耦，简化开发 （2） Aop 编程支持 （3） 方便程序测试 （4） 方便和其他框架进行整合 （5） 方便进行事务操作 （6） 降低 API 开发难度 相关资源 官网 版本说明： snapshot 快照 alpha 内测 beta 公测 release 稳定版本 GA 最稳定版本 Final 正式版 Pro(professional) 专业版 Plus 加强版 Retail 零售版 DEMO 演示版 Build 内部标号 Delux 豪华版 (deluxe：豪华的，华丽的) Corporation或Enterpraise 企业版 M1 M2 M3 M是milestone的简写 里程碑的意思 RC 版本RC:(Release Candidate)，几乎就不会加入新的功能了，而主要着重于除错 SR 修正版 Trial 试用版 Shareware 共享版 Full 完全版 下载： https://repo.spring.io/ui/native/release https://repo.spring.io/ui/native/release/org/springframework/spring 注：本文档使用5.2.6 创建普通工程 新建项目 引入依赖 对应 Core中四个包引入，以及commons-logging-1.1.1.jar 新建lib文件夹并引入jar包 创建测试类 12345public class User { public void Add(){ System.out.println(&quot;add.......&quot;); }} 创建XML 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--配置User对象创建--&gt; &lt;bean id=&quot;user&quot; class=&quot;com.lxl.spring5.User&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 测试代码 123456789101112131415161718192021222324252627282930package com.lxl.spring5.testdemo;import com.lxl.spring5.User;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.context.support.FileSystemXmlApplicationContext;public class TestSpring5 { @Test public void testAdd(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定盘符路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean1.xml&quot;); //2 获取配置创建的对象，user为配置文件中id User user = context.getBean(&quot;user&quot;, User.class); System.out.println(user); user.add(); }}结果：com.intellij.rt.junit.JUnitStarter -ideVersion5 com.lxl.spring5.testdemo.TestSpring5,testAddcom.lxl.spring5.User@46daef40add.......Process finished with exit code 0 IOC【Inversion of Control】 概念原理 概念 （1） 控制反转，把对象创建和对象之间的调用过程，交给 Spring 进行管理 （2） 使用 IOC 目的：为了耦合度降低（改） （3） 上面入门案例就是 IOC 实现 原理 xml 解析、工厂模式、反射 底层 优化1 new对象---》工厂模式创建（让创建类与被创建类不耦合，比如被创建类地址改变创建类必须改变） 优化2 解析XML--》反射创建对象 IOC容器 1、IOC思想基于IOC容器完成，IOC容器底层就是对象工厂 2、Spring提供IOC容器实现两种方式：（两个接口） （1）BeanFactory：IOC容器基本实现，是Spring内部的使用接口，不提供开发人员进行使用。加载配置文件时候不创建对象，在获取对象（使用）才去创建对象 （2）ApplicationContext：BeanFactory接口的子接口，提供更多更强大的功能，一般由开发人员进行使用。加载配置文件时候就会把在配置文件对象进行创建，服务器启动时，对web运行时效果更好。 查看实现类（Ctrl+H） ApplicationContext： IOC操作Bean管理 1、什么是Bean管理 Bean管理指的是两个操作 （1）Spring创建对象 （2）Spirng注入属性 2、Bean管理操作有两种方式 （1）xml配置文件方式 1、基于xml方式创建对象 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--配置User对象创建 （1）在spring配置文件中，使用bean标签，标签里面添加对应属性，就可以实现对象创建 （2）在bean标签有很多属性，介绍常用的属性 * id属性：唯一标识，不可加特殊字符 * name :唯一表示，可加特殊字符 * class属性：类全路径（包类路径） （3）创建对象时候，默认也是执行无参数构造方法完成对象创建 --&gt; &lt;bean id=&quot;user&quot; class=&quot;com.lxl.spring5.User&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 2、基于xml方式注入属性 ​ DI：依赖注入，就是注入属性,IOC的一种具体实现，注入属性需要在创建对象的基础之上完成。 2.1set方法进行注入 创建类，定义属性和对应的set方法 123456789101112131415public class Book { //创建属性 private String bname; private String bauthor; //set方法进行注入属性 public void setBname(String bname) { this.bname = bname; } public void setBauthor(String bauthor) { this.bauthor = bauthor; }} 在spring配置文件配置对象创建，配置属性注入 1234567&lt;!--=========================set方法注入属性=============================================--&gt; &lt;!--2 set方法注入属性--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.Book&quot;&gt; &lt;!--使用property完成属性注入 name：类里面属性名称 value：向属性注入的值 --&gt; &lt;property name=&quot;bname&quot; value=&quot;易筋经&quot;&gt;&lt;/property&gt; &lt;property name=&quot;bauthor&quot; value=&quot;达摩老祖&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 2.2有参数构造进行注入 创建类，定义属性，创建属性对应有参数构造方法 123456789101112131415161718/** * 使用有参数构造注入 */public class Orders { //属性 private String oname; private String address; //有参数构造 public Orders(String oname, String address) { this.oname = oname; this.address = address; } public void ShowOrder() { System.out.println(oname+&quot;::&quot;+address); }} 配置文件创建对象，有参注入属性 12345678&lt;!--==========================有参构造注入===========================================--&gt; &lt;!--3 有参构造注入--&gt; &lt;bean id=&quot;orders&quot; class=&quot;com.lxl.spring5.Orders&quot;&gt;&lt;!-- &lt;constructor-arg index=&quot;0&quot; value=&quot;&quot;/&gt; 效果同下--&gt; &lt;constructor-arg name=&quot;oname&quot; value=&quot;电脑&quot;&gt;&lt;/constructor-arg&gt;&lt;!-- &lt;constructor-arg index=&quot;1&quot; value=&quot;&quot;/&gt;--&gt; &lt;constructor-arg name=&quot;address&quot; value=&quot;China&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 2.3p名称空间注入 使用p名称空间注入，可以简化基于xml配置方式 第一步 添加p名称空间在配置文件中 1234&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; 第二步 进行属性注入，在bean标签里面进行操作 1234567891011&lt;!--=========================set方法注入属性=============================================--&gt; &lt;!--2 set方法注入属性--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.Book&quot;&gt; &lt;!--使用property完成属性注入 name：类里面属性名称 value：向属性注入的值 --&gt; &lt;property name=&quot;bname&quot; value=&quot;易筋经&quot;&gt;&lt;/property&gt; &lt;property name=&quot;bauthor&quot; value=&quot;达摩老祖&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;!--2.1 p名称空间注入方式，已达到简化目的--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.Book&quot; p:bname=&quot;易筋经&quot; p:bauthor=&quot;达摩老祖&quot;&gt;&lt;/bean&gt; 2.4xml注入其他类型属性 **字面量：**属性设置的固定值。 null值 ： &lt;property name=&quot;bname&quot; value=&quot;易筋经&quot;&gt; 包含特殊符号： 解决：转义、特殊内容写到 即： 注入属性-外部bean （1）创建两个类 service类和dao类 （2）在service调用dao里面的方法 123456789101112131415161718192021222324package com.lxl.spring5.service;import com.lxl.spring5.dao.UserDao;import com.lxl.spring5.dao.UserDaoImpl;public class UserService { //创建UserDao类型属性，生成set方法 private UserDao userDao; public void setUserDao(UserDao userDao) { this.userDao = userDao; } public void add() { System.out.println(&quot;service add...............&quot;); // 原始创建对象// UserDao userDao = new UserDaoImpl();// userDao.update(); }} 123456789package com.lxl.spring5.dao;/** * @author Administrator */public interface UserDao { public void update();} 12345678package com.lxl.spring5.dao;public class UserDaoImpl implements UserDao{ @Override public void update() { System.out.println(&quot;dao update--------&quot;); }} （3）在spring配置文件中进行配置 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--1 service和dao对象创建--&gt; &lt;bean id=&quot;userService&quot; class=&quot;com.lxl.spring5.service.UserService&quot;&gt; &lt;!--注入userDao对象 name属性：类里面属性名称 ref属性：创建userDao对象bean标签id值 --&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDaoImpl&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;userDaoImpl&quot; class=&quot;com.lxl.spring5.dao.UserDaoImpl&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; （4）测试 12345678910111213 @Test public void testAdd(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean2.xml&quot;); //2 获取配置创建的对象，user为配置文件中id UserService userService = context.getBean(&quot;userService&quot;, UserService.class); System.out.println(userService); userService.add(); } 注入属性-内部bean ​ 当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean，为了定义innerbean，在Spring的基于XML的配置元数据中，可以在或元素内使用元素，内部bean通常是匿名的，它们的Scope一般是prototype （1）一对多两个类： 12345678910111213141516171819202122232425package com.lxl.spring5.bean;//员工类public class Emp { private String ename; private String gender; //一对多 //员工属于某一个部门，使用对象形式表示 private Dept dept; public void setDept(Dept dept) { this.dept = dept; } public void setEname(String ename) { this.ename = ename; } public void setGender(String gender) { this.gender = gender; }public void add(){ System.out.println(ename+&quot;::&quot;+gender+&quot;::&quot;+dept);}} 1234567891011121314151617package com.lxl.spring5.bean;//部门类public class Dept { private String dname; public void setDname(String dname) { this.dname = dname; } @Override public String toString() { return &quot;Dept{&quot; + &quot;dname='&quot; + dname + '\\'' + '}'; }} （2）xml配置 1234567891011&lt;!--+++++++++++++++++++注入属性-内部bean：一对多+++++++++++++++++++++++--&gt; &lt;!--内部bean--&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.bean.Emp&quot;&gt; &lt;!--设置两个普通属性--&gt; &lt;property name=&quot;ename&quot; value=&quot;lucy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;gender&quot; value=&quot;女&quot;&gt;&lt;/property&gt; &lt;!--设置对象类型属性--&gt; &lt;property name=&quot;dept&quot;&gt; &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.bean.Dept&quot;&gt; &lt;property name=&quot;dname&quot; value=&quot;安保部&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; （3）测试： 12345678910111213 @Test public void testOneToMore(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean3.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Emp emp = context.getBean(&quot;emp&quot;, Emp.class); System.out.println(emp); emp.add(); } 注入属性-级联赋值两种方式 1234567891011121314151617181920212223 &lt;!-- +++++++++++++++++++ 级联赋值两种方式 +++++++++++++++++++++++--&gt; &lt;!--级联赋值 方式一 --&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.bean.Emp&quot;&gt; &lt;!--设置两个普通属性--&gt; &lt;property name=&quot;ename&quot; value=&quot;lucy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;gender&quot; value=&quot;女&quot;&gt;&lt;/property&gt; &lt;!--级联赋值--&gt; &lt;property name=&quot;dept&quot; ref=&quot;dept&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.bean.Dept&quot;&gt; &lt;property name=&quot;dname&quot; value=&quot;财务部&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--级联赋值 方式二 --&gt;&lt;!-- 设置两个普通属性 dept.dname方式需要添加dept的get方法 --&gt;&lt;!-- &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.bean.Emp&quot;&gt; --&gt;&lt;!-- &lt;property name=&quot;ename&quot; value=&quot;lucy&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;gender&quot; value=&quot;女&quot;&gt;&lt;/property&gt; --&gt;&lt;!-- &lt;property name=&quot;dept&quot; ref=&quot;dept&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;dept.dname&quot; value=&quot;技术部&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;/bean&gt;--&gt;&lt;!-- &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.bean.Dept&quot;&gt;--&gt;&lt;!-- &lt;property name=&quot;dname&quot; value=&quot;财务部&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;/bean&gt;--&gt; 2.5xml注入集合类属性 （1）创建类，定义数组、list、map、set类型属性，生成对应set方法 12345678910111213141516171819202122232425262728293031323334353637383940package com.lxl.spring5.collectiontype;import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.Set;public class Stu { //1 数组类型属性 private String[] courses; //2 list集合类型属性 private List&lt;String&gt; list; //3 map集合类型属性 private Map&lt;String, String&gt; maps; //4 set集合类型属性 private Set&lt;String&gt; sets; public void setSets(Set&lt;String&gt; sets) { this.sets = sets; } public void setCourses(String[] courses) { this.courses = courses; } public void setList(List&lt;String&gt; list) { this.list = list; } public void setMaps(Map&lt;String, String&gt; maps) { this.maps = maps; } public void test(){ System.out.println(Arrays.toString(courses)); System.out.println(list); System.out.println(maps); System.out.println(sets); }} （2）xml配置 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- +++++++++++++++++++ 集合类属性注入 +++++++++++++++++++++++--&gt; &lt;!--1 集合类型属性注入--&gt; &lt;bean id=&quot;stu&quot; class=&quot;com.lxl.spring5.collectiontype.Stu&quot;&gt; &lt;!--数组类型属性注入--&gt; &lt;property name=&quot;courses&quot;&gt; &lt;array&gt; &lt;value&gt;java课程&lt;/value&gt; &lt;value&gt;数据库课程&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!--list类型属性注入--&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;张三&lt;/value&gt; &lt;value&gt;小三&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--map类型属性注入--&gt; &lt;property name=&quot;maps&quot;&gt; &lt;map&gt; &lt;entry key=&quot;JAVA&quot; value=&quot;java&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;PHP&quot; value=&quot;php&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--set类型属性注入--&gt; &lt;property name=&quot;sets&quot;&gt; &lt;set&gt; &lt;value&gt;MySQL&lt;/value&gt; &lt;value&gt;Redis&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; （3）测试 12345678910111213141516171819202122package com.lxl.spring5.testdemo;import com.lxl.spring5.bean.Emp;import com.lxl.spring5.collectiontype.Stu;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestCollection { @Test public void testOneToMore(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean5.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Stu stu = context.getBean(&quot;stu&quot;, Stu.class); stu.test(); }} 2.6在集合里面设置对象类型值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.lxl.spring5.collectiontype;import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.Set;public class Stu { //1 数组类型属性 private String[] courses; //2 list集合类型属性 private List&lt;String&gt; list; //3 map集合类型属性 private Map&lt;String, String&gt; maps; //4 set集合类型属性 private Set&lt;String&gt; sets; //5 List集合中为对象类型属性 private List&lt;Course&gt; courseList; public void setCourseList(List&lt;Course&gt; courseList) { this.courseList = courseList; } public void setSets(Set&lt;String&gt; sets) { this.sets = sets; } public void setCourses(String[] courses) { this.courses = courses; } public void setList(List&lt;String&gt; list) { this.list = list; } public void setMaps(Map&lt;String, String&gt; maps) { this.maps = maps; } public void test(){ System.out.println(Arrays.toString(courses)); System.out.println(list); System.out.println(maps); System.out.println(sets); System.out.println(courseList); }} 12345678910111213141516package com.lxl.spring5.collectiontype;public class Course { private String cname; public void setCname(String cname) { this.cname = cname; } @Override public String toString() { return &quot;Course{&quot; + &quot;cname='&quot; + cname + '\\'' + '}'; }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- +++++++++++++++++++ 集合类属性注入、集合内为对象情况注入 +++++++++++++++++++++++--&gt; &lt;!--1 集合类型属性注入--&gt; &lt;bean id=&quot;stu&quot; class=&quot;com.lxl.spring5.collectiontype.Stu&quot;&gt; &lt;!--数组类型属性注入--&gt; &lt;property name=&quot;courses&quot;&gt; &lt;array&gt; &lt;value&gt;java课程&lt;/value&gt; &lt;value&gt;数据库课程&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!--list类型属性注入--&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;张三&lt;/value&gt; &lt;value&gt;小三&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--map类型属性注入--&gt; &lt;property name=&quot;maps&quot;&gt; &lt;map&gt; &lt;entry key=&quot;JAVA&quot; value=&quot;java&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;PHP&quot; value=&quot;php&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--set类型属性注入--&gt; &lt;property name=&quot;sets&quot;&gt; &lt;set&gt; &lt;value&gt;MySQL&lt;/value&gt; &lt;value&gt;Redis&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;!--注入list集合类型，值是对象--&gt; &lt;property name=&quot;courseList&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;course1&quot;&gt;&lt;/ref&gt; &lt;ref bean=&quot;course2&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;course1&quot; class=&quot;com.lxl.spring5.collectiontype.Course&quot;&gt; &lt;property name=&quot;cname&quot; value=&quot;Spring5框架&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;course2&quot; class=&quot;com.lxl.spring5.collectiontype.Course&quot;&gt; &lt;property name=&quot;cname&quot; value=&quot;MyBatis框架&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 把集合注入部分提取出来 1234567891011121314package com.lxl.spring5.collectiontype;import java.util.List;public class Book { private List&lt;String&gt; list; public void setList(List&lt;String&gt; list) { this.list = list; } public void test() { System.out.println(list); }} 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; &lt;!-- +++++++++++++++++++ 集合内为对象情况注入 +++++++++++++++++++++++--&gt; &lt;!--1 加util元空间 xmlns:util=&quot;http://www.springframework.org/schema/util&quot; http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd --&gt; &lt;!--2 提取list集合类型属性注入--&gt; &lt;util:list id=&quot;bookList&quot;&gt; &lt;value&gt;易筋经&lt;/value&gt; &lt;value&gt;九阴真经&lt;/value&gt; &lt;value&gt;九阳神功&lt;/value&gt; &lt;/util:list&gt; &lt;!--3 提取list集合类型属性注入使用--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.collectiontype.Book&quot;&gt; &lt;property name=&quot;list&quot; ref=&quot;bookList&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 测试类 1234567891011121314151617181920212223242526272829303132333435package com.lxl.spring5.testdemo;import com.lxl.spring5.collectiontype.Book;import com.lxl.spring5.bean.Emp;import com.lxl.spring5.collectiontype.Stu;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestCollection { @Test public void testOneToMore(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean5.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Stu stu = context.getBean(&quot;stu&quot;, Stu.class); stu.test(); } @Test public void testOneToMore1(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean6.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Book book = context.getBean(&quot;book&quot;, Book.class); book.test(); }} 2.7FactoryBean 1、Spring有两种类型bean，一种普通bean，另外一种工厂bean（FactoryBean） 2、普通bean：在配置文件中定义bean类型就是返回类型 3、工厂bean：在配置文件定义bean类型可以和返回类型不一样 第一步 创建类，让这个类作为工厂bean，实现接口 FactoryBean 第二步 实现接口里面的方法，在实现的方法中定义返回的bean类型 1234567891011121314151617181920212223242526package com.lxl.spring5.factorybean;import com.lxl.spring5.collectiontype.Course;import org.springframework.beans.factory.FactoryBean;/**FactoryBean*/public class MyBean implements FactoryBean&lt;Course&gt; { /**定义返回bean类型*/ @Override public Course getObject() throws Exception { Course course=new Course(); course.setCname(&quot;体育课&quot;); return course; } @Override public Class&lt;?&gt; getObjectType() { return null; } @Override public boolean isSingleton() { return FactoryBean.super.isSingleton(); }} 12&lt;bean id=&quot;myBean&quot; class=&quot;com.lxl.spring5.factorybean.MyBean&quot;&gt;&lt;/bean&gt; 12345678910 @Test public void testFactoryBean(){ ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Course myBean = context.getBean(&quot;myBean&quot;, Course.class); System.out.println(myBean); } 2.8 bean 作用域 1、在Spring里面，设置创建bean实例是单实例还是多实例 2、在Spring里面，默认情况下，bean是单实例对象 3、如何设置单实例还是多实例 （1）在spring配置文件bean标签里面有属性（scope）用于设置单实例还是多实例，spring中可使用@Scope注解 （2）scope属性值 默认值，singleton，表示是单实例对象 prototype，表示是多实例对象 request：每次http请求都会创建一个bean，该作用域仅在基于web的SpringApplicationContext情形下有效。 session：在一个HTTPSession中，一个bean定义对应一个实例。该作用域仅在基于web的SpringApplicationContext情形下有效。 global-session：在一个全局的HTTPSession中，一个bean定义对应一个实例。该作用域仅在基于web的SpringApplicationContext情形下有效。 （3）singleton和prototype区别 第一 singleton单实例，prototype多实例。 第二 设置scope值是singleton时候，加载spring配置文件时候就会创建单实例对象。 设置scope值是prototype时候，不是在加载spring配置文件时候创建对象，而是在在调用getBean方法时候创建多实例对象。 2.9Bean生命周期 （1）通过构造器创建bean实例（无参数构造） （2）为bean的属性设置值和对其他bean引用（调用set方法） （3）调用bean的初始化的方法（需要进行配置初始化的方法） （4）bean可以使用了（对象获取到了） （5）当容器关闭时候，调用bean的销毁的方法（需要进行配置销毁的方法） 创建实例 ==》 根据定义填充其属性和对其他bean的引用 ==》执行bean初始化方法得到bean ==》容器关闭时销毁bean 123456789101112131415161718192021222324package com.lxl.spring5.lifecyclebean;public class Orders { //无参数构造 public Orders() { System.out.println(&quot;第一步 执行无参数构造创建bean实例&quot;); } private String oname; public void setOname(String oname) { this.oname = oname; System.out.println(&quot;第二步 调用set方法设置属性值&quot;); } //创建执行的初始化的方法 public void initMethod() { System.out.println(&quot;第三步 执行初始化的方法&quot;); } //创建执行的销毁的方法 public void destroyMethod() { System.out.println(&quot;第五步 执行销毁的方法&quot;); }} 1234&lt;!-- +++++++++++++++++++bean生命周期+++++++++++++++++++++++--&gt;&lt;bean id=&quot;orders&quot; class=&quot;com.lxl.spring5.lifecyclebean.Orders&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;oname&quot; value=&quot;键盘&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 1234567891011121314 @Test public void TestLifeCycle(){// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;);// ClassPathXmlApplicationContext 为ApplicationContext子实现类 ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean8.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Orders myBean = context.getBean(&quot;orders&quot;, Orders.class); System.out.println(&quot;第四步 获取创建的bean实例&quot;); System.out.println(myBean); context.close(); } 2.10后置处理器 123456789101112131415161718package com.lxl.spring5.lifecyclebean;import org.springframework.beans.BeansException;import org.springframework.beans.factory.config.BeanPostProcessor;public class BeforeAfter implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;后置处理器：在初始化之前执行的方法&quot;); return BeanPostProcessor.super.postProcessBeforeInitialization(bean, beanName); } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;后置处理器：在初始化之后执行的方法&quot;); return BeanPostProcessor.super.postProcessAfterInitialization(bean, beanName); }} 12345678&lt;!--+++++++++++++++++++bean生命周期、后置处理器+++++++++++++++++++++++--&gt;&lt;bean id=&quot;orders&quot; class=&quot;com.lxl.spring5.lifecyclebean.Orders&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;oname&quot; value=&quot;键盘&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--后置处理器 ,在此配置文件中的所有bean添加后置处理器--&gt;&lt;bean id=&quot;myBeanPost&quot; class=&quot;com.lxl.spring5.lifecyclebean.BeforeAfter&quot;&gt;&lt;/bean&gt; 2.12自动装配 有五种自动装配的方式，可以用来指导Spring容器用自动装配方式来进行依赖注入。 no：默认的方式是不进行自动装配，通过显式设置ref属性来进行装配。 byName：通过参数名自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byname，之后容器试图匹配、装配和该bean的属性具有相同名字的bean。 byType:：通过参数类型自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byType，之后容器试图匹配、装配和该bean的属性具有相同类型的bean。如果有多个bean符合条件，则抛出错误。 constructor：这个方式类似于byType，但是要提供给构造器参数，如果没有确定的带参数的构造器参数类型，将会抛出异常。 autodetect：首先尝试使用constructor来自动装配，如果无法工作，则使用byType方式。 1234567891011package com.lxl.spring5.autowire;/** * @author Administrator */public class Dept { @Override public String toString() { return &quot;Dept{}&quot;; }} 1234567891011121314151617181920package com.lxl.spring5.testdemo;import com.lxl.spring5.autowire.Emp;import com.lxl.spring5.lifecyclebean.Orders;import org.junit.Test;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAutoWire { @Test public void TestLifeCycle(){// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;);// ClassPathXmlApplicationContext 为ApplicationContext子实现类 ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean9.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Emp emp = context.getBean(&quot;emp&quot;, Emp.class); System.out.println(emp); }} 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; &lt;!--+++++++++++++++++++自动装配 autowire=&quot;byName/byType&quot;+++++++++++++++++++++++--&gt; &lt;!--实现自动装配 bean标签属性autowire，配置自动装配 autowire属性常用两个值： byName根据属性名称注入 ，注入值bean的id值和类属性名称一样，如下dept1不会注入。 byType根据属性类型注入 --&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.autowire.Emp&quot; autowire=&quot;byName&quot;&gt;&lt;!-- Could not autowire. There is more than one bean of 'Dept' type. Beans: dept,dept1. Properties: 'dept'--&gt;&lt;!-- &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.autowire.Emp&quot; autowire=&quot;byType&quot;&gt;--&gt;&lt;!-- 原始模式--&gt;&lt;!-- &lt;property name=&quot;dept&quot; ref=&quot;dept&quot;&gt;&lt;/property&gt;--&gt; &lt;/bean&gt; &lt;!--后置处理器 ,在此配置文件中的所有bean添加后置处理器--&gt; &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.autowire.Dept&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;dept1&quot; class=&quot;com.lxl.spring5.autowire.Dept&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 测试 1234567891011121314151617181920package com.lxl.spring5.testdemo;import com.lxl.spring5.autowire.Emp;import com.lxl.spring5.lifecyclebean.Orders;import org.junit.Test;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAutoWire { @Test public void TestLifeCycle(){// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;);// ClassPathXmlApplicationContext 为ApplicationContext子实现类 ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean9.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Emp emp = context.getBean(&quot;emp&quot;, Emp.class); System.out.println(emp); }} 2.13引入外部属性文件 1、直接配置数据库信息 （1）配置德鲁伊连接池 （2）引入德鲁伊连接池依赖jar包 ​ 复制到lib文件夹并添加项目依赖 1234567891011121314151617181920212223242526272829303132&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context =&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--+++++++++++++++++++引入外部属性文件，数据库连接池示例+++++++++++++++++++++++--&gt; &lt;!-- 原始形式，固定值--&gt;&lt;!-- &lt;bean id=&quot;dept1&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt;--&gt;&lt;!-- &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/userDb&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- 引入外部文件--&gt;&lt;!-- &lt;/bean&gt;--&gt;&lt;!-- 引入外部文件形式--&gt;&lt;!-- 引入外部文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;!-- 配置连接池--&gt; &lt;bean id=&quot;dept1&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${prop.driverClass}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;${prop.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;${prop.userName}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${prop.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; jdbc.properties 1234prop.driverClass=com.mysql.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/userDbprop.userName=rootprop.password=root （2）注解方式 1、什么是注解 （1）注解是代码特殊标记，格式：@注解名称(属性名称=属性值, 属性名称=属性值..) （2）使用注解，注解作用在类上面，方法上面，属性上面 （3）使用注解目的：简化xml配置 2、Spring针对Bean管理中创建对象提供注解 （1）@Component （2）@Service （3）@Controller （4）@Repository 上面四个注解功能是一样的，都可以用来创建bean实例，但是习惯在不同层使用不同注解。 引入aop的jar包 类注解 12345678910111213141516171819202122232425package com.lxl.spring5.annotation;import org.springframework.stereotype.Component;import org.springframework.stereotype.Controller;import org.springframework.stereotype.Repository;import org.springframework.stereotype.Service;//在注解里面value属性值可以省略不写，// 默认值是类名称，首字母小写// UserService -- userService@Component(value = &quot;userService&quot;) //等同&lt;bean id=&quot;userService&quot; class=&quot;..&quot;/&gt;/** * @author Administrator*///@Component//@Service//@Controller//@Repository // 四个注解效果相同，习惯用在不同层public class UserService { public void add() { System.out.println(&quot;service add ...&quot;); }} 开启组件扫描 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--+++++++++++++++++++注解方式，扫描包+++++++++++++++++++++++--&gt; &lt;!--添加context命名空间--&gt; &lt;!-- 开启组件扫描 1.扫描多个包，可逗号隔开 2.扫描多个包的上层目录 --&gt; &lt;context:component-scan base-package=&quot;com.lxl&quot;&gt;&lt;/context:component-scan&gt; &lt;!--示例1 use-default-filters=&quot;false&quot; 表示现在不使用默认filter，自己配置filter context:include-filter ，设置扫描哪些内容 --&gt; &lt;context:component-scan base-package=&quot;com.lxl&quot; use-default-filters=&quot;false&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; &lt;!--示例2 下面配置扫描包所有内容 context:exclude-filter： 设置哪些内容不进行扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt;&lt;/beans&gt; 测试 12345678910111213141516171819package com.lxl.spring5.testdemo;import com.lxl.spring5.annotation.UserService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;/** * @author Administrator */public class TestAnnotation { @Test public void testAnnotation() {// spring-aop-4.3.10.RELEASE 包未导入会报错：Unexpected exception parsing XML document from class path resource ApplicationContext applicationContext=new ClassPathXmlApplicationContext(&quot;bean11.xml&quot;); UserService userService = applicationContext.getBean(&quot;userService&quot;, UserService.class); userService.add(); }} 3.纯注解开发 添加配置类扫描包 12345678910package com.lxl.spring5.annotation.allbyannotation.config;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@Configuration@ComponentScan(basePackages = {&quot;com.lxl.spring5.annotation.allbyannotation&quot;})public class SpringConfig {} 测试 123456789101112131415161718package com.lxl.spring5.testdemo;import com.lxl.spring5.annotation.allbyannotation.config.SpringConfig;import com.lxl.spring5.annotation.allbyannotation.service.UserService1;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAllByAnnotation { @Test public void testAllByAnnotation(){ //加载配置类 ApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class); UserService1 userService1 = context.getBean(&quot;userService1&quot;, UserService1.class); userService1.add(); }} AOP【Aspect Oriented Programming】 AOP介绍 1、什么是AOP （1）面向切面编程（方面），利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 （2）通俗描述：不通过修改源代码方式，在主干功能里面添加新功能、比如登录的权限过滤。 AOP底层原理 1、AOP底层使用动态代理的两种情况： 第一种 有接口情况，使用JDK动态代理 ⚫ 创建接口实现类代理对象，增强类的方法 第二种 没有接口情况，使用CGLIB动态代理 ⚫ 创建子类的代理对象，增强类的方法 AOP JDK 动态代理 1、使用JDK动态代理，使用Proxy类里面的方法创建代理对象 （1）创建接口，定义方法 12345678package com.lxl.spring5.aop.jdkdynamicproxy;public interface UserDao { public int add(int a, int b); public String update(String id);} （2）创建接口实现类，实现方法 12345678910111213package com.lxl.spring5.aop.jdkdynamicproxy;public class UserDaoImpl implements UserDao { @Override public int add(int a, int b) { return a + b; } @Override public String update(String id) { return id; }} （3）使用Proxy类创建接口代理对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.lxl.spring5.aop.jdkdynamicproxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.util.Arrays;/** * 使用Proxy创建代理对象 */public class JDKProxy { public static void main(String[] args) { //创建接口实现类代理对象 Class[] interfaces = {UserDao.class};// getClassLoader类加载器// interfaces要实现的接口// Proxy.newProxyInstance(JDKProxy.class.getClassLoader(), interfaces, new InvocationHandler() {// @Override// public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {// return null;// }// }); UserDaoImpl userDao = new UserDaoImpl(); UserDao dao = (UserDao) Proxy.newProxyInstance(JDKProxy.class.getClassLoader(), interfaces, new UserDaoProxy(userDao)); int result = dao.add(1, 2); System.out.println(&quot;result:&quot; + result); }}//创建代理对象代码class UserDaoProxy implements InvocationHandler { //1 把创建的是谁的代理对象，把谁传递过来 // 有参数构造传递代理对象 private Object obj; public UserDaoProxy(Object obj) { this.obj = obj; } //增强的逻辑// invoke方法类对象创建后即被调用 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //方法之前 System.out.println(&quot;方法之前执行....&quot; + method.getName() + &quot; :传递的参数...&quot; + Arrays.toString(args)); // 被增强的方法执行 Object res = method.invoke(obj, args); // 方法之后 System.out.println(&quot;方法之后执行....&quot; + obj); return res; }} AOP主要术语 1.连接点（Joint point） 可以被增强的程序具体的方法 2.切入点（Pointcut） 实际被增强的方法（一个或一组连接点），可通过@Pointcut注解到方法上抽取共同切入点，通过切入点表达式或者匹配的方式指明切入点。 3.通知（增强）（Advice） 实际增强的部分，切面要完成的各个工作，比如前置后置日志等。通知类型：前置通知、后置通知、环绕通知、异常通知、最终通知 4.切面（Aspect） 封装 各个通知应用到切入点的过程动作 的一个 标注@Aspect的 类 AOP操作（准备） 1、Spring框架一般都是基于AspectJ实现AOP操作 （1）AspectJ不是Spring组成部分，独立AOP框架，一般把AspectJ和Spirng框架一起使用，进行AOP操作 2、基于 **AspectJ **实现AOP操作 （1）基于xml配置文件实现 （2）基于注解方式实现（使用） 3、在项目工程里面引入AOP相关依赖 4、切入点表达式 （1）切入点表达式作用：知道对哪个类里面的哪个方法进行增强 （2）语法结构： execution([权限修饰符] [返回类型] [类全路径] [方法名称 ]-([参数列表])) 举例1：对com.atguigu.dao.BookDao类里面的add进行增强 execution(* com.atguigu.dao.BookDao.add(..)) 举例2：对com.atguigu.dao.BookDao类里面的所有的方法进行增强 execution(* com.atguigu.dao.BookDao.* (..)) 举例3：对com.atguigu.dao包里面所有类，类里面所有方法进行增强 execution(* com.atguigu.dao.. (..)) AOP操作（ 注解） 1、创建类，在类里面定义方法 12345678import org.springframework.stereotype.Component;@Componentpublic class User { public void add() { System.out.println(&quot;add.......&quot;); }} 2、创建增强类（编写增强逻辑） ​ 在增强类里面，创建方法，让不同方法代表不同通知类型,@Aspect把当前类标识为一个切面供容器读取。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.lxl.spring5.aop.aspectjanno;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;//增强的类@Component//把当前类标识为一个切面供容器读取@Aspect// 生成代理对象public class UserProxy { // 前置通知 //@Before注解表示作为前置通知 @Before(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void before() { System.out.println(&quot;before.........&quot;); } //后置通知（返回通知）// 在方法返回结果之后执行// 异常时不执行 @AfterReturning(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void afterReturning() { System.out.println(&quot;afterReturning.........&quot;); } //最终通知// 方法之后执行// 异常时也执行 @After(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void after() { System.out.println(&quot;after.........&quot;); } //异常通知// 出现异常执行 @AfterThrowing(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void afterThrowing() { System.out.println(&quot;afterThrowing.........&quot;); } //环绕通知 @Around(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable { System.out.println(&quot;环绕之前.........&quot;);// 被增强的方法执行 proceedingJoinPoint.proceed(); System.out.println(&quot;环绕之后.........&quot;); }} 3、进行通知的配置 （1）在spring配置文件中，开启注解扫描 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!--=====================AOP注解使用======================--&gt; &lt;!-- 添加命名空间：aop、context--&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl.spring5.aop.aspectjanno&quot;&gt;&lt;/context:component-scan&gt; &lt;!-- 开启Aspect生成代理对象--&gt; &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt;&lt;/beans&gt; 4.测试 12345678@Testpublic void TestAopAnno() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;aop1.xml&quot;); User user= context.getBean(&quot;user&quot;,User.class); user.add();} 5.细节：共同切入点抽取 1234567891011 //共同切入点抽取 @Pointcut(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void samepoint() { } // 前置通知 //@Before注解表示作为前置通知 @Before(value = &quot;samepoint()&quot;)// @Before(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void before() { System.out.println(&quot;before.........&quot;); } 6、细节：设置增强类优先级 ​ 有多个增强类多同一个方法进行增强，设置增强类优先级，在增强类上面添加注解 @Order(数字类型值)，数字类型值越小优先级越高。 1234567891011121314151617181920212223242526import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;//增强的类@Component@Aspect@Order(1)// 生成代理对象public class PersonProxy { //共同切入点抽取 @Pointcut(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void samepoint() { } // 前置通知 //@Before注解表示作为前置通知 @Before(value = &quot;samepoint()&quot;)// @Before(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void before() { System.out.println(&quot;PersonProxy before.........&quot;); }} 7.完全使用注解开发 （1）创建配置类，不需要创建xml配置文件 @Configuration @ComponentScan(basePackages = {&quot;com.lxl&quot;}) @EnableAspectJAutoProxy(proxyTargetClass = true) public class ConfigAop AOP操作（XML） 1.创建类与增强类 12345public class Book { public void buy() { System.out.println(&quot;buy--&quot;); }} 1234567package com.lxl.spring5.aop.aopxml;public class BookProxy { public void before() { System.out.println(&quot;before----&quot;); }} 2.配置切入点 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!--=====================AOP xml操作======================--&gt; &lt;!--创建对象--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.aop.aopxml.Book&quot;/&gt; &lt;bean id=&quot;bookProxy&quot; class=&quot;com.lxl.spring5.aop.aopxml.BookProxy&quot;/&gt; &lt;!--配置aop增强--&gt; &lt;aop:config&gt; &lt;!--切入点--&gt; &lt;aop:pointcut id=&quot;p&quot; expression=&quot;execution(* com.lxl.spring5.aop.aopxml.Book.buy(..))&quot;/&gt; &lt;!--配置切面，即把通知（增强的部分bookProxy）ref应用到切入点（buy()）的过程动作--&gt; &lt;aop:aspect ref=&quot;bookProxy&quot;&gt; &lt;!--增强作用在具体的方法上--&gt; &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;p&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 3.测试 1234567891011121314151617package com.lxl.spring5.aop.aopxml;import com.lxl.spring5.aop.aopanno.User;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAopXml { @Test public void TestAopAnno() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;aop2.xml&quot;); Book book= context.getBean(&quot;book&quot;,Book.class); book.buy(); }} JdbcTemplate 依赖准备 配置连接池 jdbc.properties 1234prop.driverClass=com.mysql.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/learn?useUnicode=true&amp;characterEncoding=utf8prop.userName=rootprop.password=root 1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 引入外部文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;!-- 配置druid连接池--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${prop.driverClass}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;${prop.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;${prop.userName}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${prop.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!--注入dataSource--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 组件扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl.spring5.jdbctemplatel&quot;&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 所有数据库基本操作 创建实体类、service类，创建dao类，在dao注入jdbcTemplate对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185//=======================Dao=============================import com.lxl.spring5.jdbctemplatel.entity.Book;import java.util.List;public interface BookDao { public void add(Book book); public void delete(String id); public void updateBook(Book book); public int selectCount(); public Book selectOneBook(String bookid); public List&lt;Book&gt; selectAllBooks(); public void batchInsert(List&lt;Object[]&gt; books); public void batchUpdateBook(List&lt;Object[]&gt; batchArgs); public void batchDeleteBook(List&lt;Object[]&gt; batchArgs);}//=======================DaoImpl=============================import com.lxl.spring5.jdbctemplatel.entity.Book;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;import java.util.Arrays;import java.util.List;@Repositorypublic class BookDaoImpl implements BookDao { //注入JdbcTemplate @Autowired private JdbcTemplate jdbcTemplate; @Override public void add(Book book) { String sql = &quot;insert into book values(?,?,?) &quot;; String args[] = {book.getBookId(), book.getBookName(), book.getBookStatus()}; int update = jdbcTemplate.update(sql, args); System.out.println(&quot;更新了&quot; + update + &quot;条数据&quot;); } @Override public void updateBook(Book book) { String sql = &quot;update book set bookname=?,bookstatus=? where bookid=?&quot;; Object[] args = {book.getBookName(), book.getBookStatus(), book.getBookId()}; int update = jdbcTemplate.update(sql, args); System.out.println(update); } @Override public void delete(String id) { String sql = &quot;delete from book where bookid=?&quot;; int update = jdbcTemplate.update(sql, id); System.out.println(update); } //查询表记录数 @Override public int selectCount() { String sql = &quot;select count(*) from book&quot;; Integer count = jdbcTemplate.queryForObject(sql, Integer.class); return count; } //查询单个对象 @Override public Book selectOneBook(String bookid) { String sql = &quot;select * from book where bookid=?&quot;; Book book = jdbcTemplate.queryForObject(sql, new BeanPropertyRowMapper&lt;Book&gt;(Book.class), bookid); return book; } //查询多个对象 @Override public List&lt;Book&gt; selectAllBooks() { String sql = &quot;select * from book&quot;; List&lt;Book&gt; query = jdbcTemplate.query(sql, new BeanPropertyRowMapper&lt;Book&gt;(Book.class)); return query; } @Override public void batchInsert(List&lt;Object[]&gt; books) { String sql = &quot;insert into book values(?,?,?)&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, books); System.out.println(Arrays.toString(ints)); }// 批量修改 @Override public void batchUpdateBook(List&lt;Object[]&gt; batchArgs) { String sql = &quot;update book set bookname=?,bookstatus=? where bookid=?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); }// 批量删除 @Override public void batchDeleteBook(List&lt;Object[]&gt; batchArgs) { String sql = &quot;delete from book where bookid=?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); }}//======================Service==============================import com.lxl.spring5.jdbctemplatel.dao.BookDao;import com.lxl.spring5.jdbctemplatel.entity.Book;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class BookService { //注入dao @Autowired private BookDao bookDao; public void addBook(Book book){ bookDao.add(book); } public void updateBook(Book book){ bookDao.updateBook(book); } public void delete(String id){ bookDao.delete(id); } public int selectCount(){ return bookDao.selectCount(); } public Book selectOneBook(String bookid){ return bookDao.selectOneBook(bookid); } public List&lt;Book&gt; selectAllBooks(){ return bookDao.selectAllBooks(); } public void batchInsert(List&lt;Object[]&gt; books){ bookDao.batchInsert(books); }; public void batchUpdateBook(List&lt;Object[]&gt; books){ bookDao.batchUpdateBook(books); }; public void batchDeleteBook(List&lt;Object[]&gt; books){ bookDao.batchDeleteBook(books); };}//========================entity============================public class Book { private String BookId; private String BookName; private String BookStatus; @Override public String toString() { return &quot;Book{&quot; + &quot;BookId='&quot; + BookId + '\\'' + &quot;, BookName='&quot; + BookName + '\\'' + &quot;, BookStatus='&quot; + BookStatus + '\\'' + '}'; } public String getBookId() { return BookId; } public void setBookId(String bookId) { BookId = bookId; } public String getBookName() { return BookName; } public void setBookName(String bookName) { BookName = bookName; } public String getBookStatus() { return BookStatus; } public void setBookStatus(String bookStatus) { BookStatus = bookStatus; }} 新建对应数据库字段后测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import com.lxl.spring5.jdbctemplatel.entity.Book;import com.lxl.spring5.jdbctemplatel.service.BookService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import java.util.ArrayList;import java.util.List;public class BookTest { @Test public void jdbcTemplateTest() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;jdbctemplate1.xml&quot;);// 增加// Book book = new Book();// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// book.setBookId(&quot;1&quot;);// book.setBookName(&quot;Java宝典&quot;);// book.setBookStatus(&quot;完好&quot;);// bookService.addBook(book);// 修改，无值改为null// Book book = new Book();// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// book.setBookId(&quot;1&quot;);// book.setBookStatus(&quot;改了&quot;);// bookService.updateBook(book);// 根据id删除// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// bookService.delete(&quot;1&quot;);// 查询数目// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// int i = bookService.selectCount();// System.out.println(i);// 查询单个对象// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// Book book = bookService.selectOneBook(&quot;1&quot;);// System.out.println(book);// 查询一组对象// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// List&lt;Book&gt; book = bookService.selectAllBooks();// System.out.println(book);// 批量新增一组对象// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// List&lt;Object[]&gt; books = new ArrayList&lt;&gt;();// Object[] o1={&quot;3&quot;,&quot;java&quot;,&quot;a&quot;};// Object[] o2={&quot;4&quot;,&quot;orcle&quot;,&quot;b&quot;};// Object[] o3={&quot;5&quot;,&quot;net&quot;,&quot;v&quot;};// books.add(o1);// books.add(o2);// books.add(o3);// bookService.batchInsert(books);// 批量修改// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;();//// sql参数顺序// Object[] o1 = {&quot;java1&quot;, &quot;a3&quot;, &quot;3&quot;};// Object[] o2 = {&quot;orcle1&quot;, &quot;b4&quot;, &quot;4&quot;};// batchArgs.add(o1);// batchArgs.add(o2);//// 调用方法实现批量修改// bookService.batchUpdateBook(batchArgs); //批量删除 BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;(); Object[] o1 = {&quot;3&quot;}; Object[] o2 = {&quot;4&quot;}; batchArgs.add(o1); batchArgs.add(o2); //调用方法实现批量删除 bookService.batchDeleteBook(batchArgs); }} 可能BUG Client does not support authentication protocol requested by server; 123mysql -hlocalhost -uroot -pALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root';FLUSH PRIVILEGES; Unknown initial character set index '255' received from server. Initial client character set can be forced via the 'characterEncoding' property. 12String url后添加?useUnicode=true&amp;characterEncoding=utf8 mybatis url后添加?useUnicode=true&amp;characterEncoding=utf8 事务 1.什么是事务 （1）事务是数据库操作最基本单元，逻辑上一组操作，要么都成功，如果有一个失败所有操作都失败 （2）典型场景：银行转账 lucy 转账100元 给mary lucy少100，mary多100 2.事务四个特性（ACID） （1）原子性 （2）一致性 （3）隔离性 （4）持久性 3.事物测试环境搭建 创建数据库 123456789101112131415161718192021SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for t_account-- ----------------------------DROP TABLE IF EXISTS `t_account`;CREATE TABLE `t_account` ( `id` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `username` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `money` int NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of t_account-- ----------------------------INSERT INTO `t_account` VALUES ('1', 'lucy', 900);INSERT INTO `t_account` VALUES ('2', 'mary', 1100);SET FOREIGN_KEY_CHECKS = 1; 创建service，搭建dao，完成对象创建和注入关系 123456package com.lxl.spring5.transactionl.dao;public interface UserDao { public void reduceMoney(); public void addMoney();} 1234567891011121314151617181920212223242526package com.lxl.spring5.transactionl.dao;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;@Repositorypublic class UserDaoImpl implements UserDao { @Autowired private JdbcTemplate jdbcTemplate; //lucy转账100给mary // 少钱 @Override public void reduceMoney() { String sql = &quot;update t_account set money=money-? where username=?&quot;; jdbcTemplate.update(sql, 100, &quot;lucy&quot;); } //多钱 @Override public void addMoney() { String sql = &quot;update t_account set money=money+? where username=?&quot;; jdbcTemplate.update(sql, 100, &quot;mary&quot;); }} 1234567891011121314151617181920package com.lxl.spring5.transactionl.service;import com.lxl.spring5.transactionl.dao.UserDao;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class UserService { // 注入dao @Autowired private UserDao userDao; //转账的方法 public void accountMoney() { // lucy少100 userDao.reduceMoney(); // mary多100 userDao.addMoney(); }} 1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 引入外部文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;!-- 配置连接池--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${prop.driverClass}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;${prop.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;${prop.userName}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${prop.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!--注入dataSource--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 组件扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl.spring5.transactionl&quot;&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 测试 12345678910111213141516package com.lxl.spring5.transactionl.test;import com.lxl.spring5.transactionl.service.UserService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class transactionTest {@Test public void transactionTest() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;transaction1.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); userService.accountMoney();}} 4.出现异常 出现异常后，lucy少100，mary不会变导致数据更改错误。 解决1：try检测异常，catch手动回滚 解决2：使用Spring声明式事务，如下5所示。 1234567public void accountMoney() { // lucy少100 userDao.reduceMoney(); int i=10/0; // mary多100 userDao.addMoney();} 5.事务操作（半注解） 事务添加到JavaEE三层结构里面Service层（业务逻辑层） 在Spring进行事务管理操作 （1）有两种方式：编程式事务管理和声明式事务管理（使用） 声明式事务管理 （1）基于注解方式（使用） （2）基于xml配置文件方式 在Spring进行声明式事务管理，底层使用AOP原理 Spring事务管理API （1）提供一个接口，代表事务管理器，这个接口针对不同的框架提供不同的实现类 （2）配置事物管理器，引入名称空间 tx，开启事物注解 12345678910111213&lt;!--1.引入名称空间tx--&gt;xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&lt;!--2.创建事务管理器--&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!--3.注入数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--4.开启事务注解--&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;&gt;&lt;/tx:annotation-driven&gt;&lt;!--5.添加@Transactional注解到需要的类或者方法上，其位置决定起作用域。--&gt; 6.事务参数 propagation：事务传播行为 ​ （1）事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。 用伪代码说明： 123456789public void methodA(){ methodB(); //doSomething}@Transaction(Propagation=XXX)public void methodB(){ //doSomething} ​ 代码中methodA()方法嵌套调用了methodB()方法，methodB()的事务传播行为由@Transaction(Propagation=XXX)设置决定。这里需要注意的是methodA()并没有开启事务，某一个事务传播行为修饰的方法并不是必须要在开启事务的外围方法中调用。 ​ （2）Spring中七种事务传播行为 事务传播( propagation )行为类型 说明 PROPAGATION_REQUIRED requirde 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。 PROPAGATION_SUPPORTS supports 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY mandatory 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW requirde _new 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED not_supported以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER never 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED nested 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 ioslation：事务隔离级别，传播属性 ​ 1.首先说明一下事务并发引起的三种情况： 1) Dirty Reads 脏读 一个事务正在对数据进行更新操作，但是更新还未提交，另一个事务这时也来操作这组数据，并且读取了前一个事务还未提交的数据，而前一个事务如果操作失败进行了回滚，后一个事务读取的就是错误数据，这样就造成了脏读。前人后悔后人遭殃 2) Non-Repeatable Reads 不可重复读 一个事务多次读取同一数据，在该事务还未结束时，另一个事务也对该数据进行了操作，而且在第一个事务两次次读取之间，第二个事务对数据进行了更新，那么第一个事务前后两次读取到的数据是不同的，这样就造成了不可重复读。我用坦克你改成飞机 3) Phantom Reads 幻像读 第一个数据正在查询符合某一条件的数据，这时，另一个事务又插入了一条符合条件的数据，第一个事务在第二次查询符合同一条件的数据时，发现多了一条前一次查询时没有的数据，仿佛幻觉一样，这就是幻像读。你吃黄色面包给你塞坨黄色物体 非重复度和幻像读的区别： 非重复读是指同一查询在同一事务中多次进行，由于其他提交事务所做的修改或删除，每次返回不同的结果集，此时发生非重复读。幻像读是指同一查询在同一事务中多次进行，由于其他提交事务所做的插入操作，每次返回不同的结果集，此时发生幻像读。表面上看，区别就在于非重复读能看见其他事务提交的修改和删除，而幻像能看见其他事务提交的插入。 ​ 2.隔离级别： 1) DEFAULT （默认） 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别。另外四个与JDBC的隔离级别相对应。 2) READ_UNCOMMITTED （读未提交） 这是事务最低的隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。 3) READ_COMMITTED （读已提交） 保证一个事务修改的数据提交后才能被另外一个事务读取，另外一个事务不能读取该事务未提交的数据。这种事务隔离级别可以避免脏读出现，但是可能会出现不可重复读和幻像读。 4) REPEATABLE_READ （可重复读） 这种事务隔离级别可以防止脏读、不可重复读，但是可能出现幻像读。它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了不可重复读。 5) SERIALIZABLE（串行化） 这是花费最高代价但是最可靠的事务隔离级别，事务被处理为顺序执行。除了防止脏读、不可重复读外，还避免了幻像读。 timeout：超时时间 （1）事务需要在一定时间内进行提交，如果不提交进行回滚 （2）默认值是 -1 ，设置时间以秒单位进行计算 readOnly：是否只读 （1）读：查询操作，写：添加修改删除操作 （2）readOnly默认值false，表示可以查询，可以添加修改删除操作 （3）设置readOnly值是true，设置成true之后，只能查询 rollbackFor：回滚 （1）设置出现哪些异常进行事务回滚 noRollbackFor：不回滚 （1）设置出现哪些异常不进行事务回滚 7.事务操作（XML） 续用上面示例，添加一下配置实现以XML方式实现事物操作，去掉@Transaction注解后复制测试方法进行测试。 1234567891011121314&lt;!--添加aop命名空间--&gt; &lt;!--无需事务注解配置--&gt;&lt;!-- &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;&gt;&lt;/tx:annotation-driven&gt;--&gt; &lt;!--配置通知--&gt; &lt;tx:advice id=&quot;txadvice&quot;&gt; &lt;!--配置事务参数--&gt; &lt;tx:attributes&gt; &lt;!--指定哪种规则的方法上面添加事务--&gt; &lt;tx:method name=&quot;accountMoney&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;!--&lt;tx:method name=&quot;account*&quot;/&gt;--&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!--3 配置切入点和切面--&gt; &lt;aop:config&gt; &lt;!--配置切入点--&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* com.lxl.spring5.transactionl.service.UserService.*(..))&quot;/&gt; &lt;!--配置切面--&gt; &lt;aop:advisor advice-ref=&quot;txadvice&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:config&gt; 8.事务操作（完全注解） 添加配置类 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.lxl.spring5.transactionl;import com.alibaba.druid.pool.DruidDataSource;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.datasource.DataSourceTransactionManager;import org.springframework.transaction.annotation.EnableTransactionManagement;import javax.sql.DataSource;@Configuration//配置类@ComponentScan(basePackages = &quot;com.lxl.spring5.transactionl&quot;)//组件扫描@EnableTransactionManagement// 开启事务public class TxConfig { // 创建数据库连接池 @Bean public DruidDataSource getDruidDataSource() { DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); dataSource.setUrl(&quot;jdbc:mysql://localhost:3306/learn?useUnicode=true&amp;characterEncoding=utf8&quot;); dataSource.setUsername(&quot;root&quot;); dataSource.setPassword(&quot;root&quot;); return dataSource; } //创建JdbcTemplate对象 @Bean public JdbcTemplate getJdbcTemplate(DataSource dataSource) { //到ioc容器中根据类型找到dataSource JdbcTemplate jdbcTemplate = new JdbcTemplate(); // 注入dataSource jdbcTemplate.setDataSource(dataSource);return jdbcTemplate;} //创建事务管理器 @Bean public DataSourceTransactionManager getDataSourceTransactionManager(DataSource dataSource) { DataSourceTransactionManager transactionManager = new DataSourceTransactionManager(); transactionManager.setDataSource(dataSource); return transactionManager; }} 测试 123456789 //全注解方式测试// 先去掉@Transaction注解 @Test public void transactionAllAnnoTest() { ApplicationContext context = new AnnotationConfigApplicationContext(TxConfig.class); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); userService.accountMoney(); } Spring5新功能 Log4j2 整个Spring5框架的代码基于Java8，运行时兼容JDK9，许多不建议使用的类和方法在代码库中删除Spring 5.0框架自带了通用的日志封装 （1）Spring5已经移除Log4jConfigListener，官方建议使用Log4j2 （2）Spring5框架整合Log4j2 第一步 引入jar包 准备 导入jar包并导入依赖【2021.12 Log4j2.17.0 RCE(CVE-2021-44832)漏洞】生产建议使用2.17.1以上 创建配置文件log4j2 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--日志级别以及优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL --&gt;&lt;!--Configuration后面的status用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，可以看到log4j2内部各种详细输出--&gt;&lt;!--设置INFO会输出前面级别所有日志--&gt;&lt;configuration status=&quot;INFO&quot;&gt; &lt;!--先定义所有的appender--&gt; &lt;appenders&gt; &lt;!--输出日志信息到控制台--&gt; &lt;console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!--控制日志输出的格式--&gt; &lt;PatternLayout pattern=&quot;%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot;/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;!--然后定义logger，只有定义了logger并引入的appender，appender才会生效--&gt; &lt;!--root：用于指定项目的根日志，如果没有单独指定Logger，则会使用root作为默认的日志输出--&gt; &lt;loggers&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; 任意运行程序可见日志，更改 查看不同级别日志。 手动加日志 12345@Testpublic void manualLogTest(){ final Logger log = LoggerFactory.getLogger(transactionTest.class); log.warn(&quot;hello log4j==============&quot;);} @Nullable Spring5框架核心容器支持@Nullable注解 （1）@Nullable注解可以使用在方法上面，属性上面，参数上面，表示方法返回可以为空，属性值可以为空，参数值可以为空 （2）注解用在方法上面，方法返回值可以为空 （3）注解使用在方法参数里面，方法参数可以为空 （4）注解使用在属性上面，属性值可以为空 GenericApplicationContext Spring5核心容器支持函数式风格GenericApplicationContext,把自己new的对象加入搜spring管理中。 123456789101112//函数式风格创建对象，交给spring进行管理 @Test public void testGenericApplicationContext() { //1 创建GenericApplicationContext对象 GenericApplicationContext context = new GenericApplicationContext(); //2 调用context的方法对象注册 context.refresh(); context.registerBean(&quot;user1&quot;,User.class,() -&gt; new User()); //3 获取在spring注册的对象 // User user = (User)context.getBean(&quot;com.atguigu.spring5.test.User&quot;); User user = (User)context.getBean(&quot;user1&quot;); System.out.println(user); } Spring5支持整合JUnit5 （1）整合JUnit4 第一步 引入Spring相关针对测试依赖 测试代码 1234567891011121314151617181920212223package com.lxl.spring5.JunitTest;import com.lxl.spring5.transactionl.service.UserService;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)//单元测试框架 @ContextConfiguration(&quot;classpath:transaction1.xml&quot;)//加载配置文件public class JunitTest4 { @Autowired private UserService userService; @Test public void test1() { userService.accountMoney(); }} （2）Junit5 idea点击@Test导入包 1234567891011121314151617181920212223package com.lxl.spring5.JunitTest;import com.lxl.spring5.transactionl.service.UserService;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit.jupiter.SpringExtension;import org.springframework.test.context.junit.jupiter.SpringJUnitConfig;//@ExtendWith(SpringExtension.class)//@ContextConfiguration(&quot;classpath:transaction1.xml&quot;)//SpringJUnitConfig效果同上面两个同时使用@SpringJUnitConfig(locations = &quot;classpath:transaction1.xml&quot;)public class JunitTest5 { @Autowired private UserService userService; @Test public void test1() { userService.accountMoney(); }} Webflux 1、SpringWebflux介绍 （1）是Spring5添加新的模块，用于web开发的，功能和SpringMVC类似的，Webflux使用当前一种比较流程响应式编程出现的框架。 （2）使用传统web框架，比如SpringMVC，这些基于Servlet容器，Webflux是一种异步非阻塞的框架，异步非阻塞的框架在Servlet3.1以后才支持，核心是基于Reactor的相关API实现的。 （3）解释什么是异步非阻塞 异步和同步 非阻塞和阻塞 上面都是针对对象不一样 异步和同步针对调用者，调用者发送请求，如果等着对方回应之后才去做其他事情就是同步，如果发送请求之后不等着对方回应就去做其他事情就是异步阻塞和非阻塞针对被调用者，被调用者受到请求之后，做完请求任务之后才给出反馈就是阻塞，受到请求之后马上给出反馈然后再去做事情就是非阻塞 （4）Webflux特点： 第一 非阻塞式：在有限资源下，提高系统吞吐量和伸缩性，以Reactor为基础实现响应式编程 第二 函数式编程：Spring5框架基于java8，Webflux使用Java8函数式编程方式实现路由请求 （5）比较SpringMVC 第一 两个框架都可以使用注解方式，都运行在Tomet等容器中 第二 SpringMVC采用命令式编程，Webflux采用异步响应式编程 时间紧迫，未完待续","link":"/2021/02/24/Draft/2021/Spring/"},{"title":"Java基础深入","text":"新学四问 WHY【与前代优化了什么，弥补了什么空白】筑基，越深越稳 WHAT【框架，思维导图，主题框架】容器、并发、IO、NET、JVM、其他 HOW【如何记忆，学习资源】:https://github.com/CyC2018、博客完善、书籍深入、ANKI稳固、熟悉基础数据结构 LEVEL【不是每个都学精】精通 综合问题 增强for和普通for区别？ ​ 需要循环数组结构的数据时，建议使用普通for循环，因为for循环采用下标访问，对于数组结构的数据来说，采用下标访问比较好。需要循环链表结构的数据时，一定不要使用普通for循环，这种做法很糟糕，数据量大的时候有可能会导致系统崩溃。for循环是随机读取，增强for的底层是迭代器，属于顺序读取。ArrayList实现了RandomAccess标记性接口，可以显著提升随机读取的效率；而linkedlist没有实现RandomAccess接口，随机读取会很慢 数据结构 哈希 **核心理论：**Hash也称散列、哈希，对应的英文都是Hash。基本原理就是把任意长度的输入，通过Hash算法变成固定长度的输出。这个映射的规则就是对应的Hash算法，而原始数据映射后的二进制串就是哈希值。 Hash的特点： 1.hash值不可以反向推导出原始的数据 2.输入数据的微小变化会得到完全不同的hash值，相同的数据会得到相同的值 3.哈希算法的执行效率要高效，长的文本也能快速地计算出哈希值 4.hash算法的冲突概率要小 由于hash的原理是将输入空间的值映射成hash空间内，而hash值的空间远小于输入的空间。根据抽屉原理，一定会存在不同的输入被映射成相同输出的情况。抽屉原理：桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面放不少于两个苹果，这一现象就是我们所说的抽屉原理。 红黑树 R-B Tree，全称是Red-Black Tree，又称为“红黑树”，它一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红(Red)或黑(Black)。 红黑树的特性: （1）每个节点或者是黑色，或者是红色。 （2）根节点是黑色。 （3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！] （4）如果一个节点是红色的，则它的子节点必须是黑色的。 （5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 注意： (01) 特性(3)中的叶子节点，是只为空(NIL或null)的节点。 (02) 特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。 红黑树示意图如下： 如何阅读源码？ 方法： 1.实践断点 2.顺序：接口$\\longrightarrow$实现类（(方法【属性--》判断】)构造器方法$\\longrightarrow$常用方法$\\longrightarrow$其他方法） 3.工具：IDEA 的 Diagram 图标化查看继承结构，所有类的所有结构，双击跳转对应类.左下角Structure查看类结构。ctrl+H查看继承树。 4.注释：查看注释 5.快捷键： 6.根据已知看未知：比如已知底层看源码，线程安全看原理 项目 JDK 常用包：【java.io java.lang java.util】 【java.lang.reflect java.net javax.net.* java.nio.* java.util.concurrent.】 框架 容器 学习流程 进度： collection$\\longrightarrow$list$\\longrightarrow$set$\\longrightarrow$map$\\longrightarrow$queue 方法： 总结特点$\\longrightarrow$根据特点查看源码 $\\longrightarrow$笔记 总结 总关系图 表格总结 集合名称 结构 是否可重复 是否线程安全 是否可为空 是否有序 扩容倍数 增 删 改 查 List Vector 动态数组【Object [ ] 】 可重 安 多空共存 有序 2 ArrayList 动态数组【Object [ ] 】 可重 不安 多空共存 有序 1.5 尾💚 尾💚 💚 LinkedList 双向循环链表 可重 不安 多空共存 有序 链表无 💚 💚 Set TreeSet 红黑树【red-black tree】 不重 不安 默认非空，自定义实现Comparator接口可存储null 有序 红黑树无 O(logN) HashSet 哈希表【HashMap】 不重 不安 多空单存 无序 2 💚 O(1)💚 LinkedHashSet 双向链表【LinkedHashMap】 不重 不安 多空单存 有序 2 Map HashMap 数组+链表（1.8之后链表长度大于默认8且数组长度大于64转换为红黑树），散列（哈希）表 值可重 不安 多空键值或单空键或值 无序 2 TreeMap 红黑树（自平衡的排序二叉树） 值可重 不安 可值空共存 有序 红黑树无 LinkedHashMap 底层哈希表+双向链表，HashMap子类 值可重 不安 多空键值或单空键或值 有序 2 ConcurrentHashMap 1.8之后 数组、链表/红黑树 适合多线程 值可重 安 非空 2 【保留】Hashtable 安 Queue PriorityQueue 二叉小顶堆，每次取出权值最小元素，先进先出 安 非 Collection 集合根接口，定义子类基础操作 遍历方式 foreach Iterator listiterator:区别：List专用、遍历时可添加元素、可逆向遍历、可定位当前索引位置、可修改遍历对象。 12345Collection&lt;Person&gt; persons = new ArrayList&lt;Person&gt;();Iterator iterator = persons.iterator();while (iterator.hasNext()) { System.out.println(iterator.next); } aggregate operations 123456789Collection&lt;Person&gt; persons = new ArrayList&lt;Person&gt;();persons .stream() .forEach(new Consumer&lt;Person&gt;() { @Override public void accept(Person person) { System.out.println(person.name); } }); 1234567891011121314//在 JDK 8 以后，推荐使用聚合操作对一个集合进行操作。聚合操作通常和 lambda 表达式结合使用，让代码看起来更简洁（因此可能更难理解）。下面举几个简单的栗子：//1.使用流来遍历一个 ShapesCollection，然后输出红色的元素：myShapesCollection.stream() .filter(e -&gt; e.getColor() == Color.RED) .forEach(e -&gt; System.out.println(e.getName()));//你还可以获取一个并行流（parallelStream），当集合元素很多时使用并发可以提高效率：myShapesCollection.parallelStream() .filter(e -&gt; e.getColor() == Color.RED) .forEach(e -&gt; System.out.println(e.getName())); //聚合操作还有很多操作集合的方法，比如说你想把 Collection 中的元素都转成 String 对象，然后把它们 连起来：String joined = elements.stream() .map(Object::toString) .collect(Collectors.joining(&quot;, &quot;));//Thanks:https://blog.csdn.net/u011240877/article/details/52773577 List 插入有序可重可多空 Vector 底层数组，同步，同步让其比AL慢，内存不够默认扩100% 特点 原因（源码） 与 ArrayList 相似 线程安全 方法中有synchronized，所以性能不如ArrayList 动态扩容为原来的2倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); 可排序扩容为原来两倍 123456789101112private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; //扩容为原来两倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);} ArrayList 特点 原因（源码） 底层数组 new Object[initialCapacity]; 查优，尾增删快，其他地方增删慢 底层数组 动态扩容原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 线程不安全 方法中没有synchronized/**This class is roughly equivalent to * Vector, except that it is unsynchronized.*/ 支持快速随机访问 RandomAccess 动态扩容 123456789101112131415161718/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */ private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; //====== 动态扩容：位运算 &gt;&gt;：右移运算符 oldCapacity &gt;&gt; 1 ---》 M &gt;&gt; n = M / 2^n 即扩容50%====== int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } 参考：位运算：M &gt;&gt; n = M / 2^n 总结 LinkedList 特点 原因（源码） 底层双向链表，增删优，可操作头尾，用作栈、（双向）队列 底层双向链表 可插入空 底层双向链表 查询慢 底层双向链表，会遍历整个链表 线程不安全 方法中没有synchronized 底层双向链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 /** * Returns the (non-null) Node at the specified element index. */ Node&lt;E&gt; node(int index) { // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) { Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; } }/** * Inserts all of the elements in the specified collection into this * list, starting at the specified position. Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices). The new elements will appear * in the list in the order that they are returned by the * specified collection's iterator. * * @param index index at which to insert the first element * from the specified collection * @param c collection containing elements to be added to this list * @return {@code true} if this list changed as a result of the call * @throws IndexOutOfBoundsException {@inheritDoc} * @throws NullPointerException if the specified collection is null */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) { checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; Node&lt;E&gt; pred, succ; if (index == size) { succ = null; pred = last; } else { succ = node(index); pred = succ.prev; } for (Object o : a) { @SuppressWarnings(&quot;unchecked&quot;) E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; } if (succ == null) { last = pred; } else { pred.next = succ; succ.prev = pred; } size += numNew; modCount++; return true; } 总结 Stack 后进先出的堆栈 Set 无序不重单空，常用于去重 TreeSet 特点 原因（源码） 底层TreeMap 底层、可排序 可排序【自然排序，定制排序】 底层、可排序 线程不安全 方法中没有synchronized 默认非空，自定义实现Comparator接口可存储null 底层、可排序底层、可排序 1234567891011121314151617181920212223public interface NavigableSet&lt;E&gt; extends SortedSet&lt;E&gt; {}public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable{}// Integer能排序(有默认顺序), String能排序(有默认顺序)，可以对对象元素进行排序，但是自定义类需要实现comparable接口，重写comparaTo() 方法,否则：{@code ClassCastException}报ClassCastException异常。所有元素必须可以相互比较（相同类型），否则将会报类型转换异常ClassCastExection /** * Constructs a new, empty tree set, sorted according to the * natural ordering of its elements. All elements inserted into * the set must implement the {@link Comparable} interface. * Furthermore, all such elements must be &lt;i&gt;mutually * comparable&lt;/i&gt;: {@code e1.compareTo(e2)} must not throw a * {@code ClassCastException} for any elements {@code e1} and * {@code e2} in the set. If the user attempts to add an element * to the set that violates this constraint (for example, the user * attempts to add a string element to a set whose elements are * integers), the {@code add} call will throw a * {@code ClassCastException}. */ public TreeSet() { this(new TreeMap&lt;E,Object&gt;()); } TreeSet的排列顺序与重写的compareTo()方法的返回值有关。 return 0:元素每次进行比较，都认为是相同的元素，这是就不再向TreeSet里面插入除第一个元素以外的元素，所以TreeSet中就只插入了一个元素。 return 1:元素每次进行比较，都认为新插入的元素比上一个元素大，于是二叉树存储时，会储存在根的右侧，读取时就是正序排列，先进先出。 return -1:元素每次进行比较，都认为新插入的元素比上一个元素小，于是二叉树存储时，会储存在根的左侧，读取时就是倒序排列，先进后出。 HashSet 学习前最好先学习 HashMap 特点 原因（源码） 底层HashMap 底层HashMap 可单空 Map的key唯一 善存取 散列表定位 无序 没继承SortedSet 不安 方法中没有synchronized 不重复 不重复 2倍扩容 同hashmap 底层HashMap 123456789101112131415161718192021 private transient HashMap&lt;E,Object&gt; map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object();/** * Constructs a new set containing the elements in the specified * collection. The &lt;tt&gt;HashMap&lt;/tt&gt; is created with default load factor * (0.75) and an initial capacity sufficient to contain the elements in * the specified collection. * * @param c the collection whose elements are to be placed into this set * @throws NullPointerException if the specified collection is null */public HashSet(Collection&lt;? extends E&gt; c) { //初始16，加载因子0.75 map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c);}//HashSet中的元素都存放在HashMap的key上面，而value中的值都是统一的一个固定对象private static final Object PRESENT = new Object(); 不重复 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * Adds the specified element to this set if it is not already present. * More formally, adds the specified element &lt;tt&gt;e&lt;/tt&gt; to this set if * this set contains no element &lt;tt&gt;e2&lt;/tt&gt; such that * &lt;tt&gt;(e==null&amp;nbsp;?&amp;nbsp;e2==null&amp;nbsp;:&amp;nbsp;e.equals(e2))&lt;/tt&gt;. * If this set already contains the element, the call leaves the set * unchanged and returns &lt;tt&gt;false&lt;/tt&gt;. * * @param e element to be added to this set * @return &lt;tt&gt;true&lt;/tt&gt; if this set did not already contain the specified * element */public boolean add(E e) { return map.put(e, PRESENT)==null;} /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) { return putVal(hash(key), key, value, false, true);} /** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;} LinkedHashSet 特点 原因（源码） 底层链表【LinkedHashMap】 底层 不安 方法中没有synchronized 插入有序 LinkedHashSet在遍历时获得的迭代器是LinkedHashSet所实现的LinkedKeyIterator，它的遍历是从双向链表头开始顺序遍历，实现了有序输出。 不重复 类似不重复 底层 1234567891011121314151617181920212223242526272829303132333435/** * Constructs a new linked hash set with the same elements as the * specified collection. The linked hash set is created with an initial * capacity sufficient to hold the elements in the specified collection * and the default load factor (0.75). * * @param c the collection whose elements are to be placed into * this set * @throws NullPointerException if the specified collection is null */ public LinkedHashSet(Collection&lt;? extends E&gt; c) { super(Math.max(2*c.size(), 11), .75f, true); addAll(c); } public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable {} /** * Constructs a new, empty linked hash set. (This package private * constructor is only used by LinkedHashSet.) The backing * HashMap instance is a LinkedHashMap with the specified initial * capacity and the specified load factor. * * @param initialCapacity the initial capacity of the hash map * @param loadFactor the load factor of the hash map * @param dummy ignored (distinguishes this * constructor from other int, float constructor.) * @throws IllegalArgumentException if the initial capacity is less * than zero, or if the load factor is nonpositive */ HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } Map 键值对，键不重 HashMap 推荐资源：博客1 特点 原因（源码） 底层为数组称之为哈希桶，每个桶里面放的是链表，链表中的每个节点，就是哈希表中的每个元素。JDK8后，链表容量大于8且桶的容量大于64，转化成红黑树 底层 线程不安全 方法中没有synchronized 默认长度16 初始容量 扩容为原来的两倍 扩容 无序 无序,可空 多空键值或单空键或值 无序,可空 底层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475 /** Node 单向链表 * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; //碰撞之后形成链表 Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + &quot;=&quot; + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; }}/**红黑树 * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn * extends Node) so can be used as extension of either regular or * linked node. */static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; { TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) { super(hash, key, val, next); } /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() { for (TreeNode&lt;K,V&gt; r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } } ......} 扩容 参考 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118 /** 为什么需要扩容：哈希冲突导致的链化影像查找效率，数组以空间换时间 由power-of-two expansion newCap = oldCap &lt;&lt; 1 可知扩容为原来的两倍1010 十进制：10 原始数 number10100 十进制：20 左移一位 number = number &lt;&lt; 1; 1010 十进制：10 右移一位 number = number &gt;&gt; 1; 即 oldCap &lt;&lt; 1 即二进制向左移动两位：oldCap &lt;&lt; 1=oldCap*2 同理oldCap &gt;&gt; 1=oldCap/2 补充&gt;&gt;&gt;：无符号右移，忽略符号位，空位都以0补齐 * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * * @return the table */ final Node&lt;K,V&gt;[] resize() { //oldTab扩容前哈希表 Node&lt;K,V&gt;[] oldTab = table; //扩容之前数组长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; //扩容之前的阈值，触发本次扩容的阈值 int oldThr = threshold; //扩容之后的数组大小 //下次触发扩容的条件 int newCap, newThr = 0; //散列表已初始化，这是一次正常的扩容 if (oldCap &gt; 0) { //扩容之前数组大小达到最大阈值，则不扩容且设扩容条件为int最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } //oldCap左移一位实现数值翻倍，并且复制给newCap，newCap小于最大限制且扩容之前的数组长度 &gt;= 16 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } //oldCap==0,说明HashMap中的散列表是null //1.new HashMap（initCap，loadFactor）； //2.new HashMap（initCap）； //3.new HashMap（map）；且map有数据 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; //oldCap==0，oldThr==0 //new HashMap（map）; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;//16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//12 } //newThr为0时，通过newCap和loadFactor计算出一个newThr if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; //第一次或创建一个更大的数组 @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //扩容前数组不为空 if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { //当前node节点 Node&lt;K,V&gt; e; //当前桶位中有数据，但是数据具体是单个数据或链表或红黑树还不明确 if ((e = oldTab[j]) != null) { //方便JVM GC回收 oldTab[j] = null; //当前节点为单数据 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //当前节点已经树化 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //当前节点为链表 else { // preserve order //loHead低位链表，存放在扩容之后的数组的下标位置，与当前数组的下标位置一致。 //hiHead高位链表，存放在扩容之后的数组的下标位置为当前数组的下标位置+扩容之前的长度。 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); //低位链表有数据 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 初始容量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/**默认（缺省）数组长度 * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /**最大数组长度 * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /**缺省负载因子大小 * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /**转换为红黑树的单个链表元素阈值 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash);看下面方法 } * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */ static final int TREEIFY_THRESHOLD = 8;/** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */ final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } /**从红黑树降为链表的阈值 * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */ static final int UNTREEIFY_THRESHOLD = 6; /**转换为红黑树的数组阈值 * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; /** 哈系桶（表） * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */ transient Node&lt;K,V&gt;[] table; /** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; /**当前哈希表元素个数 * The number of key-value mappings contained in this map. */ transient int size; /**修改次数，不包含替换 * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */ transient int modCount; /**扩容阈值，当哈希表元素超过阈值时触发扩容， * The next size value at which to resize (capacity * load factor). * * @serial */ // (The javadoc description is true upon serialization. // Additionally, if the table array has not been allocated, this // field holds the initial array capacity, or zero signifying // DEFAULT_INITIAL_CAPACITY.) int threshold; /**负载因子，threshold = capacity * loadFactor * The load factor for the hash table. * * @serial */ final float loadFactor; 无序,可空 未解决问题:为什么有时候看起来有序？ 答： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124 /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } //====Key为null情况//假设 putVal(hash(null), null, value, false, true);//即 putVal(0, 0, value, false, true);//由 if ((p = tab[i = (n - 1) &amp; hash]) == null)// tab[i] = newNode(hash, key, value, null);//得tab[0]==null --&gt; tab[0] = newNode(hash, key, value, null);//key为null放在tab[0] /** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. *///作用：在table长度不长的情况下，能让key的hash值的高16位也参与路由运算,//异或：同0异1//h=0b 0010 0101 1010 1100 0011 1111 0010 1110// 0b 0010 0101 1010 1100 0011 1111 0010 1110// ^// 0b 0000 0000 0000 0000 0010 0101 1010 1100//=&gt; 0010 0101 1010 1100 0001 1010 1000 0010 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); }/** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * onlyIfAbsent true==》key存在，不插入 * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { //tab 当前散列表 //p 当前散列表元素 //n 当前散列表数组长度 //i 路由寻址结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //延迟初始化，第一次调用putVal时会初始化hashMap对象中最消耗内存的散列表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //最简单情况，寻址找到的桶位为null，直接放入数据 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { //e 不为null 找到了一个与当前要插入的key-value一致的 key元素 Node&lt;K,V&gt; e; K k; //表示桶位中的该元素与当前插入元素key完全一致，后续将替换 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //该元素已经树化时 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //链表时且不等于头元素，元素依次比较 else { for (int binCount = 0; ; ++binCount) { //最后一个元素还找不到同插入key相同的node if ((e = p.next) == null) { //插入最后 p.next = newNode(hash, key, value, null); //链表元素数量大于8，树化 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } //链表中找到同插入key的node，替换 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } //上面的e存在替换值 if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } //记录修改次数，替换不算 ++modCount; //插入后容量大于扩容阈值则扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } GET() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Returns the value to which the specified key is mapped, * or {@code null} if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * {@code k} to a value {@code v} such that {@code (key==null ? k==null : * key.equals(k))}, then this method returns {@code v}; otherwise * it returns {@code null}. (There can be at most one such mapping.) * * &lt;p&gt;A return value of {@code null} does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to {@code null}. * The {@link #containsKey containsKey} operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;}/** * Implements Map.get and related methods. * * @param hash hash for key * @param key the key * @return the node, or null if none */final Node&lt;K,V&gt; getNode(int hash, Object key) { //tab:引用当前 hashmap的散列表 //first:桶位中的头元素 //e:临时node元素 //n: table数组长度 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { //第一种情况：定位出来的桶位元素即为咱们要get的数据 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //说明当前桶位不止一个元素，可能是链表也可能是红黑树 if ((e = first.next) != null) { //第二种情况，桶升级成了红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //桶形成了链表 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null;} Remove（） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * Implements Map.remove and related methods. * * @param hash hash for key * @param key the key * @param value the value to match if matchValue, else ignored * @param matchValue if true only remove if value is equal * @param movable if false do not move other nodes while removing * @return the node, or null if none */final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { //tab:引用当前 hashmap中的散列表 //p:当前node元素 //n:表示散列表数组长度 //index:表示寻址结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) { //说明路由的桶位是有数据的，需要进行查找操作，并且删除 //node:查找到的结果 //e:当前Node的下一个元素 Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) { //说明，当前桶位要么是链表要么是红黑树 if (p instanceof TreeNode)//判断当前桶位是否升级为红黑树了 //第二种情况：红黑树查找 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else { //第三种链表 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } //判断node不为空的话，说明按照key査找到需要删除的数据了 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) { //第一种情況：node是树节点，说明需要进行树节点移除操作 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //第二种情況：桶位元素即为査找结果，则将该元素的下一个元素放至桶位中 else if (node == p) tab[index] = node.next; else //第三种情況：将当前元素p的下一个元素设置成要删除元素的下一个元素 p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; } } return null;} 构造方法 ：&gt;&gt;&gt;无符号右移，忽略符号位，空位都以0补齐 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/* ---------------- Public operations -------------- */ /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) { //限制数组与loadFactor 0&lt;initialCapacity&lt;MAXIMUM_CAPACITY // loadFactor&gt;0 if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; //tableSizeFor 返回一个大于等于当前cap的一个数字，并且这个数字一定是2的次方数，源码如下： this.threshold = tableSizeFor(initialCapacity); } /**返回一个大于等于当前cap的一个数字，并且这个数字一定是2的次方数： * &gt;&gt;&gt;：无符号右移，忽略符号位，空位都以0补齐 * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */ public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } /** * Constructs a new &lt;tt&gt;HashMap&lt;/tt&gt; with the same mappings as the * specified &lt;tt&gt;Map&lt;/tt&gt;. The &lt;tt&gt;HashMap&lt;/tt&gt; is created with * default load factor (0.75) and an initial capacity sufficient to * hold the mappings in the specified &lt;tt&gt;Map&lt;/tt&gt;. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public HashMap(Map&lt;? extends K, ? extends V&gt; m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); } LinkedHashMap 特点 原因（源码） 底层哈希表+双向链表，HashMap子类 底层 线程不安全 方法中没有synchronized 有序 有序 多空键值或单空键或值 同HashMap 扩容两倍 同hashmap 底层 put、resize扩容均使用HashMap的方法，拥有HashMap所有特性。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt;{} /** * HashMap.Node subclass for normal LinkedHashMap entries. */ static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); } } /**双向链表头 * The head (eldest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; head; /**双向链表尾 * The tail (youngest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; tail; /** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */ final boolean accessOrder;/** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the specified initial capacity and a default load factor (0.75). * * @param initialCapacity the initial capacity * @throws IllegalArgumentException if the initial capacity is negative */ public LinkedHashMap(int initialCapacity) { super(initialCapacity); accessOrder = false; } /** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the default initial capacity (16) and load factor (0.75). */ public LinkedHashMap() { super(); accessOrder = false; } /** * Constructs an insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with * the same mappings as the specified map. The &lt;tt&gt;LinkedHashMap&lt;/tt&gt; * instance is created with a default load factor (0.75) and an initial * capacity sufficient to hold the mappings in the specified map. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) { super(); accessOrder = false; putMapEntries(m, false); } /** * Constructs an empty &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with the * specified initial capacity, load factor and ordering mode. * * @param initialCapacity the initial capacity * @param loadFactor the load factor *=====此构造方法accessOrder为true时实现了按访问顺序存储元素====== *@param accessOrder the ordering mode - &lt;tt&gt;true&lt;/tt&gt; for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder; } 有序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105 /** * HashMap.Node subclass for normal LinkedHashMap entries. */ static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { //记录相邻两个key-value对象 Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); } } /**双向链表头 * The head (eldest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; head; /**双向链表尾 * The tail (youngest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; tail;/**====false则按插入顺序存储元素，如果是true则按访问顺序存储元素 * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */ final boolean accessOrder; Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) { LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p; } /** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */ final boolean accessOrder; // internal utilities//实现有序的方法//举例:放入1,2,3//1进：head=tail=1 last=null // 3进：tail=last=1--》tail=3 3.before=1 --》 1.after=3 // 2进：tail=last=3--》tail=2 2.before=3 --》 3.after=2 // link at the end of list private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) { LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; if (last == null) head = p; else { p.before = last; last.after = p; } }//HashMap的Node===============/** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */ static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + &quot;=&quot; + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; } } TreeMap 特点 原因（源码） 底层二叉树【红黑树】 底层红黑树 线程不安全 方法中没有synchronized 有序，比较通过key 底层红黑树 多值可空 底层红黑树 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697//NavigableMap接口提供针对Key的有序访问，public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable, java.io.Serializable{ /** * The comparator used to maintain order in this tree map, or * null if it uses the natural ordering of its keys. * * @serial */ private final Comparator&lt;? super K&gt; comparator; private transient Entry&lt;K,V&gt; root; /** * The number of entries in the tree */ private transient int size = 0; /** * The number of structural modifications to the tree. */ private transient int modCount = 0; /** * Node in the Tree. Doubles as a means to pass key-value pairs back to * user (see Map.Entry). */ static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color = BLACK; /** * Make a new cell with given key, value, and parent, and with * {@code null} child links, and BLACK color. */ Entry(K key, V value, Entry&lt;K,V&gt; parent) { this.key = key; this.value = value; this.parent = parent; } /** * Returns the key. * * @return the key */ public K getKey() { return key; } /** * Returns the value associated with the key. * * @return the value associated with the key */ public V getValue() { return value; } /** * Replaces the value currently associated with the key with the given * value. * * @return the value associated with the key before this method was * called */ public V setValue(V value) { V oldValue = this.value; this.value = value; return oldValue; } public boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; return valEquals(key,e.getKey()) &amp;&amp; valEquals(value,e.getValue()); } public int hashCode() { int keyHash = (key==null ? 0 : key.hashCode()); int valueHash = (value==null ? 0 : value.hashCode()); return keyHash ^ valueHash; } public String toString() { return key + &quot;=&quot; + value; } }} HashTable 线程安全，内部的方法基本都经过 synchronized，保留类不建议使用 ConcurrentHashMap 特点 原因（源码） 底层数组、链表、红黑树 底层 线程安全 线程安全 动态扩容为原来的2倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); 都安全但是ConcurrentHashMap 比 HashTable 效率要高 ConcurrentHashMap 锁粒度更细 非空 JDK1.7 : 【数组（Segment） + 数组（HashEntry） + 链表（HashEntry节点）】 ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。 JDK1.8 : Node数组+链表 / 红黑树 利用CAS+Synchronized来保证并发更新的安全，底层依然采用数组+链表+红黑树的存储结构。 底层 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. */ transient volatile Node&lt;K,V&gt;[] table; /** * The next table to use; non-null only while resizing. */ private transient volatile Node&lt;K,V&gt;[] nextTable; /** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */ private transient volatile long baseCount; /** * hash表初始化或扩容时的一个控制位标识量。 * 负数代表正在进行初始化或扩容操作 * -1代表正在初始化 * -N 表示有N-1个线程正在进行扩容操作 * 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小 * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. */ private transient volatile int sizeCtl; /** * The next table index (plus one) to split while resizing. */ private transient volatile int transferIndex; /** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */ private transient volatile int cellsBusy; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; Queue 先进先出 PriorityQueue 特点 原因（源码） 底层二叉小顶堆 保证每次取出的元素是队列中权值最小的 方法中有synchronized，所以性能不如ArrayList 非null int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); 工具类 Collections 确保内容不被修改 unmodifiableMap unmodifiableList unmodifiableSet Arrays Comparable和Comparator ​ Comparable是排序接口；若一个类实现了Comparable接口，就意味着“该类支持排序”。而Comparator是比较器；我们若需要控制某个类的次序，可以建立一个“该类的比较器”来进行排序。Comparable相当于“内部比较器”，而Comparator相当于“外部比较器”。 容器综合问题 JUC 问题：什么时候该用多线程？ ​ 在Java中，线程部分是一个重点，本篇文章说的JUC也是关于线程的。JUC就是java.util .concurrent工具包的简称。这是一个处理线程的工具包，JDK 1.5开始出现的。 多线程基础 进程(Process) 把一个任务称为一个进程 进程内部还需要同时执行多个子任务称为线程 进程和线程的关系就是：一个进程可以包含一个或多个线程（Thread），但至少会有一个线程。 多进程模式（每个进程只有一个线程） 多线程模式（一个进程有多个线程） 多进程＋多线程模式（复杂度最高） 和多线程相比， 多进程缺点在于：创建进程比创建线程开销大，尤其是在Windows系统上；进程间通信比线程间通信要慢，因为线程间通信就是读写同一个变量，速度很快。 多进程的优点在于：多进程稳定性比多线程高，因为在多进程的情况下，一个进程崩溃不会影响其他进程，而在多线程的情况下，任何一个线程崩溃会直接导致整个进程崩溃。 多线程 Java语言内置了多线程支持：一个Java程序实际上是一个JVM进程，JVM进程用一个主线程来执行main()方法，在main()方法内部，我们又可以启动多个线程。此外，JVM还有负责垃圾回收的其他工作线程等。 因此，对于大多数Java程序来说，我们说多任务，实际上是说如何使用多线程实现多任务。 ​ 和单线程相比，多线程编程的特点在于：多线程经常需要读写共享数据，并且需要同步。例如，播放电影时，就必须由一个线程播放视频，另一个线程播放音频，两个线程需要协调运行，否则画面和声音就不同步。因此，多线程编程的复杂度高，调试更困难。 ​ Java多线程编程的特点又在于：多线程模型是Java程序最基本的并发模型；后续读写网络、数据库、Web开发等都依赖Java多线程模型。 管程 ​ 管程(monitor)是保证了同一时刻只有一个进程在管程内活动,即管程内定义的操作在同一时刻只被一个进程调用(由编译器实现).但是这样并不能保证进程以设计的顺序执行。JVM中同步是基于进入和退出管程(monitor)对象实现的，每个对象都会有一个管程(monitor)对象，管程(monitor)会随着java对象一同创建和销毁。执行线程首先要持有管程对象，然后才能执行方法，当方法完成之后会释放管程，方法在执行时候会持有管程，其他线程无法再获取同一个管程 显式创建新线程 继承Thread类，实例化一个Thread实例并重写run（），然后调用它的start()方法： 希望新线程能执行指定的代码 方法一：从Thread派生一个自定义类，然后覆写run()方法： 方法二：创建Thread实例时，传入一个Runnable实例： 线程的优先级 可以对线程设定优先级，设定优先级的方法是： Thread.setPriority(int n) // 1~10, 默认值5 实现Runnable接口 与Callable不同 无返回值 无返回异常 实现run() 实现Callable接口（jdk1.5） 实现 Callable接口 1.实现 Callable接口,需要返回值类型 2.重写call方法,需要抛出异常 3.创建目标对象 4.创建执行服务: Executorservice ser= Executors. newfixed Threadpool(1) 5.提交执行: Future&lt; Boolean&gt; result1=ser. submit(t1); 6.获取结果: boolean r1= result1.get() 7.关闭服务:ser. shutdownNow() 总结：Thread为基，另外两个通过Thread 实例实现，通过接口避免单继承局限性，灵活方便同一个对象被多个线程使用，区别在Callable 有返回值，返回值通过FutureTask封装再给Thread实例，而Runnable实例直接给Thread 实例实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118package pers.lxl.mylearnproject.javase.thread;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;/** * 快速交替执行看起来像是同时执行 * 进程和线程的关系：一个进程可以包含一个或多个线程，但至少会有一个线程 * 多进程模式（每个进程只有一个线程） * 多线程模式（一个进程有多个线程） * 多进程＋多线程模式（复杂度最高） * 和多线程相比，多进程的缺点在于： * 创建进程比创建线程开销大，尤其是在Windows系统上； * 进程间通信比线程间通信要慢，因为线程间通信就是读写同一个变量，速度很快。 * 多进程的优点在于： * 多进程稳定性比多线程高，因为在多进程的情况下，一个进程崩溃不会影响其他进程，而在多线程的情况下，任何一个线程崩溃会直接导致整个进程崩溃。 * Java语言内置了多线程支持：一个Java程序实际上是一个JVM进程，JVM进程用一个主线程来执行main()方法，在main()方法内部，我们又可以启动多个线程。此外，JVM还有负责垃圾回收的其他工作线程等。 * 因此，对于大多数Java程序来说，我们说多任务，实际上是说如何使用多线程实现多任务。 * 和单线程相比，多线程编程的特点在于：多线程经常需要读写共享数据，并且需要同步。例如，播放电影时，就必须由一个线程播放视频，另一个线程播放音频，两个线程需要协调运行，否则画面和声音就不同步。因此，多线程编程的复杂度高，调试更困难。 * Java多线程编程的特点又在于： * 多线程模型是Java程序最基本的并发模型； * 后续读写网络、数据库、Web开发等都依赖Java多线程模型。 * * @author lxl **/public class HelloClass { //希望新线程能执行指定的代码，有以下几种方法： /** * 方法一：从Thread派生一个自定义类，然后覆写run()方法，启动：子类对象.start(),为避免OOP单继承局限性，不建议使用 */ static class MyThread extends Thread { @Override public void run() { System.out.println(&quot;start new thread!extends Thread&quot;); } } /** * 方法二：创建Thread实例时，传入一个Runnable实例：启动：传入目标对象+Thread对象.start()，建议使用:避免单继承局限性，灵活方便同一个对象被多个线程使用 */ static class MyRunnable implements Runnable { @Override public void run() { System.out.println(&quot;start new thread!implements Runnable&quot;); try { Thread.sleep(10); } catch (InterruptedException e) { } System.out.println(&quot;thread end.&quot;); } } /** * 方法三：实现Callable接口,与Runnable相比,Callable可以有返回值,返回值通过FutureTask【未来任务】进行封装 */ static class MyCallable implements Callable&lt;Integer&gt; { @Override public Integer call() { return 123; } }//相比之下,接口可以实现多个,而Thread只能单继承,// 继承整个Threa类开销过大,所以实现接口方式更好一些 /** * 当Java程序启动的时候，实际上是启动了一个JVM进程，然后，JVM启动主线程来执行main()方法。在main()方法中，我们又可以启动其他线程。 */ public static void main(String[] args) throws ExecutionException, InterruptedException { //创建新线程 System.out.println(&quot;main start...&quot;); //Thread thread = new Thread();//不能执行指定代码//extends Thread============== // Thread thread = new MyThread();//implements Callable============= MyCallable mc = new MyCallable(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread thread1 = new Thread(ft); thread1.start(); System.out.println(ft.get());//lambda Callbale FutureTask&lt;Integer&gt; futureTask2 = new FutureTask&lt;&gt;(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot;come in callbale&quot;); return 111; }); new thread(futureTask2,&quot;hahha&quot;).start();//hahha come in callbale while(!futuretask2.isDone()){ System.out.println(&quot;wait....&quot;) } System.out.println(futureTask2.get());//111//implements Runnable=============== Thread thread = new Thread(new MyRunnable()); //Thread thread = new Thread(() -&gt; {System.out.println(&quot;start new thread!&quot;); });//Java8引入的lambda写法 thread.start();// 启动新线程，直接调用Thread实例的run()方法是无效的，线程开启不一定立即执行，由CPU调度决定// Thread.setPriority(int n) // 1~10, 默认值5 try { Thread.sleep(20);// sleep() 可能会抛出 InterruptedException，因为异常不能跨线程传播回 main() 中，因此必须在本地进行处理。线程中抛出的其它异常也同样需要在本地进行处理。 } catch (InterruptedException e) { } System.out.println(&quot;main end...&quot;); }} 线程的状态 在Java程序中，一个线程对象只能调用一次start()方法启动新线程，并在新线程中执行run()方法。一旦run()方法执行完毕，线程就结束了。因此，Java线程的状态有以下几种： New：新创建的线程，尚未执行； Runnable：运行中的线程，正在执行run()方法的Java代码； Blocked：运行中的线程，因为某些操作被阻塞而挂起； Waiting（不见不散）：运行中的线程，因为某些操作在等待中； Timed Waiting（过时不候）：运行中的线程，因为执行sleep()方法正在计时等待； Terminated：线程已终止，因为run()方法执行完毕。 线程终止的原因有： 线程正常终止：run()方法执行到return语句返回； 线程意外终止：run()方法因为未捕获的异常导致线程终止； 对某个线程的Thread实例调用stop()方法强制终止（强烈不推荐使用）。 通过对另一个线程对象调用join()方法可以等待其执行结束； 可以指定等待时间，超过等待时间线程仍然没有结束就不再等待； 对已经运行结束的线程调用join()方法会立刻返回。 中断线程 对目标线程调用interrupt()方法可以请求中断一个线程，目标线程通过检测isInterrupted()标志获取自身是否已中断。如果目标线程处于等待状态，该线程会捕获到InterruptedException； 目标线程检测到isInterrupted()为true或者捕获了InterruptedException都应该立刻结束自身线程； 通过标志位判断需要正确使用volatile关键字； volatile关键字解决了共享变量在线程间的可见性问题。 线程属性 线程优先级 setPriority 守护线程 守护线程是指为其他线程服务的线程。在JVM中，所有非守护线程都执行完毕后，无论有没有守护线程，虚拟机都会自动退出。因此，JVM退出时，不必关心守护线程是否已结束。如何创建守护线程呢？方法和普通线程一样，只是在调用start()方法前，调用setDaemon(true)把该线程标记为守护线程： 守护线程不能持有任何需要关闭的资源，例如打开文件等，因为虚拟机退出时，守护线程没有任何机会来关闭文件，这会导致数据丢失。 应用：JVM垃圾回收、事件监听、心跳监测 Thread t = new MyThread(); //调用start()方法前，调用setDaemon(true)把该线程标记为守护线程 t.setDaemon(true); //守护线程不能持有任何需要关闭的资源，例如打开文件等，因为虚拟机退出时，守护线程没有任何机会来关闭文件，这会导致数据丢失。 t.start(); //判断是否为守护线程 System.out.println(t.isDaemon()); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788### 线程组### 处理未捕获异常处理器## 多线程编程步骤1. 创建资源类，在资源类创建属性和操作方法2. 在资源类中添加操作方法【判断、干活、通知】3. 创建多个线程，调用资源类的操作方法## synchronizedsynchronized是Java中的关键字，是一种**同步锁**。它修饰的对象有以下几种：1. 修饰一个**代码块**，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象；2. 修饰一个**方法**，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象；虽然可以使用synchronized来定义方法，但synchronized并不属于方法定义的一部分，因此，synchronized关键字**不能被继承**。如果在父类中的某个方法使用了synchronized关键字，而在子类中覆盖了这个方法，在子类中的这个方法默认情况下并不是同步的，而必须显式地在子类的这个方法中加上synchronized关键字才可以。当然，还可以在子类方法中调用父类中相应的方法，这样虽然子类中的方法不是同步的，但子类调用了父类的同步方法，因此，子类的方法也就相当于同步了。3. 修饰一个**静态的方法**，其作用的范围是整个静态方法，作用的对象是这个类的所有对象；4. 修饰一个**类**，其作用的范围是synchronized后面括号括起来的部分，作用主的对象是这个类的所有对象。​ 一个对象里面如果有多个synchronized方法，某一个时刻内，只要一个线程去调用其中的一个synchronized方法了，其它的线程都只能等待，换句话说，某一个时刻内，只能有唯一一个线程去访问这些synchronized方法，锁的是当前对象this，被锁定后，其它的线程都不能进入到当前对象的其它的synchronized方法，加个普通方法后发现和同步锁无关，换成两个对象后，不是同一把锁了，情况立刻变化。synchronized实现同步的基础：Java中的每一个对象都可以作为锁。**具体表现为以下3种形式。****对于普通同步方法，锁是当前实例对象。****对于静态同步方法，锁是当前类的Class对象。****对于同步方法块，锁是Synchonized括号里配置的对象**​ 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。也就是说如果一个实例对象的非静态同步方法获取锁后，该实例对象的其他非静态同步方法必须等待获取锁的方法释放锁后才能获取锁，可是别的实例对象的非静态同步方法因为跟该实例对象的非静态同步方法用的是不同的锁，所以毋须等待该实例对象已获取锁的非静态同步方法释放锁就可以获取他们自己的锁。所有的静态同步方法用的也是同一把锁——类对象本身，这两把锁是两个不同的对象，所以**静态同步方法与非静态同步方法之间是不会有竞态条件的。**​ 但是一旦一个静态同步方法获取锁后，其他的静态同步方法都必须等待该方法释放锁后才能获取锁，而不管是同一个实例对象的静态同步方法之间，还是不同的实例对象的静态同步方法之间，只要它们同一个类的实例对象！## 线程同步- 多线程同时读写共享变量时，会造成逻辑错误，因此需要通过synchronized同步；同步的本质就是给指定对象加锁，加锁后才能继续执行后续代码；注意加锁对象必须是同一个实例；对JVM定义的单个原子操作不需要同步。## 同步方法- 用synchronized修饰方法可以把整个方法变为同步代码块，synchronized方法加锁对象是this； 通过合理的设计和数据封装可以让一个类变为“线程安全”； 一个类没有特殊说明，默认不是thread-safe； 多线程能否安全访问某个非线程安全的实例，需要具体问题具体分析。## 死锁- Java的synchronized锁是隐式【自动上锁解锁】的**可重入锁（递归锁）**，Lock是显式的可重入锁；死锁产生的条件是多线程各自持有不同的锁，并互相试图获取对方已持有的锁，导致无限等待；避免死锁的方法是多线程获取锁的顺序要一致。```javapackage com.atguigu.sync;import java.util.concurrent.TimeUnit;/** * 演示死锁 */public class DeadLock { //创建两个对象 static Object a = new Object(); static Object b = new Object(); public static void main(String[] args) { new Thread(()-&gt;{ synchronized (a) { System.out.println(Thread.currentThread().getName()+&quot; 持有锁a，试图获取锁b&quot;); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (b) { System.out.println(Thread.currentThread().getName()+&quot; 获取锁b&quot;); } } },&quot;A&quot;).start(); new Thread(()-&gt;{ synchronized (b) { System.out.println(Thread.currentThread().getName()+&quot; 持有锁b，试图获取锁a&quot;); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (a) { System.out.println(Thread.currentThread().getName()+&quot; 获取锁a&quot;); } } },&quot;B&quot;).start(); }} 验证是否为死锁 配置JDK环境变量 项目中打开终端 输入jps -l 查看当前类的进程号 jstack -进程号可查看是否有死锁及相关信息 使用wait和notify wait和notify用于多线程协调运行： 在synchronized内部可以调用wait()使线程进入等待状态； 必须在已获得的锁对象上调用wait()方法； 在synchronized内部可以调用notify()或notifyAll()唤醒其他等待线程； 必须在已获得的锁对象上调用notify()或notifyAll()方法； 已唤醒的线程还需要重新获得锁后才能继续执行。 虚假唤醒 wait()在哪里睡就在那里醒使用时应使用while而不是if 123456if(num!=1){ this.wait(); //notifyAll（）唤醒之后会直接从this.wait（）语句开始执行，跳过了if判断}while(num!=1){ this.wait();} 使用ReentrantLock ReentrantLock可以替代synchronized进行同步； ReentrantLock获取锁更安全； 必须先获取到锁，再进入try {...}代码块，最后使用finally保证释放锁； 可以使用tryLock()尝试获取锁。 设置为ReentrantLock（true）使其成为公平锁 123456789101112131415161718192021222324252627Lock lock = new ReentrantLock(); Thread t1 = new Thread() { @Override public void run() { try { log(&quot;线程启动&quot;); log(&quot;试图占有对象：lock&quot;); //lock同步 lock.lock(); log(&quot;占有对象：lock&quot;); log(&quot;进行5秒的业务操作&quot;); Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } finally { log(&quot;释放对象：lock&quot;); //手动释放 lock.unlock(); } log(&quot;线程结束&quot;); } }; t1.setName(&quot;t1&quot;); t1.start(); 使用Condition Condition可以替代wait和notify； Condition对象必须从Lock对象获取。 12private Lock lock=new ReentrantLock();private Condition condition=lock.newCondition(); 锁 一种设计思想、不同环境使用不同的锁以达到资源最优利用。 乐观锁 VS 悲观锁 悲观锁先加锁再操作资源，乐观锁直接操作资源。 为何乐观锁能够做到不锁定同步资源也可以正确的实现线程同步呢？我们通过介绍乐观锁的主要实现方式 “CAS” 的技术原理来为大家解惑。 CAS全称 Compare And Swap（比较与交换），是一种无锁算法。在不使用锁（没有线程被阻塞）的情况下实现多线程之间的变量同步。java.util.concurrent包中的原子类就是通过CAS来实现了乐观锁。 CAS算法涉及到三个操作数： 需要读写的内存值 V。 进行比较的值 A。 要写入的新值 B。 当且仅当 V 的值等于 A 时，CAS通过原子方式用新值B来更新V的值（“比较+更新”整体是一个原子操作），否则不会执行任何操作。一般情况下，“更新”是一个不断重试的操作。 CAS问题： ABA问题 。CAS需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但是如果内存值原来是A，后来变成了B，然后又变成了A，那么CAS进行检查时会发现值没有发生变化，但是实际上是有变化的。ABA问题的解决思路就是在变量前面添加版本号，每次变量更新的时候都把版本号加一，这样变化过程就从“A－B－A”变成了“1A－2B－3A”。 JDK从1.5开始提供了AtomicStampedReference类来解决ABA问题，具体操作封装在compareAndSet()中。compareAndSet()首先检查当前引用和当前标志与预期引用和预期标志是否相等，如果都相等，则以原子方式将引用值和标志的值设置为给定的更新值。 循环时间长开销大。CAS操作如果长时间不成功，会导致其一直自旋，给CPU带来非常大的开销。 只能保证一个共享变量的原子操作 对一个共享变量执行操作时，CAS能够保证原子操作，但是对多个共享变量操作时，CAS是无法保证操作的原子性的。 Java从1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行CAS操作。 自旋锁 VS 适应性自旋锁 ​ 自旋锁不放弃CPU时间片，自旋等待锁释放，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBlockSpin来更改）没有成功获得锁，就应当挂起线程。 ​ 适应性自旋锁的自适应意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。在自旋锁中 另有三种常见的锁形式:TicketLock、CLHlock和MCSlock 无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁 无锁，没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。 偏向锁，一段代码一直被一个线程访问，那么该线程会自动获取锁，降低获取锁的代价。 偏向锁时，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。 若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时（或轻量级锁时有第三者），轻量级锁升级为重量级锁。 公平锁 VS 非公平锁 公平锁通过申请锁的顺序来获取锁 非公平锁直接获取锁，获取不到会到等待队列末尾等待，节约唤起线程的开销，提高吞吐率，但是部分线程可能会被饿死，或等待时间很长。 可重入锁 VS 非可重入锁 可重入锁（递归锁），是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者class），不会因为之前已经获取过还没释放而阻塞。 非可重入锁，反之同线程在外层获取锁时在进入内层需要先释放锁，重复调用同步数据时可能会导致死锁。 独享锁 VS 共享锁 独享锁（排他锁），该锁一次只能被一个线程拥有，加锁后不能再加其他锁，线程拥有后可读与改。 共享锁，该锁能被多个线程所拥有的，拥有后只能读不可改，其他线程可以加共享锁不能加独享锁（排它锁）。 读锁写锁 ReentrantReadWriteLock 里的两把锁 ReadLock 和 WriteLock，读锁是共享锁，写锁是独享锁，都会发生死锁。 现实中有这样一种场景：对共享资源有读和写的操作，且写操作没有读操作那么频繁。在没有写操作的时候，多个线程同时读一个资源没有任何问题，所以应该允许多个线程同时读取共享资源；但是如果一个线程想去写这些共享资源，就不应该允许其他线程对该资源进行读和写的操作了。 针对这种场景，JAVA的并发包提供了读写锁ReentrantReadWriteLock，它表示两个锁，一个是读操作相关的锁，称为共享锁；一个是写相关的锁，称为排他锁 线程进入读锁的前提条件： • 没有其他线程的写锁 • 没有写请求, 或者有写请求，但调用线程和持有锁的线程是同一个(可重入锁)。 线程进入写锁的前提条件： • 没有其他线程的读锁 • 没有其他线程的写锁 而读写锁有以下三个重要的特性： （1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 （2）重进入：读锁和写锁都支持线程重进入。 （3）锁降级：遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁。 表锁行锁 表锁整张表，而行锁锁单条数据且可能发生死锁。 使用ReadWriteLock 使用ReadWriteLock可以提高读取效率： ReadWriteLock只允许一个线程写入； ReadWriteLock允许多个线程在没有写入时同时读取； ReadWriteLock适合读多写少的场景。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package com.atguigu.readwrite;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReadWriteLock;import java.util.concurrent.locks.ReentrantReadWriteLock;//资源类class MyCache { //创建map集合 private volatile Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); //创建读写锁对象 private ReadWriteLock rwLock = new ReentrantReadWriteLock(); //放数据 public void put(String key,Object value) { //添加写锁 rwLock.writeLock().lock(); try { System.out.println(Thread.currentThread().getName()+&quot; 正在写操作&quot;+key); //暂停一会 TimeUnit.MICROSECONDS.sleep(300); //放数据 map.put(key,value); System.out.println(Thread.currentThread().getName()+&quot; 写完了&quot;+key); } catch (InterruptedException e) { e.printStackTrace(); } finally { //释放写锁 rwLock.writeLock().unlock(); } } //取数据 public Object get(String key) { //添加读锁 rwLock.readLock().lock(); Object result = null; try { System.out.println(Thread.currentThread().getName()+&quot; 正在读取操作&quot;+key); //暂停一会 TimeUnit.MICROSECONDS.sleep(300); result = map.get(key); System.out.println(Thread.currentThread().getName()+&quot; 取完了&quot;+key); } catch (InterruptedException e) { e.printStackTrace(); } finally { //释放读锁 rwLock.readLock().unlock(); } return result; }}public class ReadWriteLockDemo { public static void main(String[] args) throws InterruptedException { MyCache myCache = new MyCache(); //创建线程放数据 for (int i = 1; i &lt;=5; i++) { final int num = i; new Thread(()-&gt;{ myCache.put(num+&quot;&quot;,num+&quot;&quot;); },String.valueOf(i)).start(); } TimeUnit.MICROSECONDS.sleep(300); //创建线程取数据 for (int i = 1; i &lt;=5; i++) { final int num = i; new Thread(()-&gt;{ myCache.get(num+&quot;&quot;); },String.valueOf(i)).start(); } }} 使用StampedLock StampedLock提供了乐观读锁，可取代ReadWriteLock以进一步提升并发性能；StampedLock是不可重入锁。 使用Concurrent集合 使用java.util.concurrent包提供的线程安全的并发集合可以大大简化多线程编程： 多线程同时读写并发集合是安全的； 尽量使用Java标准库提供的并发集合，避免自己编写同步代码。 使用Atomic 使用java.util.concurrent.atomic提供的原子操作可以简化多线程编程： 原子操作实现了无锁的线程安全； 适用于计数器，累加器等。 增加值并返回新值：int addAndGet(int delta) 加1后返回新值：int incrementAndGet() 获取当前值：int get() 用CAS方式设置：int compareAndSet(int expect, int update) 使用 Future 对线程池提交一个Callable任务，可以获得一个Future对象； 可以用Future在将来某个时刻获取结果。 使用 CompletableFuture CompletableFuture可以指定异步处理流程： thenAccept()处理正常结果； exceptional()处理异常结果； thenApplyAsync()用于串行化另一个CompletableFuture； anyOf()和allOf()用于并行化多个CompletableFuture。 使用 ForkJoin Fork/Join是一种基于“分治”的算法：通过分解任务，并行执行，最后合并结果得到最终结果。 ForkJoinPool线程池可以把一个大任务分拆成小任务并行执行，任务类必须继承自RecursiveTask或RecursiveAction。 使用Fork/Join模式可以进行并行计算以提高效率。 使用 ThreadLocal ThreadLocal表示线程的“局部变量”，它确保每个线程的ThreadLocal变量都是各自独立的； ThreadLocal适合在一个线程的处理流程中保持上下文（避免了同一参数在所有方法中传递）； 使用ThreadLocal要用try ... finally结构，并在finally中清除。 集合线程安全 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) { //创建ArrayList集合// List&lt;String&gt; list = new ArrayList&lt;&gt;(); // Vector解决// List&lt;String&gt; list = new Vector&lt;&gt;(); //Collections解决// List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;()); // CopyOnWriteArrayList写时复制技术解决// List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;();// for (int i = 0; i &lt;30; i++) {// new Thread(()-&gt;{// //向集合添加内容// list.add(UUID.randomUUID().toString().substring(0,8));// //从集合获取内容// System.out.println(list);// },String.valueOf(i)).start();// } //演示Hashset// Set&lt;String&gt; set = new HashSet&lt;&gt;();// Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;();// for (int i = 0; i &lt;30; i++) {// new Thread(()-&gt;{// //向集合添加内容// set.add(UUID.randomUUID().toString().substring(0,8));// //从集合获取内容// System.out.println(set);// },String.valueOf(i)).start();// } //演示HashMap// Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); Map&lt;String,String&gt; map = new ConcurrentHashMap&lt;&gt;(); for (int i = 0; i &lt;30; i++) { String key = String.valueOf(i); new Thread(()-&gt;{ //向集合添加内容 map.put(key,UUID.randomUUID().toString().substring(0,8)); //从集合获取内容 System.out.println(map); },String.valueOf(i)).start(); } } JUC三大辅助类 JUC中提供了三种常用的辅助类，通过这些辅助类可以很好的解决线程数量过多时Lock锁的频繁操作。这三种辅助类为： • CountDownLatch: 减少计数 • CyclicBarrier: 循环栅栏 • Semaphore: 信号灯 CountDownLatch:减少计数 CountDownLatch类可以设置一个计数器，然后通过countDown方法来进行减1的操作，使用await方法等待计数器不大于0，然后继续执行await方法之后的语句。 • CountDownLatch主要有两个方法，当一个或多个线程调用await方法时，这些线程会阻塞 • 其它线程调用countDown方法会将计数器减1(调用countDown方法的线程不会阻塞) • 当计数器的值变为0时，因await方法阻塞的线程会被唤醒，继续执行 12345678910111213141516171819202122232425262728package com.atguigu.juc;import java.util.concurrent.CountDownLatch;//演示 CountDownLatchpublic class CountDownLatchDemo { //6个同学陆续离开教室之后，班长锁门 public static void main(String[] args) throws InterruptedException { //创建CountDownLatch对象，设置初始值 CountDownLatch countDownLatch = new CountDownLatch(6); //6个同学陆续离开教室之后 for (int i = 1; i &lt;=6; i++) { new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot; 号同学离开了教室&quot;); //计数 -1 countDownLatch.countDown(); },String.valueOf(i)).start(); } //等待 countDownLatch.await(); System.out.println(Thread.currentThread().getName()+&quot; 班长锁门走人了&quot;);}} CyclicBarrier: 循环栅栏 ​ CyclicBarrier看英文单词可以看出大概就是循环阻塞的意思，在使用中CyclicBarrier的构造方法第一个参数是目标障碍数，每次执行CyclicBarrier一次障碍数会加一，如果达到了目标障碍数，才会执行cyclicBarrier.await()之后的语句。可以将CyclicBarrier理解为加1操作 12345678910111213141516171819202122232425262728293031package com.atguigu.juc;import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;//集齐7颗龙珠就可以召唤神龙public class CyclicBarrierDemo {//创建固定值private static final int NUMBER = 7;public static void main(String[] args) { //创建CyclicBarrier CyclicBarrier cyclicBarrier = new CyclicBarrier(NUMBER,()-&gt;{ System.out.println(&quot;*****集齐7颗龙珠就可以召唤神龙&quot;); }); //集齐七颗龙珠过程 for (int i = 1; i &lt;=7; i++) { new Thread(()-&gt;{ try { System.out.println(Thread.currentThread().getName()+&quot; 星龙被收集到了&quot;); //等待 cyclicBarrier.await(); } catch (Exception e) { e.printStackTrace(); } },String.valueOf(i)).start(); }}} Semaphore: 信号灯 ​ Semaphore的构造方法中传入的第一个参数是最大信号量（可以看成最大线程池），每个信号量初始化为一个最多只能分发一个许可证。使用acquire方法获得许可证，release方法释放许可。场景: 抢车位, 6部汽车3个停车位 123456789101112131415161718192021222324252627282930313233343536package com.atguigu.juc;import java.util.Random;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;//6辆汽车，停3个车位public class SemaphoreDemo { public static void main(String[] args) { //创建Semaphore，设置许可数量 Semaphore semaphore = new Semaphore(3); //模拟6辆汽车 for (int i = 1; i &lt;=6; i++) { new Thread(()-&gt;{ try { //抢占 semaphore.acquire(); System.out.println(Thread.currentThread().getName()+&quot; 抢到了车位&quot;); //设置随机停车时间 TimeUnit.SECONDS.sleep(new Random().nextInt(5)); System.out.println(Thread.currentThread().getName()+&quot; ------离开了车位&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { //释放 semaphore.release(); } },String.valueOf(i)).start(); } }} 阻塞队列 BlockingQueue Concurrent包中，BlockingQueue很好的解决了多线程中，如何高效安全“传输”数据的问题。通过这些高效并且线程安全的队列类，为我们快速搭建高质量的多线程程序带来极大的便利。本文详细介绍了BlockingQueue家庭中的所有成员，包括他们各自的功能以及常见使用场景。 阻塞队列，顾名思义，首先它是一个队列, 通过一个共享的队列，可以使得数据由队列的一端输入，从另外一端输出； 当队列是空的，从队列中获取元素的操作将会被阻塞 当队列是满的，从队列中添加元素的操作将会被阻塞 试图从空的队列中获取元素的线程将会被阻塞，直到其他线程往空的队列插入新的元素 试图向已满的队列中添加新元素的线程将会被阻塞，直到其他线程从队列中移除一个或多个元素或者完全清空，使队列变得空闲起来并后续新增 常用的队列主要有以下两种： • 先进先出（FIFO）：先插入的队列的元素也最先出队列，类似于排队的功能。从某种程度上来说这种队列也体现了一种公平性 • 后进先出（LIFO）：后插入队列的元素最先出队列，这种队列优先处理最近发生的事件(栈) 在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起 为什么需要BlockingQueue 好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了 在concurrent包发布以前，在多线程环境下，我们必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。 多线程环境中，通过队列可以很容易实现数据共享，比如经典的“生产者”和“消费者”模型中，通过队列可以很便利地实现两者之间的数据共享。假设我们有若干生产者线程，另外又有若干个消费者线程。如果生产者线程需要把准备好的数据共享给消费者线程，利用队列的方式来传递数据，就可以很方便地解决他们之间的数据共享问题。但如果生产者和消费者在某个时间段内，万一发生数据处理速度不匹配的情况呢？理想情况下，如果生产者产出数据的速度大于消费者消费的速度，并且当生产出来的数据累积到一定程度的时候，那么生产者必须暂停等待一下（阻塞生产者线程），以便等待消费者线程把累积的数据处理完毕，反之亦然。 • 当队列中没有数据的情况下，消费者端的所有线程都会被自动阻塞（挂起），直到有数据放入队列 • 当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒 BlockingQueue核心方法 1.放入数据 • offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false.（本方法不阻塞当前执行方法的线程） • offer(E o, long timeout, TimeUnit unit)：可以设定等待的时间，如果在指定的时间内，还不能往队列中加入BlockingQueue，则返回失败 • put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续. 2.获取数据 • poll(time): 取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null • poll(long timeout, TimeUnit unit)：从BlockingQueue取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则知道时间超时还没有数据可取，返回失败。 • take(): 取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到BlockingQueue有新的数据被加入; • drainTo(): 一次性从BlockingQueue获取所有可用的数据对象（还可以指定获取数据的个数），通过该方法，可以提升获取数据效率；不需要多次分批加锁或释放锁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.atguigu.queue;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;//阻塞队列public class BlockingQueueDemo { public static void main(String[] args) throws InterruptedException { //创建阻塞队列 BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); //第一组// System.out.println(blockingQueue.add(&quot;a&quot;));//// System.out.println(blockingQueue.add(&quot;b&quot;));//// System.out.println(blockingQueue.add(&quot;c&quot;));//// //System.out.println(blockingQueue.element());//////// //System.out.println(blockingQueue.add(&quot;w&quot;));//// System.out.println(blockingQueue.remove());//// System.out.println(blockingQueue.remove());//// System.out.println(blockingQueue.remove());//// System.out.println(blockingQueue.remove()); //第二组// System.out.println(blockingQueue.offer(&quot;a&quot;));// System.out.println(blockingQueue.offer(&quot;b&quot;));// System.out.println(blockingQueue.offer(&quot;c&quot;));// System.out.println(blockingQueue.offer(&quot;www&quot;));//// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll()); //第三组 put 没空间阻塞 take 没数据阻塞// blockingQueue.put(&quot;a&quot;);// blockingQueue.put(&quot;b&quot;);// blockingQueue.put(&quot;c&quot;);// //blockingQueue.put(&quot;w&quot;);//// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take()); //第四组 System.out.println(blockingQueue.offer(&quot;a&quot;)); System.out.println(blockingQueue.offer(&quot;b&quot;)); System.out.println(blockingQueue.offer(&quot;c&quot;)); System.out.println(blockingQueue.offer(&quot;w&quot;,3L, TimeUnit.SECONDS)); }} ArrayBlockingQueue(常用) 基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。 ArrayBlockingQueue在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于LinkedBlockingQueue；按照实现原理来分析，ArrayBlockingQueue完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。Doug Lea之所以没这样去做，也许是因为ArrayBlockingQueue的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。 ArrayBlockingQueue和LinkedBlockingQueue间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的Node对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。而在创建ArrayBlockingQueue时，我们还可以控制对象的内部锁是否采用公平锁，默认采用非公平锁。 一句话总结: 由数组结构组成的有界阻塞队列。 LinkedBlockingQueue(常用) 基于链表的阻塞队列，同ArrayListBlockingQueue类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue可以通过构造函数指定该值），才会阻塞生产者队列，直到消费者从队列中消费掉一份 数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。而LinkedBlockingQueue之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 ArrayBlockingQueue和LinkedBlockingQueue是两个最普通也是最常用的阻塞队列，一般情况下，在处理多线程间的生产者消费者问题，使用这两个类足以。 一句话总结: 由链表结构组成的有界（但大小默认值为integer.MAX_VALUE）阻塞队列。 DelayQueue DelayQueue中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。 一句话总结: 使用优先级队列实现的延迟无界阻塞队列。 PriorityBlockingQueue 基于优先级的阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定），但需要注意的是PriorityBlockingQueue并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。 因此使用的时候要特别注意，生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。 在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是公平锁。 一句话总结: 支持优先级排序的无界阻塞队列。 SynchronousQueue 一种无缓冲的等待队列，类似于无中介的直接交易，有点像原始社会中的生产者和消费者，生产者拿着产品去集市销售给产品的最终消费者，而消费者必须亲自去集市找到所要商品的直接生产者，如果一方没有找到合适的目标，那么对不起，大家都在集市等待。相对于有缓冲的BlockingQueue来说，少了一个中间经销商的环节（缓冲区），如果有经销商，生产者直接把产品批发给经销商，而无需在意经销商最终会将这些产品卖给那些消费者，由于经销商可以库存一部分商品，因此相对于直接交易模式，总体来说采用中间经销商的模式会吞吐量高一些（可以批量买卖）；但另一方面，又因为经销商的引入，使得产品从生产者到消费者中间增加了额外的交易环节，单个产品的及时响应性能可能会降低。 声明一个SynchronousQueue有两种不同的方式，它们之间有着不太一样的行为。 公平模式和非公平模式的区别: • 公平模式：SynchronousQueue会采用公平锁，并配合一个FIFO队列来阻塞多余的生产者和消费者，从而体系整体的公平策略； • 非公平模式（SynchronousQueue默认）：SynchronousQueue采用非公平锁，同时配合一个LIFO队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。 一句话总结: 不存储元素的阻塞队列，也即单个元素的队列。 LinkedTransferQueue LinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了tryTransfer和transfer方法。 LinkedTransferQueue采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为null的节点，生产者线程就不入队了，直接就将元素填充到该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。 一句话总结: 由链表组成的无界阻塞队列。 LinkedBlockingDeque LinkedBlockingDeque是一个由链表结构组成的双向阻塞队列，即可以从队列的两端插入和移除元素。 对于一些指定的操作，在插入或者获取队列元素时如果队列状态不允许该操作可能会阻塞住该线程直到队列状态变更为允许操作，这里的阻塞一般有两种情况 • 插入元素时: 如果当前队列已满将会进入阻塞状态，一直等到队列有空的位置时再讲该元素插入，该操作可以通过设置超时参数，超时后返回 false 表示操作失败，也可以不设置超时参数一直阻塞，中断后抛出InterruptedException异常 • 读取元素时: 如果当前队列为空会阻塞住直到队列不为空然后返回元素，同样可以通过设置超时参数 一句话总结: 由链表组成的双向阻塞队列 小结 在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起 为什么需要BlockingQueue? 在concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。使用后我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了 线程池 ​ 线程池（Thread Pool）：一种线程使用模式。线程过多会带来调度开销，进而影响缓存局部性和整体性能。而线程池维护着多个线程，等待着监督管理者分配可并发执行的任务。这避免了在处理短时间任务时创建与销毁线程的代价。线程池不仅能够保证内核的充分利用，还能防止过分调度。 ​ 例子：以前单核CPU电脑，假的多线程，像马戏团小丑玩多个球，CPU需要来回切换。 现在多核电脑，多个线程各自跑在独立的CPU上，不用切换效率高。 优势： 线程池做的工作只要是控制运行的线程数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量，超出数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 特点： • 降低资源消耗: 通过重复利用已创建的线程降低线程创建和销毁造成的销耗。 • 提高响应速度: 当任务到达时，任务可以不需要等待线程创建就能立即执行。 • 提高线程的可管理性: 线程是稀缺资源，如果无限制的创建，可能会导致内存占用过多而产生OOM，并且会造成cpu过度切换（cpu切换线程是有时间成本的（需要保持当前执行线程的现场，并恢复要执行线程的现场））,不仅会销耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。方便线程并发数的管控。 • 提供更强大的功能，延时定时线程池。 • Java中的线程池是通过Executor框架实现的，该框架中用到了Executor，Executors，ExecutorService，ThreadPoolExecutor这几个类。 创建常用三种线程池的方式 12345678910111213141516171819202122232425262728293031323334package com.atguigu.pool;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;//演示线程池三种常用分类public class ThreadPoolDemo1 { public static void main(String[] args) { //一池五线程 ExecutorService threadPool1 = Executors.newFixedThreadPool(5); //5个窗口 //一池一线程 ExecutorService threadPool2 = Executors.newSingleThreadExecutor(); //一个窗口 //一池可扩容线程 ExecutorService threadPool3 = Executors.newCachedThreadPool(); //10个顾客请求 try { for (int i = 1; i &lt;=10; i++) { //执行 threadPool3.execute(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot; 办理业务&quot;); }); } }catch (Exception e) { e.printStackTrace(); }finally { //关闭 threadPool3.shutdown(); } }} 常用参数 (重点) ​ 当提交任务数大于 corePoolSize 的时候，会优先将任务放到 workQueue 阻塞队列中。当阻塞队列饱和后，会扩充线程池中线程数，直到达到 maximumPoolSize 最大线程数配置。此时，再多余的任务，则会触发线程池的拒绝策略了。 ​ 即，当提交的任务数大于（workQueue.size() + maximumPoolSize ），就会触发线程池的拒绝策略。 1234public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) { this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);} 1、corePoolSize（线程池基本大小）：当向线程池提交一个任务时，若线程池已创建的线程数小于corePoolSize，即便此时存在空闲线程，也会通过创建一个新线程来执行该任务，直到已创建的线程数大于或等于corePoolSize时，（除了利用提交新任务来创建和启动线程（按需构造），也可以通过 prestartCoreThread() 或 prestartAllCoreThreads() 方法来提前启动线程池中的基本线程。） 2、maximumPoolSize（线程池最大大小）：线程池所允许的最大线程个数。当队列满了，且已创建的线程数小于maximumPoolSize，则线程池会创建新的线程来执行任务。另外，对于无界队列，可忽略该参数。 3、keepAliveTime（线程存活保持时间）（秒）当线程池中线程数大于核心线程数时，线程的空闲时间如果超过线程存活时间，那么这个线程就会被销毁，直到线程池中的线程数小于等于核心线程数。 4、workQueue（任务队列）：用于传输和保存等待执行任务的阻塞队列。 5、threadFactory（线程工厂）：用于创建新线程。threadFactory创建的线程也是采用new Thread()方式，threadFactory创建的线程名都具有统一的风格：pool-m-thread-n（m为线程池的编号，n为线程池内的线程编号）。 6、handler（线程饱和策略）：当线程池和队列都满了，再加入线程会执行此策略。 7、unit 存活的时间单位 1ThreadPoolExecutor threadPool = new ThreadPoolExecutor(corePoolSize：10, maximumPoolSize：15, keepAliveTime：6060, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); 即初始化10个，最大15个，存活6060，单位为秒，有哪些待执行任务。 拒绝策略(重点) CallerRunsPolicy: 当触发拒绝策略，只要线程池没有关闭的话，则使用调用线程直接运行任务。一般并发比较小，性能要求不高，不允许失败。但是，由于调用者自己运行任务，如果任务提交速度过快，可能导致程序阻塞，性能效率上必然的损失较大 AbortPolicy: 丢弃任务，并抛出拒绝执行 RejectedExecutionException 异常信息。线程池默认的拒绝策略。必须处理好抛出的异常，否则会打断当前的执行流程，影响后续的任务执行。 DiscardPolicy: 直接丢弃，其他啥都没有 DiscardOldestPolicy: 当触发拒绝策略，只要线程池没有关闭的话，丢弃阻塞队列 workQueue 中最老的一个任务，并将新任务加入 线程池的种类与创建 线程池总对比 1、newCachedThreadPool：用来创建一个可以无限扩大的线程池，适用于负载较轻的场景，执行短期异步任务。（可以使得任务快速得到执行，因为任务时间执行短，可以很快结束，也不会造成cpu过度切换） 2、newFixedThreadPool：创建一个固定大小的线程池，因为采用无界的阻塞队列，所以实际线程数量永远不会变化，适用于负载较重的场景，对当前线程数量进行限制。（保证线程数可控，不会造成线程过多，导致系统负载更为严重） 3、newSingleThreadExecutor：创建一个单线程的线程池，适用于需要保证顺序执行各个任务。 4、newScheduledThreadPool：适用于执行延时或者周期性任务。 newCachedThreadPool(常用) 作用：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程. 特点: • 线程池中数量没有固定，可达到最大值（Interger. MAX_VALUE） • 线程池中的线程可进行缓存重复利用和回收（回收默认时间为1分钟） • 当线程池中，没有可用线程，会重新创建一个线程 创建方式： 1234567891011/** * 可缓存线程池 * @return */public static ExecutorService newCachedThreadPool(){ /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */ return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); } newFixedThreadPool(常用) 作用：创建一个可重用固定线程数的线程池，以共享的无界队列方式来运行这些线程。在任意点，在大多数线程会处于处理任务的活动状态。如果在所有线程处于活动状态时提交附加任务，则在有可用线程之前，附加任务将在队列中等待。如果在关闭前的执行期间由于失败而导致任何线程终止，那么一个新线程将代替它执行后续的任务（如果需要）。在某个线程被显式地关闭之前，池中的线程将一直存在。场景: 适用于可以预测线程数量的业务中，或者服务器负载较重，对线程数有严格限制的场景 特征： • 线程池中的线程处于一定的量，可以很好的控制线程的并发量 • 线程可以重复被使用，在显示关闭之前，都将一直存在 • 超出一定量的线程被提交时候需在队列中等待 创建方式： 123456789101112/** * 固定长度线程池 * @return */ public static ExecutorService newFixedThreadPool(){ /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */ return new ThreadPoolExecutor(10, 10, 0L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); } newSingleThreadExecutor(常用) 作用：创建一个使用单个 worker 线程的 Executor，以无界队列方式来运行该线程。（注意，如果因为在关闭前的执行期间出现失败而终止了此单个线程，那么如果需要，一个新线程将代替它执行后续的任务）。可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。与其他等效的newFixedThreadPool不同，可保证无需重新配置此方法所返回的执行程序即可使用其他的线程。场景: 适用于需要保证顺序执行各个任务，并且在任意时间点，不会同时有多个线程的场景。 特征： 线程池中最多执行1个线程，之后提交的线程活动将会排在队列中以此执行 创建方式： 1234567891011/** * 单一线程池 * @return */ /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */public static ExecutorService newSingleThreadExecutor(){ return new ThreadPoolExecutor(1, 1, 0L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); } newScheduleThreadPool(了解) 作用: 线程池支持定时以及周期性执行任务，创建一个corePoolSize为传入参数，最大线程数为整形的最大数的线程池。场景: 适用于需要多个后台线程执行周期任务的场景。 特征: （1）线程池中具有指定数量的线程，即便是空线程也将保留 （2）可定时或者延迟执行线程活动 创建方式: 12public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize, ThreadFactory threadFactory) { return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory); } newWorkStealingPool jdk1.8提供的线程池，底层使用的是ForkJoinPool实现，创建一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用cpu核数的线程来并行执行任务。场景: 适用于大耗时，可并行执行的场景 创建方式: 12345678public static ExecutorService newWorkStealingPool(int parallelism) { /** * parallelism：并行级别，通常默认为JVM可用的处理器个数 * factory：用于创建ForkJoinPool中使用的线程。 * handler：用于处理工作线程未处理的异常，默认为null * asyncMode：用于控制WorkQueue的工作模式:队列---反队列*/ return new ForkJoinPool(parallelism, ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true); } 底层原理 在创建了线程池后，线程池中的线程数为零 当调用execute()方法添加一个请求任务时，线程池会做出如下判断： 2.1 如果正在运行的线程数量小于corePoolSize，那么马上创建线程运行这个任务； 2.2 如果正在运行的线程数量大于或等于corePoolSize，那么将这个任务放入队列； 2.3 如果这个时候队列满了且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务； 2.4 如果队列满了且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会启动饱和拒绝策略来执行。 当一个线程完成任务时，它会从队列中取下一个任务来执行 当一个线程无事可做超过一定的时间（keepAliveTime）时，线程会判断： 4.1 如果当前运行的线程数大于corePoolSize，那么这个线程就被停掉。 4.2 所以线程池的所有任务完成后，它最终会收缩到corePoolSize的大小。 注意事项(重要) 项目中创建多线程时，使用常见的三种线程池创建方式，单一、可变、定长都有一定问题，原因是FixedThreadPool和SingleThreadExecutor底层都是用LinkedBlockingQueue实现的，这个队列最大长度为Integer.MAX_VALUE，容易导致OOM。所以实际生产一般自己通过ThreadPoolExecutor的7个参数，自定义线程池 创建线程池推荐适用ThreadPoolExecutor及其7个参数手动创建 corePoolSize线程池的核心线程数 maximumPoolSize能容纳的最大线程数 keepAliveTime空闲线程存活时间 unit 存活的时间单位 workQueue 存放提交但未执行任务的队列 threadFactory 创建线程的工厂类 handler 等待队列满后的拒绝策略 ​ 手动创建示例 12345678910111213141516171819202122232425262728293031323334package com.atguigu.pool;import java.util.concurrent.*;//自定义手动线程池创建public class ThreadPoolDemo2 { public static void main(String[] args) { ExecutorService threadPool = new ThreadPoolExecutor( 2, 5, 2L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy() ); //10个顾客请求 try { for (int i = 1; i &lt;=10; i++) { //执行 threadPool.execute(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot; 办理业务&quot;); }); } }catch (Exception e) { e.printStackTrace(); }finally { //关闭 threadPool.shutdown(); } }} 为什么不允许适用不允许Executors.的方式手动创建线程池,如下图 123456789101112131415161718192021222324252627282930313233343536373839404142 /**使用Executors创建线程池，建议手动创建线程池 * 线程池不允许使用Executors去创建，而是通过ThreadPoolExecutor的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 说明：Executors返回的线程池对象的弊端如下： * 1）FixedThreadPool和SingleThreadPool: * 允许的请求队列长度为Integer.MAX_VALUE，可能会堆积大量的请求，从而导致OOM。 * 2）CachedThreadPool: * 允许的创建线程数量为Integer.MAX_VALUE，可能会创建大量的线程，从而导致OOM。*/ Positive example 1： //org.apache.commons.lang3.concurrent.BasicThreadFactory ScheduledExecutorService executorService = new ScheduledThreadPoolExecutor(1, new BasicThreadFactory.Builder().namingPattern(&quot;example-schedule-pool-%d&quot;).daemon(true).build()); Positive example 2： ThreadFactory namedThreadFactory = new ThreadFactoryBuilder() .setNameFormat(&quot;demo-pool-%d&quot;).build(); //Common Thread Pool ExecutorService pool = new ThreadPoolExecutor(5, 200, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(1024), namedThreadFactory, new ThreadPoolExecutor.AbortPolicy()); pool.execute(()-&gt; System.out.println(Thread.currentThread().getName())); pool.shutdown();//gracefully shutdown Positive example 3： &lt;bean id=&quot;userThreadPool&quot; class=&quot;org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor&quot;&gt; &lt;property name=&quot;corePoolSize&quot; value=&quot;10&quot; /&gt; &lt;property name=&quot;maxPoolSize&quot; value=&quot;100&quot; /&gt; &lt;property name=&quot;queueCapacity&quot; value=&quot;2000&quot; /&gt; &lt;property name=&quot;threadFactory&quot; value= threadFactory /&gt; &lt;property name=&quot;rejectedExecutionHandler&quot;&gt; &lt;ref local=&quot;rejectedExecutionHandler&quot; /&gt; &lt;/property&gt; &lt;/bean&gt; //in code userThreadPool.execute(thread); 线程池流程 线程池流程 1、判断核心线程池是否已满，没满则创建一个新的工作线程来执行任务。已满则。 2、判断任务队列是否已满，没满则将新提交的任务添加在工作队列，已满则。 3、判断整个线程池是否已满，没满则创建一个新的工作线程来执行任务，已满则执行饱和策略。 （1、判断线程池中当前线程数是否大于核心线程数，如果小于，在创建一个新的线程来执行任务，如果大于则 2、判断任务队列是否已满，没满则将新提交的任务添加在工作队列，已满则。 3、判断线程池中当前线程数是否大于最大线程数，如果小于，则创建一个新的线程来执行任务，如果大于，则执行饱和策略。） 线程池为什么需要使用（阻塞）队列？ 回到了非线程池缺点中的第3点： 1、因为线程若是无限制的创建，可能会导致内存占用过多而产生OOM，并且会造成cpu过度切换。 另外回到了非线程池缺点中的第1点： 2、创建线程的消耗较高。 或者下面这个网上并不高明的回答： 2、线程池创建线程需要获取mainlock这个全局锁，影响并发效率，阻塞队列可以很好的缓冲。 线程池为什么要使用阻塞队列而不使用非阻塞队列？ 阻塞队列可以保证任务队列中没有任务时阻塞获取任务的线程，使得线程进入wait状态，释放cpu资源。 当队列中有任务时才唤醒对应线程从队列中取出消息进行执行。 使得在线程不至于一直占用cpu资源。 （线程执行完任务后通过循环再次从任务队列中取出任务进行执行，代码片段如下 while (task != null || (task = getTask()) != null) {}）。 不用阻塞队列也是可以的，不过实现起来比较麻烦而已，有好用的为啥不用呢？ 如何配置线程池 CPU密集型任务 尽量使用较小的线程池，一般为CPU核心数+1。 因为CPU密集型任务使得CPU使用率很高，若开过多的线程数，会造成CPU过度切换。 IO密集型任务 可以使用稍大的线程池，一般为2*CPU核心数。 IO密集型任务CPU使用率并不高，因此可以让CPU在等待IO的时候有其他线程去处理别的任务，充分利用CPU时间。 混合型任务 可以将任务分成IO密集型和CPU密集型任务，然后分别用不同的线程池去处理。 只要分完之后两个任务的执行时间相差不大，那么就会比串行执行来的高效。 因为如果划分之后两个任务执行时间有数据级的差距，那么拆分没有意义。 因为先执行完的任务就要等后执行完的任务，最终的时间仍然取决于后执行完的任务，而且还要加上任务拆分与合并的开销，得不偿失。 execute()和submit()方法 1、execute()，执行一个任务，没有返回值。 2、submit()，提交一个线程任务，有返回值。 submit(Callable task)能获取到它的返回值，通过future.get()获取（阻塞直到任务执行完）。一般使用FutureTask+Callable配合使用（IntentService中有体现）。 submit(Runnable task, T result)能通过传入的载体result间接获得线程的返回值。 submit(Runnable task)则是没有返回值的，就算获取它的返回值也是null。 Future.get方法会使取结果的线程进入阻塞状态，知道线程执行完成之后，唤醒取结果的线程，然后返回结果。 Fork/Join Fork/Join框架简介 Fork/Join它可以将一个大的任务拆分成多个子任务进行并行处理，最后将子任务结果合并成最后的计算结果，并进行输出。Fork/Join框架要完成两件事情： Fork：把一个复杂任务进行分拆，大事化小 Join：把分拆任务的结果进行合并 任务分割：首先Fork/Join框架需要把大的任务分割成足够小的子任务，如果子任务比较大的话还要对子任务进行继续分割 执行任务并合并结果：分割的子任务分别放到双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都放在另外一个队列里，启动一个线程从队列里取数据，然后合并这些数据。 在Java的Fork/Join框架中，使用两个类完成上述操作 • ForkJoinTask:我们要使用Fork/Join框架，首先需要创建一个ForkJoin任务。该类提供了在任务中执行fork和join的机制。通常情况下我们不需要直接集成ForkJoinTask类，只需要继承它的子类，Fork/Join框架提供了两个子类： a.RecursiveAction：用于没有返回结果的任务 b.RecursiveTask:用于有返回结果的任务 • ForkJoinPool:ForkJoinTask需要通过ForkJoinPool来执行 • RecursiveTask: 继承后可以实现递归(自己调自己)调用的任务 Fork/Join框架的实现原理 ​ ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责将存放以及将程序提交给ForkJoinPool，而ForkJoinWorkerThread负责执行这些任务。 Fork方法 ​ Fork方法的实现原理： 当我们调用ForkJoinTask的fork方法时，程序会把任务放在ForkJoinWorkerThread的pushTask的workQueue中，异步地执行这个任务，然后立即返回结果 123456public final ForkJoinTask&lt;V&gt; fork() { Thread t; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ((ForkJoinWorkerThread)t).workQueue.push(this); else ForkJoinPool.common.externalPush(this); return this; } pushTask方法把当前任务存放在ForkJoinTask数组队列里。然后再调用ForkJoinPool的signalWork()方法唤醒或创建一个工作线程来执行任务。代码如下： 123456789101112final void push(ForkJoinTask&lt;?&gt; task) { ForkJoinTask&lt;?&gt;[] a; ForkJoinPool p; int b = base, s = top, n; if ((a = array) != null) { // ignore if queue removed int m = a.length - 1; // fenced write for task visibility U.putOrderedObject(a, ((m &amp; s) &lt;&lt; ASHIFT) + ABASE, task); U.putOrderedInt(this, QTOP, s + 1); if ((n = s - b) &lt;= 1) { if ((p = pool) != null) p.signalWork(p.workQueues, this);//执行 } else if (n &gt;= m) growArray(); } } join方法 Join方法的主要作用是阻塞当前线程并等待获取结果。让我们一起看看ForkJoinTask的join方法的实现，代码如下： 123456public final V join() { int s; if ((s = doJoin() &amp; DONE_MASK) != NORMAL) reportException(s); return getRawResult(); } 它首先调用doJoin方法，通过doJoin()方法得到当前任务的状态来判断返回什么结果，任务状态有4种： 已完成（NORMAL）、被取消（CANCELLED）、信号（SIGNAL）和出现异常（EXCEPTIONAL） • 如果任务状态是已完成，则直接返回任务结果。 • 如果任务状态是被取消，则直接抛出CancellationException • 如果任务状态是抛出异常，则直接抛出对应的异常 让我们分析一下doJoin方法的实现 12345678910111213141516private int doJoin() { int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w; return (s = status) &lt; 0 ? s : ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ? (w = (wt = (ForkJoinWorkerThread)t).workQueue). tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0 ? s : wt.pool.awaitJoin(w, this, 0L) : externalAwaitDone(); } final int doExec() { int s; boolean completed; if ((s = status) &gt;= 0) { try { completed = exec(); } catch (Throwable rex) { return setExceptionalCompletion(rex); } if (completed) s = setCompletion(NORMAL); } return s; } 在doJoin()方法流程如下: 首先通过查看任务的状态，看任务是否已经执行完成，如果执行完成，则直接返回任务状态； 如果没有执行完，则从任务数组里取出任务并执行。 如果任务顺利执行完成，则设置任务状态为NORMAL，如果出现异常，则记录异常，并将任务状态设置为EXCEPTIONAL。 Fork/Join框架的异常处理 ForkJoinTask在执行的时候可能会抛出异常，但是我们没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。 getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。 入门案例 场景: 生成一个计算任务，计算1+2+3.........+1000,每100个数切分一个子任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.atguigu.forkjoin;import java.util.concurrent.*;class MyTask extends RecursiveTask&lt;Integer&gt; { //拆分差值不能超过10，计算10以内运算 private static final Integer VALUE = 10; private int begin ;//拆分开始值 private int end;//拆分结束值 private int result ; //返回结果 //创建有参数构造 public MyTask(int begin,int end) { this.begin = begin; this.end = end; } //拆分和合并过程 @Override protected Integer compute() { //判断相加两个数值是否大于10 if((end-begin)&lt;=VALUE) { //相加操作 for (int i = begin; i &lt;=end; i++) { result = result+i; } } else {//进一步拆分 //获取中间值 int middle = (begin+end)/2; //拆分左边 MyTask task01 = new MyTask(begin,middle); //拆分右边 MyTask task02 = new MyTask(middle+1,end); //调用方法拆分 task01.fork(); task02.fork(); //合并结果 result = task01.join()+task02.join(); } return result; }}public class ForkJoinDemo { public static void main(String[] args) throws ExecutionException, InterruptedException { //创建MyTask对象 MyTask myTask = new MyTask(0,100); //创建分支合并池对象 ForkJoinPool forkJoinPool = new ForkJoinPool(); ForkJoinTask&lt;Integer&gt; forkJoinTask = forkJoinPool.submit(myTask); //获取最终合并之后结果 Integer result = forkJoinTask.get(); System.out.println(result); //关闭池对象 forkJoinPool.shutdown(); }} CompletableFuture CompletableFuture简介 同步【不在等待】、异步【不在先做其他的】 CompletableFuture在Java里面被用于异步编程，异步通常意味着非阻塞，可以使得我们的任务单独运行在与主线程分离的其他线程中，并且通过回调可以在主线程中得到异步任务的执行状态，是否完成，和是否异常等信息。 CompletableFuture实现了Future, CompletionStage接口，实现了Future接口就可以兼容现在有线程池框架，而CompletionStage接口才是异步编程的接口抽象，里面定义多种异步方法，通过这两者集合，从而打造出了强大的CompletableFuture类。 Future与CompletableFuture Futrue在Java里面，通常用来表示一个异步任务的引用，比如我们将任务提交到线程池里面，然后我们会得到一个Futrue，在Future里面有isDone方法来 判断任务是否处理结束，还有get方法可以一直阻塞直到任务结束然后获取结果，但整体来说这种方式，还是同步的，因为需要客户端不断阻塞等待或者不断轮询才能知道任务是否完成。 Future的主要缺点如下： （1）不支持手动完成 我提交了一个任务，但是执行太慢了，我通过其他路径已经获取到了任务结果，现在没法把这个任务结果通知到正在执行的线程，所以必须主动取消或者一直等待它执行完成 （2）不支持进一步的非阻塞调用 通过Future的get方法会一直阻塞到任务完成，但是想在获取任务之后执行额外的任务，因为Future不支持回调函数，所以无法实现这个功能 （3）不支持链式调用 对于Future的执行结果，我们想继续传到下一个Future处理使用，从而形成一个链式的pipline调用，这在Future中是没法实现的。 （4）不支持多个Future合并 比如我们有10个Future并行执行，我们想在所有的Future运行完毕之后，执行某些函数，是没法通过Future实现的。 （5）不支持异常处理 Future的API没有任何的异常处理的api，所以在异步运行时，如果出了问题是不好定位的。 CompletableFuture入门 使用CompletableFuture 场景:主线程里面创建一个CompletableFuture，然后主线程调用get方法会阻塞，最后我们在一个子线程中使其终止。 123456789101112131415/** * 主线程里面创建一个CompletableFuture，然后主线程调用get方法会阻塞，最后我们在一个子线程中使其终止 * @param args */ public static void main(String[] args) throws Exception{ CompletableFuture&lt;String&gt; future = new CompletableFuture&lt;&gt;(); new Thread(() -&gt; { try{ System.out.println(Thread.currentThread().getName() + &quot;子线程开始干活&quot;); //子线程睡5秒 Thread.sleep(5000); //在子线程中完成主线程 future.complete(&quot;success&quot;); }catch (Exception e){ e.printStackTrace(); } }, &quot;A&quot;).start(); //主线程调用get方法阻塞 System.out.println(&quot;主线程调用get方法获取结果为: &quot; + future.get()); System.out.println(&quot;主线程完成,阻塞结束!!!!!!&quot;); } 没有返回值的异步任务 12345678910111213/** * 没有返回值的异步任务 * @param args */ public static void main(String[] args) throws Exception{ System.out.println(&quot;主线程开始&quot;); //运行一个没有返回值的异步任务 CompletableFuture&lt;Void&gt; future = CompletableFuture.runAsync(() -&gt; { try { System.out.println(&quot;子线程启动干活&quot;); Thread.sleep(5000); System.out.println(&quot;子线程完成&quot;); } catch (Exception e) { e.printStackTrace(); }}); //主线程阻塞 future.get(); System.out.println(&quot;主线程结束&quot;); } 有返回值的异步任务 1234567891011121314151617/** * 没有返回值的异步任务 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); //运行一个有返回值的异步任务 CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(&quot;子线程开始任务&quot;); Thread.sleep(5000); } catch (Exception e) { e.printStackTrace(); } return &quot;子线程完成了!&quot;; }); //主线程阻塞 String s = future.get(); System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + s); } 线程依赖 当一个线程依赖另一个线程时，可以使用 thenApply 方法来把这两个线程串行化。 123456789101112131415161718192021private static Integer num = 10; /** * 先对一个数加10,然后取平方 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(&quot;加10任务开始&quot;); num += 10; } catch (Exception e) { e.printStackTrace(); } return num; }).thenApply(integer -&gt; { return num * num; }); Integer integer = future.get(); System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + integer); } 消费处理结果 thenAccept 消费处理结果, 接收任务的处理结果，并消费处理，无返回结果。 12345678910111213141516171819public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(&quot;加10任务开始&quot;); num += 10; } catch (Exception e) { e.printStackTrace(); } return num; }).thenApply(integer -&gt; { return num * num; }).thenAccept(new Consumer&lt;Integer&gt;() { @Override public void accept(Integer integer) { System.out.println(&quot;子线程全部处理完成,最后调用了accept,结果为:&quot; + integer); } }); } 异常处理 exceptionally异常处理,出现异常时触发 12345678910111213public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { int i = 1 / 0; System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }).exceptionally(ex -&gt; { System.out.println(ex.getMessage()); return -1; }); System.out.println(future.get()); } handle类似于thenAccept/thenRun方法,是最后一步的处理调用,但是同时可以处理异常 123456789101112131415161718public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }).handle((i, ex) -&gt; { System.out.println(&quot;进入handle方法&quot;); if (ex != null) { System.out.println(&quot;发生了异常,内容为:&quot; + ex.getMessage()); return -1; } else { System.out.println(&quot;正常完成,内容为: &quot; + i); return i; } }); System.out.println(future.get()); } 结果合并 thenCompose合并两个有依赖关系的CompletableFutures的执行结果 123456789101112131415public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); //第一步加10 CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }); //合并 CompletableFuture&lt;Integer&gt; future1 = future.thenCompose(i -&gt; //再来一个CompletableFuture CompletableFuture.supplyAsync(() -&gt; { return i + 1; })); System.out.println(future.get()); System.out.println(future1.get());} thenCombine合并两个没有依赖关系的CompletableFutures任务 1234567891011121314151617181920212223public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }); CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;乘以10任务开始&quot;); num = num * 10; return num; }); //合并两个结果 CompletableFuture&lt;Object&gt; future = job1.thenCombine(job2, new BiFunction&lt;Integer, Integer, List&lt;Integer&gt;&gt;() { @Override public List&lt;Integer&gt; apply(Integer a, Integer b) { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); return list; } }); System.out.println(&quot;合并结果为:&quot; + future.get()); } 合并多个任务的结果allOf与anyOf allOf: 一系列独立的future任务，等其所有的任务执行完后做一些事情 123456789101112131415161718192021222324252627282930313233/** * 先对一个数加10,然后取平方 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); List&lt;CompletableFuture&gt; list = new ArrayList&lt;&gt;(); CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }); list.add(job1); CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;乘以10任务开始&quot;); num = num * 10; return num; }); list.add(job2); CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;减以10任务开始&quot;); num = num * 10; return num; }); list.add(job3); CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;除以10任务开始&quot;); num = num * 10; return num; }); list.add(job4); //多任务合并 List&lt;Integer&gt; collect = list.stream().map(CompletableFuture&lt;Integer&gt;::join).collect(Collectors.toList()); System.out.println(collect); } anyOf: 只要在多个future里面有一个返回，整个任务就可以结束，而不需要等到每一个future结束 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 先对一个数加10,然后取平方 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt;[] futures = new CompletableFuture[4]; CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(5000); System.out.println(&quot;加10任务开始&quot;); num += 10; return num; } catch (Exception e) { return 0; } }); futures[0] = job1; CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(2000); System.out.println(&quot;乘以10任务开始&quot;); num = num * 10; return num; } catch (Exception e) { return 1; } }); futures[1] = job2; CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(3000); System.out.println(&quot;减以10任务开始&quot;); num = num * 10; return num; } catch (Exception e) { return 2; } }); futures[2] = job3; CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(4000); System.out.println(&quot;除以10任务开始&quot;); num = num * 10; return num; } catch (Exception e) { return 3; } }); futures[3] = job4; CompletableFuture&lt;Object&gt; future = CompletableFuture.anyOf(futures); System.out.println(future.get()); } 反射 获取类的字节码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package pers.lxl.mylearnproject.javase.reflection;import java.lang.reflect.Field;import java.util.Arrays;/** * 不知对象情况调用对象方法等，通过Class实例获取class信息的方法称为反射（Reflection） * * @author lxl */public class ReClass { public static void main(String[] args) {/** 1.获取一个class的Class实例的三个方法： 方法一：class的静态变量.class： Class cls = String.class; 方法二：实例变量.getClass()： String s = &quot;Hello&quot;; Class cls1 = s.getClass(); 方法三：静态方法Class.forName(class的完整类名)： 只会让静态代码块执行 Class cls2 = Class.forName(&quot;java.lang.String&quot;); 用instanceof不但匹配指定类型，还匹配指定类型的子类。而用==判断class实例可以精确地判断数据类型，但不能作子类型比较*/ printClassInfo(&quot;&quot;.getClass()); printClassInfo(Runnable.class); printClassInfo(java.time.Month.class); printClassInfo(String[].class); printClassInfo(int.class);// ReflectLearn reflectLearn=new ReflectLearn(); printClassInfo(ReflectLearn.class); } static void printClassInfo(Class cls) {// 先获取类之后才能获取类信息// 获取类信息 System.out.println(&quot;[&quot; + cls.getSimpleName() + &quot;] begin ==================================&quot;); System.out.println(&quot;Class name: &quot; + cls.getName()); System.out.println(&quot;Simple name: &quot; + cls.getSimpleName()); if (cls.getPackage() != null) { System.out.println(&quot;Package name: &quot; + cls.getPackage().getName()); } System.out.println(&quot;is interface: &quot; + cls.isInterface()); System.out.println(&quot;is enum: &quot; + cls.isEnum()); System.out.println(&quot;is array: &quot; + cls.isArray()); System.out.println(&quot;is primitive: &quot; + cls.isPrimitive());// 方法信息// .newInstance() 无参构造，不存在抛出异常java.lang.InstantiationException try { System.out.println(&quot;无参构造： &quot;+cls.newInstance()); } catch (InstantiationException e) { System.out.println(&quot;没有无参构造造成InstantiationException&quot;); } catch (IllegalAccessException e) { System.out.println(&quot;没有无参构造造成IllegalAccessException&quot;); } System.out.println(&quot;所有实例方法： &quot;+cls.getDeclaredMethods());// System.out.println(&quot;根据名字获取方法： &quot;+ Arrays.toString(cls.getDeclaredMethods(&quot;methodShow&quot;))); System.out.println(&quot;所有构造方法： &quot;+ Arrays.toString(cls.getDeclaredConstructors()));// 属性信息 System.out.println(&quot;public Fields: &quot; + Arrays.toString(cls.getFields())); System.out.println(&quot;Fields public Name: &quot; + cls.getName()); System.out.println(&quot;All Fields: &quot; + Arrays.toString(cls.getDeclaredFields())); try { System.out.println(&quot;Get Fields By Name: &quot; + Arrays.toString(new Field[]{cls.getDeclaredField(&quot;publicFiled&quot;)})); } catch (NoSuchFieldException e) { System.out.println(&quot;没有这样一个属性报NoSuchFieldException&quot;); } System.out.println(&quot;[&quot; + cls.getSimpleName() + &quot;] end ==================================&quot; + &quot;\\n&quot;); }}class ReflectLearn { public String publicFiled; private String privateFiled; public int methodShow(){ return 1; }} IO 磁盘操作 File对象 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import java.io.File;import java.io.FilenameFilter;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/**磁盘操作**/public class FileObject { public static void main(String[] args) throws IOException {// 即使传入的文件或目录不存在，代码也不会出错，因为构造一个File对象，并不会导致任何磁盘操作,调用方法时才会磁盘操作 //绝对路径是以根目录开头的完整路径Java字符串中以//代表/// File表示文件或目录 //\\u202A系统复制产生 File file = new File(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\lixianglun\\\\fileTest.txt&quot;); //相对路径（可以用.表示当前目录，..表示上级目录）绝对路径去掉当前目录 File afile = new File(&quot;..\\\\fileTest.txt&quot;); File catalog=new File(&quot;C:\\\\Users\\\\Administrator\\\\Desktop&quot;); System.out.println(file); //返回构造方法传入的路径 System.out.println(file.getPath()); //返回绝对路径 System.out.println(file.getAbsolutePath()); //返回的是规范路径。 System.out.println(afile.getCanonicalPath()); //根据当前平台打印&quot;\\&quot;或&quot;/&quot; System.out.println(File.separator); System.out.println(file.isFile()); //判断当前文件，目录是否存在 System.out.println(file.isDirectory()); System.out.println(file.canRead()); System.out.println(file.canWrite()); System.out.println(file.canExecute()); System.out.println(afile.length()); //add、delete// if (file.createNewFile()) {// // 文件创建成功:// // TOD O:TODO提交代码会提醒// if (file.delete()) {// // 删除文件成功:// }// }// 递归列出目录下的所有文件// public static void listAllFiles(File dir) {// if (dir == null || !dir.exists()) {// return;// }// if (dir.isFile()) {// System.out.println(dir.getName());// return;// }// for (File file : dir.listFiles()) {// listAllFiles(file);// }//}//有些时候，程序需要读写一些临时文件，File对象提供了createTempFile()来创建一个临时文件，以及deleteOnExit()在JVM退出时自动删除该文件。 System.out.println(catalog.list()); //目录下的文件和子目录名 System.out.println(catalog.listFiles()); //listFiles()提供了一系列重载方法，可以过滤不想要的文件和目录 // 仅列出.exe文件 File[] fs2 = catalog.listFiles(new FilenameFilter() { @Override public boolean accept(File dir, String name) { return name.endsWith(&quot;.exe&quot;); // 返回true表示接受该文件 } }); printFiles(fs2); } /**boolean mkdir()：创建当前File对象表示的目录； //boolean mkdirs()：创建当前File对象表示的目录，并在必要时将不存在的父目录也创建出来； //boolean delete()：删除当前File对象表示的目录，当前目录必须为空才能删除成功。*/ static void printFiles(File[] files) { System.out.println(&quot;==========&quot;); if (files != null) { for (File f : files) { System.out.println(f); } } System.out.println(&quot;==========&quot;); // 构造一个Path对象 Path p1 = Paths.get(&quot;.&quot;, &quot;project&quot;, &quot;study&quot;); System.out.println(p1); // 转换为绝对路径 Path p2 = p1.toAbsolutePath(); System.out.println(p2); // 转换为规范路径 Path p3 = p2.normalize(); System.out.println(p3); // 转换为File对象 File f = p3.toFile(); System.out.println(f); // 可以直接遍历Path for (Path p : Paths.get(&quot;src/main&quot;).toAbsolutePath()) { System.out.println(&quot; &quot; + p); } }} 字节操作 InputStream 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;/**InputStream并不是一个接口，而是一个抽象类，它是所有输入流的超类。这个抽象类定义的一个最重要的方法就是int read()，签名如下：public abstract int read() throws IOException;这个方法会读取输入流的下一个字节，并返回字节表示的int值（0~255）。如果已读到末尾，返回-1表示不能继续读取了。 * @author lxl*/public class InputStreamL { public static void main(String[] args) throws IOException { //inputStream字节输入流，抽象类，只提供方法声明，不提供具体实现 // 创建一个FileInputStream对象: InputStream inputStream = new FileInputStream(&quot;fileTest.txt&quot;); for (; ; ) {//死循环，比while（true）更好，因为编译后指令更少，而且没有判断跳转，更加简洁 // 反复调用read()方法，直到返回-1 int n = inputStream.read(); if (n == -1) { break; } // 打印byte的值 System.out.println(n); } inputStream.close(); // 关闭流释放对应的底层资源，便让操作系统把资源释放掉，否则，应用程序占用的资源会越来越多，不但白白占用内存，还会影响其他应用程序的运行。// 利用缓冲区一次读取多个字节 try (InputStream input = new FileInputStream(&quot;fileTest.txt&quot;)) { // 定义1000个字节大小的缓冲区: byte[] buffer = new byte[1000]; int n; // 读取到缓冲区,read()方法是阻塞的（Blocking），必须等待read()方法返回才能执行下一行代码 while ((n = input.read(buffer)) != -1) { System.out.println(&quot;read &quot; + n + &quot; bytes.&quot;); } } try { //准备文件lol.txt其中的内容是AB，对应的ASCII分别是65 66 File f =new File(&quot;fileTest.txt&quot;); //创建基于文件的输入流 FileInputStream fis =new FileInputStream(f); //创建字节数组，其长度就是文件的长度 byte[] all =new byte[(int) f.length()]; //以字节流的形式读取文件所有内容 fis.read(all); for (byte b : all) { //打印出来是65 66 System.out.println(b); } //每次使用完流，都应该进行关闭 fis.close(); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } }}/*装饰者模式Java I/O 使用了装饰者模式来实现。以 InputStream 为例，InputStream 是抽象组件；FileInputStream 是 InputStream 的子类，属于具体组件，提供了字节流的输入操作；FilterInputStream 属于抽象装饰者，装饰者用于装饰组件，为组件提供额外的功能。例如 BufferedInputStream 为 FileInputStream 提供缓存的功能。实例化一个具有缓存功能的字节流对象时，只需要在 FileInputStream 对象上再套一层 BufferedInputStream 对象即可。FileInputStream fileInputStream = new FileInputStream(filePath);BufferedInputStream bufferedInputStream = new BufferedInputStream(fileInputStream);DataInputStream 装饰者提供了对更多数据类型进行输入的操作，比如 int、double 等基本类型。*/ OutputStream 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.io.*;/**为什么要有flush()？因为向磁盘、网络写入数据的时候，出于效率的考虑，操作系统并不是输出一个字节就立刻写入到文件或者发送到网络，而是把输出的字节先放到内存的一个缓冲区里（本质上就是一个byte[]数组），等到缓冲区写满了，再一次性写入文件或者网络。对于很多IO设备来说，一次写一个字节和一次写1000个字节，花费的时间几乎是完全一样的，所以OutputStream有个flush()方法，能强制把缓冲区内容输出。通常情况下，我们不需要调用这个flush()方法，因为缓冲区写满了OutputStream会自动调用它并且，在调用close()方法关闭OutputStream之前，也会自动调用flush()方法。特别情况，缓冲区过大，需马上发送的时候手动flush * @author lxl*/public class OutputStreamL { /**实现文件复制*/ public static void copyFile(String src, String dist) throws IOException { FileInputStream in = new FileInputStream(src); FileOutputStream out = new FileOutputStream(dist); byte[] buffer = new byte[20 * 1024]; int cnt; // read() 最多读取 buffer.length 个字节 // 返回的是实际读取的个数 // 返回 -1 的时候表示读到 eof，即文件尾 while ((cnt = in.read(buffer, 0, buffer.length)) != -1) { out.write(buffer, 0, cnt); } in.close(); } public static void main(String[] args) throws IOException { //文件存在则覆盖，不存在创建 try(OutputStream output = new FileOutputStream(&quot;F:\\\\PROJECT\\\\IDEA\\\\mylearnproject\\\\src\\\\main\\\\resources\\\\fileTest.txt&quot;)){ byte[] data = {34, 35}; // H output.write(72); // e output.write(101); // l output.write(108); // l output.write(108); // o output.write(data);// output.write(111); // o // 一次性写入若干字节 output.write(&quot;Hello瓦达无多&quot;.getBytes(&quot;UTF-8&quot;)); // } output.close();//try之后编译器在此自动为我们写入finally并调用close() }}} 字符操作 Reader InputStream是一个字节流，即以byte为单位读取，而Reader是一个字符流，即以char为单位读取 InputStream Reader 字节流，以byte为单位 字符流，以char为单位 读取字节（-1，0~255）：int read() 读取字符（-1，0~65535）：int read() 读到字节数组：int read(byte[] b) 读到字符数组：int read(char[] c) java.io.Reader是所有字符输入流的超类，它最主要的方法是： public int read() throws IOException; 这个方法读取字符流的下一个字符，并返回字符表示的int，范围是0~65535。如果已读到末尾，返回-1。 FileReader打开文件并获取Reader // 创建一个FileReader对象: Reader reader = new FileReader(&quot;src/readme.txt&quot;); // 字符编码是??? for (;😉 { int n = reader.read(); // 反复调用read()方法，直到返回-1 if (n == -1) { break; } System.out.println((char)n); // 打印char } reader.close(); // 关闭流 避免乱码问题，我们需要在创建FileReader时指定编码： 123456789101112131415161718192021 Reader reader = new FileReader(&quot;src/readme.txt&quot;, StandardCharsets.UTF_8); - try (resource)来保证Reader在无论有没有IO错误的时候都能够正确地关闭 - Reader还提供了一次性读取若干字符并填充到char[]数组的方法：public int read(char[] c) throws IOException它返回实际读入的字符个数，最大不超过char[]数组的长度。返回-1表示流结束。 - CharArrayReader把一个char[]数组变成一个Reader - try (Reader reader = new CharArrayReader(&quot;Hello&quot;.toCharArray())) {}- StringReader- InputStreamReader - 除了特殊的CharArrayReader和StringReader，普通的Reader实际上是基于InputStream构造的，因为Reader需要从InputStream中读入字节流（byte），然后，根据编码设置，再转换为char就可以实现字符流。如果我们查看FileReader的源码，它在内部实际上持有一个FileInputStream。 既然Reader本质上是一个基于InputStream的byte到char的转换器，那么，如果我们已经有一个InputStream，想把它转换为Reader，是完全可行的。InputStreamReader就是这样一个转换器，它可以把任何InputStream转换为Reader。 - // 持有InputStream: InputStream input = new FileInputStream(&quot;src/readme.txt&quot;);// 变换为Reader:Reader reader = new InputStreamReader(input, &quot;UTF-8&quot;); - try (Reader reader = new InputStreamReader(new FileInputStream(&quot;src/readme.txt&quot;), &quot;UTF-8&quot;)) {// TODO:} Writer Writer就是带编码转换器的OutputStream，它把char转换为byte并输出 OutputStream Writer 字节流，以byte为单位 字符流，以char为单位 写入字节（0~255）：void write(int b) 写入字符（0~65535）：void write(int c) 写入字节数组：void write(byte[] b) 写入字符数组：void write(char[] c) 无对应方法 写入String：void write(String s) Writer是所有字符输出流的超类，它提供的方法主要有： 写入一个字符（0~65535）：void write(int c)； 写入字符数组的所有字符：void write(char[] c)； 写入String表示的所有字符：void write(String s)。 - FileWriter 12345678 - try (Writer writer = new FileWriter(&quot;readme.txt&quot;, StandardCharsets.UTF_8)) {writer.write('H'); // 写入单个字符writer.write(&quot;Hello&quot;.toCharArray()); // 写入char[]writer.write(&quot;Hello&quot;); // 写入String}- CharArrayWriter- StringWriter- OutputStreamWriter String 的编码方式 String 可以看成一个字符序列，可以指定一个编码方式将它编码为字节序列，也可以指定一个编码方式将一个字节序列解码为 String。 String str1 = &quot;中文&quot;; d byte[] bytes = str1.getBytes(&quot;UTF-8&quot;); String str2 = new String(bytes, &quot;UTF-8&quot;); System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是可以使用一个 char 存储中文和英文，而将 String 转为 bytes[] 字节数组就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。 byte[] bytes = str1.getBytes(); 对象操作 1234567891011121314151617181920212223242526272829303132333435- 序列化是指把一个Java对象变成二进制内容，本质上就是一个byte[]数组,序列化后可以把byte[]保存到文件中，或者把byte[]通过网络传输到远程，这样，就相当于把Java对象存储到文件或者通过网络传输出去了 - 一个Java对象要能序列化，必须实现一个特殊的java.io.Serializable接口 - 把一个Java对象变为byte[]数组，需要使用ObjectOutputStream。它负责把一个Java对象写入一个字节流 - public class Main { public static void main(String[] args) throws IOException { ByteArrayOutputStream buffer = new ByteArrayOutputStream(); try (ObjectOutputStream output = new ObjectOutputStream(buffer)) { // 写入int: output.writeInt(12345); // 写入String: output.writeUTF(&quot;Hello&quot;); // 写入Object: output.writeObject(Double.valueOf(123.456)); } System.out.println(Arrays.toString(buffer.toByteArray())); } }- 反序列化，即把一个二进制内容（也就是byte[]数组）变回Java对象。有了反序列化，保存到文件中的byte[]数组又可以“变回”Java对象，或者从网络上读取byte[]并把它“变回”Java对象。 - ObjectInputStream负责从一个字节流读取Java对象 - try (ObjectInputStream input = new ObjectInputStream(...)) { int n = input.readInt(); String s = input.readUTF(); Double d = (Double) input.readObject(); } - readObject()可能抛出的异常有：ClassNotFoundException：没有找到对应的Class；InvalidClassException：Class不匹配。- 安全性 - Java的序列化机制仅适用于Java，如果需要与其它语言交换数据，必须使用通用的序列化方法，例如JSON。 网络操作 NIO 流与块 I/O 与 NIO 最重要的区别是数据打包和传输的方式，I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。 面向流的 I/O 一次处理一个字节数据：一个输入流产生一个字节数据，一个输出流消费一个字节数据。为流式数据创建过滤器非常容易，链接几个过滤器，以便每个过滤器只负责复杂处理机制的一部分。不利的一面是，面向流的 I/O 通常相当慢。 面向块的 I/O 一次处理一个数据块，按块处理数据比按流处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 I/O 包和 NIO 已经很好地集成了，java.io.* 已经以 NIO 为基础重新实现了，所以现在它可以利用 NIO 的一些特性。例如，java.io.* 包中的一些类包含以块的形式读写数据的方法，这使得即使在面向流的系统中，处理速度也会更快。 通道与缓冲区 通道 通道 Channel 是对原 I/O 包中的流的模拟，可以通过它读取和写入数据。 通道与流的不同之处在于，流只能在一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)，而通道是双向的，可以用于读、写或者同时用于读写。 通道包括以下类型： FileChannel：从文件中读写数据； DatagramChannel：通过 UDP 读写网络中数据； SocketChannel：通过 TCP 读写网络中数据； ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel,本身并不传数据。 FileChannel 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package pers.lxl.mylearnproject.javase.io.nio.channel;import java.io.IOException;import java.io.RandomAccessFile;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;public class FileChannelDemo { public static void main(String[] args) throws IOException { // 读// 需要通过使用一个InputStream、OutputStream或RandomAccessFile来获取一个FileChannel实例 RandomAccessFile aFile = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); FileChannel inChannel = aFile.getChannel(); ByteBuffer buf = ByteBuffer.allocate(48);// 获取当前通道位置 long pos = inChannel.position();// 设置当前通道位置 inChannel.position(pos + 6);// 返回通道当前大小 System.out.println(&quot;此时通道大小：&quot;+inChannel.size());// 截取(删除)文件7字节后部分 inChannel.truncate(7); int bytesRead = inChannel.read(buf); while (bytesRead != -1) { System.out.println(&quot;读取： &quot; + bytesRead); buf.flip(); while (buf.hasRemaining()) { System.out.print((char) buf.get()); } buf.clear(); bytesRead = inChannel.read(buf); } //强制将通道里的剩余数据写到磁盘中 inChannel.force(true); aFile.close(); System.out.println(&quot;操作结束&quot;); //覆盖起始位置开始的对应位置 写 RandomAccessFile aFile1 = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis(); ByteBuffer buf1 = ByteBuffer.allocate(48); FileChannel inChannel1 = aFile1.getChannel(); buf1.clear(); buf1.put(newData.getBytes()); buf1.flip(); while (buf1.hasRemaining()) { inChannel1.write(buf1); } inChannel1.close(); RandomAccessFile aaFile = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); FileChannel fromChannel = aaFile.getChannel(); RandomAccessFile bbFile = new RandomAccessFile(&quot;src/main/resources/transferFrom.txt&quot;, &quot;rw&quot;); FileChannel toChannel = bbFile.getChannel(); long position = 0; long count = fromChannel.size(); // fromChannel to toChannel toChannel.transferFrom(fromChannel, position, count); // fromChannel to toChannel fromChannel.transferTo(position, count, toChannel); aaFile.close(); bbFile.close(); System.out.println(&quot;通道传输over!&quot;); }} ServerSocketChannel （1）新的socket通道类可以运行非阻塞模式并且是可选择的，可以激活大程序（如网络服务器和中间件组件）巨大的可伸缩性和灵活性。本节中我们会看到，再也没有为每个socket连接使用一个线程的必要了，也避免了管理大量线程所需的上下文交换开销。借助新的NIO类，一个或几个线程就可以管理成百上千的活动socket连接了并且只有很少甚至可能没有性能损失。所有的socket通道类(DatagramChannel、SocketChannel和ServerSocketChannel)都继承了位于java.nio.channels.spi包中的AbstractSelectableChannel。这意味着我们可以用一个Selector对象来执行socket通道的就绪选择（readiness selection）。 （2）请注意DatagramChannel和SocketChannel实现定义读和写功能的接口而ServerSocketChannel不实现。ServerSocketChannel负责监听传入的连接和创建新的SocketChannel对象，它本身从不传输数据。 （3）在我们具体讨论每一种socket通道前，您应该了解socket和socket通道之间的关系。通道是一个连接I/O服务导管并提供与该服务交互的方法。就某个socket而言，它不会再次实现与之对应的socket通道类中的socket协议API，而java.net中已经存在的socket通道都可以被大多数协议操作重复使用。 全部socket通道类（DatagramChannel、SocketChannel和ServerSocketChannel）在被实例化时都会创建一个对等socket对象。这些是我们所熟悉的来自java.net的类（Socket、ServerSocket和DatagramSocket），它们已经被更新以识别通道。对等socket可以通过调用socket( )方法从一个通道上获取。此外，这三个java.net类现在都有getChannel( )方法。 （4）要把一个socket通道置于非阻塞模式，我们要依靠所有socket通道类的公有超级类：SelectableChannel。就绪选择（readiness selection）是一种可以用来查询通道的机制，该查询可以判断通道是否准备好执行一个目标操作，如读或写。非阻塞I/O和可选择性是紧密相连的，那也正是管理阻塞模式的API代码要在SelectableChannel超级类中定义的原因。 设置或重新设置一个通道的阻塞模式是很简单的，只要调用configureBlocking( )方法即可，传递参数值为true则设为阻塞模式，参数值为false值设为非阻塞模式。可以通过调用isBlocking( )方法来判断某个socket通道当前处于哪种模式。 ​ 非阻塞socket通常被认为是服务端使用的，因为它们使同时管理很多socket通道变得更容易。但是，在客户端使用一个或几个非阻塞模式的socket通道也是有益处的，例如，借助非阻塞socket通道，GUI程序可以专注于用户请求并且同时维护与一个或多个服务器的会话。在很多程序上，非阻塞模式都是有用的。 偶尔地，我们也会需要防止socket通道的阻塞模式被更改。API中有一个blockingLock( )方法，该方法会返回一个非透明的对象引用。返回的对象是通道实现修改阻塞模式时内部使用的。只有拥有此对象的锁的线程才能更改通道的阻塞模式。 ​ ServerSocketChannel是一个基于通道的socket监听器。它同我们所熟悉的java.net.ServerSocket执行相同的任务，不过它增加了通道语义，因此能够在非阻塞模式下运行。 ​ 由于ServerSocketChannel没有bind()方法，因此有必要取出对等的socket并使用它来绑定到一个端口以开始监听连接。我们也是使用对等ServerSocket的API来根据需要设置其他的socket选项。 ​ 同java.net.ServerSocket一样，ServerSocketChannel也有accept( )方法。一旦创建了一个ServerSocketChannel并用对等socket绑定了它，然后您就可以在其中一个上调用accept()。如果您选择在ServerSocket上调用accept( )方法，那么它会同任何其他的ServerSocket表现一样的行为：总是阻塞并返回一个java.net.Socket对象。如果您选择在ServerSocketChannel上调用accept( )方法则会返回SocketChannel类型的对象，返回的对象能够在非阻塞模式下运行。 换句话说： ​ ServerSocketChannel的accept()方法会返回SocketChannel类型对象，SocketChannel可以在非阻塞模式下运行。其它Socket的accept()方法会阻塞返回一个Socket对象。如果ServerSocketChannel以非阻塞模式被调用，当没有传入连接在等待时，ServerSocketChannel.accept( )会立即返回null。正是这种检查连接而不阻塞的能力实现了可伸缩性并降低了复杂性。可选择性也因此得到实现。我们可以使用一个选择器实例来注册ServerSocketChannel对象以实现新连接到达时自动通知的功能。 12345678910111213141516171819202122232425262728293031323334353637import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;//http://localhost:1234/public class ServerSocketChannelDemo { public static final String GREETING = &quot;Hello java nio.\\r\\n&quot;; public static void main(String[] argv) throws Exception { int port = 1234; // default if (argv.length &gt; 0) { port = Integer.parseInt(argv[0]); } ByteBuffer buffer = ByteBuffer.wrap(GREETING.getBytes());// 打开 ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(port)); ssc.configureBlocking(false); while (true) { System.out.println(&quot;Waiting for connections&quot;);// 监听新的链接，阻塞会在此阻塞住进程非阻塞会返回null SocketChannel sc = ssc.accept(); if (sc == null) { System.out.println(&quot;null&quot;); Thread.sleep(2000); } else { System.out.println(&quot;Incoming connection from: &quot; + sc.socket().getRemoteSocketAddress());// 指针指向0 buffer.rewind(); sc.write(buffer);// 关闭 sc.close(); } } }} SocketChannel ​ Java NIO中的SocketChannel是一个连接到TCP网络套接字的通道。 A selectable channel for stream-oriented connecting sockets. 以上是Java docs中对于SocketChannel的描述：SocketChannel是一种面向流连接sockets套接字的可选择通道。从这里可以看出： ​ • SocketChannel是用来连接Socket套接字 ​ •SocketChannel主要用途用来处理网络I/O的通道 ​ • SocketChannel是基于TCP连接传输 ​ • SocketChannel实现了可选择通道，可以被多路复用的 ​ 特征： （1）对于已经存在的socket不能创建SocketChannel （2）SocketChannel中提供的open接口创建的Channel并没有进行网络级联，需要使用connect接口连接到指定地址 （3）未进行连接的SocketChannle执行I/O操作时，会抛出NotYetConnectedException （4）SocketChannel支持两种I/O模式：阻塞式和非阻塞式 （5）SocketChannel支持异步关闭。如果SocketChannel在一个线程上read阻塞，另一个线程对该SocketChannel调用shutdownInput，则读阻塞的线程将返回-1表示没有读取任何数据；如果SocketChannel在一个线程上write阻塞，另一个线程对该SocketChannel调用shutdownWrite，则写阻塞的线程将抛出AsynchronousCloseException （6）SocketChannel支持设定参数 ​ SO_SNDBUF 套接字发送缓冲区大小 ​ SO_RCVBUF 套接字接收缓冲区大小 ​ SO_KEEPALIVE 保活连接 ​ O_REUSEADDR 复用地址 ​ SO_LINGER 有数据传输时延缓关闭Channel (只有在非阻塞模式下有用) ​ TCP_NODELAY 禁用Nagle算法 12345678910111213141516171819202122232425262728293031323334package pers.lxl.mylearnproject.javase.io.nio.channel;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.channels.SocketChannel;public class SocketChannelDemo { public static void main(String[] args) throws IOException { SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;www.baidu.com&quot;, 80)); SocketChannel socketChanne2 = SocketChannel.open(); socketChanne2.connect(new InetSocketAddress(&quot;www.baidu.com&quot;, 80));// 连接校验 socketChannel.isOpen(); // 测试SocketChannel是否为open状态 socketChannel.isConnected(); //测试SocketChannel是否已经被连接 socketChannel.isConnectionPending(); //测试SocketChannel是否正在进行连接 socketChannel.finishConnect(); //校验正在进行套接字连接的SocketChannel是否已经完成连接// 读写模式 前面提到SocketChannel支持阻塞和非阻塞两种模式： socketChannel.configureBlocking(false);// 通过setOptions方法可以设置socket套接字的相关参数 socketChannel.setOption(StandardSocketOptions.SO_KEEPALIVE, Boolean.TRUE).setOption(StandardSocketOptions.TCP_NODELAY, Boolean.TRUE);// 可以通过getOption获取相关参数的值。如默认的接收缓冲区大小是8192byte。 socketChannel.getOption(StandardSocketOptions.SO_KEEPALIVE); socketChannel.getOption(StandardSocketOptions.SO_RCVBUF);// 读写 ByteBuffer byteBuffer = ByteBuffer.allocate(16); socketChannel.read(byteBuffer); socketChannel.close(); System.out.println(&quot;read over&quot;); }} DatagramChannel ​ 正如SocketChannel对应Socket，ServerSocketChannel对应ServerSocket，每一个DatagramChannel对象也有一个关联的DatagramSocket对象。正如SocketChannel模拟连接导向的流协议（如TCP/IP），DatagramChannel则模拟包导向的无连接协议（如UDP/IP）。DatagramChannel是无连接的，每个数据报（datagram）都是一个自包含的实体，拥有它自己的目的地址及不依赖其他数据报的数据负载。与面向流的的socket不同，DatagramChannel可以发送单独的数据报给不同的目的地址。同样，DatagramChannel对象也可以接收来自任意地址的数据包。每个到达的数据报都含有关于它来自何处的信息（源地址）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import org.junit.jupiter.api.Test;import java.io.IOException;import java.net.InetSocketAddress;import java.net.SocketAddress;import java.nio.ByteBuffer;import java.nio.channels.DatagramChannel;import java.nio.charset.Charset;public class DatagramChannelDemo { /** * 发包的datagram * * @throws IOException * @throws InterruptedException */ @Test public void sendDatagram() throws IOException, InterruptedException { DatagramChannel sendChannel = DatagramChannel.open(); InetSocketAddress sendAddress = new InetSocketAddress(&quot;127.0.0.1&quot;, 9999); while (true) {// 发送 sendChannel.send(ByteBuffer.wrap(&quot;发包&quot;.getBytes(&quot;UTF-8&quot;)), sendAddress); System.out.println(&quot;发包端发包&quot;); Thread.sleep(1000); } } /** * 收包端 * * @throws IOException */ @Test public void receive() throws IOException { DatagramChannel receiveChannel = DatagramChannel.open(); //打开9999端口接收UDP数据包 InetSocketAddress receiveAddress = new InetSocketAddress(9999); receiveChannel.bind(receiveAddress); ByteBuffer receiveBuffer = ByteBuffer.allocate(512); while (true) { receiveBuffer.clear();// 接收 SocketAddress sendAddress = receiveChannel.receive(receiveBuffer); receiveBuffer.flip(); System.out.print(sendAddress.toString() + &quot; &quot;); System.out.println(Charset.forName(&quot;UTF-8&quot;).decode(receiveBuffer)); } } /** * 只接收和发送9999的数据包 * * @throws IOException */ @Test public void testConect1() throws IOException { DatagramChannel connChannel = DatagramChannel.open(); connChannel.bind(new InetSocketAddress(9999)); //UDP不存在真正意义上的连接，这里的连接是向特定服务地址用read和write接收发送数据包。 connChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); connChannel.write(ByteBuffer.wrap(&quot;发包&quot;.getBytes(&quot;UTF-8&quot;))); ByteBuffer readBuffer = ByteBuffer.allocate(512); //read()和write()只有在connect()后才能使用，不然会抛NotYetConnectedException异常。用read()接收时，如果没有接收到包，会抛PortUnreachableException异常。 while (true) { try { readBuffer.clear(); connChannel.read(readBuffer); readBuffer.flip(); System.out.println(Charset.forName(&quot;UTF-8&quot;).decode(readBuffer)); } catch (Exception e) { } } }} Scatter/Gather ​ Java NIO开始支持scatter/gather，scatter/gather用于描述从Channel中读取或者写入到Channel的操作。 ​ **分散（scatter）**从Channel中读取是指在读操作时将读取的数据写入多个buffer中。因此，Channel将从Channel中读取的数据“分散（scatter）”到多个Buffer中。 ​ **聚集（gather）**写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。 scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 Scattering Reads Scattering Reads是指数据从一个channel读取到多个buffer中。 1234ByteBuffer header = ByteBuffer.allocate(128); ByteBuffer body = ByteBuffer.allocate(1024); ByteBuffer[] bufferArray = { header, body }; channel.read(bufferArray); ​ 注意buffer首先被插入到数组，然后再将数组作为channel.read() 的输入参数。read()方法按照buffer在数组中的顺序将从channel中读取的数据写入到buffer，当一个buffer被写满后，channel紧接着向另一个buffer中写。 Scattering Reads在移动下一个buffer前，必须填满当前的buffer，这也意味着它不适用于动态消息(译者注：消息大小不固定)。换句话说，如果存在消息头和消息体，消息头必须完成填充（例如 128byte），Scattering Reads才能正常工作。 Gathering Writes ​ Gathering Writes是指数据从多个buffer写入到同一个channel。 1234ByteBuffer header = ByteBuffer.allocate(128); ByteBuffer body = ByteBuffer.allocate(1024); //write data into buffers ByteBuffer[] bufferArray = { header, body };channel.write(bufferArray); ​ buffers数组是write()方法的入参，write()方法会按照buffer在数组中的顺序，将数据写入到channel，注意只有position和limit之间的数据才会被写入。因此，如果一个buffer的容量为128byte，但是仅仅包含58byte的数据，那么这58byte的数据将被写入到channel中。因此与Scattering Reads相反，Gathering Writes能较好的处理动态消息。 缓冲区 发送给一个通道的所有数据都必须首先放到缓冲区中，同样地，从通道中读取的任何数据都要先读到缓冲区中。也就是说，不会直接对通道进行读写数据，而是要先经过缓冲区。 缓冲区实质上是一个数组，但它不仅仅是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。 ​ 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。缓冲区实际上是一个容器对象，更直接的说，其实就是一个数组，在NIO库中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的； 在写入数据时，它也是写入到缓冲区中的；任何时候访问 NIO 中的数据，都是将它放到缓冲区中。而在面向流I/O系统中，所有数据都是直接写入或者直接将数据读取到Stream对象中。在NIO中，所有的缓冲区类型都继承于抽象类Buffer，最常用的就是ByteBuffer，对于Java中的基本类型，基本都有一个具体Buffer类型与之相对应，它们之间的继承关系如下图所示： 缓冲区包括以下类型： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 缓冲区状态变量 capacity：最大容量； position：当前已经读写的字节数； limit：还可以读写的字节数。 Buffer的基本用法 1、使用Buffer读写数据，一般遵循以下四个步骤： （1）写入数据到Buffer （2）调用flip()方法 （3）从Buffer中读取数据 （4）调用clear()方法或者compact()方法 当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式。在读模式下，可以读取之前写入到buffer的所有数据。一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 Buffer与IntBuffer例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package pers.lxl.mylearnproject.javase.io.nio.buffer;import org.junit.jupiter.api.Test;import java.io.IOException;import java.io.RandomAccessFile;import java.nio.ByteBuffer;import java.nio.IntBuffer;import java.nio.channels.FileChannel;public class BufferDemo { //使用Buffer的例子 @Test public void testConect2() throws IOException { RandomAccessFile aFile = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); FileChannel inChannel = aFile.getChannel(); //create buffer with capacity of 48 bytes ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = inChannel.read(buf); //read into buffer. while (bytesRead != -1) { buf.flip(); //make buffer ready for read while (buf.hasRemaining()) { System.out.print((char) buf.get()); // read 1 byte at a time } buf.clear(); //make buffer ready for writing bytesRead = inChannel.read(buf); } } //使用IntBuffer的例子 @Test public void testConect3() throws IOException { // 分配新的int缓冲区，参数为缓冲区容量 // 新缓冲区的当前位置将为零，其界限(限制位置)将为其容量。 // 它将具有一个底层实现数组，其数组偏移量将为零。 IntBuffer buffer = IntBuffer.allocate(8); for (int i = 0; i &lt; buffer.capacity(); ++i) { int j = 2 * (i + 1); // 将给定整数写入此缓冲区的当前位置，当前位置递增 buffer.put(j); } // 重设此缓冲区，将限制设置为当前位置，然后将当前位置设置为0 buffer.flip(); // 查看在当前位置和限制位置之间是否有元素 while (buffer.hasRemaining()) { // 读取此缓冲区当前位置的整数，然后当前位置递增 int j = buffer.get(); System.out.print(j + &quot; &quot;); } }} Buffer的capacity、position和limit 为了理解Buffer的工作原理，需要熟悉它的三个属性： Capacity 、 Position 、 limit position和limit的含义取决于Buffer处在读模式还是写模式。不管Buffer处在什么模式，capacity的含义总是一样的。 这里有一个关于capacity，position和limit在读写模式中的说明 （1）capacity 作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 （2）position 1）写数据到Buffer中时，position表示写入数据的当前位置，position的初始值为0。当一个byte、long等数据写到Buffer后， position会向下移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1（因为position的初始值为0）. 2）读数据到Buffer中时，position表示读入数据的当前位置，如position=2时表示已开始读入了3个byte，或从第3个byte开始读取。通过ByteBuffer.flip()切换到读模式时position会被重置为0，当Buffer从position读入数据后，position会下移到下一个可读入的数据Buffer单元。 （3）limit 1）写数据时，limit表示可对Buffer最多写入多少个数据。写模式下，limit等于Buffer的capacity。 2）读数据时，limit表示Buffer里有多少可读数据（not null的数据），因此能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）。 Buffer的类型 Java NIO 有以下Buffer类型 ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些Buffer类型代表了不同的数据类型。换句话说，就是可以通过char，short，int，long，float 或 double类型来操作缓冲区中的字节。 Buffer分配和写数据 Buffer分配 要想获得一个Buffer对象首先要进行分配。 每一个Buffer类都有一个allocate方法。 下面是一个分配48字节capacity的ByteBuffer的例子。 ByteBuffer buf = ByteBuffer.allocate(48); 这是分配一个可存储1024个字符的CharBuffer： CharBuffer buf = CharBuffer.allocate(1024); 向Buffer中写数据 写数据到Buffer有两种方式： （1）从Channel写到Buffer。 （2）通过Buffer的put()方法写到Buffer里。 从Channel写到Buffer的例子 int bytesRead = inChannel.read(buf); //read into buffer. 通过put方法写Buffer的例子： buf.put(127); put方法有很多版本，允许你以不同的方式把数据写入到Buffer中。例如， 写到一个指定的位置，或者把一个字节数组写入到Buffer flip()方法 flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 （现在能读取多少个byte、char等）。 从Buffer中读取数据 从Buffer中读取数据有两种方式： （1）从Buffer读取数据到Channel。 （2）使用get()方法从Buffer中读取数据。 从Buffer读取数据到Channel的例子： //read from buffer into channel. int bytesWritten = inChannel.write(buf); 使用get()方法从Buffer中读取数据的例子 byte aByte = buf.get(); get方法有很多版本，允许你以不同的方式从Buffer中读取数据。例如，从指定position读取，或者从Buffer中读取数据到字节数组。 Buffer几个方法 rewind()方法方法 Buffer.rewind()将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等）。 clear()与compact()方法方法 ​ 一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。 如果调用的是**clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。 如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。 如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()**方法。 compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。 mark()与reset()方法方法 通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如： buffer.mark(); //call buffer.get() a couple of times, e.g. during parsing. buffer.reset(); //set position back to mark. 缓冲区操作 缓冲区分片 ​ 在NIO中，除了可以分配或者包装一个缓冲区对象外，还可以根据现有的缓冲区对象来创建一个子缓冲区，即在现有缓冲区上切出一片来作为一个新的缓冲区，但现有的缓冲区与创建的子缓冲区在底层数组层面上是数据共享的，也就是说，子缓冲区相当于是现有缓冲区的一个视图窗口。调用**slice()**方法可以创建一个子缓冲区。 1234567891011121314151617181920212223@Testpublic void testConect2() throws IOException { ByteBuffer buffer = ByteBuffer.allocate(10); // 缓冲区中的数据0-9 for (int i = 0; i &lt; buffer.capacity(); ++i) { buffer.put((byte) i); } // 创建子缓冲区 buffer.position(3); buffer.limit(7); ByteBuffer slice = buffer.slice(); // 改变子缓冲区的内容 for (int i = 0; i &lt; slice.capacity(); ++i) { byte b = slice.get(i); b *= 10; slice.put(i, b); } buffer.position(0); buffer.limit(buffer.capacity()); while (buffer.remaining() &gt; 0) { System.out.println(buffer.get()); }} 只读缓冲区 ​ 只读缓冲区非常简单，可读禁写。可以通过调用缓冲区的asReadOnlyBuffer()方法，将任何常规缓冲区转 换为只读缓冲区，这个方法返回一个与原缓冲区完全相同的缓冲区，并与原缓冲区共享数据，只不过它是只读的。原变只读变，如果原缓冲区的内容发生了变化，只读缓冲区的内容也随之发生变化： 12345678910111213141516171819202122@Test public void testConect4() throws IOException { ByteBuffer buffer = ByteBuffer.allocate(10); // 缓冲区中的数据0-9 for (int i = 0; i &lt; buffer.capacity(); ++i) { buffer.put((byte) i); } // 创建只读缓冲区 ByteBuffer readonly = buffer.asReadOnlyBuffer(); // 改变原缓冲区的内容 for (int i = 0; i &lt; buffer.capacity(); ++i) { byte b = buffer.get(i); b *= 10; buffer.put(i, b); } readonly.position(0); readonly.limit(buffer.capacity()); // 只读缓冲区的内容也随之改变 while (readonly.remaining() &gt; 0) { System.out.println(readonly.get()); } } ​ 如果尝试修改只读缓冲区的内容，则会报ReadOnlyBufferException异常。只读缓冲区对于保护数据很有用。在将缓冲区传递给某个 对象的方法时，无法知道这个方法是否会修改缓冲区中的数据。创建一个只读的缓冲区可以保证该缓冲区不会被修改。只可以把常规缓冲区转换为只读缓冲区，而不能将只读的缓冲区转换为可写的缓冲区。 直接缓冲区 ​ 直接缓冲区是为加快I/O速度，使用一种特殊方式为其分配内存的缓冲区，JDK文档中的描述为：给定一个直接字节缓冲区，Java虚拟机将尽最大努力直接对它执行本机I/O操作。也就是说，它会在每一次调用底层操作系统的本机I/O操作之前(或之后)，尝试避免将缓冲区的内容拷贝到一个中间缓冲区中 或者从一个中间缓冲区中拷贝数据。要分配直接缓冲区，需要调用allocateDirect()方法，而不是allocate()方法，使用方式与普通缓冲区并无区别。 拷贝文件示例： 1234567891011121314151617181920 @Test public void testConect5() throws IOException { String infile = &quot;d:\\\\atguigu\\\\01.txt&quot;; FileInputStream fin = new FileInputStream(infile); FileChannel fcin = fin.getChannel(); String outfile = String.format(&quot;d:\\\\atguigu\\\\02.txt&quot;); FileOutputStream fout = new FileOutputStream(outfile); FileChannel fcout = fout.getChannel();// 使用allocateDirect，而不是allocate ByteBuffer buffer = ByteBuffer.allocateDirect(1024); while (true) { buffer.clear(); int r = fcin.read(buffer); if (r == -1) { break; } buffer.flip(); fcout.write(buffer); } } 内存映射文件I/O ​ 内存映射文件I/O是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的I/O快的多。内存映射文件I/O是通过使文件中的数据出现为 内存数组的内容来完成的，这其初听起来似乎不过就是将整个文件读到内存中，但是事实上并不是这样。一般来说，只有文件中实际读取或者写入的部分才会映射到内存中。示例代码： 1234567891011static private final int start = 0; static private final int size = 1024; static public void main(String args[]) throws Exception { RandomAccessFile raf = new RandomAccessFile(&quot;d:\\\\atguigu\\\\01.txt&quot;, &quot;rw&quot;); FileChannel fc = raf.getChannel(); MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, start, size); mbb.put(0, (byte) 97); mbb.put(1023, (byte) 122); raf.close(); } 选择器 Selector简介 1、 Selector和 Channel关系 ​ Selector 一般称 为选择器 ，也可以翻译为 多路复用器 。它是Java NIO核心组件中的一个，**用于检查一个或多个NIO Channel（通道）的状态是否处于可读、可写。**如此可以实现单线程管理多个channels,也就是可以管理多个网络链接。即使用Selector的好处在于： 使用更少的线程来就可以来处理通道了， 相比使用多个线程，避免了线程上下文切换带来的开销。NIO 实现了 IO 多路复用中的 Reactor 模型，一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel 上的事件，从而让一个线程就可以处理多个事件。通过配置监听的通道 Channel 为非阻塞，那么当 Channel 上的 IO 事件还未到达时，就不会进入阻塞状态一直等待，而是继续轮询其它 Channel，找到 IO 事件已经到达的 Channel 执行。因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件，对于 IO 密集型的应用具有很好地性能。 2、 可选择通道 (SelectableChannel) （1）不是所有的Channel都可以被Selector 复用的。比方说，FileChannel就不能被选择器复用。判断一个Channel 能被Selector 复用，有一个前提：判断他是否继承了一个抽象类SelectableChannel。如果继承了SelectableChannel，则可以被复用，否则不能。 （2）SelectableChannel类提供了实现通道的可选择性所需要的公共方法。它是所有支持就绪检查的通道类的父类。所有socket通道，都继承了SelectableChannel类都是可选择的，包括从管道(Pipe)对象的中获得的通道。而FileChannel类，没有继承SelectableChannel，因此是不是可选通道。 （3）一个通道可以被注册到多个选择器上，但对每个选择器而言只能被注册一次,即通道与选择器一对多。通道和选择器之间的关系，使用注册的方式完成。SelectableChannel可以被注册到Selector对象上，在注册的时候，需要指定通道的哪些操作，是Selector感兴趣的。 3、Channel注册到注册到Selector （1）使用Channel.register（Selector sel，int ops）方法，将一个通道注册到一个选择器时。第一个参数，指定通道要注册的选择器。第二个参数指定选择器需要查询的通道操作。 （2）可以供选择器查询的通道操作四类： 可读 : SelectionKey.OP_READ 可写 : SelectionKey.OP_WRITE 连接 : SelectionKey.OP_CONNECT 接收 : SelectionKey.OP_ACCEPT Selector对通道的多操作类型用“位或”操作符实现： 比如：int key = SelectionKey.OP_READ | SelectionKey.OP_WRITE ; （3）选择器查询的不是通道的操作，而是通道的某个操作的一种就绪状态。什么是操作的就绪状态？一旦通道具备完成某个操作的条件，表示该通道的某个操作已经就绪，就可以被Selector查询到，程序可以对通道进行对应的操作。比方说，某个SocketChannel通道可以连接到一个服务器，则处于“连接就绪”(OP_CONNECT)。再比方说，一个ServerSocketChannel服务器通道准备好接收新进入的连接，则处于“接收就绪”（OP_ACCEPT）状态。还比方说，一个有数据可读的通道，可以说是“读就绪”(OP_READ)。一个等待写数据的通道可以说是“写就绪”(OP_WRITE)。 4、选择键(SelectionKey) （1）Channel注册到后，并且一旦通道处于某种就绪的状态，就可以被选择器查询到。这个工作，使用选择器Selector的select（）方法完成。select方法的作用，对感兴趣的通道操作，进行就绪状态的查询。 （2）Selector可以不断的查询Channel中发生的操作的就绪状态。并且挑选感兴趣的操作就绪状态。一旦通道有操作的就绪状态达成，并且是Selector感兴趣的操作，就会被Selector选中，放入选择键集合中。 （3）一个选择键，首先是包含了注册在Selector的通道操作的类型，比方说SelectionKey.OP_READ。也包含了特定的通道与特定的选择器之间的注册关系。 开发应用程序是，选择键是编程的关键。NIO的编程，就是根据对应的选择键，进行不同的业务逻辑处理。 （4）选择键的概念，和事件的概念比较相似。一个选择键类似监听器模式里边的一个事件。由于Selector不是事件触发的模式，而是主动去查询的模式，所以不叫事件Event，而是叫SelectionKey选择键。 NIO 常常被叫做非阻塞 IO，主要是因为 NIO 在网络通信中的非阻塞特性被广泛使用。 Selector的使用方法 1.Selector 的创建 通过调用Selector.open()方法创建一个Selector对象，如下： // 1、获取Selector选择器 Selector selector = Selector.open(); 2.注册 Channel 到 Selector 要实现Selector管理Channel，需要将channel注册到相应的Selector上 12345// 1、获取Selector选择器 Selector selector = Selector.open(); // 2、获取通道 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 3.设置为非阻塞 serverSocketChannel.configureBlocking(false); // 4、绑定连接 serverSocketChannel.bind(new InetSocketAddress(9999)); // 5、将通道注册到选择器上,并制定监听事件为：“接收”事件 serverSocketChannel.register(selector,SelectionKey.OP_ACCEPT); 上面通过调用通道的register()方法会将它注册到一个选择器上。 首先需要注意的是： （1）与Selector一起使用时，Channel必须处于非阻塞模式下，否则将抛出异常IllegalBlockingModeException。这意味着，FileChannel不能与Selector一起使用，因为FileChannel不能切换到非阻塞模式，而套接字相关的所有的通道都可以。 （2）一个通道，并没有一定要支持所有的四种操作。比如服务器通道ServerSocketChannel支持Accept 接受操作，而SocketChannel客户端通道则不支持。可以通过通道上的validOps()方法，来获取特定通道下所有支持的操作集合。 3.轮询查询就绪操作轮询查询就绪操作 （1）通过Selector的select（）方法，可以查询出已经就绪的通道操作，这些就绪的状态集合，包存在一个元素是SelectionKey对象的Set集合中。 （2）下面是Selector几个重载的查询select()方法： select():阻塞到至少有一个通道在你注册的事件上就绪了。 select(long timeout)：和select()一样，但最长阻塞事件为timeout毫秒。 selectNow():非阻塞，只要有通道就绪就立刻返回。 select()方法返回的int值，表示有多少通道已经就绪，更准确的说，是自前一次select方法以来到这一次select方法之间的时间段上，有多少通道变成就绪状态。 例如：首次调用select()方法，如果有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。 一旦调用select()方法，并且返回值不为0时，在Selector中有一个selectedKeys()方法，用来访问已选择键集合，迭代集合的每一个选择键元素，根据就绪操作的类型，完成对应的操作： 12345678910111213 Set selectedKeys = selector.selectedKeys();Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if(key.isAcceptable()) { // a connection was accepted by a ServerSocketChannel. } else if (key.isConnectable()) { // a connection was established with a remote server. } else if (key.isReadable()) { // a channel is ready for reading } else if (key.isWritable()) { // a channel is ready for writing } keyIterator.remove(); } 4.停止选择的方法停止选择的方法 ​ 选择器执行选择的过程，系统底层会依次询问每个通道是否已经就绪，这个过程可能会造成调用线程进入阻塞状态,那么我们有以下三种方式可以唤醒在select（）方法中阻塞的线程。 wakeup()方法 ：通过调用Selector对象的wakeup（）方法让处在阻塞状态的select()方法立刻返回 该方法使得选择器上的第一个还没有返回的选择操作立即返回。如果当前没有进行中的选择操作，那么下一次对select()方法的一次调用将立即返回。 close()方法 ：通过close（）方法关闭Selector， 该方法使得任何一个在选择操作中阻塞的线程都被唤醒（类似wakeup（）），同时使得注册到该Selector的所有Channel被注销，所有的键将被取消，但是Channel本身并不会关闭。 示例代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import org.junit.jupiter.api.Test;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.nio.charset.StandardCharsets;import java.util.Date;import java.util.Iterator;import java.util.Scanner;import java.util.Set;public class SelectorDemo { //服务端 @Test public void ServerDemo() throws Exception {// 1.获取服务端通道 ServerSocketChannel ssc = ServerSocketChannel.open();// 2.绑定端口号 ssc.socket().bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000));// 3.切换非阻塞模式 ssc.configureBlocking(false);// 4.创建选择器 Selector selector = Selector.open();// 5.注册channel，并且指定感兴趣的事件是 Accept ssc.register(selector, SelectionKey.OP_ACCEPT);// 6.创建buffer ByteBuffer readBuff = ByteBuffer.allocate(1024); ByteBuffer writeBuff = ByteBuffer.allocate(128); writeBuff.put(&quot;received&quot;.getBytes()); writeBuff.flip(); while (true) {// 7.检测通道就绪状态 int nReady = selector.select();// 8.遍历选择器，获取就绪通道集合 Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = keys.iterator(); while (it.hasNext()) { SelectionKey key = it.next(); it.remove();// 判断状态 if (key.isAcceptable()) {// 创建新的连接，并且把连接注册到selector上，进行监听，声明这个channel只对读操作感兴趣。 SocketChannel socketChannel = ssc.accept();// 切换到非阻塞 socketChannel.configureBlocking(false);// 注册 socketChannel.register(selector, SelectionKey.OP_READ); } else if (key.isReadable()) {// 获取通道 SocketChannel socketChannel = (SocketChannel) key.channel(); readBuff.clear(); socketChannel.read(readBuff); readBuff.flip(); System.out.println(&quot;received : &quot; + new String(readBuff.array())); key.interestOps(SelectionKey.OP_WRITE); } else if (key.isWritable()) { writeBuff.rewind(); SocketChannel socketChannel = (SocketChannel) key.channel(); socketChannel.write(writeBuff); key.interestOps(SelectionKey.OP_READ); } } } } //客户端代码 @Test public void ClientDemo() throws Exception {// 1.获取通道，绑定主机与端口号 SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000));// 2.切换到非阻塞模式 socketChannel.configureBlocking(false);// 3.创建buffer ByteBuffer writeBuffer = ByteBuffer.allocate(32); ByteBuffer readBuffer = ByteBuffer.allocate(32);// 4.写入buffer数据 Scanner scanner = new Scanner(System.in); while (scanner.hasNext()) { String str = scanner.next();// writeBuffer.put(&quot;hello&quot;.getBytes()); writeBuffer.put((new Date().toString() + &quot;----&gt;&quot; + str).getBytes());// 5.模式切换 writeBuffer.flip(); while (true) { // 将position设回0 writeBuffer.rewind();// 6.写入通道 socketChannel.write(writeBuffer);// 7.关闭 readBuffer.clear(); socketChannel.read(readBuffer); } } }// 输入方式 客户端 public static void main(String[] args) throws Exception { // 1.获取通道，绑定主机与端口号 SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000));// 2.切换到非阻塞模式 socketChannel.configureBlocking(false);// 3.创建buffer 超过会报错Exception in thread &quot;main&quot; java.nio.BufferOverflowException ByteBuffer writeBuffer = ByteBuffer.allocate(1024); ByteBuffer readBuffer = ByteBuffer.allocate(1024);// 4.写入buffer数据 Scanner scanner = new Scanner(System.in); while (scanner.hasNext()) { String str = scanner.next();// writeBuffer.put(&quot;hello&quot;.getBytes()); writeBuffer.put((new Date().toString() + &quot;----&gt;&quot; + str).getBytes());// 5.模式切换 writeBuffer.flip(); while (true) { // 将position设回0 writeBuffer.rewind();// 6.写入通道 socketChannel.write(writeBuffer);// 7.关闭 readBuffer.clear(); socketChannel.read(readBuffer); } } }} NIO编程步骤总结 第一步：创建Selector选择器 第二步：创建ServerSocketChannel通道，并绑定监听端口 第三步：设置Channel通道是非阻塞模式 第四步：把Channel注册到Socketor选择器上，监听连接事件 第五步：调用Selector的select方法（循环调用），监测通道的就绪状况 第六步：调用selectKeys方法获取就绪channel集合 第七步：遍历就绪channel集合，判断就绪事件类型，实现具体的业务操作 第八步：根据业务，决定是否需要再次注册监听事件，重复执行第三步操作 创建选择器 Selector selector = Selector.open(); 将通道注册到选择器上 ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); 通道必须配置为非阻塞模式，否则使用选择器就没有任何意义了，因为如果通道在某个事件上被阻塞，那么服务器就不能响应其它事件，必须等待这个事件处理完毕才能去处理其它事件，显然这和选择器的作用背道而驰。 在将通道注册到选择器上时，还需要指定要注册的具体事件，主要有以下几类： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 它们在 SelectionKey 的定义如下： public static final int OP_READ = 1 &lt;&lt; 0; public static final int OP_WRITE = 1 &lt;&lt; 2; public static final int OP_CONNECT = 1 &lt;&lt; 3; public static final int OP_ACCEPT = 1 &lt;&lt; 4; 可以看出每个事件可以被当成一个位域，从而组成事件集整数。例如： int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 监听事件 int num = selector.select(); 使用 select() 来监听到达的事件，它会一直阻塞直到有至少一个事件到达。 获取到达的事件 Set keys = selector.selectedKeys(); Iterator keyIterator = keys.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { // ... } else if (key.isReadable()) { // ... } keyIterator.remove(); } 事件循环 因为一次 select() 调用不能处理完所有的事件，并且服务器端有可能需要一直监听事件，因此服务器端处理事件的代码一般会放在一个死循环内。 while (true) { int num = selector.select(); Set keys = selector.selectedKeys(); Iterator keyIterator = keys.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { // ... } else if (key.isReadable()) { // ... } keyIterator.remove(); } } Pipe Java NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。 1、创建管道 通过Pipe.open()方法打开管道。 Pipe pipe = Pipe.open(); 2、写入管道 要向管道写数据，需要访问sink通道。： Pipe.SinkChannel sinkChannel = pipe.sink(); 通过调用SinkChannel的write()方法，将数据写入SinkChannel： String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis(); ByteBuffer buf = ByteBuffer.allocate(48); buf.clear(); buf.put(newData.getBytes()); buf.flip(); while(buf.hasRemaining()) 3、从管道读取数据 访问source通道 Pipe.SourceChannel sourceChannel = pipe.source(); 调用source通道的read()方法来读取数据： ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = sourceChannel.read(buf); read()方法返回的int值会告诉我们多少字节被读进了缓冲区。 4、代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041package pers.lxl.mylearnproject.javase.io.nio.pipe;import org.junit.jupiter.api.Test;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.Pipe;public class PipeDemo {// 1.获取管道// 2获取sink通道// 3.创建缓冲区// 4.写入数据// 5.获取source通道// 6.创建source通道// 7.创建缓冲区，读取数据// 8.关闭通道 /**/ @Test public void testPipe() throws IOException { // 1、获取通道 Pipe pipe = Pipe.open(); // 2、获取sink管道，用来传送数据 Pipe.SinkChannel sinkChannel = pipe.sink(); // 3、申请一定大小的缓冲区 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); byteBuffer.put(&quot;atguigu&quot;.getBytes()); byteBuffer.flip(); // 4、sink发送数据 sinkChannel.write(byteBuffer); // 5、创建接收pipe数据的source管道 Pipe.SourceChannel sourceChannel = pipe.source(); // 6、接收数据，并保存到缓冲区中 ByteBuffer byteBuffer2 = ByteBuffer.allocate(1024); int length = sourceChannel.read(byteBuffer2); System.out.println(new String(byteBuffer2.array(), 0, length)); sourceChannel.close(); sinkChannel.close(); }} FileLock 1、 FileLock 简介 ​ 文件锁在OS中很常见，如果多个程序同时访问、修改同一个文件，很容易因为文件数据不同步而出现问题。给文件加一个锁，同一时间，只能有一个程序修改此文件，或者程序都只能读此文件，这就解决了同步问题。 ​ 文件锁是进程级别的，不是线程级别的。文件锁可以解决多个进程并发访问、修改同一个文件的问题，但不能解决多线程并发访问、修改同一文件的问题。使用文件锁时，同一进程内的多个线程，可以同时访问、修改此文件。 ​ 文件锁是当前程序所属的JVM实例持有的，一旦获取到文件锁（对文件加锁），要调用release()，或者关闭对应的FileChannel对象，或者当前JVM退出，才会释放这个锁。 ​ 一旦某个进程（比如说JVM实例）对某个文件加锁，则在释放这个锁之前，此进程不能再对此文件加锁，就是说JVM实例在同一文件上的文件锁是不重叠的（进程级别不能重复在同一文件上获取锁）。 2、文件锁分类：文件锁分类： ​ 排它锁：又叫独占锁。对文件加排它锁后，该进程可以对此文件进行读写，该进程独占此文件，其他进程不能读写此文件，直到该进程释放文件锁。 ​ 共享锁：某个进程对文件加共享锁，其他进程也可以访问此文件，但这些进程都只能读此文件，不能写。线程是安全的。只要还有一个进程持有共享锁，此文件就只能读，不能写。 3、使用示例 //创建FileChannel对象，文件锁只能通过FileChannel对象来使用 FileChannel fileChannel=new FileOutputStream(&quot;./1.txt&quot;).getChannel(); //对文件加锁 FileLock lock=fileChannel.lock(); //对此文件进行一些读写操作。 //....... //释放锁 lock.release(); 文件锁要通过FileChannel对象使用。 4、获取文件锁方法、获取文件锁方法 有4种获取文件锁的方法： lock() //对整个文件加锁，默认为排它锁。 lock(long position, long size, booean shared) //自定义加锁方式。前2个参数指定要加锁的部分（可以只对此文件的部分内容加锁），第三个参数值指定是否是共享锁。 tryLock() //对整个文件加锁，默认为排它锁。 tryLock(long position, long size, booean shared) //自定义加锁方式。 如果指定为共享锁，则其它进程可读此文件，所有进程均不能写此文件，如果某进程试图对此文件进行写操作，会抛出异常。 5、locklock与tryLocktryLock的区别 lock是阻塞式的，如果未获取到文件锁，会一直阻塞当前线程，直到获取文件锁 tryLock和lock的作用相同，只不过tryLock是非阻塞式的，tryLock是尝试获取文件锁，获取成功就返回锁对象，否则返回null，不会阻塞当前线程。 6、FileLock两个方法： boolean isShared() //此文件锁是否是共享锁 boolean isValid() //此文件锁是否还有效 在某些OS上，对某个文件加锁后，不能对此文件使用通道映射。 7、完整例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;import java.nio.channels.FileLock;import java.nio.file.Path;import java.nio.file.Paths;import java.nio.file.StandardOpenOption;public class FileLockDemo { public static void main(String[] args) throws IOException { String input = &quot;atguigu&quot;; System.out.println(&quot;输入 :&quot; + input); ByteBuffer buf = ByteBuffer.wrap(input.getBytes()); String fp = &quot;D:\\\\atguigu\\\\01.txt&quot;; Path pt = Paths.get(fp); FileChannel channel = FileChannel.open(pt, StandardOpenOption.WRITE, StandardOpenOption.APPEND); channel.position(channel.size() - 1); // position of a cursor at the end of file // 获得锁方法一：lock()，阻塞方法，当文件锁不可用时，当前进程会被挂起// lock = channel.lock(); // 无参lock()为独占锁// lock = channel.lock(0L, Long.MAX_VALUE, true); //有参lock()为共享锁，有写操作会报异常 // 获得锁方法二：trylock()，非阻塞的方法，当文件锁不可用时，tryLock()会得到null值 FileLock lock = channel.tryLock(0, Long.MAX_VALUE, false); System.out.println(&quot;共享锁shared: &quot; + lock.isShared()); channel.write(buf); channel.close(); // Releases the Lock System.out.println(&quot;写操作完成.&quot;); //读取数据 readPrint(fp); } public static void readPrint(String path) throws IOException { FileReader filereader = new FileReader(path); BufferedReader bufferedreader = new BufferedReader(filereader); String tr = bufferedreader.readLine(); System.out.println(&quot;读取内容: &quot;); while (tr != null) { System.out.println(&quot; &quot; + tr); tr = bufferedreader.readLine(); } filereader.close(); bufferedreader.close(); }} Path 1、 Path 简介 Java Path接口是Java NIO更新的一部分，同Java NIO一起已经包括在Java6和Java7中。Java Path接口是在Java7中添加到Java NIO的。Path接口位于java.nio.file包中，所以Path接口的完全限定名称为java.nio.file.Path。 Java Path实例表示文件系统中的路径。一个路径可以指向一个文件或一个目录。路径可以是绝对路径，也可以是相对路径。绝对路径包含从文件系统的根目录到它指向的文件或目录的完整路径。相对路径包含相对于其他路径的文件或目录的路径。 在许多方面，java.nio.file.Path接口类似于java.io.File类，但是有一些差别。不过，在许多情况下，可以使用Path接口来替换File类的使用。 2、创建Path实例 使用java.nio.file.Path实例必须创建一个Path实例。可以使用Paths类(java.nio.file.Paths)中的静态方法Paths.get()来创建路径实例。 示例代码: 12345678import java.nio.file.Path;import java.nio.file.Paths;public class PathDemo { public static void main(String[] args) { Path path = Paths.get(&quot;d:\\\\atguigu\\\\001.txt&quot;); }} 上述代码，可以理解为，Paths.get()方法是Path实例的工厂方法。 3、创建绝对路径 （1）创建绝对路径，通过调用Paths.get()方法，给定绝对路径文件作为参数来完成。 示例代码： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); 上述代码中，绝对路径是d:\\atguigu\\001.txt。在Java字符串中， \\是一个转义字符，需要编写\\，告诉Java编译器在字符串中写入一个\\字符。 （2）如果在Linux、MacOS等操作字体上，上面的绝对路径可能如下: Path path = Paths.get(&quot;/home/jakobjenkov/myfile.txt&quot;); 绝对路径现在为/home/jakobjenkov/myfile.txt. （3）如果在Windows机器上使用了从/开始的路径，那么路径将被解释为相对于当前驱动器。 4、创建相对路径 Java NIO Path类也可以用于处理相对路径。您可以使用Paths.get(basePath, relativePath)方法创建一个相对路径。 示例代码: //代码1 Path projects = Paths.get(&quot;d:\\atguigu&quot;, &quot;projects&quot;); //代码2 Path file = Paths.get(&quot;d:\\atguigu&quot;, &quot;projects\\002.txt&quot;); 代码1创建了一个Java Path的实例，指向路径(目录):d:\\atguigu\\projects 代码2创建了一个Path的实例，指向路径(文件):d:\\atguigu\\projects\\002.txt 5、Path.normalize() Path接口的normalize()方法可以使路径标准化。标准化意味着它将移除所有在路径字符串的中间的.和..代码，并解析路径字符串所引用的路径。 Path.normalize()示例: String originalPath = &quot;d:\\atguigu\\projects\\..\\yygh-project&quot;; Path path1 = Paths.get(originalPath); System.out.println(&quot;path1 = &quot; + path1); Path path2 = path1.normalize(); System.out.println(&quot;path2 = &quot; + path2); 输出结果：标准化的路径不包含projects..部分 Files Java NIO Files类(java.nio.file.Files)提供了几种操作文件系统中的文件的方法。以下内容介绍Java NIO Files最常用的一些方法。java.nio.file.Files类与java.nio.file.Path实例一起工作，因此在学习Files类之前，需要先了解Path类。 1、 Files.createDirectory() Files.createDirectory()方法，用于根据Path实例创建一个新目录 示例： Path path = Paths.get(&quot;d:\\sgg&quot;); try { Path newDir = Files.createDirectory(path); } catch(FileAlreadyExistsException e){ // 目录已经存在 } catch (IOException e) { // 其他发生的异常 e.printStackTrace(); } 第一行创建表示要创建的目录的Path实例。在try-catch块中，用路径作为参数调用Files.createDirectory()方法。如果创建目录成功，将返回一个Path实例，该实例指向新创建的路径。 如果该目录已经存在，则是抛出一个java.nio.file.FileAlreadyExistsException。如果出现其他错误，可能会抛出IOException。例如，如果想要的新目录的父目录不存在，则可能会抛出IOException。 2、Files.copy() （1）Files.copy()方法从一个路径拷贝一个文件到另外一个目录 示例： Path sourcePath = Paths.get(&quot;d:\\atguigu\\01.txt&quot;); Path destinationPath = Paths.get(&quot;d:\\atguigu\\002.txt&quot;); try { Files.copy(sourcePath, destinationPath); } catch(FileAlreadyExistsException e) { // 目录已经存在 } catch (IOException e) { // 其他发生的异常 e.printStackTrace(); } 首先，该示例创建两个Path实例。然后，这个例子调用Files.copy()，将两个Path实例作为参数传递。这可以让源路径引用的文件被复制到目标路径引用的文件中。 如果目标文件已经存在，则抛出一个java.nio.file.FileAlreadyExistsException异常。如果有其他错误，则会抛出一个IOException。例如，如果将该文件复制到不存在的目录，则会抛出IOException。 （2）覆盖已存在的文件 Files.copy()方法的第三个参数。如果目标文件已经存在，这个参数指示copy()方法覆盖现有的文件。 Files.copy(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING); 3、Files.move() Files.move()用于将文件从一个路径移动到另一个路径。移动文件与重命名相同，但是移动文件既可以移动到不同的目录，也可以在相同的操作中更改它的名称。 示例： Path sourcePath = Paths.get(&quot;d:\\atguigu\\01.txt&quot;); Path destinationPath = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); try { Files.move(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING); } catch (IOException e) { //移动文件失败 e.printStackTrace(); } Files.move()的第三个参数。这个参数告诉Files.move()方法来覆盖目标路径上的任何现有文件。 4、Files.delete() Files.delete()方法可以删除一个文件或者目录。 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); try { Files.delete(path); } catch (IOException e) { // 删除文件失败 e.printStackTrace(); } 创建指向要删除的文件的Path。然后调用Files.delete()方法。如果Files.delete()不能删除文件(例如，文件或目录不存在)，会抛出一个IOException。 5.Files.walkFileTree() （1）Files.walkFileTree()方法包含递归遍历目录树功能，将Path实例和FileVisitor作为参数。Path实例指向要遍历的目录，FileVisitor在遍历期间被调用。 （2）FileVisitor是一个接口，必须自己实现FileVisitor接口，并将实现的实例传递给walkFileTree()方法。在目录遍历过程中，您的FileVisitor实现的每个方法都将被调用。如果不需要实现所有这些方法，那么可以扩展SimpleFileVisitor类，它包含FileVisitor接口中所有方法的默认实现。 （3）FileVisitor接口的方法中，每个都返回一个FileVisitResult枚举实例。FileVisitResult枚举包含以下四个选项: CONTINUE 继续 TERMINATE 终止 SKIP_SIBLING 跳过同级 SKIP_SUBTREE 跳过子级 （4）查找一个名为001.txt的文件示例： Path rootPath = Paths.get(&quot;d:\\atguigu&quot;); String fileToFind = File.separator + &quot;001.txt&quot;; try { Files.walkFileTree(rootPath, new SimpleFileVisitor() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { String fileString = file.toAbsolutePath().toString(); //System.out.println(&quot;pathString = &quot; + fileString); if(fileString.endsWith(fileToFind)){ System.out.println(&quot;file found at path: &quot; + file.toAbsolutePath()); return FileVisitResult.TERMINATE; } return FileVisitResult.CONTINUE; } }); } catch(IOException e){ e.printStackTrace(); } （5）java.nio.file.Files类包含许多其他的函数，有关这些方法的更多信息，请查看java.nio.file.Files类的JavaDoc。 AsynchronousFileChannel 在Java 7中，Java NIO中添加了AsynchronousFileChannel，也就是是异步地将数据写入文件。 1、创建 AsynchronousFileChannel 通过静态方法open()创建 示例： Path path = Paths.get(&quot;d:\\atguigu\\01.txt&quot;); try { AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); } catch (IOException e) { e.printStackTrace(); } open()方法的第一个参数指向与AsynchronousFileChannel相关联文件的Path实例。 第二个参数是一个或多个打开选项，它告诉AsynchronousFileChannel在文件上执行什么操作。在本例中，我们使用了StandardOpenOption.READ选项，表示该文件将被打开阅读。 2、通过Future读取数据 可以通过两种方式从AsynchronousFileChannel读取数据。第一种方式是调用返回Future的read()方法 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; Future operation = fileChannel.read(buffer, position); while(!operation.isDone()); buffer.flip(); byte[] data = new byte[buffer.limit()]; buffer.get(data); System.out.println(new String(data)); buffer.clear(); 上述代码： （1）创建了一个AsynchronousFileChannel， （2）创建一个ByteBuffer，它被传递给read()方法作为参数，以及一个0的位置。 （3）在调用read()之后，循环，直到返回的isDone()方法返回true。 （4）读取操作完成后，数据读取到ByteBuffer中，然后打印到System.out中。 3、通过CompletionHandler读取数据 第二种方法是调用read()方法，该方法将一个CompletionHandler作为参数 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; fileChannel.read(buffer, position, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() { @Override public void completed(Integer result, ByteBuffer attachment) { System.out.println(&quot;result = &quot; + result); attachment.flip(); byte[] data = new byte[attachment.limit()]; attachment.get(data); System.out.println(new String(data)); attachment.clear(); } @Override public void failed(Throwable exc, ByteBuffer attachment) { } }); （1）读取操作完成，将调用CompletionHandler的completed()方法。 （2）对于completed()方法的参数传递一个整数，它告诉我们读取了多少字节，以及传递给read()方法的“附件”。“附件”是read()方法的第三个参数。在本代码中，它是ByteBuffer，数据也被读取。 （3）如果读取操作失败，则将调用CompletionHandler的failed()方法。 4、通过Future写数据 和读取一样，可以通过两种方式将数据写入一个AsynchronousFileChannel 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.WRITE); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; buffer.put(&quot;atguigu data&quot;.getBytes()); buffer.flip(); Future operation = fileChannel.write(buffer, position); buffer.clear(); while(!operation.isDone()); System.out.println(&quot;Write over&quot;); 首先，AsynchronousFileChannel以写模式打开。然后创建一个ByteBuffer，并将一些数据写入其中。然后，ByteBuffer中的数据被写入到文件中。最后，示例检查返回的Future，以查看写操作完成时的情况。 注意，文件必须已经存在。如果该文件不存在，那么write()方法将抛出一个java.nio.file.NoSuchFileException。 5、通过CompletionHandler写数据 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); if(!Files.exists(path)){ try { Files.createFile(path); } catch (IOException e) { e.printStackTrace(); } } AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.WRITE); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; buffer.put(&quot;atguigu data&quot;.getBytes()); buffer.flip(); fileChannel.write(buffer, position, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() { @Override public void completed(Integer result, ByteBuffer attachment) { System.out.println(&quot;bytes written: &quot; + result); } @Override public void failed(Throwable exc, ByteBuffer attachment) { System.out.println(&quot;Write failed&quot;); exc.printStackTrace(); } }); 当写操作完成时，将会调用CompletionHandler的completed()方法。如果写失败，则会调用failed()方法。 字符集（Charset） java中使用Charset来表示字符集编码对象 Charset常用静态方法 public static Charset forName(String charsetName)//通过编码类型获得Charset对象 public static SortedMap&lt;String,Charset&gt; availableCharsets()//获得系统支持的所有编码方式 public static Charset defaultCharset()//获得虚拟机默认的编码方式 public static boolean isSupported(String charsetName)//判断是否支持该编码类型 Charset常用普通方法 public final String name()//获得Charset对象的编码类型(String) public abstract CharsetEncoder newEncoder()//获得编码器对象 public abstract CharsetDecoder newDecoder()//获得解码器对象 代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import org.junit.jupiter.api.Test;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.charset.CharacterCodingException;import java.nio.charset.Charset;import java.nio.charset.CharsetDecoder;import java.nio.charset.CharsetEncoder;import java.util.Map;import java.util.Set;public class CharsetDemo { @Test public void charSetEncoderAndDecoder() throws CharacterCodingException { Charset charset = Charset.forName(&quot;UTF-8&quot;); //1.获取编码器 CharsetEncoder charsetEncoder = charset.newEncoder(); // 2.获取解码器 CharsetDecoder charsetDecoder = charset.newDecoder(); // 3.获取需要解码编码的数据 CharBuffer charBuffer = CharBuffer.allocate(1024); charBuffer.put(&quot;字符集编码解码&quot;); charBuffer.flip(); //4.编码 ByteBuffer byteBuffer = charsetEncoder.encode(charBuffer); System.out.println(&quot;编码后：&quot;); for (int i = 0; i &lt; byteBuffer.limit(); i++) { System.out.println(byteBuffer.get()); } //5.解码 byteBuffer.flip(); CharBuffer charBuffer1 = charsetDecoder.decode(byteBuffer); System.out.println(&quot;解码后：&quot;); System.out.println(charBuffer1.toString()); System.out.println(&quot;指定其他格式解码:&quot;); Charset charset1 = Charset.forName(&quot;GBK&quot;); byteBuffer.flip(); CharBuffer charBuffer2 = charset1.decode(byteBuffer); System.out.println(charBuffer2.toString()); //6.获取Charset所支持的字符编码 Map&lt;String, Charset&gt; map = Charset.availableCharsets(); Set&lt;Map.Entry&lt;String, Charset&gt;&gt; set = map.entrySet(); for (Map.Entry&lt;String, Charset&gt; entry : set) { System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue().toString()); } }} NIO多线程聊天室例子 服务端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.*;import java.nio.charset.Charset;import java.util.Iterator;import java.util.Set;//服务器端public class ChatServer { //服务器端启动的方法 public void startServer() throws IOException { //1 创建Selector选择器 Selector selector = Selector.open(); //2 创建ServerSocketChannel通道 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //3 为channel通道绑定监听端口 serverSocketChannel.bind(new InetSocketAddress(8000)); //设置非阻塞模式 serverSocketChannel.configureBlocking(false); //4 把channel通道注册到selector选择器上 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); System.out.println(&quot;服务器已经启动成功了&quot;); //5 循环，等待有新链接接入 while (true) for (; ; ) { //获取channel数量 int readChannels = selector.select(); if (readChannels == 0) { continue; } //获取可用的channel Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); //遍历集合 Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) { SelectionKey selectionKey = iterator.next(); //移除set集合当前selectionKey iterator.remove(); //6 根据就绪状态，调用对应方法实现具体业务操作 //6.1 如果accept状态 if (selectionKey.isAcceptable()) { acceptOperator(serverSocketChannel, selector); } //6.2 如果可读状态 if (selectionKey.isReadable()) { readOperator(selector, selectionKey); } } } } //处理可读状态操作 private void readOperator(Selector selector, SelectionKey selectionKey) throws IOException { //1 从SelectionKey获取到已经就绪的通道 SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); //2 创建buffer ByteBuffer byteBuffer = ByteBuffer.allocate(1024); //3 循环读取客户端消息 int readLength = socketChannel.read(byteBuffer); String message = &quot;&quot;; if (readLength &gt; 0) { //切换读模式 byteBuffer.flip(); //读取内容 message += Charset.forName(&quot;UTF-8&quot;).decode(byteBuffer); } //4 将channel再次注册到选择器上，监听可读状态 socketChannel.register(selector, SelectionKey.OP_READ); //5 把客户端发送消息，广播到其他客户端 if (message.length() &gt; 0) { //广播给其他客户端 System.out.println(message); castOtherClient(message, selector, socketChannel); } } //广播到其他客户端 private void castOtherClient(String message, Selector selector, SocketChannel socketChannel) throws IOException { //1 获取所有已经接入channel Set&lt;SelectionKey&gt; selectionKeySet = selector.keys(); //2 循环想所有channel广播消息 for (SelectionKey selectionKey : selectionKeySet) { //获取每个channel Channel tarChannel = selectionKey.channel(); //不需要给自己发送 if (tarChannel instanceof SocketChannel &amp;&amp; tarChannel != socketChannel) { ((SocketChannel) tarChannel).write(Charset.forName(&quot;UTF-8&quot;).encode(message)); } } } //处理接入状态操作 private void acceptOperator(ServerSocketChannel serverSocketChannel, Selector selector) throws IOException { //1 接入状态，创建socketChannel SocketChannel socketChannel = serverSocketChannel.accept(); //2 把socketChannel设置非阻塞模式 socketChannel.configureBlocking(false); //3 把channel注册到selector选择器上，监听可读状态 socketChannel.register(selector, SelectionKey.OP_READ); //4 客户端回复信息 socketChannel.write(Charset.forName(&quot;UTF-8&quot;).encode(&quot;欢迎进入聊天室，请注意隐私安全&quot;)); } //启动主方法 public static void main(String[] args) { try { new ChatServer().startServer(); } catch (IOException e) { e.printStackTrace(); } }} 客户端代码 123456789101112131415161718192021222324252627282930import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.util.Scanner;//客户端public class ChatClient { //启动客户端方法 public void startClient(String name) throws IOException { //连接服务端 SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000)); //接收服务端响应数据 Selector selector = Selector.open(); socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); //创建线程 new Thread(new ClientThread(selector)).start(); //向服务器端发送消息 Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()) { String msg = scanner.nextLine(); if (msg.length() &gt; 0) { socketChannel.write(Charset.forName(&quot;UTF-8&quot;).encode(name + &quot; : &quot; + msg)); } } }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.util.Iterator;import java.util.Set;public class ClientThread implements Runnable { private Selector selector; public ClientThread(Selector selector) { this.selector = selector; } @Override public void run() { try { for (; ; ) { //获取channel数量 int readChannels = selector.select(); if (readChannels == 0) { continue; } //获取可用的channel Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); //遍历集合 Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) { SelectionKey selectionKey = iterator.next(); //移除set集合当前selectionKey iterator.remove(); //如果可读状态 if (selectionKey.isReadable()) { readOperator(selector, selectionKey); } } } } catch (Exception e) { } } //处理可读状态操作 private void readOperator(Selector selector, SelectionKey selectionKey) throws IOException { //1 从SelectionKey获取到已经就绪的通道 SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); //2 创建buffer ByteBuffer byteBuffer = ByteBuffer.allocate(1024); //3 循环读取客户端消息 int readLength = socketChannel.read(byteBuffer); String message = &quot;&quot;; if (readLength &gt; 0) { //切换读模式 byteBuffer.flip(); //读取内容 message += Charset.forName(&quot;UTF-8&quot;).decode(byteBuffer); } //4 将channel再次注册到选择器上，监听可读状态 socketChannel.register(selector, SelectionKey.OP_READ); //5 把客户端发送消息，广播到其他客户端 if (message.length() &gt; 0) { //广播给其他客户端 System.out.println(message); } }} 12345678import java.io.IOException;public class AClient { public static void main(String[] args) throws IOException { new ChatClient().startClient(&quot;lxl&quot;); }} 12345678import java.io.IOException;public class BClient { public static void main(String[] args) throws IOException { new ChatClient().startClient(&quot;xqm&quot;); }} others Filter模式 通过一个“基础”组件再叠加各种“附加”功能组件的模式，称之为Filter模式（或者装饰器模式：Decorator）。它可以让我们通过少量的类来实现各种功能的组合 编写FilterInputStream 操作Zip ZipInputStream JarInputStream是从ZipInputStream派生 读取zip包 创建一个ZipInputStream，通常是传入一个FileInputStream作为数据源，然后，循环调用getNextEntry()，直到返回null，表示zip流结束。 一个ZipEntry表示一个压缩文件或目录，如果是压缩文件，我们就用read()方法不断读取，直到返回-1： try (ZipInputStream zip = new ZipInputStream(new FileInputStream(...))) { ZipEntry entry = null; while ((entry = zip.getNextEntry()) != null) { String name = entry.getName(); if (!entry.isDirectory()) { int n; while ((n = zip.read()) != -1) { ... } } } } 写入zip包 可以直接写入内容到zip包。我们要先创建一个ZipOutputStream，通常是包装一个FileOutputStream，然后，每写入一个文件前，先调用putNextEntry()，然后用write()写入byte[]数据，写入完毕后调用closeEntry()结束这个文件的打包。 try (ZipOutputStream zip = new ZipOutputStream(new FileOutputStream(...))) { File[] files = ... for (File file : files) { zip.putNextEntry(new ZipEntry(file.getName())); zip.write(getFileDataAsBytes(file)); zip.closeEntry(); } } 读取classpath资源 把资源存储在classpath中可以避免文件路径依赖； Class对象的getResourceAsStream()可以从classpath中读取指定资源； 根据classpath读取资源时，需要检查返回的InputStream是否为null。 使用Files byte[] data = Files.readAllBytes(Paths.get(&quot;/path/to/file.txt&quot;)); // 默认使用UTF-8编码读取: String content1 = Files.readString(Paths.get(&quot;/path/to/file.txt&quot;)); // 可指定编码: String content2 = Files.readString(Paths.get(&quot;/path/to/file.txt&quot;), StandardCharsets.ISO_8859_1); // 按行读取并返回每行内容: List lines = Files.readAllLines(Paths.get(&quot;/path/to/file.txt&quot;)); / 写入二进制文件: byte[] data = ... Files.write(Paths.get(&quot;/path/to/file.txt&quot;), data); // 写入文本并指定编码: Files.writeString(Paths.get(&quot;/path/to/file.txt&quot;), &quot;文本内容...&quot;, StandardCharsets.ISO_8859_1); // 按行写入文本: List lines = ... Files.write(Paths.get(&quot;/path/to/file.txt&quot;), lines); 此外，Files工具类还有copy()、delete()、exists()、move()等快捷方法操作文件和目录。 最后需要特别注意的是，Files提供的读写方法，受内存限制，只能读写小文件，例如配置文件等，不可一次读入几个G的大文件。读写大型文件仍然要使用文件流，每次只读写一部分文件内容。 内存映射文件 内存映射文件 I/O 是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的 I/O 快得多。 向内存映射文件写入可能是危险的，只是改变数组的单个元素这样的简单操作，就可能会直接修改磁盘上的文件。修改数据与将数据保存到磁盘是没有分开的。 下面代码行将文件的前 1024 个字节映射到内存中，map() 方法返回一个 MappedByteBuffer，它是 ByteBuffer 的子类。因此，可以像使用其他任何 ByteBuffer 一样使用新映射的缓冲区，操作系统会在需要时负责执行映射。 MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, 0, 1024); OFFICE PIC CSV HTML 压缩文件 IO NIO 面向流 面向缓冲 阻塞IO 非阻塞IO 无 选择器 NET OSI七层模型 TCP/IP五层模型 三次握手、四次挥手 IP 域名 端口 通信协议 一次请求的过程 面向对象 JDK新特性 操作符 流程控制 初始化与清理 权限控制 复用类 多态 接口 内部类 异常 字符串 正则表达式 泛型 数组 枚举 注解 并发 图形化 其他基础 Array [JVM与上层技术](JVM 与上层技术) hashcode与 equals与 == 相关概念 1.equals(Object obj)方法用来判断两个对象是否“相同”，如果“相同”则返回true，否则返回false。 hashCode()方法返回一个int数，在Object类中的默认实现是“将该对象的内部地址转换成一个整数返回”。 2.若两个对象equals(Object obj)返回true，则hashCode（）有必要也返回相同的int数。 3.若两个对象equals(Object obj)返回false，则hashCode（）不一定返回不同的int数。 4.若两个对象hashCode（）返回相同int数，则equals（Object obj）不一定返回true。 5.若两个对象hashCode（）返回不同int数，则equals（Object obj）一定返回false。 6.同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 7.若重写了equals(Object obj)方法，则有必要重写hashCode()方法。 什么时候需要重写hashcode与equals？ 对象比较 equals与==区别？ 基本类型中，==判断值是否相等，引用类型中比较的是地址是否相等 equals只能用于引用类型，equals具体判断要在判断对象中看equals实现,默认是判断地址，重写之后看具体实现，一般是比较对象内容。 面向对象 封装继承多态 封装 ​ 将一系列操作封装成一个功能，隐藏实现细节，施加权限控制，让用户不再操心功能实现，减少代码冗余。 继承（extends） ​ 消除重复创建相同功能类，继承以复制基类功能属性并覆盖已进行扩展修改。 ​ 继承初始化链： 12345678/**存在继承的情况下，初始化顺序为： 父类（静态变量、静态语句块） 子类（静态变量、静态语句块） 父类（实例变量、普通语句块） 父类（构造函数） 子类（实例变量、普通语句块） 子类（构造函数）*/ 静态--&gt;实例--》普通语句块--》构造 多态 接口和实现类，相同接口有不同实现类以实现不同样式的相同功能。 final、finally、finalize ​ final通常含义为不可改变，无法改变由设计和效率两个方面。修饰类不可被继承（类中方法被隐式生命为final），修饰方法不可被覆盖（效果同被private修饰的方法，也被隐式final了），修饰变量不可被修改（基本变量修饰后即为常量），修饰对象只是限制了其引用地址但不会限制其对象值。finally是在try..catch语句中try执行后且未被将程序终止最后必须执行的语句块、finalize用于对象回收，属于Object的方法，调用并不一定会马上回收对象。 向上/下转型 向上即父用子，向下即子用父。 123456Father f1 = new Son(); // 这就叫 upcasting （向上转型)// 现在 f1 引用指向一个Son对象//并不是所有的对象都可以向下转型，只有当这个对象原本就是子类对象通过向上转型得到的时候才能够成功转型。Son s1 = (Son)f1; // 这就叫 downcasting (向下转型)// 现在f1 还是指向 Son对象 内部类、匿名内部类 没有名字的类或接口实现所以只能使用一次，无法被其他类实例化，用来简化代码编写。 匿名类可继承、重写父类的方法 使用匿名类时，必然是在某个类中直接用匿名类创建对象，因此匿名类一定是内部类 匿名类可以访问外嵌类中的成员变量和方法，匿名类的类体中不可以声明static成员变量和static方法 由于匿名类是一个子类，但没有类名，所以在用匿名类创建对象时，要直接使用父类的构造方法 static ​ 方便在没有创建对象的情况下来进行调用（方法/变量），静态方法中不能依赖非静态方法、属性，反之非静态成员方法中可访问静态成员变量、方法。 静态变量在内存中只有一个，非静态变量随对象创建而创建，各个变量之间互不影响。static不能用来修饰局部变量。静态代码块只有在类加载的时候创建一次，适合于只需要进行一次初始化的操作。静只静，非可静 CAP、BASE理论 AQS JDK新版本特征 Lambda 表达式 流Stream（） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package pers.lxl.mylearnproject.programbase.newjdk;import java.util.Arrays;import java.util.IntSummaryStatistics;import java.util.List;import java.util.Random;import java.util.stream.Collectors;/** * JDK8 新特性stream（） * 以声明的方式处理数据，类似用sql从数据库查询数据的方式来提供对集合的运算和表达的高阶抽象 * 其将要处理的数据看成一种流，流在管道中传输，其可以再管道的节点中进行筛选排序聚合等一系列操作 * Stream（流）是一个来自数据源的元素队列并支持聚合操作 * 元素是特定类型的对象，形成一个队列。 Java中的Stream并不会存储元素，而是按需计算。 * 数据源 流的来源。 可以是集合，数组，I/O channel， 产生器generator 等。 * 聚合操作 类似SQL语句一样的操作， 比如filter, map, reduce, find, match, sorted等。 * 和以前的Collection操作不同， Stream操作还有两个基础的特征： * Pipelining: 中间操作都会返回流对象本身。 这样多个操作可以串联成一个管道， 如同流式风格（fluent style）。 这样做可以对操作进行优化， 比如延迟执行(laziness)和短路(short-circuiting)。 * 内部迭代： 以前对集合遍历都是通过Iterator或者For-Each的方式, 显式的在集合外部进行迭代， 这叫做外部迭代。 Stream提供了内部迭代的方式， 通过访问者模式(Visitor)实现。 */public class StreamL { public static void main(String[] args) {// 1.生成流 //stream() − 为集合创建串行流。 //parallelStream() − 为集合创建并行流。 List&lt;String&gt; strings = Arrays.asList(&quot;abc&quot;, &quot;&quot;, &quot;bc&quot;, &quot;efg&quot;, &quot;abcd&quot;, &quot;&quot;, &quot;jkl&quot;); List&lt;String&gt; filtered = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList()); List&lt;String&gt; filteredParallelStream = strings.parallelStream().filter(string -&gt; !string.isEmpty()).collect(Collectors.toList());// 2.聚合操作// 迭代数据foreach(),limit() 获取指定数量的流,sorted() 对流进行排序。 Random random = new Random(); random.ints().limit(10).sorted().forEach(System.out::println);// map() 映射每个元素到对应的结果 List&lt;Integer&gt; numbers = Arrays.asList(3, 2, 2, 3, 7, 3, 5);// 获取对应的平方数 List&lt;Integer&gt; squaresList = numbers.stream().map( i -&gt; i*i).distinct().collect(Collectors.toList());// filter() 通过设置的条件过滤出元素// 获取空字符串的数量 long count = strings.stream().filter(string -&gt; string.isEmpty()).count();// 并行（parallel）程序// parallelStream 是流并行处理程序的代替方法。以下实例我们使用 parallelStream 来输出空字符串的数量：// 获取空字符串的数量// 3.Collectors// Collectors 类实现归约操作，例如将流转换成集合和聚合元素。Collectors 可用于返回列表或字符串： System.out.println(&quot;筛选列表: &quot; + filtered); String mergedString = strings.stream().filter(string -&gt; !string.isEmpty()).collect(Collectors.joining(&quot;, &quot;)); System.out.println(&quot;合并字符串: &quot; + mergedString);// 4.统计// 一些产生统计结果的收集器也非常有用。它们主要用于int、double、long等基本类型上，它们可以用来产生类似如下的统计结果。 IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics(); System.out.println(&quot;列表中最大的数 : &quot; + stats.getMax()); System.out.println(&quot;列表中最小的数 : &quot; + stats.getMin()); System.out.println(&quot;所有数之和 : &quot; + stats.getSum()); System.out.println(&quot;平均数 : &quot; + stats.getAverage()); }} 其他 局部变量需要显示初始化 保留字： 1）48个关键字：abstract、assert、boolean、break、byte、case、catch、char、class、continue、default、do、double、else、enum、extends、final、finally、float、for、if、implements、import、int、interface、instanceof、long、native、new、package、private、protected、public、return、short、static、strictfp、super、switch、synchronized、this、throw、throws、transient、try、void、volatile、while。 2）2个保留字（现在没用以后可能用到作为关键字）：goto、const。 3）3个特殊直接量：true、false、null。 基础类库（从上到下简化了解） 【java.io java.lang java.util】 【java.lang.reflect java.net javax.net.* java.nio.* java.util.concurrent.】 【java.lang.annotation javax.annotation.* java.lang.refjava.math java.rmi.*javax.rmi. java.security. javax.security. java.sqljavax.sql. javax.transaction. *java.text javax.xml.org.w3c.dom.org.xml.sax. javax.crypto. * javax.imageio. javax.jws. * java.util.jar java.util.logging java.util.prefs java.util.regex java.util.zip】 访问权限 private default protected public 同一个类中 √ √ √ √ 同一个包中 √ √ √ 子类中 √ √ 全局范围内 √ 基本数据类型 数据类型 位数 默认值 取值范围 举例说明 byte(位) 8 0 -2^7 --- 2^7-1 byte b = 10; boolean(布尔值) 8 false true、false boolean b = true; short(短整数) 16 0 -2^15 --- 2^15-1 short s = 10; char(字符) 16 空 0 --- 2^16-1 char c = 'c'; int(整数) 32 0 -2^31 --- 2^31-1 int i = 10; float(单精度) 32 0.0 -2^31 --- 2^31-1 float f = 10.0f; long(长整数) 64 0 -2^63 --- 2^63-1 long l = 10l; double(双精度) 64 0.0 -2^63 --- 2^63-1 double d = 10.0d; 面向对象 封装继承多态， 自动类型转换 1.若参与运算的数据类型不同，则先转换成同一类型，然后进行运算。 2.转换按数据长度增加的方向进行，以保证精度不降低。例如int型和long型运算时，先把int量转成long型后再进行运算。 3.所有的浮点运算都是以双精度进行的，即使仅含float单精度量运算的表达式，也要先转换成double型，再作运算。 4.char型和short型参与运算时，必须先转换成int型。 5.在赋值运算中，赋值号两边的数据类型不同时，需要把右边表达式的类型将转换为左边变量的类型。如果右边表达式的数据类型长度比左边长时，将丢失一部分数据，这样会降低精度。 下图表示了类型自动转换的规则： String、StringBuffer、StringBuilder 区别 1234567891011121314151617181920212223242526** * String使用+拼接字符串，如果在循环中，在循环中， * 每次循环都会创建新的字符串对象，然后扔掉旧的字符串。 * 这样，绝大部分字符串都是临时对象，不但浪费内存，还会影响GC效率 * 为高效拼接字符串Java标准库提供了StringBuilder，它是一个可变对象，可以预分配缓冲区 * 对于普通的字符串+操作，并不需要我们将其改写为StringBuilder， * 因为Java编译器在编译时就自动把多个连续的+操作编码为StringConcatFactory的操作。 * 在运行期，StringConcatFactory会自动把字符串连接操作优化为数组复制或者StringBuilder操作 * StringBuffer通过同步来保证多个线程操作StringBuffer也是安全的，但是同步会带来执行速度的下降 * StringBuilder和StringBuffer接口完全相同，现在完全没有必要使用StringBuffer * @author lxl*/public class StringBuilderL { public static void main(String[] args) { StringBuffer sb = new StringBuffer(1024); for (int i = 0; i &lt; 1000; i++) { sb.append('s'); sb.append(i); } sb.append(&quot;good &quot;) .append(&quot;LXL&quot;) .insert(0, &quot;DD&quot;);//链式操作 String s = sb.toString(); System.out.println(s); }} JDK与JRE区别 12#查看JDK路径 java -verbose JDK ​ JDK(Java SE Development Kit)，Java标准开发包，它提供了编译、运行Java程序所需的各种工具和资源，包括Java编译器、Java运行时环境，以及常用的Java类库等。 JRE ​ JRE( Java Runtime Environment) 、Java运行环境，用于解释执行Java的字节码文件。普通用户而只需要安装 JRE（Java Runtime Environment）来运行 Java 程序。而程序开发者必须安装JDK来编译、调试程序。 精度遗失问题 ​ float 还是double都是浮点数，而计算机是二进制的，浮点数会失去一定的精确度，只能用来科学或工程计算，不能用来商业计算。根本原因是:十进制值通常没有完全相同的二进制表示形式;十进制数的二进制表示形式可能不精确。只能无限接近于那个值 BigDecimal 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package pers.lxl.mylearnproject.javase.coreclass;import com.sun.istack.internal.NotNull;import java.math.BigDecimal;import java.math.RoundingMode;import static java.math.BigDecimal.ROUND_CEILING;/** * Java在java.math包中提供的API类BigDecimal，用来对超过16位有效位的数进行精确的运算。 * 双精度浮点型变量double可以处理16位有效数。在实际应用中，需要对更大或者更小的数进行运算和处理。 * float和double只能用来做科学计算或者是工程计算，在商业计算中要用java.math.BigDecimal。 * BigDecimal所创建的是对象，我们不能使用传统的+、-、*、/等算术运算符直接对其对象进行数学运算，而必须调用其相对应的方法。 * 方法中的参数也必须是BigDecimal的对象。构造器是类的特殊方法，专门用来创建对象，特别是带有参数的对象。 * * @author Administrator * * 参考：https://blog.csdn.net/qq_35868412/article/details/89029288 */public class BigDecimalL { public static void main(String[] args) {// 精度遗失问题// float 还是double都是浮点数，而计算机是二进制的，浮点数会失去一定的精确度，只能用来科学或工程计算，不能用来商业计算。根本原因是:十进制值通常没有完全相同的二进制表示形式;十进制数的二进制表示形式可能不精确。只能无限接近于那个值// 0.19999999999999998 System.out.println(0.3 - 0.1);// 0.020000000000000004 System.out.println(0.2 * 0.1);// 0.30000000000000004 System.out.println(0.2 + 0.1);// 2.9999999999999996 System.out.println(0.3 / 0.1);//金融项目中需要准确计算，不能有精度遗失问题，所以商业使用需要用BigDecimal==============================//BigDecimal(int) 创建一个具有参数所指定整数值的对象。//BigDecimal(double) 创建一个具有参数所指定双精度值的对象。 //不推荐使用//BigDecimal(long) 创建一个具有参数所指定长整数值的对象。//BigDecimal(String) 创建一个具有参数所指定以字符串表示的数值的对象。//推荐使用//add(BigDecimal) BigDecimal对象中的值相加，然后返回这个对象。//subtract(BigDecimal) BigDecimal对象中的值相减，然后返回这个对象。//multiply(BigDecimal) BigDecimal对象中的值相乘，然后返回这个对象。//divide(BigDecimal) BigDecimal对象中的值相除，然后返回这个对象。//toString() 将BigDecimal对象的数值转换成字符串。//doubleValue() 将BigDecimal对象中的值以双精度数返回。//floatValue() 将BigDecimal对象中的值以单精度数返回。//longValue() 将BigDecimal对象中的值以长整数返回。//intValue() 将BigDecimal对象中的值以整数返回。 BigDecimal bd1 = new BigDecimal(&quot;4264.24&quot;); BigDecimal bd2 = new BigDecimal(&quot;2132.1200&quot;); BigDecimal bd3 = new BigDecimal(&quot;213200&quot;); BigDecimal bd6 = new BigDecimal(&quot;4.0000&quot;); BigDecimal bd7 = new BigDecimal(&quot;3.0&quot;);// 加 System.out.println(&quot;加&quot;+bd1.add(bd3));// 减 System.out.println(&quot;减&quot;+bd1.subtract(bd1));// 乘 System.out.println(&quot;乘&quot;+bd6.multiply(bd7));// 除// 不分配保留小数点保留位数的非整除运算会导致报错java.lang.ArithmeticException：Non-terminating decimal expansion; no exact representable decimal result.// System.out.println(bd1.divide(bd3));// 多传入两个参数即可避免// divide(BigDecimal，保留小数点后几位小数，舍入模式)//舍入模式//ROUND_CEILING //向正无穷方向舍入//ROUND_DOWN //向零方向舍入//ROUND_FLOOR //向负无穷方向舍入//ROUND_HALF_DOWN //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向下舍入, 例如1.55 保留一位小数结果为1.5//ROUND_HALF_EVEN //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，如果保留位数是奇数，使用ROUND_HALF_UP，如果是偶数，使用ROUND_HALF_DOWN//ROUND_HALF_UP //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向上舍入, 1.55保留一位小数结果为1.6,也就是我们常说的“四舍五入”//ROUND_UNNECESSARY //计算结果是精确的，不需要舍入模式//ROUND_UP //向远离0的方向舍入 System.out.println(&quot;除&quot;+bd6.divide(bd7,2,ROUND_CEILING)); //scale()表示小数位数 System.out.println(bd1.scale()); System.out.println(bd2.scale()); System.out.println(bd3.scale()); //stripTrailingZeros()方法，可以将一个BigDecimal格式化为一个相等的，但去掉了末尾0的BigDecimal BigDecimal d1 = bd1.stripTrailingZeros(); BigDecimal d2 = bd2.stripTrailingZeros(); BigDecimal d3 = bd3.stripTrailingZeros(); //scale()返回负数，例如，-2，表示这个数是个整数，并且末尾有2个0 System.out.println(d1.scale()); System.out.println(d2.scale()); System.out.println(d3.scale()); //除法时，存在无法除尽的情况，这时，就必须指定精度以及如何进行截断,还可同事求余数 //四舍五入 BigDecimal bd4 = bd1.setScale(4, RoundingMode.HALF_UP); //直接截断 BigDecimal bd5 = bd1.setScale(4, RoundingMode.DOWN); //divideAndRemainder()方法时，返回的数组包含两个BigDecimal，分别是商和余数， // 其中商总是整数，余数不会大于除数。我们可以利用这个方法判断两个BigDecimal是否是整数倍数 BigDecimal[] dr = bd1.divideAndRemainder(bd2); if (dr[1].signum() == 0) { System.out.println(&quot;bd1是bd2的整&quot; + dr[0] + &quot;倍&quot;); } //在比较两个BigDecimal的值是否相等时，要特别注意，使用equals()方法不但要求两个BigDecimal的值相等，还要求它们的scale()相等 //必须使用compareTo()方法来比较，它根据两个值的大小分别返回负数、正数和0，分别表示小于、大于和等于// JDK的描述：1、参数类型为double的构造方法的结果有一定的不可预知性。有人可能认为在Java中写入newBigDecimal(0.1)所创建的BigDecimal正好等于 0.1（非标度值 1，其标度为 1），// 但是它实际上等于0.1000000000000000055511151231257827021181583404541015625。这是因为0.1无法准确地表示为 double（或者说对于该情况，不能表示为任何有限长度的二进制小数）。// 这样，传入到构造方法的值不会正好等于 0.1（虽然表面上等于该值）。// 2、另一方面，String 构造方法是完全可预知的：写入 newBigDecimal(&quot;0.1&quot;) 将创建一个 BigDecimal，它正好等于预期的 0.1。因此，比较而言，通常建议优先使用String构造方法// 当double必须用作BigDecimal的源时，请使用Double.toString(double)转成String，然后使用String构造方法，或使用BigDecimal的静态方法valueOf }} BigInteger 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package pers.lxl.mylearnproject.javase.coreclass;import java.math.BigInteger;/**CPU原生提供的整型最大范围是64位long型整数 * 如果我们使用的整数范围超过了long型怎么办？这个时候，就只能用软件来模拟一个大整数。 * java.math.BigInteger就是用来表示任意大小的整数。 * BigInteger内部用一个int[]数组来模拟一个非常大的整数： * @author Administrator*/public class BigIntegerL { public static void main(String[] args) { BigInteger bi = new BigInteger(&quot;12131231231233&quot;); BigInteger bi1 = new BigInteger(&quot;121321211111111&quot;); System.out.println(bi.pow(5)); //BigInteger做运算的时候，只能使用实例方法,和long型整数运算比，BigInteger不会有范围限制，但缺点是速度比较慢。 BigInteger sum = bi.add(bi1); /*转换为byte：byteValue() 转换为short：shortValue() 转换为int：intValue() 转换为long：longValue() 转换为float：floatValue() 转换为double：doubleValue() 通过上述方法，可以把BigInteger转换成基本类型。 如果BigInteger表示的范围超过了基本类型的范围， 转换时将丢失高位信息，即结果不一定是准确的。如果需要准确地转换成基本类型， 可以使用intValueExact()、longValueExact()等方法，在转换时如果超出范围， 将直接抛出ArithmeticException异常 如果BigInteger的值甚至超过了float的最大范围（3.4x1038），那么返回的float为Infinity 其他常用方法： BigInteger abs() 返回大整数的绝对值 BigInteger add(BigInteger val) 返回两个大整数的和 BigInteger and(BigInteger val) 返回两个大整数的按位与的结果 BigInteger andNot(BigInteger val) 返回两个大整数与非的结果 BigInteger divide(BigInteger val) 返回两个大整数的商 BigInteger gcd(BigInteger val) 返回大整数的最大公约数 BigInteger max(BigInteger val) 返回两个大整数的最大者 BigInteger min(BigInteger val) 返回两个大整数的最小者 BigInteger mod(BigInteger val) 用当前大整数对val求模 BigInteger multiply(BigInteger val) 返回两个大整数的积 BigInteger negate() 返回当前大整数的相反数 BigInteger not() 返回当前大整数的非 BigInteger or(BigInteger val) 返回两个大整数的按位或 BigInteger pow(int exponent) 返回当前大整数的exponent次方 BigInteger remainder(BigInteger val) 返回当前大整数除以val的余数 BigInteger leftShift(int n) 将当前大整数左移n位后返回 BigInteger rightShift(int n) 将当前大整数右移n位后返回 BigInteger subtract(BigInteger val)返回两个大整数相减的结果 byte[] toByteArray(BigInteger val)将大整数转换成二进制反码保存在byte数组中 String toString() 将当前大整数转换成十进制的字符串形式 BigInteger xor(BigInteger val) 返回两个大整数的异或 */ }} 注解 注解用途 1，什么是注解 注解也叫元数据，例如我们常见的@Override和@Deprecated，注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解 **注解类型:**一般常用的注解可以分为三类： 一类是Java自带的标准注解，包括@Override（标明重写某个方法）、@Deprecated（标明某个类或方法过时）和@SuppressWarnings（标明要忽略的警告），使用这些注解后编译器就会进行检查。 一类为元注解，元注解是用于定义注解的注解，包括@Retention（标明注解被保留的阶段）、@Target（标明注解使用的范围）、@Inherited（标明注解可继承）、@Documented（标明是否生成javadoc文档） 一类为自定义注解，可以根据自己的需求定义注解 注解与xml区别： 注解：是一种分散式的元数据，与源代码紧绑定耦合。 xml：是一种集中式的元数据，与源代码无绑定 当然网上存在各种XML与注解的辩论哪个更好，这里不作评论和介绍，主要介绍一下注解的主要用途: 生成文档，通过代码里标识的元数据生成javadoc文档。 编译检查，通过代码里标识的元数据让编译器在编译期间进行检查验证。 编译时动态处理，编译时通过代码里标识的元数据动态处理，例如动态生成代码。 运行时动态处理，运行时通过代码里标识的元数据动态处理，例如使用反射注入实例 注解作用域 @Target({ElementType.TYPE}) //Class,interface,enum @Target({ElementType.METHOD}) @Target({ElementType.FIELD}) @Target({ElementType.PARAMETER}) 注解原理 实际运用示例 @Excel 参考资源： 1.https://blog.csdn.net/xushiyu1996818/article/details/91983557","link":"/2021/02/24/Draft/2021/Java/"},{"title":"GIS","text":"MAPBOX cgcs2000 Mapbox VUE使用 安装：npm i @cgcs2000/mapbox-gl MAPBOX源码编译、切片流程 准备：熟悉linux 安装、网络、文件等基本操作，JS、HTML等熟悉，Mapbox熟悉，SQL熟悉，，， 注意参数：同一套坐标、经纬度等 Ubuntu环境准备===》其他格式数据===》shp数据===》Windows导入postgresql数据库===》通过ogr2ogr转换为geojson===》通过tippecanoe切片（参数自控）》~~数据打包（传输更快）~~windows共享文件夹=》准备mapbox2000 js css文件====》mapbox前端代码导入数据并显示====》准备图标数据===》filter分类图标显示=====》分层数据显示====》细节优化（颜色，光，字体大小，图片大小）》加入点击显示位置》其他点击功能====》wfts底图切换==》控件加载===》wfs，wms数据加载（不同数据源）==》其他模型加入 详细步骤 环境准备 Ubuntu虚拟机： 网络环境，数据共享，Postgresql，Tippecanoe，PROJ.4、GEOS和GDAL（ogr2ogr），gcc、g++、make windows： Postgresql,VMware,Ubuntu镜像（官方指定），Navicat（方便查看数据和建立空间数据库），VSCode 详细步骤： 安装Postgresql数据库，环境配置，导入shp Windows安装Postgresql 下载 环境变量添加 设置外网访问（pg_hba.conf） 1234567891011# TYPE DATABASE USER ADDRESS METHOD# IPv4 local connections:host all all 127.0.0.1/32 md5host all all 0.0.0.0/0 md5# IPv6 local connections:host all all ::1/128 md5# Allow replication connections from localhost, by a user with the# replication privilege.#host replication postgres 127.0.0.1/32 md5#host replication postgres ::1/128 md5 建立空间数据库,目标数据库执行以下语句 1234create extension postgis;create extension postgis_topology;create extension fuzzystrmatch;create extension postgis_tiger_geocoder; 在SHP文件夹新建txt文件，粘贴以下代码，导入数据到数据库，相关参数如下，改成bat后缀，双击运行后输入密码，显示类似 insert 1成功 123shp2pgsql -s 4544 -c -W “GBK” DJQ5120812018.shp public.DJQ5120812018 | psql -d shp2pgsqldemo -U postgres -W示例 ：shp2pgsql -s 4544 -c -W &quot;UTF-8&quot; ADDRESS.shp public.rrrrr| psql -h 192.168.22.128 -d postgres -U postgres -W 编码格式 SHP数据 public下的rrrrr表 导入数据库地址 数据库名 用户名 也可分为两个步骤，先转换为sql语句，再导入,步骤同上 12shp2pgsql -s 4544 -c -W “GBK” DJQ5120812018.shp&gt;DJQ5120812018.sqlpsql -d shp2pgsqldemo -U postgres -f DJQ5120812018.sql -W 可能出现的问题 解决办法 乱码 更改编码 导入中断，数据库无数据 数据过大 参数 含义 -s 空间参考标识符（SRID） -d 重新建立表，并插入数据 -a 在同一个表中增加数据 -c 建立新表，并插入数据(缺省) -p 只创建表 -g 指定要创建的表的空间字段名称(在追加数据时有用) -D 使用dump方式，比缺省生成sql的速度快 -G 使用类型geography -k 保持标识符（列名，模式，属性）大小写。 -i 将所有整型都转为标准的32-bit整数 -I 在几何列上建立GIST索引 -S 生成简单几何，而非MULTI几何 -t 指定几何的维度 -w 指定输出格式为WKT -W 输入的dbf文件编码方式 -N 指定几何为空时的操作 -n 只导入dbf文件 -T 指定表的表空间 -X 指定索引的表空间 -? 帮助 通过ogr2ogr转换为geojson 12ogr2ogr -f &quot;GeoJSON&quot; ./asstln.json PG:&quot;host=localhost dbname=postgres user=postgres password=111111&quot; -sql &quot;select * from asstln&quot; 目标文件名 导出数据库连接信息 导出表名 通过tippecanoe切片（参数自控） 单数据源 12tippecanoe -e tracenln -pC -Z0 -z20 -f tracenln.json 目标文件夹 切片等级空值 源文件名 单层多数据源合并 123#where确定级别ogr2ogr -f &quot;GeoJSON&quot; ./veg_py.json PG:&quot;host=126.10.9.16 dbname=postgres user=postgres password=724111&quot; -sql &quot;select * from veg_py fscale=10&quot; tippecanoe -e tracenln -pC -Z0 -z20 -f tracenln1.json tracenln2.json tracenln3.json 参数查看 数据打包 复制到编程端（传输更快） windows文件共享文件夹，或Tomcat，或在线服务器以便数据调用，注意解决触跨域资源访问 准备mapbox2000 js css文件 windows 安装 git Node.JS 安装说明 Yarn安装、配置、镜像源修改 12npm install --global yarnyarn --version Node.js安装本地插件构建工具node-gyp GitHub Mapbox源码地址：https://github.com/mapbox/mapbox-gl-js 2000坐标源码， 项目编译 yarn install 安装headless-gl，并将node_modules/headless-gl/deps/windows/dll/x64/*.dll 复制到c:\\windows\\system32 npm install gl yarn run start-debug yarn run build-dev 准备好js和css文件 mapbox 前端代码导入地图数据并显示 3D模型 针对对应的建筑数据，进行建筑物3D显示 123456789101112131415161718paint: { // 'fill-color': 'red', // 'fill-opacity': 1, 'fill-extrusion-color': '#f5f4ee', // use an 'interpolate' expression to add a smooth transition effect to the // buildings as the user zooms in 'fill-extrusion-height': [ &quot;interpolate&quot;, [&quot;linear&quot;], [&quot;zoom&quot;], 15, 0, 15.05, [&quot;get&quot;, &quot;height&quot;] ], 'fill-extrusion-base': [ &quot;interpolate&quot;, [&quot;linear&quot;], [&quot;zoom&quot;], 15, 0, 15.05, [&quot;get&quot;, &quot;min_height&quot;] ], 'fill-extrusion-opacity': 0.85 }, vscode 插件服务器 插件安装 Live Server 准备图标数据，PBF字体数据 https://github.com/mapbox/spritezero 从零生成图标资源工具，可网上下载 生成如图形式文件， filter 分类图标显示 123456789101112131415'filter': [ 'any', [ '==', 'fcode', '4206002500'//2 ] ,[ '==', 'fcode', '4305010500'//2 ] ], 分层数据显示 1&quot;layers&quot;: []//中越靠前的在底层 细节优化（颜色，光，字体大小，图片大小） 控件加载 Supermap查看 加入点击显示相关信息 123456789101112131415//弹出框​ map.on('click', function (e) {​ var features = map.queryRenderedFeatures(e.point, {​ layers: ['13'] // replace this with the id of the layer​ });​ if (!features.length) {​ return;​ }​ var feature = features[0];​ var popup = new mapboxgl.Popup({ offset: [0, -15] })​ .setLngLat(feature.geometry.coordinates)​ .setHTML('&lt;h3&gt;' + feature.properties.shortname + '&lt;/h3&gt;&lt;p&gt;' + feature.properties.name + '&lt;/p&gt;')​ .addTo(map);​ }); wfts 底图切换 wfs，wms 数据加载（不同数据源） 定位 数据查询 数据可视化 Supermap查看 优化 窗口样式优化，弹出框样式优化，字体等调节 一些小技巧 待更新 Mapbox源码编译 环境准备 GIT环境搭建： 详细点击：（一）windows 安装 git Node.JS环境搭建： 详细点击：（一）Node.JS 安装说明 Yarn环境搭建： 详细点击：（一）Yarn安装、配置、镜像源修改 12npm install --global yarnyarn --version Npm and node-gyp依赖安装 详细点击：（二）Node.js安装本地插件构建工具node-gyp 其他地址： GitHub Mapbox源码地址：https://github.com/mapbox/mapbox-gl-js 项目编译 yarn install 安装headless-gl，并将node_modules/headless-gl/deps/windows/dll/x64/*.dll 复制到c:\\windows\\system32 npm install gl yarn run start-debug yarn run build-dev debug/index.html中代码最上方增加token mapboxgl.accessToken='pk.eyJ1IjoibGltbiIsImEiOiJja2t1bG1na2IxZGU0MnZvNmlzY3FhZXM4In0.oQx4VguycOR4TK80Pyusmw'; var map = window.map = new mapboxgl.Map({ MAPBOX专业术语 矢量瓦片： 栅格瓦片： MAPBOX学习 中文文档：http://www.mapbox.cn/mapbox-gl-js/api/ Styles (8) 为地图添加生成的图标 为地图添加动画图标 为地图生成及添加缺失的图标 为地图添加图标 使用自定义样式展示地图 显示卫星地图 改变一个地图的样式 显示一个地图 Layers (30) 用3D形式呈现建筑物 拉伸多边形以绘制3D室内地图 添加3D模型 调整图层不透明度 为线添加动画效果 为一系列图像添加动画效果 为点添加动画效果 按照缩放级别改变建筑颜色 更改标注的大小写 显示具有自定义属性的HTML聚类 创建样式聚类 使用按钮更改图层颜色 添加自定义样式图层 给线添加数据驱动属性的样式。 给圆添加数据驱动属性的样式 显示多种文本格式并设置其样式 为多边形添加图案 在标签下添加新图层 添加 GeoJSON 线 绘制 GeoJSON 点 添加 GeoJSON 多边形 创建热力图图层 添加晕暄 使用表达式创建渐变色线条 设置海洋深度数据样式 显示和隐藏图层 改变行政边界世界观 根据缩放级别更新等值线图层 变量标签位置 可视化人口密度 Sources (9) 将本地JSON数据与矢量切片图形连接 添加影像 添加实时数据 更新实时要素 添加栅格切片数据源 添加一个第三方矢量切片来源 添加一个矢量图片数据源 添加一个视频 添加一个 WMS 源 User interaction (17) 基于周边声音给3D建筑添加动画效果 禁用地图旋转 创建可拖动的点 创建可拖动的标记（Marker） 通过文本输入筛选符号 在 map view 中筛选要素 通过切换列表筛选符号 创建悬停效果 显示非交互式地图 更改地图的语言 高亮包含相似数据的部分 从点击点周围选择特征 限制地图平移在某一区域 获取鼠标下点的特征 切换交互 创建时间滑动条 高亮一个选择框范围内的特征 Camera (11) 使地图相机环绕一点运动 为路线中的点添加动画效果 将地图居中于被单击的符号上 缓慢飞至某个位置 将地图缩放至边界框内 飞至某一位置 使用游戏式控件浏览地图 跳至一系列地点 以幻灯片形式播放地图位置。 根据滚动位置飞到某处 设置 pitch 和 bearing Controls and overlays (16) 为标记(marker)添加动画效果 改变注释的默认位置 使用 Markers 添加自定义图标 禁用滚轮缩放 全屏查看地图 定位用户 在不同地图之间滑动 显示驾驶方向 显示已绘制的多边形区域 添加地理编码器 利用地点名称添加标记 点击时显示多边形信息 悬浮时显示弹出窗 点击时显示一个弹出窗 显示一个弹出窗 将弹出窗口附加到 marker 实例 Geocoder (8) 从其他数据源中补充进一步的地理编码查询结果 接收输入坐标至地理编码器 使用地理编码器时采用自定义渲染功能 将地理编码的结果限制在指定地区范围内 在使用地理编码器的过程中结合使用自定义相机动画 在地图上添加位置搜索框 将地理编码器进行指定语言的本地化 在Geocoder产生结果后设置一个点 Browser support (1) 检查浏览器支持 Internationalization support (2) 使用本地生成的表意文字 为从右至左书写的脚本提供支持 SUPERMAP 一个mapbox华丽外衣与装备 Cusium 3D、2.5D、2D地图展示 源码解析准备 官网 中文文档 注册 源码：下载 环境：node、npm 使用：VUE 运行：方法一： 解压 打开命令行，进入当前目录 安装依赖，输入:cnpm install,等待安装完成，然后输入: node server.js 1234$ cd ./Cesium-1.40$ cnpm install ... Installed 37 packages Linked 569 latest versions$ npm start 或者 node server.js(新版本是 node server.cjs)Cesium development server running locally. Connect to http://localhost:8080/复制 方法二： 123这里如果使用 express发布遇到问题，可以使用 http-server$ npm install http-server -g$ http-server -c-1 (如果只输入http-server，更新代码后，页面不会同步更新) 方法三：VSCode 插件运行 获取Token 使用 12Cesium.Ion.defaultAccessToken = 'token';var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;, {}) 主页说明 Cesium ion Cesium ion is your hub for discovering 3D content and tiling your own data for streaming. CesiumJS and ion work together to enable you to build world class 3D mapping applications. Sign up for a free account to get your access token required for using ion's Bing Maps global imagery and Cesium World Terrain assets. Local links Documentation The complete API documentation and reference. Hello World The simplest possible Cesium application. Cesium Viewer A sample Cesium reference application which allows you to browse the globe and select from a variety of imagery and terrain layers as well as load CZML, GeoJSON, and other formats supported by Cesium. Sandcastle Cesium's live code editor and example gallery. Browse examples highlighting features of the Cesium API and edit and run them all in your web browser. Cesium applications created in Sandcastle can be saved and downloaded. Run tests Run Cesium's full suite of unit tests External links Cesium Learn about the Cesium team and the Cesium ion platform Cesium Blog Follow project news, developer tips, and posts about Cesium's innovative technology Tutorials Step-by-step guides for learning to use CesiumJS Community Forum Questions and feedback welcome from all skill levels GitHub The official CesiumJS GitHub repository User Stories Demos and information about projects using CesiumJS WebGL Report Provides technical information about WebGL capabilities supported by your browser 界面介绍 自项目编写 环境：VUE2 ，cesium 1.89 C:\\Users\\Administrator&gt;node -v v14.16.0 C:\\Users\\Administrator&gt;npm -v 6.14.11 项目创建 1vue init webpack gislearn 安装cesium 1.89 1npm i cesium 修改webpack.prod.conf.js 修改 --Cesium--标记处 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163'use strict'const path = require('path')const utils = require('./utils')const webpack = require('webpack')const config = require('../config')const merge = require('webpack-merge')const baseWebpackConfig = require('./webpack.base.conf')const CopyWebpackPlugin = require('copy-webpack-plugin')const HtmlWebpackPlugin = require('html-webpack-plugin')const ExtractTextPlugin = require('extract-text-webpack-plugin')const OptimizeCSSPlugin = require('optimize-css-assets-webpack-plugin')const UglifyJsPlugin = require('uglifyjs-webpack-plugin')// 定义路径--Cesium--const cesiumSource = 'node_modules/cesium/Source'const cesiumWorkers = '../Build/Cesium/Workers'const env = process.env.NODE_ENV === 'testing' ? require('../config/test.env') : require('../config/prod.env')const webpackConfig = merge(baseWebpackConfig, { module: { rules: utils.styleLoaders({ sourceMap: config.build.productionSourceMap, extract: true, usePostCSS: true }) }, devtool: config.build.productionSourceMap ? config.build.devtool : false, output: { path: config.build.assetsRoot, filename: utils.assetsPath('js/[name].[chunkhash].js'), chunkFilename: utils.assetsPath('js/[id].[chunkhash].js') }, plugins: [ // 添加如下插件--Cesium-- new CopyWebpackPlugin([{ from: path.join(cesiumSource, cesiumWorkers), to: 'Workers' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Assets'), to: 'Assets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Widgets'), to: 'Widgets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'ThirdParty/Workers'), to: 'ThirdParty/Workers' }]), new webpack.DefinePlugin({ // 注意这里和dev的配置不同 // 定义Cesium从哪里加载资源，如果使用默认的''，却变成了绝对路径了，所以这里使用'./'，使用相对路径 CESIUM_BASE_URL: JSON.stringify('./') }), // http://vuejs.github.io/vue-loader/en/workflow/production.html new webpack.DefinePlugin({ 'process.env': env }), new UglifyJsPlugin({ uglifyOptions: { compress: { warnings: false } }, sourceMap: config.build.productionSourceMap, parallel: true }), // extract css into its own file new ExtractTextPlugin({ filename: utils.assetsPath('css/[name].[contenthash].css'), // Setting the following option to `false` will not extract CSS from codesplit chunks. // Their CSS will instead be inserted dynamically with style-loader when the codesplit chunk has been loaded by webpack. // It's currently set to `true` because we are seeing that sourcemaps are included in the codesplit bundle as well when it's `false`, // increasing file size: https://github.com/vuejs-templates/webpack/issues/1110 allChunks: true, }), // Compress extracted CSS. We are using this plugin so that possible // duplicated CSS from different components can be deduped. new OptimizeCSSPlugin({ cssProcessorOptions: config.build.productionSourceMap ? { safe: true, map: { inline: false } } : { safe: true } }), // generate dist index.html with correct asset hash for caching. // you can customize output by editing /index.html // see https://github.com/ampedandwired/html-webpack-plugin new HtmlWebpackPlugin({ filename: process.env.NODE_ENV === 'testing' ? 'index.html' : config.build.index, template: 'index.html', inject: true, minify: { removeComments: true, collapseWhitespace: true, removeAttributeQuotes: true // more options: // https://github.com/kangax/html-minifier#options-quick-reference }, // necessary to consistently work with multiple chunks via CommonsChunkPlugin chunksSortMode: 'dependency' }), // keep module.id stable when vendor modules does not change new webpack.HashedModuleIdsPlugin(), // enable scope hoisting new webpack.optimize.ModuleConcatenationPlugin(), // split vendor js into its own file new webpack.optimize.CommonsChunkPlugin({ name: 'vendor', minChunks(module) { // any required modules inside node_modules are extracted to vendor return ( module.resource &amp;&amp; /\\.js$/.test(module.resource) &amp;&amp; module.resource.indexOf( path.join(__dirname, '../node_modules') ) === 0 ) } }), // extract webpack runtime and module manifest to its own file in order to // prevent vendor hash from being updated whenever app bundle is updated new webpack.optimize.CommonsChunkPlugin({ name: 'manifest', minChunks: Infinity }), // This instance extracts shared chunks from code splitted chunks and bundles them // in a separate chunk, similar to the vendor chunk // see: https://webpack.js.org/plugins/commons-chunk-plugin/#extra-async-commons-chunk new webpack.optimize.CommonsChunkPlugin({ name: 'app', async: 'vendor-async', children: true, minChunks: 3 }), // copy custom static assets new CopyWebpackPlugin([ { from: path.resolve(__dirname, '../static'), to: config.build.assetsSubDirectory, ignore: ['.*'] } ]) ]})if (config.build.productionGzip) { const CompressionWebpackPlugin = require('compression-webpack-plugin') webpackConfig.plugins.push( new CompressionWebpackPlugin({ asset: '[path].gz[query]', algorithm: 'gzip', test: new RegExp( '\\\\.(' + config.build.productionGzipExtensions.join('|') + ')$' ), threshold: 10240, minRatio: 0.8 }) )}if (config.build.bundleAnalyzerReport) { const BundleAnalyzerPlugin = require('webpack-bundle-analyzer').BundleAnalyzerPlugin webpackConfig.plugins.push(new BundleAnalyzerPlugin())}module.exports = webpackConfig webpack.dev.conf.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110'use strict'const utils = require('./utils')const webpack = require('webpack')const config = require('../config')const merge = require('webpack-merge')const path = require('path')const baseWebpackConfig = require('./webpack.base.conf')const CopyWebpackPlugin = require('copy-webpack-plugin')const HtmlWebpackPlugin = require('html-webpack-plugin')const FriendlyErrorsPlugin = require('friendly-errors-webpack-plugin')const portfinder = require('portfinder')const HOST = process.env.HOSTconst PORT = process.env.PORT &amp;&amp; Number(process.env.PORT)// 定义路径：--Cesium--const cesiumSource = 'node_modules/cesium/Source'const cesiumWorkers = '../Build/Cesium/Workers'const devWebpackConfig = merge(baseWebpackConfig, { module: { rules: utils.styleLoaders({ sourceMap: config.dev.cssSourceMap, usePostCSS: true }) }, // cheap-module-eval-source-map is faster for development devtool: config.dev.devtool, // these devServer options should be customized in /config/index.js devServer: { clientLogLevel: 'warning', historyApiFallback: { rewrites: [ { from: /.*/, to: path.posix.join(config.dev.assetsPublicPath, 'index.html') }, ], }, hot: true, contentBase: false, // since we use CopyWebpackPlugin. compress: true, host: HOST || config.dev.host, port: PORT || config.dev.port, open: config.dev.autoOpenBrowser, overlay: config.dev.errorOverlay ? { warnings: false, errors: true } : false, publicPath: config.dev.assetsPublicPath, proxy: config.dev.proxyTable, quiet: true, // necessary for FriendlyErrorsPlugin watchOptions: { poll: config.dev.poll, } }, plugins: [ // 添加如下插件--Cesium-- new CopyWebpackPlugin([{ from: path.join(cesiumSource, cesiumWorkers), to: 'Workers' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Assets'), to: 'Assets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Widgets'), to: 'Widgets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'ThirdParty/Workers'), to: 'ThirdParty/Workers' }]), new webpack.DefinePlugin({ // Define relative base path in cesium for loading assets CESIUM_BASE_URL: JSON.stringify('') }), new webpack.DefinePlugin({ 'process.env': require('../config/dev.env') }), new webpack.HotModuleReplacementPlugin(), new webpack.NamedModulesPlugin(), // HMR shows correct file names in console on update. new webpack.NoEmitOnErrorsPlugin(), // https://github.com/ampedandwired/html-webpack-plugin new HtmlWebpackPlugin({ filename: 'index.html', template: 'index.html', inject: true }), // copy custom static assets new CopyWebpackPlugin([ { from: path.resolve(__dirname, '../static'), to: config.dev.assetsSubDirectory, ignore: ['.*'] } ]) ]})module.exports = new Promise((resolve, reject) =&gt; { portfinder.basePort = process.env.PORT || config.dev.port portfinder.getPort((err, port) =&gt; { if (err) { reject(err) } else { // publish the new Port, necessary for e2e tests process.env.PORT = port // add port to devServer config devWebpackConfig.devServer.port = port // Add FriendlyErrorsPlugin devWebpackConfig.plugins.push(new FriendlyErrorsPlugin({ compilationSuccessInfo: { messages: [`Your application is running here: http://${devWebpackConfig.devServer.host}:${port}`], }, onErrors: config.dev.notifyOnErrors ? utils.createNotifierCallback() : undefined })) resolve(devWebpackConfig) } })}) webpack.base.conf.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130'use strict'const path = require('path')const utils = require('./utils')const config = require('../config')const vueLoaderConfig = require('./vue-loader.conf')//定义 Cesium 源码路径 --Cesium--const cesiumSource = '../node_modules/cesium/Source'const cesiumWorkers = path.join(cesiumSource, 'Workers');function resolve(dir) { return path.join(__dirname, '..', dir)}const createLintingRule = () =&gt; ({ test: /\\.(js|vue)$/, loader: 'eslint-loader', enforce: 'pre', include: [resolve('src'), resolve('test')], options: { formatter: require('eslint-friendly-formatter'), emitWarning: !config.dev.showEslintErrorsInOverlay }})module.exports = { context: path.resolve(__dirname, '../'), entry: { app: './src/main.js' }, // output: { // path: config.build.assetsRoot, // filename: '[name].js', // publicPath: process.env.NODE_ENV === 'production' // ? config.build.assetsPublicPath // : config.dev.assetsPublicPath // }, // 让webpack正确处理多行字符串，在output中添加sourcePrefix:' '--Cesium-- output: { path: config.build.assetsRoot, filename: '[name].js', publicPath: process.env.NODE_ENV === 'production' ? config.build.assetsPublicPath : config.dev.assetsPublicPath, sourcePrefix: ' ' //让webpack正确处理多行字符串 }, // resolve: { // extensions: ['.js', '.vue', '.json'], // alias: { // 'vue$': 'vue/dist/vue.esm.js', // '@': resolve('src'), // } // }, // 需要在resolve中设置cesium别名，这样在引入的时候就可以根据别名找到Cesium的包。--Cesium-- // （注：也可以不设置别名，导包是直接导入'cesium/Source/Cesium.js'就行。其实设置别名的目的就是让“别名”指向/ node_modules / cesium / Source目录） resolve: { extensions: ['.js', '.vue', '.json'], alias: { 'vue$': 'vue/dist/vue.esm.js', '@': resolve('src'), cesium: path.resolve(__dirname, cesiumSource) //设置cesium别名 } }, module: { // 解决报错 1.87版本无效，只能暂时退版本--Cesium-- // npm install @open-wc/webpack-import-meta-loader --save-dev // error in ./node_modules/cesium/Source/ThirdParty/zip.js // Module parse failed: Unexpected token(6400:57) // You may need an appropriate loader to handle this file type. rules: [ { test: /\\.js$/, use: { loader: '@open-wc/webpack-import-meta-loader', }, }, // 创建工程师会选择 ESLint 代码校验 // Use ESLint to lint your code? Yes // ...(config.dev.useEslint ? [createLintingRule()] : []), { test: /\\.vue$/, loader: 'vue-loader', options: vueLoaderConfig }, { test: /\\.js$/, loader: 'babel-loader', include: [resolve('src'), resolve('test'), resolve('node_modules/webpack-dev-server/client')] }, { test: /\\.(png|jpe?g|gif|svg)(\\?.*)?$/, loader: 'url-loader', options: { limit: 10000, name: utils.assetsPath('img/[name].[hash:7].[ext]') } }, { test: /\\.(mp4|webm|ogg|mp3|wav|flac|aac)(\\?.*)?$/, loader: 'url-loader', options: { limit: 10000, name: utils.assetsPath('media/[name].[hash:7].[ext]') } }, { test: /\\.(woff2?|eot|ttf|otf)(\\?.*)?$/, loader: 'url-loader', options: { limit: 10000, name: utils.assetsPath('fonts/[name].[hash:7].[ext]') } } ], unknownContextCritical: false,//阻止依赖警告--Cesium-- }, node: { // prevent webpack from injecting useless setImmediate polyfill because Vue // source contains it (although only uses it if it's native). setImmediate: false, // prevent webpack from injecting mocks to Node native modules // that does not make sense for the client dgram: 'empty', fs: 'empty', net: 'empty', tls: 'empty', child_process: 'empty' }} 创建地图组件 src\\view\\earth.vue 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135&lt;template&gt; &lt;div class=&quot;container&quot;&gt; &lt;div id=&quot;cesiumContainer&quot;&gt;&lt;/div&gt; &lt;div id=&quot;credit&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/template&gt; &lt;script&gt;// 这里不能使用 import Cesium from 'cesium/Cesium' 导入模块，因为Cesium 1.63 版本以后使用的是ES6。应该使用一下方式// import { Viewer } from &quot;cesium/Cesium&quot;;import * as Cesium from &quot;cesium/Cesium&quot;; //正确import &quot;cesium/Widgets/widgets.css&quot;;export default { name: &quot;earth&quot;, data() { return {}; }, mounted() { // let viewer = new Viewer(&quot;cesiumContainer&quot;); // 1. Geocoder : 查找位置工具，查找到之后会将镜头对准找到的地址，默认使用bing地图 // 2. Home Button :视角返回初始位置. // 3. Scene Mode Picker : 选择视角的模式，有三种：3D，2D，哥伦布视图(CV) // 4. Base Layer Picker : 图层选择器，选择要显示的地图服务和地形服务. // 5. Navigation Help Button :导航帮助按钮，显示默认的地图控制帮助. // 6. Animation : 动画器件，控制视图动画的播放速度. // 7. Timeline :时间线,指示当前时间，并允许用户跳到特定的时间. // 8. Credits Display :版权显示，显示数据归属，必选 // 9. Fullscreen Button :全屏按钮. Cesium.Ion.defaultAccessToken =&quot;自己的token&quot;; var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;, { // geocoder: false, // homeButton: false, // sceneModePicker: false, // baseLayerPicker: false, // navigationHelpButton: false, // animation: false, creditContainer: &quot;credit&quot;, // timeline: false, // fullscreenButton: false, // vrButton: false, // skyBox : new Cesium.SkyBox({ // sources : { // positiveX : 'stars/TychoSkymapII.t3_08192x04096_80_px.jpg', // negativeX : 'stars/TychoSkymapII.t3_08192x04096_80_mx.jpg', // positiveY : 'stars/TychoSkymapII.t3_08192x04096_80_py.jpg', // negativeY : 'stars/TychoSkymapII.t3_08192x04096_80_my.jpg', // positiveZ : 'stars/TychoSkymapII.t3_08192x04096_80_pz.jpg', // negativeZ : 'stars/TychoSkymapII.t3_08192x04096_80_mz.jpg' // } // }) }); // 显示帧速(FPS) viewer.scene.debugShowFramesPerSecond = true; // 绘制形状 // 方式一： var redBox = viewer.entities.add({ name: &quot;Red box with black outline&quot;, position: Cesium.Cartesian3.fromDegrees(-107.0, 40.0, 300000.0), box: { dimensions: new Cesium.Cartesian3(400000.0, 300000.0, 500000.0), material: Cesium.Color.BLUE.withAlpha(0.5), outline: true, outlineColor: Cesium.Color.BLACK, }, }); viewer.zoomTo(viewer.entities); // 方式二： var czml = [ { id: &quot;document&quot;, name: &quot;box&quot;, version: &quot;1.0&quot;, }, { id: &quot;shape2&quot;, name: &quot;Red box with black outline&quot;, position: { cartographicDegrees: [-107.0, 50.0, 300000.0], }, box: { dimensions: { cartesian: [400000.0, 300000.0, 500000.0], }, material: { solidColor: { color: { rgba: [255, 0, 0, 128], }, }, }, outline: true, outlineColor: { rgba: [0, 0, 0, 255], }, }, }, ]; // 重复创建Viewer会添加多个窗口 // var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;); var dataSourcePromise = Cesium.CzmlDataSource.load(czml); viewer.dataSources.add(dataSourcePromise); viewer.zoomTo(dataSourcePromise); },};&lt;/script&gt; &lt;style scoped&gt;.container { width: 100%; height: 100%;}#cesiumContainer { width: 100%; height: 100%;}/* 不占据空间，无法点击，右上角按钮组，左下角动画控件 ，时间线，logo信息*//* .cesium-viewer-toolbar,.cesium-viewer-animationContainer,.cesium-viewer-timelineContainer,.cesium-viewer-bottom { display: none;} *//* 全屏按钮 *//* .cesium-viewer-fullscreenContainer { position: absolute; top: -999em;} *//* 注：全屏按钮不能通过display:none的方式来达到隐藏的目的，这是因为生成的按钮控件的行内样式设置了display属性，会覆盖引入的css属性&lt;div class=&quot;cesium-viewer-fullscreenContainer&quot; style=&quot;display: block;&quot;&gt;...&lt;/div&gt; */&lt;/style&gt; 添加路由 src\\router\\index.js 12345678910111213141516171819202122import Vue from 'vue'import Router from 'vue-router'import HelloWorld from '@/components/HelloWorld'import earth from '@/view/earth'Vue.use(Router)export default new Router({ routes: [ { path: '/', name: 'HelloWorld', component: HelloWorld }, { path: '/earth', name: 'earth', component: earth } ]}) 运行报错1 error in ./node_modules/cesium/Source/ThirdParty/zip.js 解决 1npm i @open-wc/webpack-import-meta-loader -S vue.config.js 或 webpack.base.conf.js配置 123456789101112131415module.exports = { configureWebpack: { module: { rules: [ { test: /\\.js$/, use: { loader: '@open-wc/webpack-import-meta-loader', }, }, ], }, },} 运行报错2 error in (webpack)/hot/dev-server.js Module build failed: TypeError: Cannot read property 'length' of undefined at Object.module.exports (F:\\demo\\cesiumtest-master\\node_modules@open-wc\\webpack-import-meta-loader\\webpack-import-meta-loader.js:20:63) 解决 点击报错跳转到对应文件行，将其注释即可 1234567module.exports = function (source) { const path = require('path'); const relativePath = this.context.substring( // this.context.indexOf(this.rootContext) + this.rootContext.length + 1, this.resource.lastIndexOf(path.sep) + 1, ); 运行 npm run dev http://localhost:8081/#/earth npm run build 打包成功 部署dist http://xx.xx.xx.11:8084/#/earth 成功 功能开发 控件显示控制 Geocoder : 查找位置工具，查找到之后会将镜头对准找到的地址，默认使用bing地图 Home Button :视角返回初始位置. Scene Mode Picker : 选择视角的模式，有三种：3D，2D，哥伦布视图(CV) Base Layer Picker : 图层选择器，选择要显示的地图服务和地形服务. Navigation Help Button :导航帮助按钮，显示默认的地图控制帮助. Animation : 动画器件，控制视图动画的播放速度. Timeline :时间线,指示当前时间，并允许用户跳到特定的时间. Credits Display :版权显示，显示数据归属，必选 Fullscreen Button :全屏按钮. js方式 官方文档 123456789101112131415161718192021222324var viewer = new Viewer(&quot;cesiumContainer&quot;, { geocoder: false, homeButton: false, sceneModePicker: false, baseLayerPicker: false, navigationHelpButton: false, animation: false, // creditContainer: &quot;credit&quot;, timeline: false, fullscreenButton: false, vrButton: false, // skyBox : new Cesium.SkyBox({ // sources : { // positiveX : 'stars/TychoSkymapII.t3_08192x04096_80_px.jpg', // negativeX : 'stars/TychoSkymapII.t3_08192x04096_80_mx.jpg', // positiveY : 'stars/TychoSkymapII.t3_08192x04096_80_py.jpg', // negativeY : 'stars/TychoSkymapII.t3_08192x04096_80_my.jpg', // positiveZ : 'stars/TychoSkymapII.t3_08192x04096_80_pz.jpg', // negativeZ : 'stars/TychoSkymapII.t3_08192x04096_80_mz.jpg' // } // })});// 显示帧速(FPS)viewer.scene.debugShowFramesPerSecond = true; css方式 12345678910/* 不占据空间，无法点击 */.cesium-viewer-toolbar, /* 右上角按钮组 */.cesium-viewer-animationContainer, /* 左下角动画控件 */.cesium-viewer-timelineContainer, /* 时间线 */.cesium-viewer-bottom /* logo信息 */{ display: none;}.cesium-viewer-fullscreenContainer /* 全屏按钮 */{ position: absolute; top: -999em; } 创建形状 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 绘制形状 正方体 // 方式一：Entity//其他形状：http://cesium.xin/cesium/Documentation/Entity.html?classFilter=entity var redBox = viewer.entities.add({ name: &quot;Red box with black outline&quot;, position: Cesium.Cartesian3.fromDegrees(-107.0, 40.0, 300000.0), box: { dimensions: new Cesium.Cartesian3(400000.0, 300000.0, 500000.0), material: Cesium.Color.BLUE.withAlpha(0.5), outline: true, outlineColor: Cesium.Color.BLACK, }, }); viewer.zoomTo(viewer.entities); // 方式二：CZML(可创建几何形状也可创建动画) var czml = [ { id: &quot;document&quot;, name: &quot;box&quot;, version: &quot;1.0&quot;, }, { id: &quot;shape2&quot;, name: &quot;Red box with black outline&quot;, position: { cartographicDegrees: [-107.0, 50.0, 300000.0], }, box: { dimensions: { cartesian: [400000.0, 300000.0, 500000.0], }, material: { solidColor: { color: { rgba: [255, 0, 0, 128], }, }, }, outline: true, outlineColor: { rgba: [0, 0, 0, 255], }, }, }, ]; // 重复创建Viewer会添加多个窗口 // var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;); var dataSourcePromise = Cesium.CzmlDataSource.load(czml); viewer.dataSources.add(dataSourcePromise); viewer.zoomTo(dataSourcePromise); 添加不同类型数据 概览 Web Map Service (WMS) - 一种OGC标准，从分布式地理数据库中通过地图的地理范围来请求切片。 Cesium使用 WebMapServiceImageryProvider去支持这种。 Tile Map Service (TMS) - 一种访问地图切片的REST接口。 可以用CesiumLab， MapTiler 或者 GDAL2Tiles . Cesium中使用TileMapServiceImageryProvider. OpenGIS Web Map Tile Service (WMTS) - 一种OGC标准，主要是为预渲染的地图切片形式. Cesium中使用 WebMapTileServiceImageryProvider. OpenStreetMap - 访问 OpenStreetMap 切片 或者 任意 Slippy map tiles.有很多方法 发布这种服务 .Cesium中使用createOpenStreetMapImageryProvider. Bing 地图 - 使用Bing 地图 REST 服务访问切片. 在这里 https://www.bingmapsportal.com/创建Bing地图的key. Cesium中使用 BingMapsImageryProvider. Esri ArcGIS MapServer - 使用 ArcGIS Server REST API 访问存储在ArcGIS Server上的切片。Cesium中使用ArcGisMapServerImageryProvider. Google Earth Enterprise - 访问Google Earth 企业版服务器发布的影像切片。Cesium中 GoogleEarthImageryProvider. Mapbox - 使用 Mapbox API访问切片. 在这里新建用户，并且创建一个 access token. Cesium中使用 MapboxImageryProvider. 普通图片文件 - 使用一张普通图片创建影像图层. Cesium中使用 SingleTileImageryProvider. 自定义切片机制 - 使用UrlTemplateImageryProvider, 可以通过 URL 模板连接各种影像资源 。比如TMS服务的URL模板是： //cesiumjs.org/tilesets/imagery/naturalearthii/{z}/{x}/{reverseY}.jpg. 切片坐标 - 用来显示全球是如何被切片的，支持多种切片规则，画出每个切片的地理边界，并且用文字标注每个切片的level，x，y坐标。 百度地图 - 用来加载百度默认地图或者自定义样式地图，请联系我们。 跨域问题解决 12345678910111213141516171819module.exports = { dev: { // Paths assetsSubDirectory: 'static', assetsPublicPath: '/', //接口地址原本是/satellite/z={z}&amp;x={x}&amp;y={y} 但是为了匹配代理地址 调用时需在前面加一个 /satellite, 因此接口地址需要写成这样的即可生效 /satellite/satellite/z={z}&amp;x={x}&amp;y={y} proxyTable: { '/ArcGIS': {//代理的目的：只要是/satellite开头的路径都往localhost:3000进行转发 target: 'https://sampleserver1.arcgisonline.com', //后端接口地址 设置代理服务器地址 转发地址 ws: true,//WebSocket协议 changeOrigin: true, //表示是否改变原域名；这个一定要选择为true; 是否允许跨域[ 如果接口跨域 则要配置这个参数] secure: false, // 如果是https接口 需要配置这个参数 pathRewrite: {// 把程序中的地址转换成“真实地址”+‘/satellite’后面的部分如‘/satellite/satellite/z={z}&amp;x={x}&amp;y={y}'，被转换成'http://localhost:3000/satellite/z={z}&amp;x={x}&amp;y={y}' '^/ArcGIS': ''//修改pathRewrite地址 将前缀'^satellite'转为'/satellite' } }, }, 加载不同类型数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 加载不同类型数据 从上到下依次覆盖============================================== // 数据类型1： -----------------------------------Web Map Service (WMS) ------------------------------------- // var provider = new Cesium.WebMapServiceImageryProvider({ // // url: &quot;https://sampleserver1.arcgisonline.com/ArcGIS/services/Specialty/ESRI_StatesCitiesRivers_USA/MapServer/WMSServer&quot;, // url: &quot;ArcGIS/ArcGIS/services/Specialty/ESRI_StatesCitiesRivers_USA/MapServer/WMSServer&quot;, // layers: &quot;0&quot;, // // proxy: new Cesium.DefaultProxy(&quot;/proxy/&quot;), // }); // viewer.imageryLayers.addImageryProvider(provider); // 数据类型2：-----------------------------------OpenGIS Web Map Tile Service (WMTS)----------------------------------- // var shadedRelief2 = new Cesium.WebMapTileServiceImageryProvider({ // url: &quot;http://basemap.nationalmap.gov/arcgis/rest/services/USGSShadedReliefOnly/MapServer/WMTS/tile/1.0.0/USGSShadedReliefOnly/{Style}/{TileMatrixSet}/{TileMatrix}/{TileRow}/{TileCol}.jpg&quot;, // layer: &quot;USGSShadedReliefOnly&quot;, // style: &quot;default&quot;, // format: &quot;image/jpeg&quot;, // tileMatrixSetID: &quot;default028mm&quot;, // maximumLevel: 19, // credit: new Cesium.Credit(&quot;U. S. Geological Survey&quot;), // }); // viewer.imageryLayers.addImageryProvider(shadedRelief2); // url:&quot;http://61.175.211.102/arcgis/rest/services/wzmap/map/MapServer/WMTS?service=WMTS&amp;request=GetTile&amp;layer=wzmap&amp;style=default&amp;tilematriX={TileMatrix}&amp;tilerow={TileRow}&amp;tilecoL={TileCol}&quot; // 数据类型3： -----------------------------------Tile Map Service (TMS)----------------------------------- // var tms = new Cesium.TileMapServiceImageryProvider({ // url: &quot;../images/cesium_maptiler/Cesium_Logo_Color&quot;, // fileExtension: &quot;png&quot;, // maximumLevel: 4, // rectangle: new Cesium.Rectangle( // Cesium.Math.toRadians(-120.0), // Cesium.Math.toRadians(20.0), // Cesium.Math.toRadians(-60.0), // Cesium.Math.toRadians(40.0) // ), // }); // viewer.imageryLayers.addImageryProvider(tms); // 数据类型4： -----------------------------------ArcGis----------------------------------- // var esri = new Cesium.ArcGisMapServerImageryProvider({ // url: &quot;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer&quot;, // }); // viewer.imageryLayers.addImageryProvider(esri); // 数据类型5：-----------------------------------Mapbox----------------------------------- var mapbox = new Cesium.MapboxImageryProvider({ mapId: &quot;mapbox.mapbox-streets-v8&quot;, accessToken: &quot;pk.eyJ1IjoibGltbiIsImEiOiJja2t1bG1na2IxZGU0MnZvNmlzY3FhZXM4In0.oQx4VguycOR4TK80Pyusmw&quot;, }); viewer.imageryLayers.addImageryProvider(mapbox); Leaflet an open-source JavaScript library for mobile-friendly interactive maps Leaflet 是一个为建设移动设备友好的互动地图，而开发的现代的、开源的 JavaScript 库。虽然代码仅有 38 KB，但它具有开发人员开发在线地图的大部分功能。Leaflet设计坚持简便、高性能和可用性好的思想，在所有主要桌面和移动平台能高效运作，在现代浏览器上会利用HTML5和CSS3的优势，同时也支持旧的浏览器访问。支持插件扩展，有一个友好、易于使用的API文档和一个简单的、可读的源代码。详细见官方网站https://leafletjs.com/ 3.3.1地图 3.3.1.1矢量电子地图 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;!--添加leaflet样式--&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;!--添加leafletjs包--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;!--添加坐标库--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], //切图原点 } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30.25168, 120.16179], 4); //设置比例尺和中心点级别 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.1.2影像电子地图 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加影像地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.1.3手动加载/移除地图 3.3.2图层 3.3.2.1弹出信息框 点击地图弹出信息，示例代码bindPopup 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;!--添加leaflet样式--&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;!--添加leafletjs包--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;!--添加坐标库--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], //切图原点 } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30.25168, 120.16179], 4); //设置比例尺和中心点级别 L.marker([30.25168, 120.16179]).addTo(map).bindPopup(&quot;&lt;b&gt;Hello world!&lt;/b&gt;&lt;br /&gt;I am a popup.&quot;).openPopup(); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.2.2矢量图层（wms） 加载WMS地图服务，支持filter筛选 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;调用WMS&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet.wms.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 // L.WMS.source var wmsurl = L.WMS.overlay(&quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wms&quot;, { &quot;transparent&quot;: true, &quot;srs&quot;: &quot;EPSG:4490&quot;, &quot;layers&quot;: &quot;lsyt_dmdz_csbj0619&quot;, &quot;format&quot;: &quot;image/png&quot;, &quot;CQL_FILTER&quot;: &quot;quxian='余杭区'&quot; }); map.addLayer(wmsurl); map.on('click', function (e) { alert(&quot;You clicked the map at &quot; + e.latlng); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.2.3矢量图层（wfs） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;加载WFS电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 map.on(&quot;click&quot;, function (e) { alert(&quot;You clicked the map at &quot; + e.latlng); }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3查询 3.3.3.1点查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;点查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 map.on(&quot;click&quot;, function (e) { // alert(&quot;You clicked the map at &quot; + e.latlng); var point = e.latlng.lng + &quot;,&quot; + e.latlng.lat + &quot;,&quot; + e.latlng.lng + &quot;,&quot; + e.latlng.lat;// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;You clicked the map at &quot; + text.features[1].properties['quxian']); } }); }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3.2面查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;面查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 //var draw=new L.Control.Draw(map,drawControl.options.rectangle).enable(); var options = { position: &quot;topleft&quot;, draw: { polyline: false, polygon: false, circle: false, marker: false, circlemarker: false, rectangle: { shapeOptions: { clickable: true, }, }, }, }; var drawControl = new L.Control.Draw(options); map.addControl(drawControl); map.on(L.Draw.Event.CREATED, function (e) { var type = e.layerType; if (type == &quot;rectangle&quot;) { var bound = e.layer.getBounds(); var point = bound._southWest.lng + &quot;,&quot; + bound._southWest.lat + &quot;,&quot; + bound._northEast.lng + &quot;,&quot; + bound._northEast.lat; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert( &quot;你选择的是 &quot; + text.features[1].properties[&quot;quxian&quot;] ); }, }); } }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3.3属性查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;点查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } .h1 { z-index: 1009; position: absolute; left: 100px; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt; &lt;h1&gt;点击地图根据属性查询&lt;/h1&gt; &lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 map.on(&quot;click&quot;, function (e) { var point = e.latlng.lng + &quot;,&quot; + e.latlng.lat + &quot;,&quot; + e.latlng.lng + &quot;,&quot; + e.latlng.lat; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=&quot;; qurl += &quot; quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;根据属性选择的是 &quot; + text.features[0].properties[&quot;quxian&quot;]); }, }); }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3.4组合查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;面查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 //var draw=new L.Control.Draw(map,drawControl.options.rectangle).enable(); var options = { position: &quot;topleft&quot;, draw: { polyline: false, polygon: false, circle: false, marker: false, circlemarker: false, rectangle: { shapeOptions: { clickable: true, }, }, }, }; var drawControl = new L.Control.Draw(options); map.addControl(drawControl); map.on(L.Draw.Event.CREATED, function (e) { var type = e.layerType; if (type == &quot;rectangle&quot;) { var bound = e.layer.getBounds(); var point = bound._southWest.lng + &quot;,&quot; + bound._southWest.lat + &quot;,&quot; + bound._northEast.lng + &quot;,&quot; + bound._northEast.lat; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); if (!text.features.length) { alert(&quot;你选择的是非上城区，无查询结果！&quot;); } else { alert(&quot;你选择的是 &quot; + text.features[0].properties[&quot;quxian&quot;]); } }, }); } }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; ArcGIS ArcGIS API for JavaScript就是ESRI公司用JavaScipt语言编写的一套程序接口。用户可以通过调用API获取ArcGIS server、geoserver等提供的服务，例如浏览、编辑、渲染地图，以及一些常用的空间分析功能。API中包含了ArcGIS API for JavaScript中每个类的详细描述。使用API查找每个类的构造函数选项以及属性、方法和事件。 详情见官方网站https://developers.arcgis.com/javascript/3/jsapi/ 3.1.1地图 3.1.1.1矢量电子地图 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with Vertor&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.1.2影像电子地图 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with Raster&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver&quot;, { id: &quot;raster&quot;, } ); map.addLayer(layer); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.1.3手动加载/移除地图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with Control&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } .details span { ​ cursor: pointer; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var vector = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); // map.addLayer(vector); var raster = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver&quot;, { id: &quot;raster&quot;, } ); // map.addLayer(raster); $(&quot;#details span&quot;).click(function () { var name = $(this).attr(&quot;name&quot;); switch (name) { case &quot;vector&quot;: if (!map.getLayer(&quot;vector&quot;)) map.addLayer(vector); break; case &quot;raster&quot;: if (!map.getLayer(&quot;raster&quot;)) map.addLayer(raster); break; case &quot;rvector&quot;: if (map.getLayer(&quot;vector&quot;)) map.removeLayer(vector); break; case &quot;rraster&quot;: if (map.getLayer(&quot;raster&quot;)) map.removeLayer(raster); break; } }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt; ​ &lt;span name=&quot;vector&quot;&gt;添加矢量图层&lt;/span&gt; ​ &lt;span name=&quot;raster&quot;&gt;添加影像图层&lt;/span&gt; ​ &lt;span name=&quot;rvector&quot;&gt;移除矢量图层&lt;/span&gt; ​ &lt;span name=&quot;rraster&quot;&gt;移除影像图层&lt;/span&gt; &lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.2图层 3.1.2.1弹出信息框 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with infoWindow&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WFSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WFSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); map.on('click', function (e) { map.infoWindow.setTitle(&quot;详细信息&quot;); map.infoWindow.setContent(&quot;当前坐标：&quot; + e.mapPoint.x + &quot;,&quot; + e.mapPoint.y); map.infoWindow.show(e.mapPoint); }) }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.2.2矢量图层（wms） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with wmslayer&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.2.3矢量图层（wfs） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with wfslayer&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map, symbol; require([ &quot;esri/map&quot;, &quot;esri/layers/WFSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/symbols/SimpleLineSymbol&quot;, &quot;esri/symbols/SimpleFillSymbol&quot;, &quot;esri/graphic&quot;, &quot;esri/geometry/Extent&quot;, &quot;esri/geometry/Polygon&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WFSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, SimpleLineSymbol, SimpleFillSymbol, Graphic, Extent, Polygon, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; $.ajax({ url: url, success: function (data) { console.log(data); var text = JSON.parse(data); for (var i = 0; i &lt; text.features.length; i++) { symbol = new SimpleFillSymbol(); if (text.features[i].geometry.type == &quot;Polygon&quot;) { var polygonjson = { &quot;rings&quot;: text.features[i].geometry.coordinates[0], &quot;spatialReference&quot;: { &quot;wkid&quot;: 4490 } } var poly = new Polygon(polygonjson) var graphic = new Graphic(poly, symbol); //var gra = new Graphic(text.features[i].geometry, symbol); map.graphics.add(graphic); } else if (text.features[i].geometry.type == &quot;MultiPolygon&quot;) { var len = text.features[i].geometry.coordinates.length; for (var k = 0; k &lt; len; k++) { var polygonjson = { &quot;rings&quot;: text.features[i].geometry.coordinates[k], &quot;spatialReference&quot;: { &quot;wkid&quot;: 4490 } } var poly = new Polygon(polygonjson) var graphic = new Graphic(poly, symbol); map.graphics.add(graphic); } } else { alert(); } } } }); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3查询 3.1.3.1点查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query By Point&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { // basemap: &quot;streets&quot;, // center: [120.1594, 30.2571], center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { // id: layerInfo.LayerName, id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); }); map.on(&quot;click&quot;, function (e) { console.log(&quot;获取到的点&quot; + e); var point = e.mapPoint.x + &quot;,&quot; + e.mapPoint.y + &quot;,&quot; + e.mapPoint.x + &quot;,&quot; + e.mapPoint.y;// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); $('#details').html(text.features[1].properties['quxian']); } }); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3.2面查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query By Rectangle&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map, toolbar; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/toolbars/draw&quot;, &quot;esri/symbols/SimpleLineSymbol&quot;, &quot;esri/symbols/SimpleFillSymbol&quot;, &quot;esri/graphic&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, Draw, SimpleLineSymbol, SimpleFillSymbol, Graphic, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { // basemap: &quot;streets&quot;, // center: [120.1594, 30.2571], center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { // id: layerInfo.LayerName, id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); toolbar = new Draw(map); toolbar.on(&quot;draw-end&quot;, showinfo); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }); function showinfo(evt) { var symbol; toolbar.deactivate(); map.graphics.clear(); symbol = new SimpleFillSymbol(); var graphic = new Graphic(evt.geometry, symbol); map.graphics.add(graphic); getAttr(evt.geometry); } function getAttr(geo) { var point = geo.cache._extent.xmin + &quot;,&quot; + geo.cache._extent.ymin + &quot;,&quot; + geo.cache._extent.xmax + &quot;,&quot; + geo.cache._extent.ymax; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); var attr = &quot;&quot;; for (var i = 0; i &lt; text.features.length; i++) { attr += text.features[i].properties[&quot;quxian&quot;] + &quot;&lt;/br&gt;&quot;; } $(&quot;#details&quot;).html(attr); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }, }); } }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3.3属性查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query by Attr&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); }); map.on(&quot;click&quot;, function (e) { console.log(&quot;获取到的点&quot; + e); var point = &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); var attr = &quot;&quot;; for (var i = 0; i &lt; text.features.length; i++) { attr += text.features[i].properties[&quot;quxian&quot;] + &quot;&lt;/br&gt;&quot;; } $(&quot;#details&quot;).html(attr); }, }); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;点击地图实现按属性查询（区县=上城区）&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3.4组合查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map, toolbar; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/toolbars/draw&quot;, &quot;esri/symbols/SimpleLineSymbol&quot;, &quot;esri/symbols/SimpleFillSymbol&quot;, &quot;esri/graphic&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, Draw, SimpleLineSymbol, SimpleFillSymbol, Graphic, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); toolbar = new Draw(map); toolbar.on(&quot;draw-end&quot;, showinfo); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }); function showinfo(evt) { var symbol; toolbar.deactivate(); map.graphics.clear(); symbol = new SimpleFillSymbol(); var graphic = new Graphic(evt.geometry, symbol); map.graphics.add(graphic); getAttr(evt.geometry); } function getAttr(geo) { var point = geo.cache._extent.xmin + &quot;,&quot; + geo.cache._extent.ymin + &quot;,&quot; + geo.cache._extent.xmax + &quot;,&quot; + geo.cache._extent.ymax; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); var attr = &quot;&quot;; for (var i = 0; i &lt; text.features.length; i++) { attr += text.features[i].properties[&quot;quxian&quot;] + &quot;&lt;/br&gt;&quot;; } $(&quot;#details&quot;).html(attr); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }, }); } }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;在框选范围内查询属性（区县=上城区）的数据&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; Openlayers OpenLayers 是一个专为Web GIS 客户端开发提供的JavaScript 类库包，用于实现标准格式发布的地图数据访问。OpenLayers实现访问地理空间数据的方法都符合行业标准。OpenLayers 支持Open GIS 协会制定的WMS（Web Mapping Service）和WFS（Web Feature Service）等网络服务规范，可以通过远程服务的方式，将以OGC 服务形式发布的地图数据加载到基于浏览器的OpenLayers 客户端中进行显示。详细见官方网站https://openlayers.org/ 地图 矢量电子地图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot;&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = 'image/png'; var bounds = [119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439 ]; var projection = new ol.proj.Projection({ code: 'EPSG:4490', units: 'degrees', axisOrientation: 'neu', global: true }); var fullExtent = [118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = (-tileCoord[2] - 1); var url = 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/' + z + '/' + y + '/' + x; console.log(url); return url; }, projection: 'EPSG:4490' }) }); var map = new ol.Map({ target: 'map', layers: [ tileLayer ], view: new ol.View({ projection: projection }) }); map.getView().fit(bounds, map.getSize()); //非常重要 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 影像电子地图 添加电子地图 var format = 'image/png'; var bounds = [119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439 ]; var projection = new ol.proj.Projection({ code: 'EPSG:4490', units: 'degrees', axisOrientation: 'neu', global: true }); var fullExtent = [118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = (-tileCoord[2] - 1); var url = 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver/tile/' + z + '/' + y + '/' + x; console.log(url); return url; }, projection: 'EPSG:4490' }) }); var map = new ol.Map({ target: 'map', layers: [ tileLayer ], view: new ol.View({ projection: projection }) }); map.getView().fit(bounds, map.getSize()); //非常重要 图层 弹出框信息框 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;弹出信息框&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;style&gt; #map { height: 100%; width: 100%; } #mypopup { background: #fff; }&lt;/style&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;div id=&quot;mypopup&quot;&gt; &lt;div id=&quot;mypopup-content&quot;&gt;内容自定义&lt;/div&gt; &lt;button id=&quot;closeOverlay&quot;&gt;关闭&lt;/button&gt; &lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; // console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var info = $(&quot;#mypopup&quot;); var content = $(&quot;#mypopup-content&quot;); var overlay = new ol.Overlay({ element: info[0], autopan: true, autoPanMargin: 20, positioning: &quot;center-center&quot;, }); //map.addOverlay(overlay); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer], overlays: [overlay], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 $(&quot;#closeOverlay&quot;).click(function () { info.hide(); return false; }); map.on(&quot;click&quot;, function (e) { var location = e.coordinate; content.html(&quot;这是信息框&quot;); info.show(); overlay.setPosition(location); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 矢量图层（wms） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot;&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = 'image/png'; var bounds = [119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439 ]; var projection = new ol.proj.Projection({ code: 'EPSG:4490', units: 'degrees', axisOrientation: 'neu', global: true }); var fullExtent = [118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = (-tileCoord[2] - 1); var url = 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/' + z + '/' + y + '/' + x; console.log(url); return url; }, projection: 'EPSG:4490' }) }); var untiled = new ol.layer.Image({ source: new ol.source.ImageWMS({ ratio: 1, //url: 'http://126.10.2.27:8080/geoserver/gpserver/wms', url: 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms?', params: { 'FORMAT': format, 'VERSION': '1.1.1', &quot;LAYERS&quot;: 'lsyt_dmdz_qxbj0619', &quot;exceptions&quot;: 'application/vnd.ogc.se_inimage', } }) }); var map = new ol.Map({ target: 'map', layers: [ tileLayer, untiled ], view: new ol.View({ projection: projection }) }); map.getView().fit(bounds, map.getSize()); //非常重要 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 矢量图层（wfs） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;添加WFS电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 查询 点查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;点查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 map.on(&quot;click&quot;, function (e) { var location = e.coordinate; var point = location[0] + &quot;,&quot; + location[1] + &quot;,&quot; + location[0] + &quot;,&quot; + location[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;You clicked the map at &quot; + text.features[1].properties['quxian']); } }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 面查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;框选查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 var dragbox = new ol.interaction.DragBox(); map.addInteraction(dragbox); dragbox.on(&quot;boxend&quot;, function () { var extent = dragbox.getGeometry().getExtent(); var point = extent[0] + &quot;,&quot; + extent[1] + &quot;,&quot; + extent[0] + &quot;,&quot; + extent[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;You clicked the map at &quot; + text.features[0].properties['quxian']); } }); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 属性查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;属性查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 map.on(&quot;click&quot;, function (e) { var location = e.coordinate; var point = location[0] + &quot;,&quot; + location[1] + &quot;,&quot; + location[0] + &quot;,&quot; + location[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=&quot;; qurl += &quot; quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;根据属性选择的是&quot; + text.features[0].properties['quxian']); } }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 组合查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;组合查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 map.on(&quot;click&quot;, function (e) { var location = e.coordinate; var point = location[0] + &quot;,&quot; + location[1] + &quot;,&quot; + location[0] + &quot;,&quot; + location[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); if (!text.features.length) { alert(&quot;你选择的是非上城区，无查询结果！&quot;); } else { alert(&quot;你选择的是 &quot; + text.features[0].properties[&quot;quxian&quot;]); } } }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; Cygwin windows使用linux环境（部分机型失败） Tippecanoe Tippecanoe是Mapbox的一个开源切片工具，项目地址：https://github.com/mapbox/tippecanoe，Mapbox常规的切片方法tilelive-copy参见另一篇博客。Tippecanoe主要在处理大数据量时有很大的优势，具有很高的效率，并且有很多参数可以控制。Tippecanoe只能处理GeoJSON，因此在切片前需要将矢量数据转换为GeoJSON，推荐使用ogr2ogr工具转换。切片以后的格式为mbtiles，可自行导入mongodb等数据库。 目的：根据你的数据创造一个可自由缩放的视图 引用地址：https://my.oschina.net/u/1464512/blog/1631972 常用tippecanoe参数设置 GEOJSON GeoSever GeoServer是OGC Web服务器规范的J2EE实现，利用GeoServer可以方便地发布地图数据，允许用户对特征数据进行更新、删除、插入操作，通过GeoServer可以比较容易地在用户之间迅速共享空间地理信息。GeoServer是开源软件。 GeoServer主要包含如下一些特点： 兼容WMS和WFS特性 支持PostGIS、Shapefile、ArcSDE、Oracle、VPF、MySQL、MapInfo 支持上百种投影 能够将网络地图输出为JPEG、GIF、PNG、SVG、KML等格式 能够运行在任何基于J2EE/Servlet容器之上 嵌入MapBuilder支持AJAX的地图客户端OpenLayers 引用地址 WMS &amp;&amp; WFS WMS是由服务器将一地图图像发送给客户端，而WFS是服务器将矢量数据发送给客户端，也就是在使用WMS时地图由服务器绘制，在使用WFS时地图由客户端绘制。 WMS 1.2.1 WMS服务简介 Web Map Service（网络地图服务），简称WMS，由开放地理信息联盟（Open GeoSpatial Consortium，OGC）制定。该规范定义了Web客户端从网络地图服务器获取地图的接口标准。一个WMS可以动态地生成具有地理参考数据的地图，这些地图通常用GIF、JPEG或PNG等图像格式等，使用者通过指定的参数获取相应的地图图片。 1.2.2 服务操作列表 WMS实现规范由三个基础性操作协议(GetCapabilities，GetMap和GetFeatureInfo)组成，这些协议共同构成了利用WMS创建和叠加显示不同来源的远程异构地图服务的基础。WMS服务操作列表见下表所示。 操作 实现要求 描述 GetCapabilities 强制实现 暂关闭 GetMap 强制实现 获取地图图片。该操作根据客户端发出的请求参数在服务端进行检索，服务器端返回一个地图图像，其地理空间参数和大小参数是已经明确定义的，返回的地图图像可以是GIF、JPEG、PNG或SVG格式。 GetFeatureInfo 选择实现 1.2.3服务操作的参数列表 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WMS” request 1个(必选) 字符类型，请求的操作名称，值为“GetCapabilities” version 0或1个(可选) 字符类型，值为请求的WMS的版本号 format 0或1个(可选) MIME类型，值为服务元数据的输出格式 GetMap操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型标识值为“WMS” request 1个(必选) 字符类型，值为“GetMap” version 1个(必选) 字符类型，值为请求的WMS的版本号 layers 1个(必选) 字符类型，值为一个或多个地图图层列表，多个图层之间用”,”隔开 styles 1个(必选) 字符类型，值为请求图层的地图渲染样式 CRS 1个(必选) 字符类型，值为坐标参照系统 BBOX 1个(必选) Wkt格式，值为某个CRS下的地图边界范围的坐标序列 width 1个(必选) 整型类型，值为地图图片的像素宽度 height 1个(必选) 整型类型，值为地图图片的像素高度 format 1个(必选) 字符类型，值为地图的输出格式 transparent 0或1个(可选) 字符类型，值为true或者false，用来表示地图图层是否透明(默认情况下是不透明的) filter 0或1个(可选) 请求要素的过滤条件 1.2.4 接口调用示例 实例名称 调用实例 GetCapabilities 暂关闭 GetMap http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wms?SERVICE=WMS&amp;VERSION=1.1.1&amp;REQUEST=GetMap&amp;FORMAT=image%2Fpng&amp;TRANSPARENT=true&amp;LAYERS=gpserver%3Alsyt_dmdz_csbj0619&amp;CQL_FILTER=quxian%3D'滨江区'&amp;SRS=EPSG%3A4490&amp;STYLES=&amp;WIDTH=768&amp;HEIGHT=546&amp;BBOX=120.05413055419922%2C30.084686279296875%2C120.31780242919922%2C30.272140502929688 WFS 1.3.1 WFS服务简介 Web Feature Service（网络要素服务），简称WFS，由开放地理信息联盟（Open GeoSpatial Consortium，OGC）制定。该规范主要对OpenGIS简单要素的数据编辑操作进行规范，从而使服务器端和客户端能够在要素层面进行“通讯”。其返回结果的是XML格式的WFS服务元数据文档，通过该文档用户能够了解：WFS服务器支持的所有操作操作列表，GetFeature操作返回的数据格式，可用的坐标参照系统列表，操作异常信息的列表，WFS服务提供商的相关信息，WFS服务器的可用要素类列表等。 1.3.2 服务操作列表 WFS服务接口规范定义了GetCapabilities，DescribeFeatureType、GetFeature、Transaction等操作。其中GetCapabilities，DescribeFeatureType和GetFeature为必须实现的操作，也即只要实现了这三个操作的服务均可称为WFS服务。WFS的操作见下表所示。 操作 实现要求 描述 GetCapabilities 强制实现 GetCapabilities请求用于查询WFS服务的能力信息，包括支持的操作、支持的格式、空间坐标、包含的资源等。它主要的目的是使客户端在使用GetFeature请求前可以对WFS服务有一个基本的了解，从而可以设置正确的参数。 DescribeFeatureType 强制实现 用于生成一个 Schema 描述，该 Schema 描述了 WFS 服务提供的要素类型（Feature Type），以及要素类型的结构信息。该 Schema 还定义了 WFS 服务所期望的要素实例在输入时如何编码以及输出时如何生成要素实例。 GetFeature 强制实现 GetFeature用于向WFS的客户端程序提供查询特定地理信息的能力，通过GetFeature操作可以由指定的属性条件、空间条件或者两者叠加的条件进行空间查询。 Transaction 选择实现 允许Transaction操作，使客户端可对服务器端所提供的地图要素类执插入，更新，删除等命令 1.3.3服务操作的参数列表 GetCapabilities操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WFS” request 1个(必选) 字符类型，请求的操作名称，值为“GetCapabilities” versions 0或1个(可选) 字符类型，值为请求的WFS的版本号 DescribeFeatureType操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WFS” request 1个(必选) 字符类型，请求的操作名称，值为“DescribeFeatureType” typeName 0或1个(可选) 字符类型，值为要素类型的列表，多个值之间用“，”隔开，默认解析包括的全部要素类型 outputFormat 0或1个(可选) MIME类型，值为输出格式 GetFeature操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型标识值为“WFS” request 1个(必选) 字符类型，请求的操作值为“GetFeature” typeName 1个(必选) 字符类型，值为请求的要素类型的名称，多个名称之间用“，”隔开 version 0或1个(可选) 字符类型，值为请求的WFS的版本号 outputFormat 0或1个(可选) MIME类型，值为输出格式 resultType 0或1个(可选) 字符类型，值为请求的结果类型 propertyName 0或1个(可选) 字符类型，值为请求要素的属性名，多个值之间用“，”隔开 featureVersion 0或1个(可选) 字符类型，值为要素的版本，值为ALL返回请求的要素的所有版本，没有值默认为返回请求要素的最新版本 maxFeature 0或1个(可选) 整型类型，值为请求要素的最大数，默认值为满足查询的所有结果集 expiry 0或1个(可选) 数字类型，要素被锁定的时间 SRSName 0或1个(可选) 字符类型，值为坐标系统名 featureID 0或1个(可选) 字符类型，值为要素的ID，多个ID之间用“，”隔开 filter 0或1个(可选) 请求要素的过滤条件 bBox 0或1个(可选) Wkt格式，请求指定要素查询范围，可以替代featureId和filter参数 1.3.4 接口调用示例 实例名称 调用实例 GetCapabilities http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wfs?service=wfs&amp;version=1.1.0&amp;request=getcapabilities DescribeFeatureType http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.1.0&amp;request=describefeaturetype&amp;typename=lsyt_dmdz_csbj0619 GetFeature http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wfs?service=wfs&amp;version=1.1.0&amp;request=getfeature&amp;typename=lsyt_dmdz_csbj0619&amp;outputFormat=json&amp;CQL_FILTER=zhenjie='闲林街道' WMTS WMTS 1.1 WMTS服务简介 ​ Web Map Tile Service（网络地图瓦片服务），简称WMTS，由开放地理信息联盟（Open GeoSpatial Consortium，OGC）制定，是和WMS并列的重要OGC规范之一。WMTS标准定义了一些操作，这些操作允许用户访问切片地图，是OGC首个支持RESTful访问的服务标准。WMTS不同于WMS,它最重要的特征是采用缓存技术能够缓解WebGIS服务器端数据处理的压力，提高交互响应速度，大幅改善在线地图应用客户端的用户体验。WMTS是OGC主推的缓存技术规范，是目前各种缓存技术相互兼容的一种方法。 WMTS的切片坐标系统和其组织方式可参考下图： 1.2 服务操作列表 WMTS服务支持RESTful访问，其接口包括GetCapabilities、GetTile和GetFeatureInfo3个操作，这些操作允许用户访问切片地图。 操作 操作 描述 GetCapabilities 强制实现 获取WMTS的能力文档（即元数据文档），里面包含服务的信息 GetTile 强制实现 该操作根据客户端发出的请求参数在服务端进行检索，服务器端返回地图瓦片图像 GetFeatureInfo 选择实现 1.3 服务操作的参数列表 GetCapabilities操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WMTS” request 1个(必选) 字符类型，请求的操作名称，值为“GetCapabilities” GetTile操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型标识值为“WMTS” request 1个(必选) 字符类型，请求的操作值为“GetTile” version 1个(必选) 字符类型，值为请求的WMTS的版本号 layer 1个(必选) 字符类型，值为请求的图层名称 style 1个(必选) 字符类型，值为请求图层的渲染样式 format 1个(必选) 字符类型，值为瓦片地图的输出格式 tileMatrixSet 1个(必选) 字符类型，瓦片矩阵数据集，其值在服务的元数据文档中指定 tileMatrix 1个(必选) 字符类型，瓦片矩阵，其值在服务的元数据文档中指定 tileRow 1个(必选) 整型类型，值为大于0的整数，表示瓦片矩阵的行号 tileCol 1个(必选) 整型类型，值为大于0的整数，表示瓦片矩阵的列号 1.4接口调用示例 实例名称 调用实例 GetCapabilities操作 http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/WMTS?service=wmts&amp;request=GetCapabilities GetTile操作 http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/WMTS?service=wmts&amp;request=getTile&amp;version=1.0.0&amp;layer=hzsyvector&amp;Style=&amp;tilematrixset=hzsyvector&amp;tilematrix=3&amp;tilerow=5&amp;tilecol=7&amp;format=png ****","link":"/2021/02/22/Draft/2021/GIS/"},{"title":"魑魅先生 | 资源篇","text":"JVM 与上层技术 魑魅先生 | 资源篇 魑魅先生 | 资源篇 一.程序员书屋（编程相关，Kindle 相关 一共112个 G 资源） Java资源101个 G 囊括大部分 Java 开发小白到全栈大神所需阅读全部书籍，设计模式、网络、算法、框架、职场经验 ​ 关注公众号发送 小书屋 获取下载链接 不仅如此，还有 Kindle 各类电子书籍11个G 索引在此点击 搜索之后有需要的 关注公众号发送 Kindle获取下载链接！ 二.学习网站（持续更新中） 1.编程网站： Refactoring.Guru ​ 设计模式在线学习，程序员之间的沟通语言，学习必不可少的知识，有各种代码示范，UML图，动漫演示。 Visualgo ​ 算法在线可视化过程，理解更加容易，过程速度可调，步骤可控，代码可观。 Github ​ 将自己的学习代码进行版本化管理，将学习资源整理归纳，学习别人的优秀项目，获取全球最新资源，好处就不一一列举了，每个程序员都了解。 CSDN ​ 一个人的成就往往不在于得到多少，而在于输出多少，一个有贡献的程序员往往是会分享的。这个博客网站汇聚很多大牛，也有很多 demo，让你可以不重复造轮子。 2.设计网站： doyoudo ​ 学习 Ae、Ps、Pr、C4D、乐理等等。印象中小白老师的生动课堂是让我对设计感兴趣第一原因，也是第一个让我有想上付费课冲动的学习网站。 Colordrop ​ 配色网站，直观且复制方便，妈妈再也不用担心我的配色了。 创客贴 ​ 不会 PS 没关系，创客贴直接拉动修改元素，一切都直接设计好了，小白式拖动便可快速完成设计更改，还可以直接修改 GIF 动图！ 3.通用网站： Bilibili 不仅仅有鬼畜，你想有的教程都有，摄影、设计、编程、绘画等，是现代学习者不可缺少聚集地。 Coursera 国外学习网站，免费资源很多，可以接触更加及时的知识。 三.软件推荐（持续更新中） 电脑篇 英语语法书写纠正，毕竟语法纠正除了老师就只有他了。 自媒体必备，Markdown最佳利器。 字体中央管理，免费商用就够你用一辈子了，版权问题不用考虑了 Markdown图床管理优秀软件之一 有他还要啥百度云，配合chrome使用更香 你有个ipad放那盖泡面不如拿来用作你的第二屏幕 艾宾浩斯遗忘曲线最佳实践，记忆任何知识，自主卡片形式 目前使用过的最佳思维导图软件 三端互传文件，无缝同步，在鸿蒙系统出来之前它可能一直是我的主力 安卓可用，电脑操控手机，可无线控制，上班划水利器，免费开源 抓包，测试，网络模拟，新世界，一不小心从抓包到入狱 浏览器插件 ​ chrome有了它，啥会员都不用开。 ​ 有了它，任何复杂页面都可以变得小清新。 ipad篇 （待更新[如何抓包获取各种围哎皮，如何高效率利用学习软件]） 手机篇 （待更新[各种黑科技小工具]） ​ 魑魅先生，一只拼搏成为魑魅大能的小鬼(●—●) 四.设计资源（持续更新中） ​ 包括各大海报，字体，笔刷，素材，LOGO，全家桶等等，公众号回复设计资源获取下载链接 ​ ! 五.视频资源（持续更新中） Java，Python相关基础到架构课程，项目实战 ​ ​ 音乐相关 ​ 吉他，尤克里里，乐理，编曲，作词，相关软件，声源 ​ ​ 绘画 语言 摄影 ​ 包含美国摄影学院摄影教材，学习摄影不可缺少的书籍 最后想说，资源虽多，但如果只当一个收藏党那永远不会是你的。只有在这飞速发展的时代，卸去浮躁，一点点消化，才会让成为你茁壮成长的养分。 ​ 资源分享，部分资源请勿作商业用途，如有侵权，联系删除。如果有能力，请支持正版！","link":"/2021/02/20/Draft/2021/%E8%B5%84%E6%BA%90%E7%AF%87/"},{"title":"魑魅先生 | 每日算法","text":"新学四问 WHY【与前代优化了什么，弥补了什么空白】编程的灵魂，不管什么语言都得需要。 WHAT【框架，思维导图，主题框架】程序员专业词汇，日常英语 HOW【如何记忆，学习资源】多想多写多用，Leetcode LEVEL【不是每个都学精】实际运用 学前准备 labuladong/fucking-algorithm: 刷算法全靠套路，认准 labuladong 就够了！English version supported! Crack LeetCode, not only how, but also why. (github.com) MisterBooo/LeetCodeAnimation: Demonstrate all the questions on LeetCode in the form of animation.（用动画的形式呈现解LeetCode题目的思路） (github.com) 一.基本数据结构 第一章 算法定义 在特定计算模型下，在信息处理过程中为了解决某一类问题而设计的一个指令序列。 要素： 输入：待处理的信息，即对具体问题的描述。 输出：经过处理之后得到的信息，即问题的答案。 确定性：任一算法都可以描述为由若干种基本操作组成的序列。 可行性：在相应的计算模型中，每一基本操作都可以实现，且能够在常数时间内完成。 有穷性：对于任何输入，按照算法，经过有穷次基本操作都可以得到正确的输出。 1.2性能分析预评价 三个层次：合法程序，确定尺度度量算法效率，通过对算法设计编写效率高，能处理大规模数据的程序， 时间复杂度 T(n) 度量算法执行速度并评价其效率，算法需要多少时间才能得到结果 ==》针对不同规模的输入，算法的执行时间各是多少 ==》统一规模算法处理时间也不相同 空间复杂度 算法所需存储空间 1.3 算法复杂度及其分析 1.3.1 O(1)⎯⎯取非极端元素 1.3.2 O(logn)⎯⎯进制转换 1.3.3 O(n)⎯⎯数组求和 1.3.4 O(n2)⎯⎯起泡排序 1.3.5 O(2r)⎯⎯幂函数 1.4 计算模型 1.4.1 可解性 1.4.2 有效可解 1.4.3 下界 1.5 递归 1.5.1 线性递归 1.5.2 递归算法的复杂度分析 1.5.3 二分递归 1.5.4 多分支递归 第二章 栈与队列 栈与队列最简单基本，但也是最重要的。JVM，CPU，Java提供对应内建类， 2.1 栈 后进先出（Last-in-first-out，LIFO），比如浏览器访问记录与回退，编辑回退。 元素： 栈容量，栈顶指针，初始化 进栈push()、出栈pop()，查栈顶peek() Java java.util.Stack ：push()、pop()、peek()（功能等价于top()）、getSize()以及empty()（功能等价于isEmpty()） 应用：数组倒置，括号匹配算法， 三种实现方式 数组 链表 LinkedList 2.2 队列 先进先出（First-In-First-Out, FIFO），羽毛球筒 2.2.1 队列ADT Queue 接口 元素 队头、队尾、队尾加元素add()，队头删除元素poll(),查队头元素peek() 2.2.2 基于数组的实现 顺序数组，整体移动 循环数组， 性能分析，O(1)。 2.2.3 队列应用实例 循环分配器，Josephus 环 2.3 链表 数组长度必须固定，在空间效率及适应性方面还存在不足。 2.3.1 单链表 元素 首节点，末节点 ​ 链表的第一个和最后一个节点，分别称作链表的首节点（Head）和末节点（Tail）。末节点的特征是，其next 引用为空。如此定义的链表，称作单链表（Singly linkedlist）。 ​ 与数组类似，单链表中的元素也具有一个线性次序⎯⎯若P 的next 引用指向S，则P 就是S的直接前驱，而S 是P 的直接后继。与数组不同的是，单链表的长度不再固定，而是可以根据实际需要不断变化。如此一来，包含n 个元素的单链表只需占用O(n)空间⎯⎯这要比定长数组更为灵活。 2.4 位置 2.5 双端队列 第三章 向量、列表与序列 ​ 序列（Sequence），就是依次排列的多个对象，就是一组对象之间的后继与前驱关系，是数据结构设计的基础。两种典型的序列：向量（Vector）和列表（List）。 3.1 向量与数组 3.1.1 向量ADT 第四章 树 ​ 前面所有的数据结构根据其实现方式，可以划分为基于数组实现和基于链表实现，其各有长短，数组善查找读取，修改耗时，链表反之。两者有点能结合?树或许可以回答这个问题。 术语及性质 节点的深度、树的深度与高度 ​ 树中的元素也称作节点（Node），每个节点的深度都是一个非负整数；深度为0 的节点有且仅有一个，称作树根（Root）；对于深度为k (k≥1)的每个节点u，都有且仅有一个深度为k-1 的节点v 与之对应，称作u 的父亲（Parent）或父节点。定义四.2 若节点v 是节点u 的父亲，则u 称作v 的孩子（Child），并在二者之间建立一条树边（Edge）。同一节点的孩子互称“兄弟”（Sibling）。树中所有节点的最大深度，称作树的深度或高度。树中节点的数目，总是等于边数加一。 度、内部节点与外部节点 ​ 任一节点的孩子数目，称作它的“度”（Degree）。至少拥有一个孩子的节点称作“内部节点”（Internal node）；没有任何孩子的节点则称作 “外部节点”（External node）或“叶子”（Leaf）。 路径 ​ 由树中k+1 节点通过树边首尾衔接而构成的序列{ (v0, v1), (v1, v2), …, (vk-1, vk) | k ≥ 0}，称作树中长度为k 的一条路径（Path）。由单个节点、零条边构成的路径也是合法的，其长度为0。树中任何两个节点之间都存在唯一的一条路径。若v 是u 的父亲，则depth(v) + 1 = depth(u)。从树根通往任一节点的路径长度，恰好等于该节点的深度。 祖先、后代、子树和节点的高度 每个节点都是自己的“祖先”（Ancestor），也是自己的“后代”（Descendent）； 若v 是u 的父节点的祖先，则v 也是u 的祖先； 若u 的父节点是v 的后代，则u 也是v 的后代。 除节点本身以外的祖先（后代），称作真祖先（后代）。任一节点v 的深度，等于其真祖先的数目。任一节点v 的祖先，在每一深度上最多只有一个。树T 中每一节点v 的所有后代也构成一棵树，称作T 的“以v 为根的子树（Subtree）”若子树v 的深度（高度）为h，则称v 的高度为h，记作height(v) = h。对于叶子节点u 的任何祖先v，必有depth(v) + height(v) ≥ depth(u)。 共同祖先及最低共同祖先 在树T 中，若节点u 和v 都是节点a 的后代，则称节点a 为节点u 和v 的共同祖先（Commonancestor）。每一对节点至少存在一个共同祖先。在一对节点u 和v 的所有共同祖先中，深度最大者称为它们的最低共同祖先（Lowerestcommon ancestor），记作lca(u, v)。每一对节点的最低共同祖先必存在且唯一。 有序树、m 叉树 在树T 中，若在每个节点的所有孩子之间都可以定义某一线性次序，则称T 为一棵“有序树（Ordered tree）”每个内部节点均为m 度的有序树，称作m 叉树。 二叉树 每个节点均不超过2 度的有序树，称作二叉树（Binary tree）。不含1 度节点的二叉树，称作真二叉树（Proper binary tree），否则称作非真二叉树 （Improper binary tree）。在二叉树中，深度为k 的节点不超过2k 个。高度为h 的二叉树最多包含2h+1-1 个节点。由n 个节点构成的二叉树，高度至少为⎣log2n⎦。在二叉树中，叶子总是比2 度节点多一个。 满二叉树与完全二叉树 若二叉树T 中所有叶子的深度完全相同，则称之为满二叉树（Full binary tree）高度为h 的二叉树是满的，当且仅当它拥有2h 匹叶子、2h+1-1 个节点。若在一棵满二叉树中，从最右侧起将相邻的若干匹叶子节点摘除掉，则得到的二叉树称作完全二叉树（Complete binary tree）。由n 个节点构成的完全二叉树，高度h = ⎣log2n⎦。在由固定数目的节点所组成的所有二叉树中，完全二叉树的高度最低。 第五章 优先队列 第六章 映射与词典 第七章 查找树 第八章 串 第九章 图 第十章 算法思想 二、常用算法 排序 1、冒泡排序 2、选择排序 3、插入排序 4、希尔排序 5、归并排序 6、快速排序 然后基准两边分别快速排序 7、堆排序 8、计数排序 9、桶排序 10、基数排序 查找 1. 顺序查找 2. 二分查找 3. 插值查找 4. 斐波那契查找 5. 树表查找 6. 分块查找 7. 哈希查找 三、算法题 2.18题目：序号-题目 思路1： 123思路1优点：缺点： 代码1： 1代码1 思路2： 123思路2优点：缺点： 代码2： 1代码2 参考文献：数据结构预算法(Java描述)邓俊辉","link":"/2021/02/19/Draft/2021/%E6%AF%8F%E6%97%A5%E7%AE%97%E6%B3%95/"},{"title":"五分钟搭建在线博客","text":"博客已成为程序员学习与输出重要一环，当你时间宝贵或者没有必要从零构建个人博客的话，一个可以进行快速搭建的静态博客就显得比较重要了。不但丰富的主题可以任你选择，各个组件还可以任你搭配。有的甚至无需服务器和域名注册，分分钟实现自己分享的小基地。 目前比较流行的开源框架有 Hexo、WordPress、VuePress、Hugo、Solo、Halo 、Jekyll 白嫖党【无需服务器，免费域名】喜欢的无个人域名无法被百度搜索 Jekyll、Hugo、Hexo 本博客使用 Hexo 接下来开始Hexo博客快速搭建 环境准备 Github账号，新库名:用户名.github.io node.js、npm、JS、Git 安装与基础学习 图床 Gitee+PicGo 初步搭建 创建博客文件夹，shift+右键进入目录cmd 安装hexo npm install -g hexo-cli 初始化文件夹 hexo init hexo_blog 进入博客文件夹 cd E:\\my\\hexo_blog 安装博客需要的依赖文件 npm install Hexo命令 hexo cl #清理 hexo g #生成 hexo s #本地服务 hexo d #发布 测试http://locakhost:4000 或127.0.0.1:4000 初步搭建完成 Hexo相关配置 项目目录说明 _config.yml配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117title: 博客名subtitle: 副标题description: 博客描述keywords: SEO搜索关键词author: 文章作者language: zh-CN #语言timezone: ''#时区# URL## If your site is put in a subdirectory, set url as 'http://example.com/child' and root as '/child/'url: http://MrDemonlxl.github.io #网址root: /permalink: :year/:month/:day/:title/permalink_defaults:pretty_urls: trailing_index: true # Set to false to remove trailing 'index.html' from permalinks trailing_html: true # Set to false to remove trailing '.html' from permalinks# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: enable: true # Open external links in new tab field: site # Apply to the whole site exclude: ''filename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: '' wrap: true hljs: falseprismjs: enable: false preprocess: true line_number: true tab_replace: ''# Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Metadata elements## https://developer.mozilla.org/en-US/docs/Web/HTML/Element/metameta_generator: true# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss## updated_option supports 'mtime', 'date', 'empty'updated_option: 'mtime'# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Include / Exclude file(s)## include:/exclude: options only apply to the 'source/' folderinclude:exclude:ignore:# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: amazing#landscape# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy: type: git repo: https://github.com/MrDemonlxl/MrDemonlxl.github.io.git branch: maincomment: #评论 type: gitalk language: zh-CN #zh-CN #Localization language key, en, zh-CN and zh-TW are currently available. owner: MrDemonlxl # (required) GitHub user name repo: MrDemonlxl.github.io # (required) GitHub repository name client_id: # (required) OAuth application client id client_secret: # (required) OAuth application client secret admin: [''] create_issue_manually: true distraction_free_mode: false has_hot_recommend: true # 是否有热门推荐 has_latest_comment: true #是否有最新评论 # 主题选择 https://hexo.io/themes/ 选择喜欢主题，下载相应主题包放入主题文件夹，进行对应主题说明配置。 # 继续学习完善 https://hexo.io/zh-cn/docs/index.html # 效果预览 https://mrdemonlxl.github.io/","link":"/2021/02/18/Draft/2021/%E4%BA%94%E5%88%86%E9%92%9F%E6%90%AD%E5%BB%BA%E5%9C%A8%E7%BA%BF%E5%8D%9A%E5%AE%A2/"},{"title":"如何学习一个新知识","text":"个人学习流程工具记录，如何将一个学习目标最高效率化 学习==》熟练==》大师==》创造 新学四问 WHY【与前代优化了什么，弥补了什么空白】 WHAT【框架，思维导图，主题框架】 HOW【如何记忆，学习资源】 LEVEL【不是每个都学精】 回顾四问【补充】 WHY【与前代优化了什么，弥补了什么空白】 WHAT【框架，思维导图，主题框架】 HOW【如何记忆，学习资源】 LEVEL【不是每个都学精】 工具 Xmaind MarginNote Typora Anki 印象笔记 专项工具 坚果（笔记在线同步） 百度在线脑图 processon 文件系统 阿里云网盘 坚果云 移动硬盘 流程 确定官方文档 Google 为什么学【与前代优化了什么】，学什么【框架，思维导图】，如何学【重复运用，直接背诵】，学到哪【会用即可】 资源准备 Xmaind，MarginNote，Goodnotes【思维导图，笔记】，百度在线脑图 Typora，印象笔记 导入思维导图 Markdown 笔记细化，方便整理分享 Anki 艾宾浩斯记忆稳固 结构化====》细化====》固化====》更新 代码学习：思考&gt;敲代码&gt;看 方法 分类学习 自问自答 重复记忆 实操运用 最高标准 笔记方法 合理计划 督促验收 实践发现未知 固定笔记模板并不断细化 80%时间在基础，18%在框架，2%在英文，视频更细，博客更快，官方文档为王 技术的进步是一点一滴实操而来的 DEMO塑造，笔记+项目代码 网络方法 费曼学习法","link":"/2021/02/09/Draft/2021/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9F%A5%E8%AF%86/"},{"title":"Interesting Programs","text":"一些有趣的编程项目，回到乐趣本身。 编程绘画，图形，深度学习等CRUD意外有趣的东西。 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】：研究源码、解析思想、业务流程 LEVEL【不是每个都学精】： 进度：上篇 【】 大数据 深度学习 机器学习 快查 引用：","link":"/2021/01/20/Draft/2021/Interesting%20Programs/"},{"title":"Markdown","text":"Markdown作为自媒体人非常方便的一个工具，可以再各大自媒体网站通用，实现一篇多发的效果。其语法简单，几乎可以抛弃鼠标，生成一切样式。 符号 符号(英文输入状态下，开头输入“:”+英文字母有提示) Markdown箭头的输入方法汇总 普通箭头 箭头形状 MarkDown $\\uparrow$ $\\uparrow$ $\\Uparrow$ $\\Uparrow$ $\\downarrow$ $\\downarrow$ $\\Downarrow$ $\\Downarrow$ $\\leftarrow$ $\\leftarrow$ $\\Leftarrow$ $\\Leftarrow$ $\\rightarrow$ $\\rightarrow$ $\\Rightarrow$ $\\Rightarrow$ $\\updownarrow$ $\\updownarrow$ $\\Updownarrow$ $\\Updownarrow$ $\\leftrightarrow$ $\\leftrightarrow$ $\\Leftrightarrow$ $\\Leftrightarrow$ 长箭头 箭头形状 MarkDown $\\longleftarrow$ $\\longleftarrow$ $\\Longleftarrow$ $\\Longleftarrow$ $\\longrightarrow$ $\\longrightarrow$ $\\Longrightarrow$ $\\Longrightarrow$ $\\longleftrightarrow$ $\\longleftrightarrow$ $\\Longleftrightarrow$ $\\Longleftrightarrow$ 其他箭头 箭头形状 MarkDown $\\twoheadrightarrow$ $\\twoheadrightarrow$ $\\rightarrowtail$ $\\rightarrowtail$ $\\looparrowright$ $\\looparrowright$ $\\curvearrowright$ $\\curvearrowright$ $\\circlearrowright$ $\\circlearrowright$ $\\Rsh$ $\\Rsh$ $\\multimap$ $\\multimap$ $\\leftrightsquigarrow$ $\\leftrightsquigarrow$ $\\rightsquigarrow$ $\\rightsquigarrow$ $\\leadsto$ $\\leadsto$ $\\nearrow$ $\\nearrow$ $\\searrow$ $\\searrow$ $\\swarrow$ $\\swarrow$ $\\nwarrow$ $\\nwarrow$ $\\nleftarrow$ $\\nleftarrow$ $\\nLeftarrow$ $\\nLeftarrow$ $\\nrightarrow$ $\\nrightarrow$ $\\nRightarrow$ $\\nRightarrow$ $\\nleftrightarrow$ $\\nleftrightarrow$ $\\nLeftrightarrow$ $\\nLeftrightarrow$ $\\dashrightarrow$ $\\dashrightarrow$ $\\dashleftarrow$ $\\dashleftarrow$ $\\leftleftarrows$ $\\leftleftarrows$ $\\leftrightarrows$ $\\leftrightarrows$ $\\Lleftarrow$ $\\Lleftarrow$ $\\twoheadleftarrow$ $\\twoheadleftarrow$ $\\leftarrowtail$ $\\leftarrowtail$ $\\looparrowleft$ $\\looparrowleft$ $\\curvearrowleft$ $\\curvearrowleft$ 箭头形状 MarkDown $\\circlearrowleft$ $\\circlearrowleft$ $\\Lsh$ $\\Lsh$ $\\mapsto$ $\\mapsto$ $\\hookleftarrow$ $\\hookleftarrow$ $\\hookrightarrow$ $\\hookrightarrow$ $\\upharpoonright$ $\\upharpoonright$ $\\upharpoonleft$ $\\upharpoonleft$ $\\downharpoonright$ $\\downharpoonright$ $\\downharpoonleft$ $\\downharpoonleft$ $\\leftharpoonup$ $\\leftharpoonup$ $\\rightharpoonup$ $\\rightharpoonup$ $\\rightharpoondown$ $\\rightharpoondown$ $\\leftharpoondown$ $\\leftharpoondown$ $\\upuparrows$ $\\upuparrows$ $\\downdownarrows$ $\\downdownarrows$ $\\rightrightarrows$ $\\rightrightarrows$ $\\rightleftarrows$ $\\rightleftarrows$ $\\leftleftarrows$ $\\leftleftarrows$ $\\leftrightarrows$ $\\leftrightarrows$ $\\rightleftharpoons$ $\\rightleftharpoons$ $\\leftrightharpoons$ $\\leftrightharpoons$ 分割线 LaTeX符号 语法：$\\clubsuit$ $\\triangleright$ 格式 1234567891011&amp;emsp;首行缩进~~删除线~~~缩小~`代码` --- 分割线&lt;font color=red&gt;字体颜色&lt;/font&gt;+空格[空格]空格 勾选框[内容文字](#标题) 页内跳转[内容文字](跳转目标文件的相对路径)下划线 html 方法：&lt;span style=&quot;border-bottom:2px dashed yellow;&quot;&gt;文字&lt;/span&gt;下划线 typory 方法（ctrl+u）： &lt;u&gt;awdw&lt;/u&gt; 图 图片（hexo） 图 + 横竖图 ![张芷溪](https://wx1.sinaimg.cn/large/b5d1b710ly1g6bz7n92s7j212w0nr1kx.jpg) + 横图4 ![v4](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191022182226.png) + 竖图5 ![电池](https://cdn.jsdelivr.net/gh/removeif/blog_image/img/2019/20191024145940.jpg) 导图 状态转移图 stateDiagram %% 单程生命周期起点是实心圆，终点是同心圆，内圆为实心。 %%这个例子包含是3个状态Still, Moving 和 Crash. 从Still状态可以转移到Moving，从Moving可以转移到Still 或者 Crash。不能从Still转移到Crash [*] --> Still Still --> [*] Still --> Moving: A transition note right of Moving Moving可以转移到Still或者Crash end note Moving --> Still Moving --> Crash Crash --> [*] 类图 classDiagram class Duck{ -String beakColor - double weight +swim() +quack() #count() +getPrice(count) double +someAbstractMethod() * -someStaticMethod() $ } class Shape{ %% This whole line is a comment classDiagram class Shape noOfVertices draw() } class Color{ RED BLUE GREEN WHITE BLACK } class Relation{ } classK ..> classL : 依赖关系 classA --|> classB : 继承关系（泛化） classM ..|> classN : 实现关系 classG --> classH : 关联关系 classE --o classF : 聚合关系 classC --* classD : 组合关系 Customer \"1\" --> \"*\" Ticket Student \"1\" --> \"1..*\" Course Galaxy --> \"many\" Star : Contains Sky \"1\"--> \"1\" Sun Parent \"1\" -- \"0..2\" Children Person \"1\" -- \"2\" Eyes 饼图 pie title Pie Chart \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 150 \"Cows\" : 150 渲染效果 graph LR id1(Start)-->id2(Stop) style id1 fill:#f9f,stroke:#333,stroke-width:4px; style id2 fill:#f00,stroke:#000,stroke-width:2px,stroke-dasharray:5,5; 基础fontawesome支持 graph TD B[\"fa:fa-twitter for peace\"] B-->C[fa:fa-ban forbidden] B-->D(fa:fa-spinner); B-->E(A fa:fa-camerra-retro perhaps?); 连线 graph TD A1==TEXT===B1 A2-->|text|B2 A3..-B3 节点形状 graph TD B[bname] C(cname) D((dname)) E>ename] F{fname} 甘特图 关键词说明： title—标题 dateFormat—日期格式 section—模块 Completed—已经完成 Active—当前正在进行 Future—后续待处理 crit—关键阶段 gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d future task : des3, after des2, 5d future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and json :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to ,mermaid :1d 流程图 graph TD A[方形] -->B(圆角) B --> C{条件a} C -->|a=1| D[结果1] C -->|a=2| E[结果2] F[竖向流程图] 时序图示例 123456789Title:时序图示例客户端-&gt;服务端: 我想找你拿下数据 SYN服务端--&gt;客户端: 我收到你的请求啦 ACK+SYN客户端-&gt;&gt;服务端: 我收到你的确认啦，我们开始通信吧 ACKNote right of 服务端: 我是一个服务端Note left of 客户端: 我是一个客户端Note over 服务端,客户端: TCP 三次握手participant 观察者 公式 $$ \\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\ \\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\ \\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\ \\end{vmatrix} ${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}} $$ 表格 ctl+回车新增行 左对齐 右对齐 居中对齐 单元格 单元格 单元格 单元格 单元格 单元格 代码 1hello! Hexo可折叠 main.java >folded1可折叠代码块 脚注 文字内容 [1] todo列表 +空格[空格]空格 勾选框+ [ ] [ ] [ ] 读书 《》 专业 （设计模式代码Demo， 网络， JVM， VUE项目， Linux） 音乐 吉他谱扒谱学习 运动 画画 Typory 自定义元素样式（修改CSS） 快捷键 其他 emoji【外可访】，Typora 快捷键WIN+。 🎈🎈 🤲 😎 inserted =&gt; inserted 29th =&gt; 29th H20 =&gt; H2O basic footnote[1:1] here is an inline footnote[^2](inline footnote) and another one[2] and another one[3] footnote content The HTML specification is maintained by the W3C. flowchat st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something…|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options); basic footnote content ↩︎ ↩︎ paragraph ↩︎ footnote content with some markdown ↩︎","link":"/2021/01/20/Draft/2021/Markdown/"},{"title":"Office","text":"Office高级教程记录，边用边记录 WORD PPT EXCEL 相对地址、绝对地址 相对地址在公式中会相变变化，绝对反之，绝对地址在相对地址B1基础上添加$比如$B$1 函数篇 替换 =LEFT(C2835,LEN(C2835)-1)&amp;&quot;路-散盘&quot; 提取 =MID（） 结合 =CONCATENATE(A2,B2) 随机 =CHOOSE(RANDBETWEEN(1,2),&quot;出让&quot;,&quot;划拨&quot;) 随机数 =RANDBETWEEN(5,100) 奇偶 =MOD(ROW(),2) 替换，插入 =REPLACE(A1,2,,&quot;09&quot;) 查找 =FIND(&quot;K&quot;,A2,1) 模糊匹配删除 计算重复数量 Countif（统计范围，统计对象） =INDEX(Sheet0!A:AA,ROW(),MATCH(INDIRECT((CHAR(COLUMN()+ 64)&amp;1)),Sheet0!A1:AA1,0))复制对应列名单元格","link":"/2021/01/20/Draft/2021/Office/"},{"title":"SpringMVC","text":"SpringMVC 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度：【一天 】 综合问题 运行流程？ 常用注解？ 如何解决中文乱码？ 重定向？转发？ springmvc的控制器是单例的吗?是线程安全的吗？ 一、简介 二、流程 DispatcherServlet调用链 重要成员 1234567891011121314151617181920212223242526272829303132333435// 文件上传组件/** MultipartResolver used by this servlet */private MultipartResolver multipartResolver;// 资源定位组件/** LocaleResolver used by this servlet */private LocaleResolver localeResolver;// 主题解析组件/** ThemeResolver used by this servlet */private ThemeResolver themeResolver;// 处理器映射器组件集合/** List of HandlerMappings used by this servlet */private List&lt;HandlerMapping&gt; handlerMappings;// 处理器适配器组件集合/** List of HandlerAdapters used by this servlet */private List&lt;HandlerAdapter&gt; handlerAdapters;// 异常处理解析器集合/** List of HandlerExceptionResolvers used by this servlet */private List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers;// 视图名解析器/** RequestToViewNameTranslator used by this servlet */private RequestToViewNameTranslator viewNameTranslator;// 重定向及FlashMap存储组件/** FlashMapManager used by this servlet */private FlashMapManager flashMapManager;// 视图解析组件集合/** List of ViewResolvers used by this servlet */private List&lt;ViewResolver&gt; viewResolvers; 流程 DispatcherServlet ==&gt; HandlerMapping ==&gt; HandlerExecutionChain(Handler、HandlerInterceptor) ==&gt; HandlerAdapter ==&gt; Handler ==&gt; ModelAndView ==&gt;ViewResolver ==&gt; View 第一步：发起请求到前端控制器/中央控制器(DispatcherServlet) 第二步：前端控制器请求处理器映射器HandlerMapping查找 Handler （可以根据xml配置、注解进行查找） 第三步：处理器映射器HandlerMapping向前端控制器返回Handler，HandlerMapping会把请求映射为HandlerExecutionChain对象（包含一个Handler处理器（页面控制器）对象，多个HandlerInterceptor拦截器对象），通过这种策略模式，很容易添加新的映射策略 第四步：前端控制器调用处理器适配器HandlerAdapter去执行Handler 第五步：处理器适配器HandlerAdapter将会根据适配的结果去执行Handler 第六步：Handler执行完成给适配器返回ModelAndView 第七步：处理器适配器HandlerAdapter向前端控制器返回ModelAndView （ModelAndView是springmvc框架的一个底层对象，包括 Model和view） 第八步：前端控制器请求视图解析器ViewResolver去进行视图解析 （根据逻辑视图名解析成真正的视图(jsp)），通过这种策略很容易更换其他视图技术，只需要更改视图解析器即可 第九步：视图解析器ViewResolver向前端控制器返回View 第十步：前端控制器进行视图渲染 （视图渲染将模型数据(在ModelAndView对象中)填充到request域） 第十一步：前端控制器向用户响应结果 三、拦截器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.jeethink.framework.interceptor;import java.lang.reflect.Method;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.stereotype.Component;import org.springframework.web.method.HandlerMethod;import org.springframework.web.servlet.handler.HandlerInterceptorAdapter;import com.alibaba.fastjson.JSONObject;import com.jeethink.common.utils.ServletUtils;import com.jeethink.framework.interceptor.annotation.RepeatSubmit;import com.jeethink.framework.web.domain.AjaxResult;/** * 防止重复提交拦截器 * @author 官方网址 */@Componentpublic abstract class RepeatSubmitInterceptor extends HandlerInterceptorAdapter{ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { if (handler instanceof HandlerMethod) { HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); RepeatSubmit annotation = method.getAnnotation(RepeatSubmit.class); if (annotation != null) { if (this.isRepeatSubmit(request)) { AjaxResult ajaxResult = AjaxResult.error(&quot;不允许重复提交，请稍后再试&quot;); ServletUtils.renderString(response, JSONObject.toJSONString(ajaxResult)); return false; } } return true; } else { return super.preHandle(request, response, handler); } } /** * 验证是否重复提交由子类实现具体的防重复提交的规则 * * @param httpServletRequest * @return * @throws Exception */ public abstract boolean isRepeatSubmit(HttpServletRequest request);} 四、重定向转发 （1）转发：在返回值前面加&quot;forward:&quot;，譬如&quot;forward:user.do?name=method4&quot; （2）重定向：在返回值前面加&quot;redirect:&quot;，譬如&quot;redirect:http://www.baidu.com&quot; 五、常用注解 引用：","link":"/2021/01/20/Draft/2021/SpringMVC/"},{"title":"模板","text":"介绍 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源（官方文档、视频资源、项目地址）】： LEVEL【不是每个都学精】： 进度： 【】 快查 一、简介 知识背景 章节总结 章节问题 。。。？ 二、 引用：","link":"/2021/01/20/Draft/2021/template/"},{"title":"工具篇","text":"工具篇 工具篇 工欲善其事，必先利其器 编程 IDEA 插件 1234567891011121314Alibaba Java Coding GuidelinesCamelCaseEasyCodeIDE Eval ResetKey Promoter XLeetCode EditorLive Coding in PythonMaven HelperMyBatisXPylintPythonRainbow BracketsTabnine Al Code CompletionTranslation GIT http://gitimmersion.com/index.html http://try.github.io/ 在线实操学习网站 工作流程 工作区、暂存区、版本库 **工作区：**就是你在电脑里能看到的目录。 **暂存区：**英文叫 stage 或 index。一般存放在 .git 目录下的 index 文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。 **版本库：**工作区有一个隐藏目录 .git，这个不算工作区，而是 Git 的版本库。 .git文件夹目录结构 ├─hooks ├─info ├─objects │ ├─info │ └─pack └─refs ├─heads └─tags GIT命令【同linux】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114本地创建仓库#初始化仓库，项目下会生成.git文件夹，后面加目录名时创建指定目录为仓库，无则当前目录为仓库git init newreponame#将文件纳入版本控制，将文件添加到暂存区， 文件名.文件后缀 可用*通配符 git add .添加所有git add *.*#初始化项目版本，将暂存区内容添加到仓库git commit -m 'commit信息，linux使用单引号，windows使用双引号'#克隆仓库到指定目录git clone &lt;repo&gt; &lt;directory&gt;配置管理#查看所有配置git config --list#编辑当前仓库配置 --global针对所有仓库git config -e #设置当前用户配置信息 --global针对所有仓库git config --global user.name &quot;runoob&quot;git config --global user.email test@runoob.com基本命令#提交与修改#创建与提交快照git add #添加文件到暂存区git status #查看仓库当前的状态，显示有变更的文件。git diff #比较文件的不同，即暂存区和工作区的差异。git commit #提交暂存区到本地仓库。git reset #回退版本。git rm #删除工作区文件。git mv #移动或重命名工作区文件。#提交日志git log #查看历史提交记录git blame &lt;file&gt; #以列表形式查看指定文件的历史修改记录#远程操作git remote #远程仓库操作git fetch #从远程获取代码库git pull #下载远程代码并合并git push #上传远程代码并合并分支管理#显示所有分支/+分支名创建分支 命令：git branch (branchname)#切换分支命令:git checkout (branchname) git checkout -b (branchname) #创建新分支并立即切换到该分支下#当你切换分支的时候，Git 会用该分支的最后提交的快照替换你的工作目录的内容， 所以多个分支不需要多个目录。#合并分支git merge #合并冲突处理后add告诉git冲突已解决git add 文件 #删除分支git branch -d (branchname)历史记录git log #查看历史提交记录。 --oneline 查看简洁版本 --graph 分支历史 --reverse逆向展示 --author指定用户 --since 、 --before， --until、 --after 指定日期 http://git-scm.com/docs/git-loggit blame &lt;file&gt; #以列表形式查看指定文件的历史修改记录。标签git tag -a v1.0 #创建标签git log --oneline --decorate --graph / git tag#查看标签git tag -a v0.9 85fc7e7#发布后追加标签git tag -a &lt;tagname&gt; -m &quot;runoob.com标签&quot;#指定标签信息命令：git tag -s &lt;tagname&gt; -m &quot;runoob.com标签&quot;PGP#签名标签命令：远程仓库git remote add [shortname] [url]#添加一个新的远程仓库ssh-keygen -t rsa -C &quot;youremail@example.com&quot;#生成 SSH Key，~/ 下生成 .ssh 文件夹，进去，打开 id_rsa.pub，复制里面的 key #github中Account =&gt; Settings SSH and GPGkeys ssh -T git@github.com#验证是否成功git remote/git remote -v #查看配置的所有远程库/远程库实际地址git fetch#从远程仓库下载新分支与数据，该命令执行完后需要执行 git merge 远程分支到你所在的分支。git remote rm [别名]#删除远程仓库服务器搭建centos#1、安装Gityum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-develyum install git#创建一个git用户组和用户，用来运行git服务：groupadd gituseradd git -g git#2、创建证书登录#收集所有需要登录的用户的公钥，公钥位于id_rsa.pub文件中，把我们的公钥导入到/home/git/.ssh/authorized_keys文件里，一行一个。#如果没有该文件创建它：cd /home/git/mkdir .sshchmod 755 .sshtouch .ssh/authorized_keyschmod 644 .ssh/authorized_keys#3、初始化Git仓库#首先我们选定一个目录作为Git仓库，假定是/home/gitrepo/runoob.git，在/home/gitrepo目录下输入命令：cd /homemkdir gitrepochown git:git gitrepo/cd gitrepogit init --bare runoob.gitInitialized empty Git repository in /home/gitrepo/runoob.git/#以上命令Git创建一个空仓库，服务器上的Git仓库通常都以.git结尾。然后，把仓库所属用户改为git：chown -R git:git runoob.git#4、克隆仓库git clone git@192.168.45.4:/home/gitrepo/runoob.gitCloning into 'runoob'...warning: You appear to have cloned an empty repository.Checking connectivity... done.#192.168.45.4 为 Git 所在服务器 ip ，你需要将其修改为你自己的 Git 服务 ip。#这样我们的 Git 服务器安装就完成。其他命令#同linuxtouch 文件名#创建文件mkdir 路径#创建文件夹echo 'runoob.com' &gt; test.txt#创建内容文'runoob.com'的test.txt文件 ​ git config --global --list 显示 git 配置信息 ​ git config --global user.name “yourname”，git config --global user.email “email@email.com ”设置全局用户名和邮箱 ​ ssh-keygen -t rsa -C “邮箱” ​ clip &lt; ~/.ssh/id_rsa.pub 复制密匙，在github设置中新建SSH key粘贴即可 VSCode 插件 123456789101112Auto Close TagAuto Rename TagChinese(Simplified（简体中文）Langua..CSS NavigationESLintGitLens-Git superchargedHTML CSS SupportJavaScript (ES6)code snippeLive Serveropen in browserVeturvscode-icons 电脑应用 Synergy键鼠共享工具 坚果云，文件同步工具 Quicker PicGO Snipaste ScreenToGif 学习 流程图 在线流程图 ProcessOn 离线流程图 XMaind 网站推荐 艺术 记录 时光序【备忘，提醒，TODO】 插件 Edge Vimium Navigating the page j, Scroll down (scrollDown) k, Scroll up (scrollUp) gg Scroll to the top of the page (scrollToTop) G Scroll to the bottom of the page (scrollToBottom) d Scroll a half page down (scrollPageDown) u Scroll a half page up (scrollPageUp) Scroll a full page down (scrollFullPageDown) Scroll a full page up (scrollFullPageUp) h Scroll left (scrollLeft) l Scroll right (scrollRight) r Reload the page (reload) yy Copy the current URL to the clipboard (copyCurrentUrl) p Open the clipboard's URL in the current tab (openCopiedUrlInCurrentTab) P Open the clipboard's URL in a new tab (openCopiedUrlInNewTab) i Enter insert mode (enterInsertMode) v Enter visual mode (enterVisualMode) gi Focus the first text input on the page (focusInput) f Open a link in the current tab (LinkHints.activateMode) F Open a link in a new tab (LinkHints.activateModeToOpenInNewTab) Open a link in a new tab &amp; switch to it (LinkHints.activateModeToOpenInNewForegroundTab) gf Select the next frame on the page (nextFrame) gF Select the page's main/top frame (mainFrame) Using the vomnibar o Open URL, bookmark or history entry (Vomnibar.activate) O Open URL, bookmark or history entry in a new tab (Vomnibar.activateInNewTab) b Open a bookmark (Vomnibar.activateBookmarks) B Open a bookmark in a new tab (Vomnibar.activateBookmarksInNewTab) T Search through your open tabs (Vomnibar.activateTabSelection) Using find / Enter find mode (enterFindMode) n Cycle forward to the next find match (performFind) N Cycle backward to the previous find match (performBackwardsFind) Navigating history H Go back in history (goBack) L Go forward in history (goForward) Manipulating tabs t Create new tab (createTab) J, gT Go one tab left (previousTab) K, gt Go one tab right (nextTab) ^ Go to previously-visited tab (visitPreviousTab) g0 Go to the first tab (firstTab) g$ Go to the last tab (lastTab) yt Duplicate current tab (duplicateTab) Pin or unpin current tab (togglePinTab) Mute or unmute current tab (toggleMuteTab) x Close current tab (removeTab) X Restore closed tab (restoreTab) Miscellaneous ? Show help (showHelp)","link":"/2021/01/20/Draft/2021/%E5%B7%A5%E5%85%B7%E7%AF%87/"},{"title":"摄影摄像","text":"摄影摄像，记录技术，分析大佬、自己作品以督促进步。 一、摄影 构图 工具 修图 创意 二、摄像 剪辑 工具 三、省美 + 横竖图 ![]() + 横图4 ![]() + 竖图5 ![]() 引用：","link":"/2021/01/20/Draft/2021/%E6%91%84%E5%BD%B1%E6%91%84%E5%83%8F/"},{"title":"架构师","text":"系统设计、管理、业务经验，架构知识学习记录。开发之外【实施，测试，维护，部署，人际沟通】 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源（官方文档、视频资源）】： LEVEL【不是每个都学精】： 一、设计经验 系统设计 系统设计工具 Visual Paradigm 数据库设计 Navicat 二、管理经验 三、业务经验 四、开发之外 【实施，测试，维护，部署，人际沟通】 代码规范","link":"/2021/01/20/Draft/2021/%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"title":"MongoDB","text":"最像关系型数据库的非关系型数据库 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】：菜鸟、官方文档 LEVEL【不是每个都学精】： 进度：【一天】 快查 命令 解释 备注 show dbs 查看所有数据库 db 显示当前数据库对象或集合 use 连接到一个指定的数据库 db.createCollection(name, options) 创建集合 show collections 或 show tables 查看集合 关系图 一、简介 C++语言编写的，是一个基于分布式文件存储的开源数据库系统。 在高负载的情况下，添加更多的节点，可以保证服务器性能。 MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。 MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 环境：win10 二、基础概念 SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接,MongoDB不支持 primary key primary key 主键,MongoDB自动将_id字段设置为主键 默认数据库为db文件夹需要自己创建，数据放在data文件夹中 文档(Document) ​ 文档是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。一个简单的文档例子如下： 1{&quot;site&quot;:&quot;www.runoob.com&quot;, &quot;name&quot;:&quot;菜鸟教程&quot;} 下表列出了 RDBMS 与 MongoDB 对应的术语： RDBMS MongoDB 数据库 数据库 表格 集合 行 文档 列 字段 表联合 嵌入文档 主键 主键 (MongoDB 提供了 key 为 _id ) 数据库服务和客户端 Mysqld/Oracle mongod mysql/sqlplus mongo 需要注意的是： 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 文档键命名规范： 键不能含有\\0 (空字符)。这个字符用来表示键的结尾。 .和$有特别的意义，只有在特定环境下才能使用。 以下划线&quot;_&quot;开头的键是保留的(不是严格要求的)。 集合 / 表 集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。 集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 比如，我们可以将以下不同数据结构的文档插入到集合中： 123{&quot;site&quot;:&quot;www.baidu.com&quot;}{&quot;site&quot;:&quot;www.google.com&quot;,&quot;name&quot;:&quot;Google&quot;}{&quot;site&quot;:&quot;www.runoob.com&quot;,&quot;name&quot;:&quot;菜鸟教程&quot;,&quot;num&quot;:5} 当第一个文档插入时，集合就会被创建。 合法的集合名 集合名不能是空字符串&quot;&quot;。 集合名不能含有\\0字符（空字符)，这个字符表示集合名的结尾。 集合名不能以&quot;system.&quot;开头，这是为系统集合保留的前缀。 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。 如下实例： 1db.col.findOne() capped collections Capped collections 就是固定大小的collection。 它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 &quot;RRD&quot; 概念类似。 Capped collections 是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能，和标准的 collection 不同，你必须要显式的创建一个capped collection，指定一个 collection 的大小，单位是字节。collection 的数据存储空间值提前分配的。 Capped collections 可以按照文档的插入顺序保存到集合中，而且这些文档在磁盘上存放位置也是按照插入顺序来保存的，所以当我们更新Capped collections 中文档的时候，更新后的文档不可以超过之前文档的大小，这样话就可以确保所有文档在磁盘上的位置一直保持不变。 由于 Capped collection 是按照文档的插入顺序而不是使用索引确定插入位置，这样的话可以提高增添数据的效率。MongoDB 的操作日志文件 oplog.rs 就是利用 Capped Collection 来实现的。 要注意的是指定的存储大小包含了数据库的头信息。 1db.createCollection(&quot;mycoll&quot;, {capped:true, size:100000}) 在 capped collection 中，你能添加新的对象。 能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。 使用 Capped Collection 不能删除一个文档，可以使用 drop() 方法删除 collection 所有的行。 删除之后，你必须显式的重新创建这个 collection。 在32bit机器中，capped collection 最大存储为 1e9( 1X109)个字节。 元数据 数据库的信息是存储在集合中。它们使用了系统的命名空间： 1dbname.system.* 在MongoDB数据库中名字空间 .system.* 是包含多种系统信息的特殊集合(Collection)，如下: 集合命名空间 描述 dbname.system.namespaces 列出所有名字空间。 dbname.system.indexes 列出所有索引。 dbname.system.profile 包含数据库概要(profile)信息。 dbname.system.users 列出所有可访问数据库的用户。 dbname.local.sources 包含复制对端（slave）的服务器信息和状态。 对于修改系统集合中的对象有如下限制。 在插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的drop index命令将自动更新相关信息)。 是可修改的。 是可删除的。 MongoDB 数据类型 下表为MongoDB中常用的几种数据类型。 数据类型 描述 String 字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 Integer 整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 Boolean 布尔值。用于存储布尔值（真/假）。 Double 双精度浮点值。用于存储浮点值。 Min/Max keys 将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。 Array 用于将数组或列表或多个值存储为一个键。 Timestamp 时间戳。记录文档修改或添加的具体时间。 Object 用于内嵌文档。 Null 用于创建空值。 Symbol 符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 Date 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 Object ID 对象 ID。用于创建文档的 ID。 Binary Data 二进制数据。用于存储二进制数据。 Code 代码类型。用于在文档中存储 JavaScript 代码。 Regular expression 正则表达式类型。用于存储正则表达式。 下面说明下几种重要的数据类型。 ObjectId ObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是： 前 4 个字节表示创建 unix 时间戳,格林尼治时间 UTC 时间，比北京时间晚了 8 个小时 接下来的 3 个字节是机器标识码 紧接的两个字节由进程 id 组成 PID 最后三个字节是随机数 MongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象 由于 ObjectId 中保存了创建的时间戳，所以你不需要为你的文档保存时间戳字段，你可以通过 getTimestamp 函数来获取文档的创建时间: 123&gt; var newObject = ObjectId()&gt; newObject.getTimestamp()ISODate(&quot;2017-11-25T07:21:10Z&quot;) ObjectId 转为字符串 12&gt; newObject.str5a1919e63df83ce79df8b38f 字符串 BSON 字符串都是 UTF-8 编码。 时间戳 BSON 有一个特殊的时间戳类型用于 MongoDB 内部使用，与普通的 日期 类型不相关。 时间戳值是一个 64 位的值。其中： 前32位是一个 time_t 值（与Unix新纪元相差的秒数） 后32位是在某秒中操作的一个递增的序数 在单个 mongod 实例中，时间戳值通常是唯一的。 在复制集中， oplog 有一个 ts 字段。这个字段中的值使用BSON时间戳表示了操作时间。 BSON 时间戳类型主要用于 MongoDB 内部使用。在大多数情况下的应用开发中，你可以使用 BSON 日期类型。 日期 表示当前距离 Unix新纪元（1970年1月1日）的毫秒数。日期类型是有符号的, 负数表示 1970 年之前的日期。 12345678910&gt; var mydate1 = new Date() //格林尼治时间&gt; mydate1ISODate(&quot;2018-03-04T14:58:51.233Z&quot;)&gt; typeof mydate1object&gt; var mydate2 = ISODate() //格林尼治时间&gt; mydate2ISODate(&quot;2018-03-04T15:00:45.479Z&quot;)&gt; typeof mydate2object 这样创建的时间是日期类型，可以使用 JS 中的 Date 类型的方法。 返回一个时间类型的字符串： 12345&gt; var mydate1str = mydate1.toString()&gt; mydate1strSun Mar 04 2018 14:58:51 GMT+0000 (UTC) &gt; typeof mydate1strstring 或者 12&gt; Date()Sun Mar 04 2018 15:02:59 GMT+0000 (UTC) 三、安装（win） 下载 安装 创建默认数据文件夹 \\data\\db 与 \\data\\log \\MongoDB.log文件 手动运行MongoDB服务器 12345678#安装目录\\bin\\mongod --dbpath 自己定义的数据文件夹（c:\\data\\db）F:\\Softs\\Installed\\MongoDB\\bin\\mongod --dbpath F:\\Softs\\Installed\\MongoDB\\data\\db#成功结果：{&quot;t&quot;:{&quot;$date&quot;:&quot;2022-01-18T14:50:04.760+08:00&quot;},&quot;s&quot;:&quot;I&quot;, &quot;c&quot;:&quot;CONTROL&quot;, &quot;id&quot;:23285, &quot;ctx&quot;:&quot;-&quot;,&quot;msg&quot;:&quot;Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'&quot;}{&quot;t&quot;:{&quot;$date&quot;:&quot;2022-01-18T14:50:04.761+08:00&quot;},&quot;s&quot;:&quot;I&quot;, &quot;c&quot;:&quot;NETWORK&quot;, &quot;id&quot;:4915701, &quot;ctx&quot;:&quot;-&quot;,&quot;msg&quot;:&quot;Initialized wire specification&quot;,&quot;attr&quot;:{&quot;spec&quot;:{&quot;incomingExternalClient&quot;:{&quot;minWireVersion&quot;:0,&quot;maxWireVersion&quot;:13},&quot;incomingInternalClient&quot;:{&quot;minWireVersion&quot;:0,&quot;maxWireVersion&quot;:13},&quot;outgoing&quot;:{&quot;minWireVersion&quot;:0,&quot;maxWireVersion&quot;:13},&quot;isInternalClient&quot;:true}}}。。。 四、连接 12345678910111213141516171819202122232425#一#将其配置成服务后启动服务mongod --dbpath &quot;F:\\Softs\\Installed\\MongoDB\\data\\db&quot; --logpath &quot;F:\\Softs\\Installed\\MongoDB\\data\\log\\MongoDB.log&quot; --install --serviceName &quot;MongoDB&quot;PS F:\\Softs\\Installed\\MongoDB\\bin&gt; net start mongodb请求的服务已经启动。#二#配置bin目录环境变量后，用mongo连接C:\\Users\\Administrator&gt;mongoMongoDB shell version v5.0.6-rc0connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session { &quot;id&quot; : UUID(&quot;4f5de15b-0c12-4dde-8cc4-325ae6b23114&quot;) }MongoDB server version: 5.0.6-rc0#关闭#不为系统服务时关闭mongo命令窗口#系统服务NET stop MongoDB (关闭服务)#删除进程mongod --dbpath &quot;d:\\mongodb\\data\\db&quot; --logpath &quot;d:\\mongodb\\data\\log\\MongoDB.log&quot; --remove --serviceName &quot;MongoDB&quot; 五、数据库 1234567891011121314151617181920212223242526272829303132#创建数据库，没有创建，有则切换，#数据命名规则：#不能是空字符串（&quot;&quot;)。#不得含有' '（空格)、.、$、/、\\和\\0 (空字符)。#应全部小写。#最多64字节。#有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。#admin： 从权限的角度来看，这是&quot;root&quot;数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。#local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合#config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。&gt; use userswitched to db user#查看当前数据库名&gt; dbuser#插入数据&gt; db.user.insert({&quot;name&quot;:&quot;lxl&quot;})WriteResult({ &quot;nInserted&quot; : 1 })#查看数据库列表，有数据的数据库才会显示&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GBuser 0.000GB#删除数据库&gt; use userswitched to db user&gt; db.dropDatabase(){ &quot;ok&quot; : 1 } 六、集合 1234567891011121314151617181920#查看所有数据库&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB&gt; use studentswitched to db student#创建集合数据&gt; db.createCollection(&quot;students&quot;){ &quot;ok&quot; : 1 }#查看集合show collections 或 show tables&gt; show tablesstudents&gt; db.student.drop()false#删除集合数据&gt; db.students.drop()true&gt; show tables&gt; options 参数： 字段 类型 描述 capped 布尔 （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。 当该值为 true 时，必须指定 size 参数。 autoIndexId 布尔 3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。 size 数值 （可选）为固定集合指定一个最大值，即字节数。 如果 capped 为 true，也需要指定该字段。 max 数值 （可选）指定固定集合中包含文档的最大数量。 七、文档 增 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&gt; show tables&gt; dbstudent&gt; show collections&gt; dbstudent#插入文档 insert（）&gt; db.student.insert({name:&quot;lxl&quot;})WriteResult({ &quot;nInserted&quot; : 1 })#查看文档 find()&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }# 将文档声明成变量再插入&gt; document=({age:18}){ &quot;age&quot; : 18 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 18 }&gt; document=({&quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;),age:19}){ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }# 插入文档 save（），有_id时重复则修改&gt; db.student.save(document)WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; document=({&quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;),age:20}){ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 20 }#插入文档 insetOne（）id重复报错 db.collection.insertMany()一次插入多个文档&gt; db.student.insertOne(document)WriteError({ &quot;index&quot; : 0, &quot;code&quot; : 11000, &quot;errmsg&quot; : &quot;E11000 duplicate key error collection: student.student index: _id_ dup key: { _id: ObjectId('61e7ae456e48f4bf534731d7') }&quot;, &quot;op&quot; : { &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 20 }}) :WriteError({ &quot;index&quot; : 0, &quot;code&quot; : 11000, &quot;errmsg&quot; : &quot;E11000 duplicate key error collection: student.student index: _id_ dup key: { _id: ObjectId('61e7ae456e48f4bf534731d7') }&quot;, &quot;op&quot; : { &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 20 }})WriteError@src/mongo/shell/bulk_api.js:465:48mergeBatchResults@src/mongo/shell/bulk_api.js:871:49executeBatch@src/mongo/shell/bulk_api.js:940:13Bulk/this.execute@src/mongo/shell/bulk_api.js:1182:21DBCollection.prototype.insertOne@src/mongo/shell/crud_api.js:264:9@(shell):1:1#插入文档replaceOne（）&gt; db.student.replaceOne(document)uncaught exception: TypeError: can't convert undefined to object :DBCollection.prototype.replaceOne@src/mongo/shell/crud_api.js:477:16@(shell):1:1&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; 改 模板 123456789db.collection.update( &lt;query&gt;, &lt;update&gt;, { upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; }) 参数说明： query : update的查询条件，类似sql update查询内where后面的。 update : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的 upsert : 可选，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。 multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 writeConcern :可选，抛出异常的级别。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }#更新第一条文档&gt; db.student.update({&quot;name&quot;:&quot;lxl&quot;},{$set:{&quot;name&quot;:&quot;lll&quot;}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; document=({age:21}){ &quot;age&quot; : 21 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 21 }&gt; document=({age:21}){ &quot;age&quot; : 21 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 21 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 21 }#更新类型必须相同&gt; db.student.update({&quot;age&quot;:&quot;21&quot;},{$set:{&quot;age&quot;:&quot;22&quot;}})WriteResult({ &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 0 })#多条文档相同只更新第一条&gt; db.student.update({&quot;age&quot;:21},{$set:{&quot;age&quot;:22}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 22 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 21 }&gt; db.student.update({&quot;age&quot;:22},{$set:{&quot;age&quot;:21}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 21 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 21 }#,{multi:true}将全部符合条件的文档都更新&gt; db.student.update({&quot;age&quot;:21},{$set:{&quot;age&quot;:22}},{multi:true})WriteResult({ &quot;nMatched&quot; : 2, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 2 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 22 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 22 }&gt; 删 1234567db.collection.remove( &lt;query&gt;, { justOne: &lt;boolean&gt;, writeConcern: &lt;document&gt; }) 参数说明： query :（可选）删除的文档的条件。 justOne : （可选）如果设为 true 或 1，则只删除一个文档，如果不设置该参数，或使用默认值 false，则删除所有匹配条件的文档。 writeConcern :（可选）抛出异常的级别 12345678910111213141516171819202122232425262728293031323334353637&gt; db.student.find({&quot;age&quot;:22}){ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 22 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 22 }#删除所有复合条件的文档&gt; db.student.remove({&quot;age&quot;:22})WriteResult({ &quot;nRemoved&quot; : 2 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b3d76e48f4bf534731da&quot;), &quot;age&quot; : 21 }{ &quot;_id&quot; : ObjectId(&quot;61e7b3d96e48f4bf534731db&quot;), &quot;age&quot; : 21 }#设置justOne为true或1，删除符合条件的第一条文档&gt; db.student.remove({&quot;age&quot;:22},1)WriteResult({ &quot;nRemoved&quot; : 0 })&gt; db.student.remove({&quot;age&quot;:21},1)WriteResult({ &quot;nRemoved&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b3d96e48f4bf534731db&quot;), &quot;age&quot; : 21 }#删除所有文档&gt; db.student.remove()uncaught exception: Error: remove needs a query :DBCollection.prototype._parseRemove@src/mongo/shell/collection.js:364:15DBCollection.prototype.remove@src/mongo/shell/collection.js:394:18@(shell):1:1&gt; db.student.remove({})WriteResult({ &quot;nRemoved&quot; : 3 })&gt; db.student.find()&gt; 查 1db.collection.find(query, projection) query ：可选，使用查询操作符指定查询条件 projection ：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值， 只需省略该参数即可（默认省略）。 如果你需要以易读的方式来读取数据，可以使用 pretty() 方法，语法格式如下： 1&gt;db.col.find().pretty() pretty() 方法以格式化的方式来显示所有文档。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&gt; db.student.remove({})WriteResult({ &quot;nRemoved&quot; : 2 })&gt; db.student.insert({&quot;age&quot;:22,name:&quot;a1&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.insert({&quot;age&quot;:11,name:&quot;a2&quot;}) WriteResult({ &quot;nInserted&quot; : 1 })#查询第一个&gt; db.student.findOne(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }&gt; db.student.find().pretty(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#查询全部&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 小于$lt&gt; db.student.find({age:{$lt:20}}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#查询小于$lt的指定字段&gt; db.student.find({age:{$lt:20}}，{&quot;_id&quot;:1}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;)}#查询小于$lt的指定外字段&gt; db.student.find({age:{$lt:20}}，{&quot;_id&quot;:0}){ &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 小于或等于$lt&gt; db.student.find({age:{$lte:22}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 大于$lt&gt; db.student.find({age:{$gt:20}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }#条件查询 大于或等于$lt&gt; db.student.find({age:{$gte:11}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 不等于$lt&gt; db.student.find({age:{$ne:11}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }&gt; db.student.insert({&quot;age&quot;:11,name:&quot;a3&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }#条件查询 and $ne&gt; db.student.find({age:{$ne:22},name:&quot;a2&quot;}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; db.student.insert({&quot;age&quot;:33,name:&quot;a3&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }#条件查询 or $or 缺少冒号&gt; db.student.find({$or[age:{$ne:11},name:&quot;a2&quot;]})uncaught exception: SyntaxError: missing : after property id :@(shell):1:20#条件查询 or $or 缺少括号&gt; db.student.find({$or:[age:{$ne:11},name:&quot;a2&quot;]})uncaught exception: SyntaxError: missing ] after element list :@(shell):1:25#条件查询 or $or&gt; db.student.find({$or:[{age:{$ne:11}},{name:&quot;a2&quot;}]}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }#条件查询 or + and&gt; db.student.find({name:&quot;a3&quot;,$or:[{age:{$ne:11}},{name:&quot;a2&quot;}]}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }&gt; $type 操作符 类型 数字 备注 Double 1 String 2 Object 3 Array 4 Binary data 5 Undefined 6 已废弃。 Object id 7 Boolean 8 Date 9 Null 10 Regular Expression 11 JavaScript 13 Symbol 14 JavaScript (with scope) 15 32-bit integer 16 Timestamp 17 64-bit integer 18 Min key 255 Query with -1. Max key 127 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&gt; db.student.insert({&quot;age&quot;:33,name:1})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find({&quot;name&quot;:{$type:'32-bit integer'}})Error: error: { &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;Unknown type name alias: 32-bit integer&quot;, &quot;code&quot; : 2, &quot;codeName&quot; : &quot;BadValue&quot;}&gt; db.student.find({&quot;name&quot;:{$type:'integer'}})Error: error: { &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;Unknown type name alias: integer&quot;, &quot;code&quot; : 2, &quot;codeName&quot; : &quot;BadValue&quot;}&gt; db.student.find({&quot;name&quot;:{$type:16}})&gt; db.student.find({&quot;name&quot;:{$type:2}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }&gt; db.student.find({&quot;name&quot;:{$type:18}})#类型数字默认为double 即1 类型&gt; db.student.find({&quot;name&quot;:{$type:1}}){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find({&quot;name&quot;:{$type:'Double'}})Error: error: { &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;Unknown type name alias: Double&quot;, &quot;code&quot; : 2, &quot;codeName&quot; : &quot;BadValue&quot;}&gt; db.student.find({&quot;name&quot;:{$type:Double}})uncaught exception: ReferenceError: Double is not defined :@(shell):1:26&gt; db.student.find({&quot;name&quot;:{$type:double}})uncaught exception: ReferenceError: double is not defined :@(shell):1:26#类型小写引号&gt; db.student.find({&quot;name&quot;:{$type:'double'}}){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; Limit() 方法 12345678910111213141516&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; db.student.find({name:{$type:1}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#条件筛选后限制查询数量&gt; db.student.find({name:{$type:2}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; Skip() 方法 12345678910111213141516171819&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; db.student.find({name:{$type:1}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find({name:{$type:2}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#跳过对应数量的限制查询的数据&gt; db.student.find({name:{$type:2}}).limit(2).skip(1){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }&gt; sort() 方法 12345678910111213&gt; db.student.find().sort({age:1}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().sort({age:-1}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }&gt; 八、索引 createIndex() 方法` 1234567891011121314151617181920212223242526272829303132333435&gt; db.student.find().sort({age:1}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().sort({age:-1}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }&gt; db.student.createIndex({&quot;age&quot;:1}){ &quot;numIndexesBefore&quot; : 1, &quot;numIndexesAfter&quot; : 2, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}&gt; db.student.createIndex({&quot;age&quot;:-1}){ &quot;numIndexesBefore&quot; : 2, &quot;numIndexesAfter&quot; : 3, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}#创建索引 1为正序，可使用多个字段创建索引&gt; db.student.createIndex({&quot;age&quot;:-1,&quot;name&quot;:1}){ &quot;numIndexesBefore&quot; : 3, &quot;numIndexesAfter&quot; : 4, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}&gt; Parameter Type Description background Boolean 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 &quot;background&quot; 可选参数。 &quot;background&quot; 默认值为false。 unique Boolean 建立的索引是否唯一。指定为true创建唯一索引。默认值为false. name string 索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。 dropDups Boolean **3.0+版本已废弃。**在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false. sparse Boolean 对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false. expireAfterSeconds integer 指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。 v index version 索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。 weights document 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。 default_language string 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语 language_override string 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language. 九、聚合 aggregate() 方法 处理数据(诸如统计平均值，求和等) 12345678910111213&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#通过name判断相同name有多少数据&gt; db.student.aggregate([{$group:{_id:&quot;$name&quot;,num_tutorial:{$sum:1}}}]){ &quot;_id&quot; : &quot;a2&quot;, &quot;num_tutorial&quot; : 1 }{ &quot;_id&quot; : &quot;a3&quot;, &quot;num_tutorial&quot; : 2 }{ &quot;_id&quot; : &quot;a1&quot;, &quot;num_tutorial&quot; : 1 }{ &quot;_id&quot; : 1, &quot;num_tutorial&quot; : 1 }&gt; 聚合的表达式: 表达式 描述 实例 $sum 计算总和。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$sum : &quot;$likes&quot;}}}]) $avg 计算平均值 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$avg : &quot;$likes&quot;}}}]) $min 获取集合中所有文档对应值得最小值。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$min : &quot;$likes&quot;}}}]) $max 获取集合中所有文档对应值得最大值。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$max : &quot;$likes&quot;}}}]) $push 将值加入一个数组中，不会判断是否有重复的值。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, url : {$push: &quot;$url&quot;}}}]) $addToSet 将值加入一个数组中，会判断是否有重复的值，若相同的值在数组中已经存在了，则不加入。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, url : {$addToSet : &quot;$url&quot;}}}]) $first 根据资源文档的排序获取第一个文档数据。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, first_url : {$first : &quot;$url&quot;}}}]) $last 根据资源文档的排序获取最后一个文档数据 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, last_url : {$last : &quot;$url&quot;}}}]) 管道 管道在Unix和Linux中一般用于将当前命令的输出结果作为下一个命令的参数。 MongoDB的聚合管道将MongoDB文档在一个管道处理完毕后将结果传递给下一个管道处理。管道操作是可以重复的。 表达式：处理输入文档并输出。表达式是无状态的，只能用于计算当前聚合管道的文档，不能处理其它的文档。 这里我们介绍一下聚合框架中常用的几个操作： $project：修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。 $match：用于过滤数据，只输出符合条件的文档。$match使用MongoDB的标准查询操作。 $limit：用来限制MongoDB聚合管道返回的文档数。 $skip：在聚合管道中跳过指定数量的文档，并返回余下的文档。 $unwind：将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。 $group：将集合中的文档分组，可用于统计结果。 $sort：将输入文档排序后输出。 $geoNear：输出接近某一地理位置的有序文档。 12345678910111213141516171819202122232425262728&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#只显示默认的id与name&gt; db.student.aggregate({$project:{name:1}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;name&quot; : 1 }#去掉默认id，只显示name&gt; db.student.aggregate({$project:{_id:0,name:1}}){ &quot;name&quot; : &quot;a1&quot; }{ &quot;name&quot; : &quot;a2&quot; }{ &quot;name&quot; : &quot;a3&quot; }{ &quot;name&quot; : &quot;a3&quot; }{ &quot;name&quot; : 1 }#显示前三条之后的数据&gt; db.student.aggregate({$skip:3}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#显示大于10 小于等于25的一条数据&gt; db.student.aggregate( [{ $match : { age : { $gt:10,$lte:25}}},{ $group:{ _id: null, count: { $sum: 1 } } } ] );{ &quot;_id&quot; : null, &quot;count&quot; : 3 }&gt; 十、复制 数据冗余备份，硬件故障，服务中断数据恢复。无需停机维护，分布式读取数据。 mongodb的复制至少需要两个节点。其中一个是主节点，负责处理客户端请求，其余的都是从节点，负责复制主节点上的数据。 mongodb各个节点常见的搭配方式为：一主一从、一主多从。 主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 MongoDB复制结构图如下所示： 以上结构图中，客户端从主节点读取数据，在客户端写入数据到主节点时， 主节点与从节点进行数据交互保障数据的一致性。 副本集特征： N 个节点的集群 任何节点可作为主节点 所有写入操作都在主节点上 自动故障转移 自动恢复 MongoDB副本集设置 在本教程中我们使用同一个MongoDB来做MongoDB主从的实验， 操作步骤如下： 1、关闭正在运行的MongoDB服务器。 现在我们通过指定 --replSet 选项来启动mongoDB。--replSet 基本语法格式如下： 1mongod --port &quot;PORT&quot; --dbpath &quot;YOUR_DB_DATA_PATH&quot; --replSet &quot;REPLICA_SET_INSTANCE_NAME&quot; 示例 1mongod --port 27017 --dbpath &quot;F:\\Softs\\Installed\\MongoDB\\data&quot; --replSet rs0 示例启动一个名为rs0的MongoDB实例，其端口号为27017。启动后打开命令提示框并连接上mongoDB服务。在Mongo客户端使用命令rs.initiate()来启动一个新的副本集。我们可以使用rs.conf()来查看副本集的配置查看副本集状态使用 rs.status() 命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155#关闭服务C:\\Users\\Administrator&gt;net stop mongodb#创建rs0实例C:\\Users\\Administrator&gt;mongod --port 27017 --dbpath &quot;F:\\Softs\\Installed\\MongoDB\\data&quot; --replSet rs0#启动C:\\Users\\Administrator&gt;mongoMongoDB shell version v5.0.6-rc0connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session { &quot;id&quot; : UUID(&quot;a7bc72f5-0db4-4de3-b3c1-4f457b526e8b&quot;) }MongoDB server version: 5.0.6-rc0================Warning: the &quot;mongo&quot; shell has been superseded by &quot;mongosh&quot;,which delivers improved usability and compatibility.The &quot;mongo&quot; shell has been deprecated and will be removed inan upcoming release.For installation instructions, seehttps://docs.mongodb.com/mongodb-shell/install/================#启动新的副本集&gt; rs.initiate(){ &quot;info2&quot; : &quot;no configuration specified. Using a default configuration for the set&quot;, &quot;me&quot; : &quot;localhost:27017&quot;, &quot;ok&quot; : 1}查看副本配置rs0:SECONDARY&gt; rs.conf(){ &quot;_id&quot; : &quot;rs0&quot;, &quot;version&quot; : 1, &quot;term&quot; : 1, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;host&quot; : &quot;localhost:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;secondaryDelaySecs&quot; : NumberLong(0), &quot;votes&quot; : 1 } ], &quot;protocolVersion&quot; : NumberLong(1), &quot;writeConcernMajorityJournalDefault&quot; : true, &quot;settings&quot; : { &quot;chainingAllowed&quot; : true, &quot;heartbeatIntervalMillis&quot; : 2000, &quot;heartbeatTimeoutSecs&quot; : 10, &quot;electionTimeoutMillis&quot; : 10000, &quot;catchUpTimeoutMillis&quot; : -1, &quot;catchUpTakeoverDelayMillis&quot; : 30000, &quot;getLastErrorModes&quot; : { }, &quot;getLastErrorDefaults&quot; : { &quot;w&quot; : 1, &quot;wtimeout&quot; : 0 }, &quot;replicaSetId&quot; : ObjectId(&quot;61ea43d55303298c0a702c41&quot;) }}#查看副本集状态rs0:PRIMARY&gt; rs.status(){ &quot;set&quot; : &quot;rs0&quot;, &quot;date&quot; : ISODate(&quot;2022-01-21T05:26:33.032Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;majorityVoteCount&quot; : 1, &quot;writeMajorityCount&quot; : 1, &quot;votingMembersCount&quot; : 1, &quot;writableVotingMembersCount&quot; : 1, &quot;optimes&quot; : { &quot;lastCommittedOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;lastCommittedWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;readConcernMajorityOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;appliedOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;durableOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;lastAppliedWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;) }, &quot;lastStableRecoveryTimestamp&quot; : Timestamp(1642742741, 1), &quot;electionCandidateMetrics&quot; : { &quot;lastElectionReason&quot; : &quot;electionTimeout&quot;, &quot;lastElectionDate&quot; : ISODate(&quot;2022-01-21T05:25:42.687Z&quot;), &quot;electionTerm&quot; : NumberLong(1), &quot;lastCommittedOpTimeAtElection&quot; : { &quot;ts&quot; : Timestamp(1642742741, 1), &quot;t&quot; : NumberLong(-1) }, &quot;lastSeenOpTimeAtElection&quot; : { &quot;ts&quot; : Timestamp(1642742741, 1), &quot;t&quot; : NumberLong(-1) }, &quot;numVotesNeeded&quot; : 1, &quot;priorityAtElection&quot; : 1, &quot;electionTimeoutMillis&quot; : NumberLong(10000), &quot;newTermStartDate&quot; : ISODate(&quot;2022-01-21T05:25:42.963Z&quot;), &quot;wMajorityWriteAvailabilityDate&quot; : ISODate(&quot;2022-01-21T05:25:43.131Z&quot;) }, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;name&quot; : &quot;localhost:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 203, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDate&quot; : ISODate(&quot;2022-01-21T05:26:23Z&quot;), &quot;lastAppliedWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;Could not find member to sync from&quot;, &quot;electionTime&quot; : Timestamp(1642742742, 1), &quot;electionDate&quot; : ISODate(&quot;2022-01-21T05:25:42Z&quot;), &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 1, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; } ], &quot;ok&quot; : 1, &quot;$clusterTime&quot; : { &quot;clusterTime&quot; : Timestamp(1642742783, 1), &quot;signature&quot; : { &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) } }, &quot;operationTime&quot; : Timestamp(1642742783, 1)}rs0:PRIMARY&gt; 副本集添加成员 添加副本集的成员，我们需要使用多台服务器来启动mongo服务。进入Mongo客户端，并使用rs.add()方法来添加副本集的成员。 语法 rs.add() 命令基本语法格式如下： 1&gt;rs.add(HOST_NAME:PORT) 实例 假设你已经启动了一个名为mongod1.net，端口号为27017的Mongo服务。 在客户端命令窗口使用rs.add() 命令将其添加到副本集中，命令如下所示： 12&gt;rs.add(&quot;mongod1.net:27017&quot;)&gt; MongoDB中你只能通过主节点将Mongo服务添加到副本集中， 判断当前运行的Mongo服务是否为主节点可以使用命令db.isMaster() 。 MongoDB的副本集与我们常见的主从有所不同，主从在主机宕机后所有服务将停止，而副本集在主机宕机后，副本会接管主节点成为主节点，不会出现宕机的情况。 十一、分片 分片 在Mongodb里面存在另一种集群，就是分片技术,可以满足MongoDB数据量大量增长的需求。 当MongoDB存储海量的数据时，一台机器可能不足以存储数据，也可能不足以提供可接受的读写吞吐量。这时，我们就可以通过在多台机器上分割数据，使得数据库系统能存储和处理更多的数据。 为什么使用分片 复制所有的写入操作到主节点 延迟的敏感数据会在主节点查询 单个副本集限制在12个节点 当请求量巨大时会出现内存不足。 本地磁盘不足 垂直扩展价格昂贵 MongoDB分片 下图展示了在MongoDB中使用分片集群结构分布： 上图中主要有如下所述三个主要组件： Shard: 用于存储实际的数据块，实际生产环境中一个shard server角色可由几台机器组个一个replica set承担，防止主机单点故障 Config Server: mongod 实例，存储了整个 ClusterMetadata，其中包括 chunk 信息。 Query Routers: 前端路由，客户端由此接入，且让整个集群看上去像单一数据库，前端应用可以透明使用。 分片实例 分片结构端口分布如下： 123456Shard Server 1：27020Shard Server 2：27021Shard Server 3：27022Shard Server 4：27023Config Server ：27100Route Process：40000 步骤一：启动Shard Server 12345678[root@100 /]# mkdir -p /www/mongoDB/shard/s0[root@100 /]# mkdir -p /www/mongoDB/shard/s1[root@100 /]# mkdir -p /www/mongoDB/shard/s2[root@100 /]# mkdir -p /www/mongoDB/shard/s3[root@100 /]# mkdir -p /www/mongoDB/shard/log[root@100 /]# /usr/local/mongoDB/bin/mongod --port 27020 --dbpath=/www/mongoDB/shard/s0 --logpath=/www/mongoDB/shard/log/s0.log --logappend --fork....[root@100 /]# /usr/local/mongoDB/bin/mongod --port 27023 --dbpath=/www/mongoDB/shard/s3 --logpath=/www/mongoDB/shard/log/s3.log --logappend --fork 步骤二： 启动Config Server 12[root@100 /]# mkdir -p /www/mongoDB/shard/config[root@100 /]# /usr/local/mongoDB/bin/mongod --port 27100 --dbpath=/www/mongoDB/shard/config --logpath=/www/mongoDB/shard/log/config.log --logappend --fork **注意：**这里我们完全可以像启动普通mongodb服务一样启动，不需要添加—shardsvr和configsvr参数。因为这两个参数的作用就是改变启动端口的，所以我们自行指定了端口就可以。 步骤三： 启动Route Process 1/usr/local/mongoDB/bin/mongos --port 40000 --configdb localhost:27100 --fork --logpath=/www/mongoDB/shard/log/route.log --chunkSize 500 mongos启动参数中，chunkSize这一项是用来指定chunk的大小的，单位是MB，默认大小为200MB. 步骤四： 配置Sharding 接下来，我们使用MongoDB Shell登录到mongos，添加Shard节点 123456789101112[root@100 shard]# /usr/local/mongoDB/bin/mongo admin --port 40000MongoDB shell version: 2.0.7connecting to: 127.0.0.1:40000/adminmongos&gt; db.runCommand({ addshard:&quot;localhost:27020&quot; }){ &quot;shardAdded&quot; : &quot;shard0000&quot;, &quot;ok&quot; : 1 }......mongos&gt; db.runCommand({ addshard:&quot;localhost:27029&quot; }){ &quot;shardAdded&quot; : &quot;shard0009&quot;, &quot;ok&quot; : 1 }mongos&gt; db.runCommand({ enablesharding:&quot;test&quot; }) #设置分片存储的数据库{ &quot;ok&quot; : 1 }mongos&gt; db.runCommand({ shardcollection: &quot;test.log&quot;, key: { id:1,time:1}}){ &quot;collectionsharded&quot; : &quot;test.log&quot;, &quot;ok&quot; : 1 } 步骤五： 程序代码内无需太大更改，直接按照连接普通的mongo数据库那样，将数据库连接接入接口40000 十二、安装其他工具 下载后将其bin目录下复制到data下bin目录下： https://www.mongodb.com/try/download/database-tools 十三、备份 / 恢复 数据备份 备份(mongodump)与恢复(mongorestore)。在Mongodb中我们使用mongodump命令来备份MongoDB数据。该命令可以导出所有数据到指定目录中。 mongodump命令可以通过参数指定导出的数据量级转存的服务器。 语法 mongodump命令脚本语法如下： 1&gt;mongodump -h dbhost -d dbname -o dbdirectory -h： MongoDB 所在服务器地址，例如：127.0.0.1，当然也可以指定端口号：127.0.0.1:27017 -d： 需要备份的数据库实例，例如：test -o： 备份的数据存放位置，例如：c:\\data\\dump，当然该目录需要提前建立，在备份完成后，系统自动在dump目录下建立一个test目录，这个目录里面存放该数据库实例的备份数据。 实例 在本地使用 27017 启动你的mongod服务。打开命令提示符窗口，进入MongoDB安装目录的bin目录输入命令mongodump: 1&gt;mongodump #即mongod 执行以上命令后，客户端会连接到ip为 127.0.0.1 端口号为 27017 的MongoDB服务上，并备份所有数据到 bin/dump/ 目录中。命令输出结果如下： mongodump 命令可选参数列表如下所示： 语法 描述 实例 mongodump --host HOST_NAME --port PORT_NUMBER 该命令将备份所有MongoDB数据 mongodump --host runoob.com --port 27017 mongodump --dbpath DB_PATH --out BACKUP_DIRECTORY mongodump --dbpath /data/db/ --out /data/backup/ mongodump --collection COLLECTION --db DB_NAME 该命令将备份指定数据库的集合。 mongodump --collection mycol --db test 数据恢复 mongodb使用 mongorestore 命令来恢复备份的数据。 语法 mongorestore命令脚本语法如下： 1&gt;mongorestore -h &lt;hostname&gt;&lt;:port&gt; -d dbname &lt;path&gt; --host &lt;:port&gt;, -h &lt;:port&gt;： MongoDB所在服务器地址，默认为： localhost:27017 --db , -d ： 需要恢复的数据库实例，例如：test，当然这个名称也可以和备份时候的不一样，比如test2 --drop： 恢复的时候，先删除当前数据，然后恢复备份的数据。就是说，恢复后，备份后添加修改的数据都会被删除，慎用哦！ ： mongorestore 最后的一个参数，设置备份数据所在位置，例如：c:\\data\\dump\\test。 你不能同时指定 和 --dir 选项，--dir也可以设置备份目录。 --dir： 指定备份的目录 你不能同时指定 和 --dir 选项。 接下来我们执行以下命令: 1&gt;mongorestore 执行以上命令输出结果如下： 十四、监控 在你已经安装部署并允许MongoDB服务后，你必须要了解MongoDB的运行情况，并查看MongoDB的性能。这样在大流量得情况下可以很好的应对并保证MongoDB正常运作。 MongoDB中提供了mongostat 和 mongotop 两个命令来监控MongoDB的运行情况。 mongostat 命令 mongostat是mongodb自带的状态检测工具，在命令行下使用。它会间隔固定时间获取mongodb的当前运行状态，并输出。如果你发现数据库突然变慢或者有其他问题的话，你第一手的操作就考虑采用mongostat来查看mongo的状态。 启动你的Mongod服务，进入到你安装的MongoDB目录下的bin目录， 然后输入mongostat命令，如下所示： 1D:\\set up\\mongodb\\bin&gt;mongostat 以上命令输出结果如下： mongotop 命令 mongotop也是mongodb下的一个内置工具，mongotop提供了一个方法，用来跟踪一个MongoDB的实例，查看哪些大量的时间花费在读取和写入数据。 mongotop提供每个集合的水平的统计数据。默认情况下，mongotop返回值的每一秒。 启动你的Mongod服务，进入到你安装的MongoDB目录下的bin目录， 然后输入mongotop命令，如下所示： 1D:\\set up\\mongodb\\bin&gt;mongotop 以上命令执行输出结果如下： 带参数实例 1E:\\mongodb-win32-x86_64-2.2.1\\bin&gt;mongotop 10 后面的10是**参数 ，可以不使用，等待的时间长度，以秒为单位，mongotop等待调用之间。通过的默认mongotop返回数据的每一秒。 1E:\\mongodb-win32-x86_64-2.2.1\\bin&gt;mongotop --locks 报告每个数据库的锁的使用中，使用mongotop - 锁，这将产生以下输出： 输出结果字段说明： ns： 包含数据库命名空间，后者结合了数据库名称和集合。 db： 包含数据库的名称。名为 . 的数据库针对全局锁定，而非特定数据库。 total： mongod花费的时间工作在这个命名空间提供总额。 read： 提供了大量的时间，这mongod花费在执行读操作，在此命名空间。 write： 提供这个命名空间进行写操作，这mongod花了大量的时间。 十五、MongoDBCompass使用 MongoDB可视化工具 mongo命令连接服务器后打开工具 点击连接默认的服务器、接口和None认证的服务 十六、Java使用 文档 引入依赖 123456&lt;!--MongoDB--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver-sync&lt;/artifactId&gt; &lt;version&gt;4.4.1&lt;/version&gt;&lt;/dependency&gt; 连接 1234567891011121314151617181920212223242526272829303132333435363738394041424344package pers.lxl.mylearnproject.programbase.nosql.mongolearn;import static com.mongodb.client.model.Filters.eq;import org.bson.Document;import com.mongodb.client.MongoClient;import com.mongodb.client.MongoClients;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoDatabase;public class HelloMongoDB {//1.添加依赖// &lt;dependency&gt;// &lt;groupId&gt;org.mongodb&lt;/groupId&gt;// &lt;artifactId&gt;mongodb-driver-sync&lt;/artifactId&gt;// &lt;version&gt;4.4.1&lt;/version&gt;// &lt;/dependency&gt;// 2.连接到数据库public static void main( String args[] ){// try{// // 连接到 mongodb 服务// MongoClient mongoClient = new MongoClient( &quot;localhost&quot; , 27017 );//// // 连接到数据库// MongoDatabase mongoDatabase = mongoClient.getDatabase(&quot;mycol&quot;);// System.out.println(&quot;Connect to database successfully&quot;);//// }catch(Exception e){// System.err.println( e.getClass().getName() + &quot;: &quot; + e.getMessage() );// }// Replace the uri string with your MongoDB deployment's connection string// IllegalArgumentException: Unsupported compressor 'disabled'// String uri = &quot;mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodb&quot;;// Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: com/mongodb/internal/connection/InternalConnectionPoolSettings 升级springboot版本 String uri = &quot;mongodb://127.0.0.1:27017/?gssapiServiceName=mongodb&quot;; try (MongoClient mongoClient = MongoClients.create(uri)) {// show dbs MongoDatabase database = mongoClient.getDatabase(&quot;student&quot;);// show collections 或 show tables MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;student&quot;); Document doc = collection.find(eq(&quot;age&quot;, 11)).first(); System.out.println(doc.toJson()+&quot;==========&quot;); }}} 十七、关系 MongoDB 的关系表示多个文档之间在逻辑上的相互联系。 文档间可以通过嵌入和引用来建立联系。 MongoDB 中的关系可以是： 1:1 (1对1) 1: N (1对多) N: 1 (多对1) N: N (多对多) 接下来我们来考虑下用户与用户地址的关系。 一个用户可以有多个地址，所以是一对多的关系。 以下是 user 文档的简单结构： 123456{ &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;), &quot;name&quot;: &quot;Tom Hanks&quot;, &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;} 以下是 address 文档的简单结构： 1234567{ &quot;_id&quot;:ObjectId(&quot;52ffc4a5d85242602e000000&quot;), &quot;building&quot;: &quot;22 A, Indiana Apt&quot;, &quot;pincode&quot;: 123456, &quot;city&quot;: &quot;Los Angeles&quot;, &quot;state&quot;: &quot;California&quot;} 嵌入式关系 使用嵌入式方法，我们可以把用户地址嵌入到用户的文档中： 12345678910111213141516171819{ &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;), &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;, &quot;address&quot;: [ { &quot;building&quot;: &quot;22 A, Indiana Apt&quot;, &quot;pincode&quot;: 123456, &quot;city&quot;: &quot;Los Angeles&quot;, &quot;state&quot;: &quot;California&quot; }, { &quot;building&quot;: &quot;170 A, Acropolis Apt&quot;, &quot;pincode&quot;: 456789, &quot;city&quot;: &quot;Chicago&quot;, &quot;state&quot;: &quot;Illinois&quot; }]} 以上数据保存在单一的文档中，可以比较容易的获取和维护数据。 你可以这样查询用户的地址： 1&gt;db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;},{&quot;address&quot;:1}) 注意：以上查询中 db 和 users 表示数据库和集合。 这种数据结构的缺点是，如果用户和用户地址在不断增加，数据量不断变大，会影响读写性能。 引用式关系 引用式关系是设计数据库时经常用到的方法，这种方法把用户数据文档和用户地址数据文档分开，通过引用文档的 id 字段来建立关系。 12345678910{ &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;), &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;, &quot;address_ids&quot;: [ ObjectId(&quot;52ffc4a5d85242602e000000&quot;), ObjectId(&quot;52ffc4a5d85242602e000001&quot;) ]} 以上实例中，用户文档的 address_ids 字段包含用户地址的对象id（ObjectId）数组。 我们可以读取这些用户地址的对象id（ObjectId）来获取用户的详细地址信息。 这种方法需要两次查询，第一次查询用户地址的对象id（ObjectId），第二次通过查询的id获取用户的详细地址信息。 12&gt;var result = db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;},{&quot;address_ids&quot;:1})&gt;var addresses = db.address.find({&quot;_id&quot;:{&quot;$in&quot;:result[&quot;address_ids&quot;]}}) 十八、数据库引用 在上一章节MongoDB关系中我们提到了MongoDB的引用来规范数据结构文档。 MongoDB 引用有两种： 手动引用（Manual References） DBRefs DBRefs vs 手动引用 考虑这样的一个场景，我们在不同的集合中 (address_home, address_office, address_mailing, 等)存储不同的地址（住址，办公室地址，邮件地址等）。 这样，我们在调用不同地址时，也需要指定集合，一个文档从多个集合引用文档，我们应该使用 DBRefs。 使用 DBRefs DBRef的形式： 1{ $ref : , $id : , $db : } 三个字段表示的意义为： $ref：集合名称 $id：引用的id $db:数据库名称，可选参数 以下实例中用户数据文档使用了 DBRef, 字段 address： 12345678910{ &quot;_id&quot;:ObjectId(&quot;53402597d852426020000002&quot;), &quot;address&quot;: { &quot;$ref&quot;: &quot;address_home&quot;, &quot;$id&quot;: ObjectId(&quot;534009e4d852427820000002&quot;), &quot;$db&quot;: &quot;runoob&quot;}, &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;} address DBRef 字段指定了引用的地址文档是在 runoob 数据库下的 address_home 集合，id 为 534009e4d852427820000002。 以下代码中，我们通过指定 $ref 参数（address_home 集合）来查找集合中指定id的用户地址信息： 123&gt;var user = db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;})&gt;var dbRef = user.address&gt;db[dbRef.$ref].findOne({&quot;_id&quot;:(dbRef.$id)}) 以上实例返回了 address_home 集合中的地址数据： 1234567{ &quot;_id&quot; : ObjectId(&quot;534009e4d852427820000002&quot;), &quot;building&quot; : &quot;22 A, Indiana Apt&quot;, &quot;pincode&quot; : 123456, &quot;city&quot; : &quot;Los Angeles&quot;, &quot;state&quot; : &quot;California&quot;} 十九、覆盖索引查询 官方的MongoDB的文档中说明，覆盖查询是以下的查询： 所有的查询字段是索引的一部分 所有的查询返回字段在同一个索引中 由于所有出现在查询中的字段是索引的一部分， MongoDB 无需在整个数据文档中检索匹配查询条件和返回使用相同索引的查询结果。 因为索引存在于RAM中，从索引中获取数据比通过扫描文档读取数据要快得多。 使用覆盖索引查询 为了测试覆盖索引查询，使用以下 users 集合: 12345678{ &quot;_id&quot;: ObjectId(&quot;53402597d852426020000002&quot;), &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;, &quot;user_name&quot;: &quot;tombenzamin&quot;} 我们在 users 集合中创建联合索引，字段为 gender 和 user_name : 1&gt;db.users.ensureIndex({gender:1,user_name:1}) 现在，该索引会覆盖以下查询： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}) 也就是说，对于上述查询，MongoDB的不会去数据库文件中查找。相反，它会从索引中提取数据，这是非常快速的数据查询。 由于我们的索引中不包括 _id 字段，_id在查询中会默认返回，我们可以在MongoDB的查询结果集中排除它。 下面的实例没有排除_id，查询就不会被覆盖： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1}) 最后，如果是以下的查询，不能使用覆盖索引查询： 所有索引字段是一个数组 所有索引字段是一个子文档 二十、查询分析 MongoDB 查询分析可以确保我们所建立的索引是否有效，是查询语句性能分析的重要工具。 MongoDB 查询分析常用函数有：explain() 和 hint()。 使用 explain() explain 操作提供了查询信息，使用索引及查询统计等。有利于我们对索引的优化。 接下来我们在 users 集合中创建 gender 和 user_name 的索引： 1&gt;db.users.ensureIndex({gender:1,user_name:1}) 现在在查询语句中使用 explain ： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}).explain() 以上的 explain() 查询返回如下结果： 1234567891011121314151617181920212223242526272829303132{ &quot;cursor&quot; : &quot;BtreeCursor gender_1_user_name_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 1, &quot;nscannedObjects&quot; : 0, &quot;nscanned&quot; : 1, &quot;nscannedObjectsAllPlans&quot; : 0, &quot;nscannedAllPlans&quot; : 1, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : true,//**indexOnly**: 字段为 true ，表示我们使用了索引。 &quot;nYields&quot; : 0, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 0, &quot;indexBounds&quot; : { &quot;gender&quot; : [ [ &quot;M&quot;, &quot;M&quot; ] ], &quot;user_name&quot; : [ [ { &quot;$minElement&quot; : 1 }, { &quot;$maxElement&quot; : 1 } ] ] }} 现在，我们看看这个结果集的字段： indexOnly: 字段为 true ，表示我们使用了索引。 cursor：因为这个查询使用了索引，MongoDB 中索引存储在B树结构中，所以这是也使用了 BtreeCursor 类型的游标。如果没有使用索引，游标的类型是 BasicCursor。这个键还会给出你所使用的索引的名称，你通过这个名称可以查看当前数据库下的system.indexes集合（系统自动创建，由于存储索引信息，这个稍微会提到）来得到索引的详细信息。 n：当前查询返回的文档数量。 nscanned/nscannedObjects：表明当前这次查询一共扫描了集合中多少个文档，我们的目的是，让这个数值和返回文档的数量越接近越好。 millis：当前查询所需时间，毫秒数。 indexBounds：当前查询具体使用的索引。 使用 hint() 虽然MongoDB查询优化器一般工作的很不错，但是也可以使用 hint 来强制 MongoDB 使用一个指定的索引。 这种方法某些情形下会提升性能。 一个有索引的 collection 并且执行一个多字段的查询(一些字段已经索引了)。 如下查询实例指定了使用 gender 和 user_name 索引字段来查询： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}).hint({gender:1,user_name:1}) 可以使用 explain() 函数来分析以上查询： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}).hint({gender:1,user_name:1}).explain() 二一、MongoDB 原子操作 mongodb不支持事务，所以，在你的项目中应用时，要注意这点。无论什么设计，都不要要求mongodb保证数据的完整性。 但是mongodb提供了许多原子操作，比如文档的保存，修改，删除等，都是原子操作。 所谓原子操作就是要么这个文档保存到Mongodb，要么没有保存到Mongodb，不会出现查询到的文档没有保存完整的情况。 原子操作数据模型 考虑下面的例子，图书馆的书籍及结账信息。 实例说明了在一个相同的文档中如何确保嵌入字段关联原子操作（update：更新）的字段是同步的。 1234567891011book = { _id: 123456789, title: &quot;MongoDB: The Definitive Guide&quot;, author: [ &quot;Kristina Chodorow&quot;, &quot;Mike Dirolf&quot; ], published_date: ISODate(&quot;2010-09-24&quot;), pages: 216, language: &quot;English&quot;, publisher_id: &quot;oreilly&quot;, available: 3, checkout: [ { by: &quot;joe&quot;, date: ISODate(&quot;2012-10-15&quot;) } ] } 你可以使用 db.collection.findAndModify() 方法来判断书籍是否可结算并更新新的结算信息。 在同一个文档中嵌入的 available 和 checkout 字段来确保这些字段是同步更新的: 12345678910db.books.findAndModify ( { query: { _id: 123456789, available: { $gt: 0 } }, update: { $inc: { available: -1 }, $push: { checkout: { by: &quot;abc&quot;, date: new Date() } } }} ) 原子操作常用命令 $set 用来指定一个键并更新键值，若键不存在并创建。 1{ $set : { field : value } } $unset 用来删除一个键。 1{ $unset : { field : 1} } $inc $inc可以对文档的某个值为数字型（只能为满足要求的数字）的键进行增减的操作。 1{ $inc : { field : value } } $push 用法： 1{ $push : { field : value } } 把value追加到field里面去，field一定要是数组类型才行，如果field不存在，会新增一个数组类型加进去。 $pushAll 同$push,只是一次可以追加多个值到一个数组字段内。 1{ $pushAll : { field : value_array } } $pull 从数组field内删除一个等于value值。 1{ $pull : { field : _value } } $addToSet 增加一个值到数组内，而且只有当这个值不在数组内才增加。 $pop 删除数组的第一个或最后一个元素 1{ $pop : { field : 1 } } $rename 修改字段名称 1{ $rename : { old_field_name : new_field_name } } $bit 位操作，integer类型 1{$bit : { field : {and : 5}}} 偏移操作符 12345&gt; t.find() { &quot;_id&quot; : ObjectId(&quot;4b97e62bf1d8c7152c9ccb74&quot;), &quot;title&quot; : &quot;ABC&quot;, &quot;comments&quot; : [ { &quot;by&quot; : &quot;joe&quot;, &quot;votes&quot; : 3 }, { &quot;by&quot; : &quot;jane&quot;, &quot;votes&quot; : 7 } ] } &gt; t.update( {'comments.by':'joe'}, {$inc:{'comments.$.votes':1}}, false, true ) &gt; t.find() { &quot;_id&quot; : ObjectId(&quot;4b97e62bf1d8c7152c9ccb74&quot;), &quot;title&quot; : &quot;ABC&quot;, &quot;comments&quot; : [ { &quot;by&quot; : &quot;joe&quot;, &quot;votes&quot; : 4 }, { &quot;by&quot; : &quot;jane&quot;, &quot;votes&quot; : 7 } ] } 二二、高级索引 考虑以下文档集合（users ）: 12345678910111213{ &quot;address&quot;: { &quot;city&quot;: &quot;Los Angeles&quot;, &quot;state&quot;: &quot;California&quot;, &quot;pincode&quot;: &quot;123&quot; }, &quot;tags&quot;: [ &quot;music&quot;, &quot;cricket&quot;, &quot;blogs&quot; ], &quot;name&quot;: &quot;Tom Benzamin&quot;} 以上文档包含了 address 子文档和 tags 数组。 索引数组字段 假设我们基于标签来检索用户，为此我们需要对集合中的数组 tags 建立索引。 在数组中创建索引，需要对数组中的每个字段依次建立索引。所以在我们为数组 tags 创建索引时，会为 music、cricket、blogs三个值建立单独的索引。 使用以下命令创建数组索引： 1234567891011121314#3.0弃用&gt;db.users.ensureIndex({&quot;tags&quot;:1})&gt; db.users.ensureIndex({&quot;tags&quot;:1})uncaught exception: TypeError: db.users.ensureIndex is not a function :@(shell):1:1&gt; db.users.createIndex({&quot;tags&quot;:1}){ &quot;numIndexesBefore&quot; : 1, &quot;numIndexesAfter&quot; : 2, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}&gt; 创建索引后，我们可以这样检索集合的 tags 字段： 1&gt;db.users.find({tags:&quot;cricket&quot;}) 为了验证我们使用使用了索引，可以使用 explain 命令： 1&gt;db.users.find({tags:&quot;cricket&quot;}).explain() 以上命令执行结果中会显示 &quot;cursor&quot; : &quot;BtreeCursor tags_1&quot; ，则表示已经使用了索引。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970db.users.find({tags:&quot;cricket&quot;}).explain(){ &quot;explainVersion&quot; : &quot;1&quot;, &quot;queryPlanner&quot; : { &quot;namespace&quot; : &quot;users.users&quot;, &quot;indexFilterSet&quot; : false, &quot;parsedQuery&quot; : { &quot;tags&quot; : { &quot;$eq&quot; : &quot;cricket&quot; } }, &quot;queryHash&quot; : &quot;9D3B61A7&quot;, &quot;planCacheKey&quot; : &quot;3C3D3201&quot;, &quot;maxIndexedOrSolutionsReached&quot; : false, &quot;maxIndexedAndSolutionsReached&quot; : false, &quot;maxScansToExplodeReached&quot; : false, &quot;winningPlan&quot; : { &quot;stage&quot; : &quot;FETCH&quot;, &quot;inputStage&quot; : { &quot;stage&quot; : &quot;IXSCAN&quot;, &quot;keyPattern&quot; : { &quot;tags&quot; : 1 }, &quot;indexName&quot; : &quot;tags_1&quot;, &quot;isMultiKey&quot; : true, &quot;multiKeyPaths&quot; : { &quot;tags&quot; : [ &quot;tags&quot; ] }, &quot;isUnique&quot; : false, &quot;isSparse&quot; : false, &quot;isPartial&quot; : false, &quot;indexVersion&quot; : 2, &quot;direction&quot; : &quot;forward&quot;, &quot;indexBounds&quot; : { &quot;tags&quot; : [ &quot;[\\&quot;cricket\\&quot;, \\&quot;cricket\\&quot;]&quot; ] } } }, &quot;rejectedPlans&quot; : [ ] }, &quot;command&quot; : { &quot;find&quot; : &quot;users&quot;, &quot;filter&quot; : { &quot;tags&quot; : &quot;cricket&quot; }, &quot;$db&quot; : &quot;users&quot; }, &quot;serverInfo&quot; : { &quot;host&quot; : &quot;BF-202103261718&quot;, &quot;port&quot; : 27017, &quot;version&quot; : &quot;5.0.6-rc0&quot;, &quot;gitVersion&quot; : &quot;60af56dfe1a17c60bbd628163fda0a161105b6c0&quot; }, &quot;serverParameters&quot; : { &quot;internalQueryFacetBufferSizeBytes&quot; : 104857600, &quot;internalQueryFacetMaxOutputDocSizeBytes&quot; : 104857600, &quot;internalLookupStageIntermediateDocumentMaxSizeBytes&quot; : 104857600, &quot;internalDocumentSourceGroupMaxMemoryBytes&quot; : 104857600, &quot;internalQueryMaxBlockingSortMemoryUsageBytes&quot; : 104857600, &quot;internalQueryProhibitBlockingMergeOnMongoS&quot; : 0, &quot;internalQueryMaxAddToSetBytes&quot; : 104857600, &quot;internalDocumentSourceSetWindowFieldsMaxMemoryBytes&quot; : 104857600 }, &quot;ok&quot; : 1}&gt; 索引子文档字段 假设我们需要通过city、state、pincode字段来检索文档，由于这些字段是子文档的字段，所以我们需要对子文档建立索引。 为子文档的三个字段创建索引，命令如下： 1&gt;db.users.ensureIndex({&quot;address.city&quot;:1,&quot;address.state&quot;:1,&quot;address.pincode&quot;:1}) 一旦创建索引，我们可以使用子文档的字段来检索数据： 1&gt;db.users.find({&quot;address.city&quot;:&quot;Los Angeles&quot;}) 查询表达不一定遵循指定的索引的顺序，mongodb 会自动优化。所以上面创建的索引将支持以下查询： 1&gt;db.users.find({&quot;address.state&quot;:&quot;California&quot;,&quot;address.city&quot;:&quot;Los Angeles&quot;}) 同样支持以下查询： 1&gt;db.users.find({&quot;address.city&quot;:&quot;Los Angeles&quot;,&quot;address.state&quot;:&quot;California&quot;,&quot;address.pincode&quot;:&quot;123&quot;}) 二三、索引限制 额外开销 每个索引占据一定的存储空间，在进行插入，更新和删除操作时也需要对索引进行操作。所以，如果你很少对集合进行读取操作，建议不使用索引。 内存(RAM)使用 由于索引是存储在内存(RAM)中,你应该确保该索引的大小不超过内存的限制。 如果索引的大小大于内存的限制，MongoDB会删除一些索引，这将导致性能下降。 查询限制 索引不能被以下的查询使用： 正则表达式及非操作符，如 $nin, $not, 等。 算术运算符，如 $mod, 等。 $where 子句 所以，检测你的语句是否使用索引是一个好的习惯，可以用explain来查看。 索引键限制 从2.6版本开始，如果现有的索引字段的值超过索引键的限制，MongoDB中不会创建索引。 插入文档超过索引键限制 如果文档的索引字段值超过了索引键的限制，MongoDB不会将任何文档转换成索引的集合。与mongorestore和mongoimport工具类似。 最大范围 集合中索引不能超过64个 索引名的长度不能超过128个字符 一个复合索引最多可以有31个字段 二四、MongoDB ObjectId 在前面几个章节中我们已经使用了MongoDB 的对象 Id(ObjectId)。 在本章节中，我们将了解的ObjectId的结构。 ObjectId 是一个12字节 BSON 类型数据，有以下格式： 前4个字节表示时间戳 接下来的3个字节是机器标识码 紧接的两个字节由进程id组成（PID） 最后三个字节是随机数。 MongoDB中存储的文档必须有一个&quot;_id&quot;键。这个键的值可以是任何类型的，默认是个ObjectId对象。 在一个集合里面，每个文档都有唯一的&quot;_id&quot;值，来确保集合里面每个文档都能被唯一标识。 MongoDB采用ObjectId，而不是其他比较常规的做法（比如自动增加的主键）的主要原因，因为在多个 服务器上同步自动增加主键值既费力还费时。 创建新的ObjectId 使用以下代码生成新的ObjectId： 1&gt;newObjectId = ObjectId() 上面的语句返回以下唯一生成的id： 1ObjectId(&quot;5349b4ddd2781d08c09890f3&quot;) 你也可以使用生成的id来取代MongoDB自动生成的ObjectId： 1&gt;myObjectId = ObjectId(&quot;5349b4ddd2781d08c09890f4&quot;) 创建文档的时间戳 由于 ObjectId 中存储了 4 个字节的时间戳，所以你不需要为你的文档保存时间戳字段，你可以通过 getTimestamp 函数来获取文档的创建时间: 1&gt;ObjectId(&quot;5349b4ddd2781d08c09890f4&quot;).getTimestamp() 以上代码将返回 ISO 格式的文档创建时间： 1ISODate(&quot;2014-04-12T21:49:17Z&quot;) ObjectId 转换为字符串 在某些情况下，您可能需要将ObjectId转换为字符串格式。你可以使用下面的代码： 1&gt;new ObjectId().str 以上代码将返回Guid格式的字符串：： 15349b4ddd2781d08c09890f3 二五、 Map Reduce Map-Reduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。 MongoDB提供的Map-Reduce非常灵活，对于大规模数据分析也相当实用。 MapReduce 命令 以下是MapReduce的基本语法： 12345678910&gt;db.collection.mapReduce( function() {emit(key,value);}, //map 函数 function(key,values) {return reduceFunction}, //reduce 函数 { out: collection, query: document, sort: document, limit: number }) 使用 MapReduce 要实现两个函数 Map 函数和 Reduce 函数,Map 函数调用 emit(key, value), 遍历 collection 中所有的记录, 将 key 与 value 传递给 Reduce 函数进行处理。 Map 函数必须调用 emit(key, value) 返回键值对。 参数说明: map ：映射函数 (生成键值对序列,作为 reduce 函数参数)。 reduce 统计函数，reduce函数的任务就是将key-values变成key-value，也就是把values数组变成一个单一的值value。。 out 统计结果存放集合 (不指定则使用临时集合,在客户端断开后自动删除)。 query 一个筛选条件，只有满足条件的文档才会调用map函数。（query。limit，sort可以随意组合） sort 和limit结合的sort排序参数（也是在发往map函数前给文档排序），可以优化分组机制 limit 发往map函数的文档数量的上限（要是没有limit，单独使用sort的用处不大） 以下实例在集合 orders 中查找 status:&quot;A&quot; 的数据，并根据 cust_id 来分组，并计算 amount 的总和。 使用 MapReduce 考虑以下文档结构存储用户的文章，文档存储了用户的 user_name 和文章的 status 字段： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;disabled&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 }) 现在，我们将在 posts 集合中使用 mapReduce 函数来选取已发布的文章(status:&quot;active&quot;)，并通过user_name分组，计算每个用户的文章数： 12345678&gt;db.posts.mapReduce( function() { emit(this.user_name,1); }, function(key, values) {return Array.sum(values)}, { query:{status:&quot;active&quot;}, out:&quot;post_total&quot; }) 以上 mapReduce 输出结果为： 1234567891011{ &quot;result&quot; : &quot;post_total&quot;, &quot;timeMillis&quot; : 23, &quot;counts&quot; : { &quot;input&quot; : 5, &quot;emit&quot; : 5, &quot;reduce&quot; : 1, &quot;output&quot; : 2 }, &quot;ok&quot; : 1} 结果表明，共有 5 个符合查询条件（status:&quot;active&quot;）的文档， 在map函数中生成了 5 个键值对文档，最后使用reduce函数将相同的键值分为 2 组。 具体参数说明： result：储存结果的collection的名字,这是个临时集合，MapReduce的连接关闭后自动就被删除了。 timeMillis：执行花费的时间，毫秒为单位 input：满足条件被发送到map函数的文档个数 emit：在map函数中emit被调用的次数，也就是所有集合中的数据总量 ouput：结果集合中的文档个数**（count对调试非常有帮助）** ok：是否成功，成功为1 err：如果失败，这里可以有失败原因，不过从经验上来看，原因比较模糊，作用不大 使用 find 操作符来查看 mapReduce 的查询结果： 12345678&gt;db.posts.mapReduce( function() { emit(this.user_name,1); }, function(key, values) {return Array.sum(values)}, { query:{status:&quot;active&quot;}, out:&quot;post_total&quot; }).find() 以上查询显示如下结果: 12{ &quot;_id&quot; : &quot;mark&quot;, &quot;value&quot; : 4 }{ &quot;_id&quot; : &quot;runoob&quot;, &quot;value&quot; : 1 } 用类似的方式，MapReduce可以被用来构建大型复杂的聚合查询。 Map函数和Reduce函数可以使用 JavaScript 来实现，使得MapReduce的使用非常灵活和强大。 二六、全文检索 全文检索对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。 这个过程类似于通过字典中的检索字表查字的过程。 MongoDB 从 2.4 版本开始支持全文检索，目前支持15种语言的全文索引。 danish dutch english finnish french german hungarian italian norwegian portuguese romanian russian spanish swedish turkish 启用全文检索 MongoDB 在 2.6 版本以后是默认开启全文检索的，如果你使用之前的版本，你需要使用以下代码来启用全文检索: 1&gt;db.adminCommand({setParameter:true,textSearchEnabled:true}) 或者使用命令： 1mongod --setParameter textSearchEnabled=true 创建全文索引 考虑以下 posts 集合的文档数据，包含了文章内容（post_text）及标签(tags)： 1234567{ &quot;post_text&quot;: &quot;enjoy the mongodb articles on Runoob&quot;, &quot;tags&quot;: [ &quot;mongodb&quot;, &quot;runoob&quot; ]} 我们可以对 post_text 字段建立全文索引，这样我们可以搜索文章内的内容： 1&gt;db.posts.ensureIndex({post_text:&quot;text&quot;}) 使用全文索引 现在我们已经对 post_text 建立了全文索引，我们可以搜索文章中的关键词 runoob： 1&gt;db.posts.find({$text:{$search:&quot;runoob&quot;}}) 以下命令返回了如下包含 runoob 关键词的文档数据： 12345{ &quot;_id&quot; : ObjectId(&quot;53493d14d852429c10000002&quot;), &quot;post_text&quot; : &quot;enjoy the mongodb articles on Runoob&quot;, &quot;tags&quot; : [ &quot;mongodb&quot;, &quot;runoob&quot; ]} 如果你使用的是旧版本的 MongoDB，你可以使用以下命令： 1&gt;db.posts.runCommand(&quot;text&quot;,{search:&quot;runoob&quot;}) 使用全文索引可以提高搜索效率。 删除全文索引 删除已存在的全文索引，可以使用 find 命令查找索引名： 1&gt;db.posts.getIndexes() 通过以上命令获取索引名，本例的索引名为post_text_text，执行以下命令来删除索引： 1&gt;db.posts.dropIndex(&quot;post_text_text&quot;) 二七、正则表达式 正则表达式是使用单个字符串来描述、匹配一系列符合某个句法规则的字符串。 许多程序设计语言都支持利用正则表达式进行字符串操作。 MongoDB 使用 $regex 操作符来设置匹配字符串的正则表达式。 MongoDB使用PCRE (Perl Compatible Regular Expression) 作为正则表达式语言。 不同于全文检索，我们使用正则表达式不需要做任何配置。 考虑以下 posts 集合的文档结构，该文档包含了文章内容和标签： 1234567{ &quot;post_text&quot;: &quot;enjoy the mongodb articles on runoob&quot;, &quot;tags&quot;: [ &quot;mongodb&quot;, &quot;runoob&quot; ]} 使用正则表达式 以下命令使用正则表达式查找包含 runoob 字符串的文章： 1&gt;db.posts.find({post_text:{$regex:&quot;runoob&quot;}}) 以上查询也可以写为： 1&gt;db.posts.find({post_text:/runoob/}) 不区分大小写的正则表达式 如果检索需要不区分大小写，我们可以设置 $options 为 $i。 以下命令将查找不区分大小写的字符串 runoob： 1&gt;db.posts.find({post_text:{$regex:&quot;runoob&quot;,$options:&quot;$i&quot;}}) 集合中会返回所有包含字符串 runoob 的数据，且不区分大小写： 12345{ &quot;_id&quot; : ObjectId(&quot;53493d37d852429c10000004&quot;), &quot;post_text&quot; : &quot;hey! this is my post on runoob&quot;, &quot;tags&quot; : [ &quot;runoob&quot; ]} 数组元素使用正则表达式 我们还可以在数组字段中使用正则表达式来查找内容。 这在标签的实现上非常有用，如果你需要查找包含以 run 开头的标签数据(ru 或 run 或 runoob)， 你可以使用以下代码： 1&gt;db.posts.find({tags:{$regex:&quot;run&quot;}}) 优化正则表达式查询 如果你的文档中字段设置了索引，那么使用索引相比于正则表达式匹配查找所有的数据查询速度更快。 如果正则表达式是前缀表达式，所有匹配的数据将以指定的前缀字符串为开始。例如： 如果正则表达式为 ^tut ，查询语句将查找以 tut 为开头的字符串。 这里面使用正则表达式有两点需要注意： 正则表达式中使用变量。一定要使用eval将组合的字符串进行转换，不能直接将字符串拼接后传入给表达式。否则没有报错信息，只是结果为空！实例如下： 1var name=eval(&quot;/&quot; + 变量值key +&quot;/i&quot;); 以下是模糊查询包含title关键词, 且不区分大小写: 1title:eval(&quot;/&quot;+title+&quot;/i&quot;) // 等同于 title:{$regex:title,$Option:&quot;$i&quot;} 二八、管理工具: Rockmongo RockMongo是PHP5写的一个MongoDB管理工具。 通过 Rockmongo 你可以管理 MongoDB服务，数据库，集合，文档，索引等等。 它提供了非常人性化的操作。类似 phpMyAdmin（PHP开发的MySql管理工具）。 Rockmongo 下载地址：https://github.com/iwind/rockmongo 简介 主要特征： 使用宽松的New BSD License协议 速度快，安装简单 支持多语言（目前提供中文、英文、日文、巴西葡萄牙语、法语、德语、俄语、意大利语） 系统 可以配置多个主机，每个主机可以有多个管理员 需要管理员密码才能登入操作，确保数据库的安全性 服务器 服务器信息 (WEB服务器, PHP, PHP.ini相关指令 ...) 状态 数据库信息 数据库 查询，创建和删除 执行命令和Javascript代码 统计信息 集合（相当于表） 强大的查询工具 读数据，写数据，更改数据，复制数据，删除数据 查询、创建和删除索引 清空数据 批量删除和更改数据 统计信息 GridFS 查看分块 下载文件 安装 需求 一个能运行PHP的Web服务器，比如Apache Httpd, Nginx ... PHP - 需要PHP v5.1.6或更高版本，需要支持SESSION 为了能连接MongoDB，你需要安装php_mongo扩展 快速安装 下载安装包 解压到你的网站目录下 用编辑器打开config.php，修改host, port, admins等参数 在浏览器中访问index.php，比如说：http://localhost/rockmongo/index.php 使用用户名和密码登录，默认为&quot;admin&quot;和&quot;admin&quot; 开始玩转MongoDB! 二九、GridFS GridFS 用于存储和恢复那些超过16M（BSON文件限制）的文件(如：图片、音频、视频等)。 GridFS 也是文件存储的一种方式，但是它是存储在MonoDB的集合中。 GridFS 可以更好的存储大于16M的文件。 GridFS 会将大文件对象分割成多个小的chunk(文件片段),一般为256k/个,每个chunk将作为MongoDB的一个文档(document)被存储在chunks集合中。 GridFS 用两个集合来存储一个文件：fs.files与fs.chunks。 每个文件的实际内容被存在chunks(二进制数据)中,和文件有关的meta数据(filename,content_type,还有用户自定义的属性)将会被存在files集合中。 以下是简单的 fs.files 集合文档： 1234567{ &quot;filename&quot;: &quot;test.txt&quot;, &quot;chunkSize&quot;: NumberInt(261120), &quot;uploadDate&quot;: ISODate(&quot;2014-04-13T11:32:33.557Z&quot;), &quot;md5&quot;: &quot;7b762939321e146569b07f72c62cca4f&quot;, &quot;length&quot;: NumberInt(646)} 以下是简单的 fs.chunks 集合文档： 12345{ &quot;files_id&quot;: ObjectId(&quot;534a75d19f54bfec8a2fe44b&quot;), &quot;n&quot;: NumberInt(0), &quot;data&quot;: &quot;Mongo Binary Data&quot;} GridFS 添加文件 现在我们使用 GridFS 的 put 命令来存储 mp3 文件。 调用 MongoDB 安装目录下bin的 mongofiles.exe工具。 打开命令提示符，进入到MongoDB的安装目录的bin目录中，找到mongofiles.exe，并输入下面的代码： 1&gt;mongofiles.exe -d gridfs put song.mp3 -d gridfs 指定存储文件的数据库名称，如果不存在该数据库，MongoDB会自动创建。如果不存在该数据库，MongoDB会自动创建。Song.mp3 是音频文件名。 使用以下命令来查看数据库中文件的文档： 1&gt;db.fs.files.find() 以上命令执行后返回以下文档数据： 1234567{ _id: ObjectId('534a811bf8b4aa4d33fdf94d'), filename: &quot;song.mp3&quot;, chunkSize: 261120, uploadDate: new Date(1397391643474), md5: &quot;e4f53379c909f7bed2e9d631e15c1c41&quot;, length: 10401959 } 我们可以看到 fs.chunks 集合中所有的区块，以下我们得到了文件的 _id 值，我们可以根据这个 _id 获取区块(chunk)的数据： 1&gt;db.fs.chunks.find({files_id:ObjectId('534a811bf8b4aa4d33fdf94d')}) 以上实例中，查询返回了 40 个文档的数据，意味着mp3文件被存储在40个区块中。 三十、固定集合（Capped Collections） MongoDB 固定集合（Capped Collections）是性能出色且有着固定大小的集合，对于大小固定，我们可以想象其就像一个环形队列，当集合空间用完后，再插入的元素就会覆盖最初始的头部的元素！ 创建固定集合 我们通过createCollection来创建一个固定集合，且capped选项设置为true： 1&gt;db.createCollection(&quot;cappedLogCollection&quot;,{capped:true,size:10000}) 还可以指定文档个数,加上max:1000属性： 1&gt;db.createCollection(&quot;cappedLogCollection&quot;,{capped:true,size:10000,max:1000}) 判断集合是否为固定集合: 1&gt;db.cappedLogCollection.isCapped() 如果需要将已存在的集合转换为固定集合可以使用以下命令： 1&gt;db.runCommand({&quot;convertToCapped&quot;:&quot;posts&quot;,size:10000}) 以上代码将我们已存在的 posts 集合转换为固定集合。 固定集合查询 固定集合文档按照插入顺序储存的,默认情况下查询就是按照插入顺序返回的,也可以使用$natural调整返回顺序。 1&gt;db.cappedLogCollection.find().sort({$natural:-1}) 固定集合的功能特点 可以插入及更新,但更新不能超出collection的大小,否则更新失败,不允许删除,但是可以调用drop()删除集合中的所有行,但是drop后需要显式地重建集合。 在32位机子上一个cappped collection的最大值约为482.5M,64位上只受系统文件大小的限制。 固定集合属性及用法 属性 属性1:对固定集合进行插入速度极快 属性2:按照插入顺序的查询输出速度极快 属性3:能够在插入最新数据时,淘汰最早的数据 用法 用法1:储存日志信息 用法2:缓存一些少量的文档 三一、自动增长 MongoDB 没有像 SQL 一样有自动增长的功能， MongoDB 的 _id 是系统自动生成的12字节唯一标识。 但在某些情况下，我们可能需要实现 ObjectId 自动增长功能。 由于 MongoDB 没有实现这个功能，我们可以通过编程的方式来实现，以下我们将在 counters 集合中实现_id字段自动增长。 使用 counters 集合 考虑以下 products 文档。我们希望 _id 字段实现 从 1,2,3,4 到 n 的自动增长功能。 12345{ &quot;_id&quot;:1, &quot;product_name&quot;: &quot;Apple iPhone&quot;, &quot;category&quot;: &quot;mobiles&quot;} 为此，创建 counters 集合，序列字段值可以实现自动长： 1&gt;db.createCollection(&quot;counters&quot;) 现在我们向 counters 集合中插入以下文档，使用 productid 作为 key: 1234{ &quot;_id&quot;:&quot;productid&quot;, &quot;sequence_value&quot;: 0} sequence_value 字段是序列通过自动增长后的一个值。 使用以下命令插入 counters 集合的序列文档中： 1&gt;db.counters.insert({_id:&quot;productid&quot;,sequence_value:0}) 创建 Javascript 函数 现在，我们创建函数 getNextSequenceValue 来作为序列名的输入， 指定的序列会自动增长 1 并返回最新序列值。在本文的实例中序列名为 productid 。 123456789&gt;function getNextSequenceValue(sequenceName){ var sequenceDocument = db.counters.findAndModify( { query:{_id: sequenceName }, update: {$inc:{sequence_value:1}}, &quot;new&quot;:true }); return sequenceDocument.sequence_value;} 使用 Javascript 函数 接下来我们将使用 getNextSequenceValue 函数创建一个新的文档， 并设置文档 _id 自动为返回的序列值： 123456789&gt;db.products.insert({ &quot;_id&quot;:getNextSequenceValue(&quot;productid&quot;), &quot;product_name&quot;:&quot;Apple iPhone&quot;, &quot;category&quot;:&quot;mobiles&quot;})&gt;db.products.insert({ &quot;_id&quot;:getNextSequenceValue(&quot;productid&quot;), &quot;product_name&quot;:&quot;Samsung S3&quot;, &quot;category&quot;:&quot;mobiles&quot;}) 就如你所看到的，我们使用 getNextSequenceValue 函数来设置 _id 字段。 为了验证函数是否有效，我们可以使用以下命令读取文档： 1&gt;db.products.find() 以上命令将返回以下结果，我们发现 _id 字段是自增长的： 123{ &quot;_id&quot; : 1, &quot;product_name&quot; : &quot;Apple iPhone&quot;, &quot;category&quot; : &quot;mobiles&quot;}{ &quot;_id&quot; : 2, &quot;product_name&quot; : &quot;Samsung S3&quot;, &quot;category&quot; : &quot;mobiles&quot; }","link":"/2021/01/20/Draft/2021/MongoDB/"},{"title":"Docker","text":"Docker，开源基于GO语言的应用容器引擎，可对应用和相关依赖包打包到轻量可以移植的Docker容器中，可发布到任何Linux中，容器性能开销极低。 新学四问 WHY【与前代优化了什么，弥补了什么空白】：自动化开发部署流程，环境 WHAT【框架，思维导图，主题框架】：安装实例， HOW【如何记忆，学习资源（官方文档、视频资源、项目地址）】：Docker 教程 LEVEL【不是每个都学精】：熟练操作 进度： 【】 一、简介 是什么？ 问题：为什么会有 docker 的出现 一款产 品从开发到上线，从操作系统，到运行环境，再到应用配置。作为开发+运维之间的协作我们需要关心很多东西，这也是很多互联网公司都不得不面对的问题，特别是各种版本的迭代之后，不同版本环境的兼容，对运维人员都是考验 Docker之所以发展如此迅速，也是因为它对此给出了一个标准化的解决方案。 环境配置如此麻烦，换一台机器，就要重来一次，费力费时。很多人想到，能不能从根本上解决问题，软件可以带环境安装?也就是说，安装的时候，把原始环境-模-样地复制过来。开发人员利用Docker可以消除协作编码时“在我的机器上可正常工作”的问题。 之前在服务器配置一个应用的运行环境，要安装各种软件，就拿尚硅谷电商项目的环境来说吧，Java/TomcatMySQL/JDBC驱动包等。安装和配置这些东西有多麻烦就不说了，它还不能跨平台。假如我们是在Windows上安装的这些环境，到了Linux 又得重新装。况且就算不跨操作系统，换另一台同样操作系统的服务器，要移植应用也是非常麻烦的。 传统上认为，软件编码开发/测试结束后，所产出的成果即是程序或是能够编译执行的二进制字节码等java为例)。而为了让这程序可以顺利执行，开发团队也得准备完整的部署文件，让维运团队得以部署应用程式，开发需要清楚的告诉运维部署团队，用的全部配置文件+所有软件环境。不过，即便如此，仍然常常发生部署失败的状况。Docker镜 像的设计，使得Docker得以打过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运.作。 docker理念 Docker是基于Go语言实现的云开源项目。 Docker的主要目标是“Build, Ship[ and Run Any App,Anywhere&quot;，也就是通过对应用组件的封装、分发、部署、运行等生命期的管理，使用户的APP (可以是一个WEB应用或数据库应用等等)及其运行环境能够做到“一次封装，到处运行”。 Linux容器技术的出现就解决了这样一 一个问题，而Docker就是在它的基础上发展过来的。将应用运行在Docker容器上面，而Docker容器在任何操作系统上都是一-致的，这就实现了跨平台、跨服务器。只需要一次配置好环境，换到别的机子上就可以一键部署好，大大简化了操作 一句话 解决了运行环境和配置问题的软件容器，方便做持续集成并有助于整体发布的容器虚拟化技术 能干嘛 之前的虚拟机技术 虚拟机**(virtual machine)**就是带环境安装的一种解决方案。 它可以在一种操作系统里面运行另一种作系统，比如在Windows系统里面运行Linux系统。应用程序对此毫无感知，因为虚拟机看上去跟真实系统- -模-样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。 虚拟机的缺点: 1、资源占用多 2、冗余步骤多 3、启动慢 容器虚拟化技术 由于前面虛拟机存在这些缺点，Linux 发展出了另一种虚拟化技术: Linux 容器(Linux Containers,缩为LXC)。 Linux容器不是模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。. 比较了Docker和传统虚拟化方式的不同之处: 1、传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程; 2、而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机为轻便。 3、每个容器之间互相隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源。 开发/运维(DevOps) 一次构建、随处运行， 更快速的应用交付和部署 ​ 传统的应用开发完成后，需要提供一堆安装程序和配置说明文档，安装部署后需根据配置文档进行繁杂的配置才能正常运行。Docker化 之后只需要交付少量容器镜像文件，在正式生产环境加载镜像并运行即可，应用安装配置在镜像里已经内置好，大大节省部署配置和测 试验证时间。 更便捷的升级和扩缩容 ​ 随着微服务架构和Docker的发展，大量的应用会通过微服务方式架构，应用的开发构建将变成搭乐高积木一样，每个Docker容器将变成-块“积木”，应用的升级将变得非常容易。当现有的容器不足以支撑业务处理时，可通过镜像运行新的容器进行快速扩容，使应用系统的扩容从原先的天级变成分钟级甚至秒级。 更简单的系统运维 ​ 应用容器化运行后，生产环境运行的应用可与开发、测试环境的应用高度--致，容器会将应用程序相关的环境和状态完全封装起来，不会因为底层基础架构和操作系统的不一致性给应用带来影响，产生新的BUG。当出现程序异常时，也可以通过测试环境的相同容器进行快速定位和修复。 更高效的计算资源利用 ​ Docker是内核级虚拟化，其不像传统的虚拟化技术一样 需要额外的Hypervisor支持，所以在-台物理机上可以运行很多个容器实例，可大大提升物理服务器的CPU和内存的利用率。 企业级 新浪 美团 蘑菇街 去哪下 1、官网 docker官网： https://www.docker.com/ docker中文网站: https://www.docker-cn.com/ 2、仓库 Docker Hub官网：https://hub.docker.com/ 1.应用场景 Web 应用的自动化打包和发布。 自动化测试和持续集成、发布。 在服务型环境中部署和调整数据库或其他的后台应用。 从头编译或者扩展现有的 OpenShift 或 Cloud Foundry 平台来搭建自己的 PaaS 环境。 2.优点 Docker 是一个用于开发，交付和运行应用程序的开放平台。Docker 使您能够将应用程序与基础架构分开，从而可以快速交付软件。借助 Docker，您可以与管理应用程序相同的方式来管理基础架构。通过利用 Docker 的方法来快速交付，测试和部署代码，您可以大大减少编写代码和在生产环境中运行代码之间的延迟。 1、快速，一致地交付您的应用程序 Docker 允许开发人员使用您提供的应用程序或服务的本地容器在标准化环境中工作，从而简化了开发的生命周期。 容器非常适合持续集成和持续交付（CI / CD）工作流程，请考虑以下示例方案： 您的开发人员在本地编写代码，并使用 Docker 容器与同事共享他们的工作。 他们使用 Docker 将其应用程序推送到测试环境中，并执行自动或手动测试。 当开发人员发现错误时，他们可以在开发环境中对其进行修复，然后将其重新部署到测试环境中，以进行测试和验证。 测试完成后，将修补程序推送给生产环境，就像将更新的镜像推送到生产环境一样简单。 2、响应式部署和扩展 Docker 是基于容器的平台，允许高度可移植的工作负载。Docker 容器可以在开发人员的本机上，数据中心的物理或虚拟机上，云服务上或混合环境中运行。 Docker 的可移植性和轻量级的特性，还可以使您轻松地完成动态管理的工作负担，并根据业务需求指示，实时扩展或拆除应用程序和服务。 3、在同一硬件上运行更多工作负载 Docker 轻巧快速。它为基于虚拟机管理程序的虚拟机提供了可行、经济、高效的替代方案，因此您可以利用更多的计算能力来实现业务目标。Docker 非常适合于高密度环境以及中小型部署，而您可以用更少的资源做更多的事情。 二、安装 安装方法一 前提说明 CentOS Docker安装 Docker支持以下的CentOS版本: CentOS 7 (64-bit) CentOS 6.5 (64-bit)或更高的版本 前提条件 目前，CentOS 仅发行版本中的内核支持Docker。 Docker运行在CentOS 7.上，要求系统为64位、系统内核版本为3.10以上。 Docker运行在CentOS-6.5或更高的版本的CentOS上，要求系统为64位、系统内核版本为2.6.32-431或者更高版本。 Docker 的基本组成 docker架构图 镜像( image ) Docker镜像(lmage)就是-一个只读的模板。镜像可以用来创建Docker容器，个镜像可以创建很多容器 容器( container) Docker利用容器(Container) 独立运行的一个或一组应用。容器是用镜像创建的运行实例。 它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。 可以把容器看做是一个简 易版的Linux环境(包括root用户权限、进程空间、用户空间和网络空间等)和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角， 唯- -区别在于容器的最上面那-层是可读可写的。 仓库( repository) 仓库(Repository) 是集中存放镜像文件的场所。 仓库(Repository)和仓库注册服务器(Registry) 是有区别的。仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多镜像， 每个镜像有不同的标签(tag) 。 仓库分为公开仓库(Public) 和私有仓库(Private) 两种形式。 最大的公开仓库是Docker Hub(ttps://hub. docker.com/) 存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云、网易云等 小总结 () 需要正确的理解仓储/镜像/容器这几个概念: Docker本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一-个可交付的运行环境，这个打好的运行环境就似乎image镜像文件。只有通过这个镜像文件才能生成Docker容器。image文件可以看作是容器的模板。Docker根据image文件生成容器的实例。同一个image文件，可以生成多个同时运行的容器实例。 image文件生成的容器实例，本身也是一一个文件，称为镜像文件。 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一-个对应的运行实例，也就是我们的容器至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。| 2.1 卸载系统之前安装的docker Uninstall old versions. 一步一步往下执行就行 12345678sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine SET UP THE REPOSITORY Install the yum-utils package (which provides the yum-config-manager utility) and set up the stable repository. 1234sudo yum install -y yum-utilssudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo INSTALL DOCKER ENGINE 1.Install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: 1sudo yum install docker-ce docker-ce-cli containerd.io Start Docker 1sudo systemctl start docker Verify that Docker Engine is installed correctly by running the hello-world image. 1sudo docker run hello-world 设置 docker 开机自启动 1sudo systemctl enable docker 设置 Docker 镜像加速 登录 aliyun.com 在控制台找到容器镜像服务 docker 进入 对应容器 123docker ps #查看主机有哪些容器docker exec -it 容器ID/NAME /bin/bash #进入对应容器exit; #退出容器[快捷键 ctrl + p/q] image文件生成的容器实例，本身也是一一个文件，称为镜像文件。 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一-个对应的运行实例，也就是我们的容器至于仓储，就是放了一堆镜像的地方，我们可以把镜像发布到仓储中，需要的时候从仓储中拉下来就可以了。| 安装方法二 Centos6.8安装Docker 1、yum install -y epel-release 2、yum install -y docker-io 3、安装后的配置文件： etc/sysconfig/docker 4、启动 Docker后台服务: service docker start 5、docker version 验证 Centos7.0安装Docker https://docs.docker.com/engine/install/centos/ 永远的helloworld 阿里云镜像加速 是什么 ​ https://promotion.aliyun.com/ntms/act/kubernetes.html 注册一个属于自己的阿里云账户( 可复用淘宝账号) 获得加速器地址连接 ​ 登录阿里云开发者平台 ​ 获取加速器地址 配置本机Docker运行镜像加速器 ​ 鉴于国内网络问题，后续拉取Docker镜像十分缓慢，我们可以需要配置加速器来解决， 我使用的是阿里云的本人自己账号的镜像地址(需要自己注册有一个属于你自己的): ht:po/. mirror aliyuncns .com vim /etc/sysconfig/docker 将获得的自己账户下的阿里云加速地址配置进 other_ args-=&quot;--registry-mirror=https://你自 己的账号加速信息.mirror .aliyuncs.com 重新启动 Docker 后台服务：service docker restart Linux系统下配置完加速器需要检查是否生效 网易云加速 基本上同上述阿里云 启动Docker后台容器(测试运行 hello-world ) ​ docker run hello world run干了什么 底层原理 Docker是怎样工作的 Docker是一个Client-Server结构的系统，Docker守 护进程运行在主机上，然后通过Socket连 接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器。容器，是一个运行时环境，就是我们前面说到的集装箱。 为什么Docker比较比vm快 1、docker有着比虚拟机更少的抽象层。由亍docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 2、docker利用的是宿主机的内核,而不需要Guest OS。因此,当新建一个 容器时,docker不需要和虚拟机一样 重新加载- - 个操作系统内核仍而避免引寻、加载操作系统内核返个比较费时费资源的过程,当新建--个虚拟机时,虚拟机软件需要加载GuestOS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返个过程,因此新建一-个docker容器只需要几秒钟。 三、 Docker常用命令 帮助命令 123456docker Versiondocker infodocker --help 自己查看官网解释，高手都是自己练出来的，百度上只不过是翻译了下，加了点例子 镜像命令 docker images 列出本机上的镜像 OPTIONS 说明： 1234-a 列出本地所有的镜像(含中间映射层)-q 只显示镜像ID--digests 显示镜像的摘要信息--no-trunc 显示完整的镜像信息 docker search 某个XXX镜像的名字 ​ 网站 www.dockerhub.com ​ docker search [OPTIONS] 镜像名字 ​ OPTIONS 说明 123--no-trun 显示完整的镜像描述-s 列出收藏数不小于指定值的镜像--automated 只列出 automated build类型的镜像 docker pull 某个镜像的名字 ​ 下载镜像 ​ docker pull 镜像名字[:TAG] docker rmi 某个XXX镜像的名字ID ​ 删除镜像 ​ 删除单个 docker rm -f 镜像ID ​ 删除多个 docker rm -f 镜像名1:TAG 镜像名2:TAG ​ 删除多个 docker rmi -f $ 容器命令 有镜像才能创建容器，这是根本前提(下载一个Centos镜像演示) ​ docker pull centos 新建并启动容器 ​ docker run [OPTIONS] IMAGE [COMMAND][ARG] ​ OPTIONS 说明 123456789101112OPTIONS说明(常用) :有些是一个减号，有些是两个减号--name=&quot;容器新名字&quot;:为容器指定一个名称;-d:后台运行容器，并返回容器ID， 也即启动守护式容器;-i:以交互模式运行容器，通常与-t同时使用;-t:为容器重新分配一个伪输入终端，通常与-i同时使用;-P:随机端口映射;-p:指定端口映射，有以下四种格式ip:hostPort:containerPortip::containerPorthostPort:containerPortcontainerPort 列出当前所有正在运行的容器 ​ docker ps [OPTIONS] 1234567OPTIONS说明(常用) :-a :列出当前所有正在运行的容器+历史上运行过的-|:显示最近创建的容器。-n:显示最近n个创建的容器。-q :静默模式，只显示容器编号。--no-trunc :不截断输出。 退出容器 两种退出方式 ​ exit 容器停止退出 ​ ctrl+P+Q 容器不停止退出 启动容器 docker start 容器ID或容器签名 重启容器 docker restart 容器ID或容器签名 停止容器 docker stop 容器ID或容器签名 强制停止容器 docker kill 容器ID或容器签名 删除已停止的容器 docker rm 容器ID -f ​ 一次性删除多个容器 ​ docker rm -f $(docker ps -a -q) ​ docker ps -a -q | xargs docker rm 重要 启动守护式容器 #使用镜像centos:latest以后台模式启动一个容器 docker run -d centos 问题:然后docker ps -a进行查看,会发现容器已经退出 很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程. 容器运行的命令如果不是那些一直挂起的命令 (比如运行top，tail) ，就是会自动退出的。 这个是docker的机制问题,比如你的web容器，我们以nginx为例，正常情况下,我们配置启动服务只需要启动响应的service即可。例如 service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用,这样的容器后台启动后，会立即自杀因为他觉得他没事可做了.所以，最佳的解决方案是将你要运行的程序以前台进程的形式运行 查看容器日志 docker logs -f -t --tail 容器ID ​ -t 是加入时间戳 ​ -f 跟随最新的日志打印 ​ --tail 数字显示最后多少条 查看容器内的进程 docker top 容器ID 查看容器内部细节 docker inspect 容器ID 进入正在运行的容器并以命令行交互 docker exec -it 容器ID bashShell 重新进入docker attach 容器ID 上述两个区别 attach 直接进入容器启动命令的终端，不会启动新的进程 exec 实在容器中打开新的终端，并且可以穷的那个新的进程 从容器内拷贝文件到主机上 docker cp 容器ID:容器内路径 目的主机路径 小总结 第 、 Docker 镜像 是什么 镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件，它包含运行某个软件所需的有内容，包括代码、运行时、库、环境变量和配置文件。 UnionFS(联合文件系统) UnionFS (状节又件示统) UnionFS (联合文件系统) : Union文件系统(UnionFS)是一一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修作为一 次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a singlevirtualfilesystem)。Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像(没有父镜像)可以制作各种具.体的应用镜像。 特性:一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文 件系统会包含所有底层的文件和目录 Docker镜像加载原理 Docker镜像加载原理: docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。 botfs(boot file system)主要包含bootloader和kernel, bootloader主 要是引导加载kernel, Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一-层与我们典型的Linux/Unix系统是- - -样的，包含boot加载器和内核。当boot加载完成之 后整个内核就都在内存中了，此时内存的使用权己由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system)，在bootfs之 上。 包含的就是典型Linux系统中的**/dev, /proc, /bin, /etc等标准目录和文件。rootfs就 是各种不同的操作系统发行版，比如Ubuntu**，Centos等等。 平时我们安装的虚拟机的Centos都是好几个G ，为什么docker这里才要200m 对于一个精简的OS, rootfs可 以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel,自只需要提供rootfs就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别，因此不同的发行版可以公用bootfs。 分层的镜像 分层的镜像 为什么 Docker纪念馆想要采用这种分层结构 最大的一个好处就是-共享资源 比如:有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像, 同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 特点 Docker镜像都是只读的，当容器启动时，一个新的可写层被加载到镜像的顶部，这一层通常被称为容器层，容器层之下都叫镜像层 Docker镜像Commit操作 docker commit 提交容器副本使之称为一个新的镜像 docker commit -m=&quot;提交的描述信息&quot; -a=&quot;作者&quot; 容器ID 要创建的目标镜像名:[标签名] 案例演示： 1、从Hub上下载tomcat镜像到本地并成功运行 docker run -d -p 8080:8080 tomcat 1234-p主机端口：docker容器端口-P:随机分配端口i:交互t:终端 2、故意删除上一步镜像生产tomcat容器的文档 3、也即当前的tomcat运行实例是一个没有文档内容的容器，以他为模板commit一个没有doc的tomcat新镜像 atguigu/tomcat02 4、启动我们的新镜像并和原来的对比 ​ 启动atuigu/tomcat02 没有doc ​ 启动原来tomcat他有doc 五 、Docker容器数据卷 是什么 先来看看Docker的理念: *将运用与运行的环境打包形成容器运行，运行可以伴随着容器，但是我们对数据的要求希望是持久化的 *容器之间希望有可能共享数据 Docker容器产生的数据，如果不通过docker commit生成新的镜像，使得数据做为镜像的一部分保存下来， 那么当容器删除后，数据自然也就没有了。 为了能保存数据在docker中我们使用卷。| 一句话：有点类似我们Redis里面的rdb和aof文件 能干嘛 卷就是目录或文件，存在于一个或多个容器中，由docker挂载到容器，但不属于联合文件系统，因此能够绕过Union FileSystem提供一些用 于持续存储或共享数据的特性: 卷的设计目的就是数据的持久化，完全独立于容器的生存周期，因此Docker不 会在容器删除时删除其挂载的数据卷 特点: 1:数据卷可在容器之间共享或重用数据 2:卷中的更改可以直接生效 3:数据卷中的更改不会包含在镜像的更新中 4:数据卷的生命周期一直持续到没有容器使用它为止 容器的持久化 容器间继承+共享数据 数据卷 容器内添加 直接命令添加 docker run -it -v /宿主机绝对路径目录:/容器内目录 镜像名 查看数据卷是否挂载成功 容器和宿主机之间数据共享 容器停止退出后，主机修改后的数据是否同步 命令(带权限) ​ docker run -it -v /宿主机绝对路径目录:/容器内目录**:ro** 镜像名 DockerFile添加 根目录下新建mydocker文件夹并进入 可在Dockerfile中使用VOLUME指令来给镜像添加一个或多个数据卷 File构建 build后生成镜像 获得一个新镜像zzyy/centos run容器 通过上述步骤，容器内的卷目录地址已经知道，对应的主机目录在哪 主机对应默认地址 备注 Docker挂载主机目录Docker访问出现cannot open directory . Permission denied 解决办法:在挂载目录后多加一个--privileged=true参数即可 数据卷容器 是什么 命名的容器挂载数据卷，其它容器通过挂载这个(父容器)实现数据共享，挂载数据卷的容器，称之为数据卷容器. 总体介绍 以上一步新建的zzyy/centos为模板并运行容器 doc1/doc2/doc3 他们已经具有容器卷 ​ /dataVolumeContainer1 ​ /dataVolumeContainer2 容器间传递共享(--volumes -from) 先启动一个父容器doc1 启动后在 dataVolumeContainer1中新增内容 doc2/doc3 继承doc1 ​ --volumes -from doc2/doc3 分别在dataVolumeContainer2各自新增内容 回到doc1可以看到02/03各自添加的都能共享了 删除doc1 doc2修改后doc3是否可以访问 删除doc02后doc3是否访问 在进一步 新建doc04继承doc03 然后删除doc03 结论：容器之间配置信息的传递，数据卷的生命周期一直持续到没有容器使用它为止 六 、 DockerFile解析 是什么 Dockerfile是用来构建Docker镜像的构建文件，由一系列命令和参数构成的脚本 构建三步骤 ​ 编写Dockerfile文件 ​ docker build ​ docker run 文件什么样？？？ ​ 熟悉的Centos为例 http://hub.docker.com/_/centos DockerFile构建过程解析 Dockerfile内容基础知识 1、每条保留字指令都必须为大写字母且后面要跟随至少一个参数 2、 指令按照从.上到下，顺序执行 3、#表示注释 4、每条指令都会创建一个新的镜像层，并对镜像进行提交 Docker执行Dockerfile的大致流程 1、 docker 从基础镜像运行一个容器 2、执行一-条指令并对容器作出修改 3、执行类似docker commit的操作提交- -个新的镜像层 4、docker再基 于刚提交的镜像运行一一个新容器 5、执行dockerfile中的 下一条指令直到所有指令都执行完成 小总结 从应用软件的角度来看，Dockerfile、 Docker镜像与Docker容器分别代表软件的三个不同阶段， Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件的运行态。 Dockerfile面向开发，Docker镜 像成为交付标准，Docker容 器则涉及部署与运维，三者缺- -不可，合力充当Docker体系的基石。 1、Dockerfile，需要定义一个Dockerfile，Dockerfile定 义了进程需要的一切东西。Dockerfile涉 及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; 2、Docker镜像，在用Dockerfile定义一文件之后，docker build时会产生- -个Docker镜像，当运行Docker镜像时，会真正开始提供服务; 3、Docker容器，容器是直接提供服务的。 DockerFile体系结构(保留字指令) 小总结 案例 Base 镜像(scratch) Docker Hub中 99%的镜像都是通过在base镜像中安装和配置需要的软件构建出来的 自定义镜像mycentos 1、编写 ​ Hub默认Centos镜像是什么情况 准备Dockerfile文件 myCentOS内容Dockerfile 1234567891011121314FROM centosMAINTAINER ZZYY&lt;zzyy167@126.com&gt;ENV MYPATH /usr/localWORKDIR $MYPATHRUN yum -y install vimRUN yum -y install net-toolsEXPOSE 80CMD echo $MYPATHCMD echo &quot;success--------------ok&quot;CMD /bin/bash 2、构建 docker build -t 新镜像名字:TAG . 3、运行 docker run -it 新镜像名字:TAG 4、列出镜像的变更历史 docker history 镜像名 CMD/ENTRYPOINT 镜像案例 都是指定一个容器启动时要运行的命令 CMD ​ Dockerfile中可以有多个CMD指令，但只有最后一个生效，CMD会被dockerrun之后的参数替换 ​ Case ​ tomcat的讲解演示 docker run -it -p 8080:8080 tomcat ls -l ENTRYPOINT ​ docker run 之后的参数会被当做参数传递给 ENTRYPOINT 之后形成新的命令组合 ​ Case 制作CMD版可以查询IP信息的容器 curl的命令解释 curl命令可以用来执行下载、发送各种HTTP请求，指定HTTP头部等操作。 如果系统没有curl可以使用yum install curl安装，也可以下载安装。 curl是将下载文件输出到stdout 使用命令: curl http://www .baidu.com 执行后，www.baidu.com的html就会显示在屏幕上了 这是最简单的使用方法。用这个命令获得了htp://curl.haxx.se指向的页面，同样，如果这里的URL指向的是--个文件或者一幅图都可以直接下载到本地。如果下载的是HTML文档，那么缺省的将只显示文件头部，即HTML文档的header。要全部显示，请加参数-i WHY 我们可以看到可执行文件找不到的报错，executable file not found。 之前我们说过，跟在镜像名后面的是command,运行时会替换CMD的默认值。 因此这里的-i替换了原来的CMD，而不是添加在原来的curl -s htp://ip.cn后面。而-i 根本不是命令，所以自然找不到。 那么如果我们希望加入-i这参数，我们就必须重新完整的输入这个命令: $ docker run myip curl -s http://ip.cn -i 自定义镜像Tomcat 1、mkdir -p /zzyy/mydockerfile/tomcat9 2、在上述目录下 touch c.txt 3、将jdk和tomcat安装的压缩包拷贝进上一步目录 4、在zzyyuse/mydockerfile/tomcat9目录下新建Dockerfile文件 123456789101112131415161718192021222324FROM centosMAINTAINER zzyy&lt;zzyybs@ 126.com&gt;#把宿主机当前上下文的c .txt拷贝到容器/usr/local/路径下COPY c.txt /usr/local/cincontainer.txt#把java与tomcat添加到容器中ADD jdk-8u171-linux x64.tar .gz /usr/local/ADD apache-tomcat-9.0.8.tar.gz /usr/ocal/#安装vim编辑器RUN yum -y install vim#设置工 作访问时候的WORKDIR路径， 登录落脚点ENV MYPATH /usr/localWORKDIR $MYPATH#配:置java与tomcat环境变量ENV JAVA_ HOME /usr/localjdk1 .8.0_ 171ENV CLASSPATH $JAVA_ HOME/lib/dt.jar:$JAVA_ HOME/lib/tools.jarENV CATALINA_ HOME /usr/local/apache-tomcat-9.0.8ENV CATALINA_ BASE /usr/ocal/apache-tomcat-9.0.8ENV PATH $PATH:$JAVA_ HOME/bin:$CATALINA_ HOME/ib:$CATALINA_ HOME/bin#容器运行时监听的端口EXPOSE 8080#启动时运行tomcat# ENTRYPOINT [&quot;/usrl/local/apache-tomcat-9.0.8/bin/startup.sh&quot; ]# CMD [&quot;/usr/local/apache-tomcat-9.0.8/bin/catalina.sh&quot;,&quot;run&quot;]CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/in/logs/catalina.out 目录内容 5、构建 构建完成 6、run 123docker run -d -p 9080:8080 -name myt9 -v /zyuse/mydockerfiletomcat9/test:/usrlocal/apache-tomcat9.0.8/webapps/test -v /zzyyuse/mydockerfile/tomcat9/tomcat9logs/:/usrlocal/apache-tomcat-9.0.8/logs -privileged=true zzyytomcat9 备注 Docker挂载主机目录Docker访问出现cannot open directory : Permission denied解决办法:在挂载目录后多加一个--privileged=true参数即可 7、验证 8、综合前 述容器卷测试的web服务test发布 web.xml 12345678&lt;?xml version=&quot;1 .0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmIns:xsi=&quot;http://www.w3.org/2001/XML Schema-instance&quot;xmIns=&quot;http://java sun.com/xm/ns/javaee&quot;xsi:schemaL ocation=&quot;http://java. sun.com/xml/ns/javaee htp:/:/java. sun.com/xml/ns/javaee/web-app_ 2_ _5.xsd&quot;id=&quot;WebApp_ ID&quot; version=&quot;2.5&quot;&gt;&lt;display-name&gt;test&lt;/display-name&gt;&lt;/web-app&gt; a.jsp 123456789101112131415&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN&quot; http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here &lt;/title&gt;&lt;/head&gt;&lt;body&gt;welcome-&lt;%=&quot;i am in docker tomcat self &quot;%&gt;&lt;br&gt;&lt;br&gt;&lt;% System.out,.printIn(&quot;==========docker tomcat self&quot;);%&gt;&lt;/body&gt;&lt;/htmI&gt; 测试 123456789101112131415&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot; pageEncoding=&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html PUBLIC“//W3C//DTD HTML 4.01 Transitional//EN&quot; http://www.w3.org/TR/html4/loose.dtd&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot;&gt;&lt;title&gt;Insert title here &lt;/title&gt;&lt;/head&gt;&lt;body&gt;welcome-&lt;%=&quot;i am in docker tomcat self &quot;%&gt;&lt;br&gt;&lt;br&gt;&lt;% System.out,.printIn(&quot;==========docker tomcat self&quot;);%&gt;&lt;/body&gt;&lt;/htmI&gt; 小总结 七 、 Docker常用安装 总体步骤 搜索镜像 拉取镜像 查看镜像 启动镜像 停止容器 移除容器 安装Mysql docker hub 上查找mysql镜像 从 docker hub(阿里云加速器)拉取mysql镜像到本地标签为5.6 使用mysql5.6镜像创建容器(也叫运行镜像) 使用mysql镜像 12345678910111213141516171819docker run -p 12345:3306 --name mysql -v /ggcc/mysql/conf:/etc/mysql/conf.d -v /ggcc/mysql/logs:/logs -v /ggcc/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6----------------------------------------------命令说明:-p 12345:3306:将主机的12345端口映射到docker容器的3306端口。-name mysq:运行服务名字-V /ggcc/mysql/conf:/etc/mysql/conf.d :将主机/zzyyuse/mysq|录下的conf/my.cnf挂载到容器的/etc/mysql/conf.d-v /ggcc/mysqlogs/logs: 将主机/zzyyuse/mysq|目 录下的logs 目录挂载到容器的/logs。-V /ggcc/mysqldata:/var/lib/mysql :将主机lzzyyuse/mysql目录下的data目录挂载到容器的/var/lib/mysql .-e MYSQL_ ROOT_ PASSWORD=123456: 初始化root用户的密码。.-d mysql:5.6:后台程序运行mysql5.6 ----------------------------------------------docker exec -it Mysql运行成功后的容器ID /bin/bash----------------------------------------------数据备份小测试docker exec mysql服务容器ID sh -c 'exec mysqldump --all-databases -uroot -p&quot;123456&quot;' &gt;/ggcc/all-database.sql 数据备份测试 安装Redis 从docker hu上(阿里云加速器)拉取redis镜像到本地标签为：3.2 使用redis3.2镜像创建容器(也叫运行镜像) ​ 使用镜像 12docker run -p 6379:6379 -v /ggcc/myredis/data:/data -v /ggcc/myredis/conf/redis.conf:/usr/local/etc/redis/redis.conf -d redis redis-server /usr/local/etc/redis/redis.conf --appendonly yes 在主机/ggcc/myredis/conf/redis.conf目录上新建redis.conf文件 vim /ggcc/myredis/conf/redis.conf/redis.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748174917501751175217531754175517561757175817591760176117621763176417651766176717681769177017711772177317741775177617771778177917801781178217831784178517861787178817891790179117921793179417951796179717981799180018011802180318041805180618071808180918101811181218131814181518161817181818191820182118221823182418251826182718281829183018311832183318341835183618371838183918401841184218431844184518461847184818491850185118521853185418551856185718581859186018611862# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Notice option &quot;include&quot; won't be rewritten by command &quot;CONFIG REWRITE&quot;# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you'd better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no &quot;bind&quot; configuration directive is specified, Redis listens# for connections from all the network interfaces available on the server.# It is possible to listen to just one or multiple selected interfaces using# the &quot;bind&quot; configuration directive, followed by one or more IP addresses.## Examples:## bind 192.168.1.100 10.0.0.1# bind 127.0.0.1 ::1## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only into# the IPv4 loopback interface address (this means Redis will be able to# accept connections only from clients running into the same computer it# is running).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#bind 127.0.0.1# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# &quot;bind&quot; directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the &quot;bind&quot; directive.protected-mode yes# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need an high backlog in order# to avoid slow clients connections issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /tmp/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Take the connection alive from the point of view of network# equipment in the middle.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# TLS/SSL ###################################### By default, TLS/SSL is disabled. To enable it, the &quot;tls-port&quot; configuration# directive can be used to define TLS-listening ports. To enable TLS on the# default port, use:## port 0# tls-port 6379# Configure a X.509 certificate and private key to use for authenticating the# server to connected clients, masters or cluster peers. These files should be# PEM formatted.## tls-cert-file redis.crt # tls-key-file redis.key# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:## tls-dh-params-file redis.dh# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL# clients and peers. Redis requires an explicit configuration of at least one# of these, and will not implicitly use the system wide configuration.## tls-ca-cert-file ca.crt# tls-ca-cert-dir /etc/ssl/certs# By default, clients (including replica servers) on a TLS port are required# to authenticate using valid client side certificates.## If &quot;no&quot; is specified, client certificates are not required and not accepted.# If &quot;optional&quot; is specified, client certificates are accepted and must be# valid if provided, but are not required.## tls-auth-clients no# tls-auth-clients optional# By default, a Redis replica does not attempt to establish a TLS connection# with its master.## Use the following directive to enable TLS on replication links.## tls-replication yes# By default, the Redis Cluster bus uses a plain TCP connection. To enable# TLS for the bus protocol, use the following directive:## tls-cluster yes# Explicitly specify TLS versions to support. Allowed values are case insensitive# and include &quot;TLSv1&quot;, &quot;TLSv1.1&quot;, &quot;TLSv1.2&quot;, &quot;TLSv1.3&quot; (OpenSSL &gt;= 1.1.1) or# any combination. To enable only TLSv1.2 and TLSv1.3, use:## tls-protocols &quot;TLSv1.2 TLSv1.3&quot;# Configure allowed ciphers. See the ciphers(1ssl) manpage for more information# about the syntax of this string.## Note: this configuration applies only to &lt;= TLSv1.2.## tls-ciphers DEFAULT:!MEDIUM# Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more# information about the syntax of this string, and specifically for TLSv1.3# ciphersuites.## tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256# When choosing a cipher, use the server's preference instead of the client# preference. By default, the server follows the client's preference.## tls-prefer-server-ciphers yes# By default, TLS session caching is enabled to allow faster and less expensive# reconnections by clients that support it. Use the following directive to disable# caching.## tls-session-caching no# Change the default number of TLS sessions cached. A zero value sets the cache# to unlimited size. The default size is 20480.## tls-session-cache-size 5000# Change the default timeout of cached TLS sessions. The default timeout is 300# seconds.## tls-session-cache-timeout 60################################# GENERAL ###################################### By default Redis does not run as a daemon. Use 'yes' if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal &quot;process is ready.&quot;# They do not enable continuous liveness pings back to your supervisor.supervised no# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to &quot;/var/run/redis.pid&quot;.## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile &quot;&quot;# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and 'databases'-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY. Basically this means# that normally a logo is displayed only in interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo yes################################ SNAPSHOTTING ################################## Save the DB on disk:## save &lt;seconds&gt; &lt;changes&gt;## Will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## In the example below the behaviour will be to save:# after 900 sec (15 min) if at least 1 key changed# after 300 sec (5 min) if at least 10 keys changed# after 60 sec if at least 10000 keys changed## Note: you can disable saving completely by commenting out all &quot;save&quot; lines.## It is also possible to remove all the previously configured save# points by adding a save directive with a single empty string argument# like in the following example:## save &quot;&quot;save 900 1save 300 10save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# For default that's set to 'yes' as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# The filename where to dump the DBdbfilename dump.rdb# Remove RDB files used by replication in instances without persistence# enabled. By default this option is disabled, however there are environments# where for regulations or other security concerns, RDB files persisted on# disk by masters in order to feed replicas, or stored on disk by replicas# in order to load them for the initial synchronization, should be deleted# ASAP. Note that this option ONLY WORKS in instances that have both AOF# and RDB persistence disabled, otherwise is completely ignored.## An alternative (and sometimes better) way to obtain the same effect is# to use diskless replication on both master and replicas instances. However# in the case of replicas, diskless is not always an option.rdb-del-sync-files no# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the &quot;requirepass&quot; configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;## However this is not enough if you are using Redis ACLs (for Redis version# 6 or greater), and the default user is not capable of running the PSYNC# command and/or other commands needed for replication. In this case it's# better to configure a special user to use with replication, and specify the# masteruser configuration as such:## masteruser &lt;username&gt;## When masteruser is specified, the replica will authenticate against its# master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) if replica-serve-stale-data is set to 'no' the replica will reply with# an error &quot;SYNC with master in progress&quot; to all the kind of commands# but to INFO, replicaOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG,# SUBSCRIBE, UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB,# COMMAND, POST, HOST: and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It's just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using 'rename-command' to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## New replicas and reconnecting replicas that are not able to continue the# replication process just receiving differences, need to do what is called a# &quot;full synchronization&quot;. An RDB file is transmitted from the master to the# replicas.## The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child# producing the RDB file finishes its work. With diskless replication instead# once the transfer starts, new replicas arriving will be queued and a new# transfer will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple# replicas will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the# server waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# -----------------------------------------------------------------------------# WARNING: RDB diskless load is experimental. Since in this setup the replica# does not immediately store an RDB on disk, it may cause data loss during# failovers. RDB diskless load + Redis modules not handling I/O reads may also# cause Redis to abort in case of I/O errors during the initial synchronization# stage with the master. Use only if your do what you are doing.# -----------------------------------------------------------------------------## Replica can load the RDB it reads from the replication link directly from the# socket, or store the RDB to a file and read that file after it was completely# recived from the master.## In many cases the disk is slower than the network, and storing and loading# the RDB file may increase replication time (and even increase the master's# Copy on Write memory and salve buffers).# However, parsing the RDB file directly from the socket may mean that we have# to flush the contents of the current database before the full rdb was# received. For this reason we have the following options:## &quot;disabled&quot; - Don't use diskless load (store the rdb file to the disk first)# &quot;on-empty-db&quot; - Use diskless load only when it is completely safe.# &quot;swapdb&quot; - Keep a copy of the current db contents in RAM while parsing# the data directly from the socket. note that this requires# sufficient memory, if you don't have it, you risk an OOM kill.repl-diskless-load disabled# Replicas send PINGs to server in a predefined interval. It's possible to# change this interval with the repl_ping_replica_period option. The default# value is 10 seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select &quot;yes&quot; Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select &quot;no&quot; the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to &quot;yes&quot; may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a# replica wants to reconnect again, often a full resync is not needed, but a# partial resync is enough, just passing the portion of data the replica# missed while disconnected.## The bigger the replication backlog, the longer the time the replica can be# disconnected and later be able to perform a partial resynchronization.## The backlog is only allocated once there is at least a replica connected.## repl-backlog-size 1mb# After a master has no longer connected replicas for some time, the backlog# will be freed. The following option configures the amount of seconds that# need to elapse, starting from the time the last replica disconnected, for# the backlog buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly &quot;partially# resynchronize&quot; with the replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO# output. It is used by Redis Sentinel in order to select a replica to promote# into a master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel# will pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in &quot;online&quot; state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the &quot;INFO replication&quot; section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# &quot;ROLE&quot; command of a master.## The listed IP and address normally reported by a replica is obtained# in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may be actually reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234############################### KEYS TRACKING ################################## Redis implements server assisted support for client side caching of values.# This is implemented using an invalidation table that remembers, using# 16 millions of slots, what clients may have certain subsets of keys. In turn# this is used in order to send invalidation messages to clients. Please# to understand more about the feature check this page:## https://redis.io/topics/client-side-caching## When tracking is enabled for a client, all the read only queries are assumed# to be cached: this will force Redis to store information in the invalidation# table. When keys are modified, such information is flushed away, and# invalidation messages are sent to the clients. However if the workload is# heavily dominated by reads, Redis could use more and more memory in order# to track the keys fetched by many clients.## For this reason it is possible to configure a maximum fill value for the# invalidation table. By default it is set to 1M of keys, and once this limit# is reached, Redis will start to evict keys in the invalidation table# even if they were not modified, just to reclaim memory: this will in turn# force the clients to invalidate the cached values. Basically the table# maximum size is a trade off between the memory you want to spend server# side to track information about who cached what, and the ability of clients# to retain cached objects in memory.## If you set the value to 0, it means there are no limits, and Redis will# retain as many keys as needed in the invalidation table.# In the &quot;stats&quot; INFO section, you can find information about the number of# keys in the invalidation table at every given moment.## Note: when key tracking is used in broadcasting mode, no memory is used# in the server side so this setting is useless.## tracking-table-max-keys 1000000################################## SECURITY #################################### Warning: since Redis is pretty fast an outside user can try up to# 1 million passwords per second against a modern box. This means that you# should use very strong passwords, otherwise they will be very easy to break.# Note that because the password is really a shared secret between the client# and the server, and should not be memorized by any human, the password# can be easily a long string from /dev/urandom or whatever, so by using a# long and unguessable password no brute force attack will be possible.# Redis ACL users are defined in the following format:## user &lt;username&gt; ... acl rules ...## For example:## user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99## The special username &quot;default&quot; is used for new connections. If this user# has the &quot;nopass&quot; rule, then new connections will be immediately authenticated# as the &quot;default&quot; user without the need of any password provided via the# AUTH command. Otherwise if the &quot;default&quot; user is not flagged with &quot;nopass&quot;# the connections will start in not authenticated state, and will require# AUTH (or the HELLO command AUTH option) in order to be authenticated and# start to work.## The ACL rules that describe what an user can do are the following:## on Enable the user: it is possible to authenticate as this user.# off Disable the user: it's no longer possible to authenticate# with this user, however the already authenticated connections# will still work.# +&lt;command&gt; Allow the execution of that command# -&lt;command&gt; Disallow the execution of that command# +@&lt;category&gt; Allow the execution of all the commands in such category# with valid categories are like @admin, @set, @sortedset, ...# and so forth, see the full list in the server.c file where# the Redis command table is described and defined.# The special category @all means all the commands, but currently# present in the server, and that will be loaded in the future# via modules.# +&lt;command&gt;|subcommand Allow a specific subcommand of an otherwise# disabled command. Note that this form is not# allowed as negative like -DEBUG|SEGFAULT, but# only additive starting with &quot;+&quot;.# allcommands Alias for +@all. Note that it implies the ability to execute# all the future commands loaded via the modules system.# nocommands Alias for -@all.# ~&lt;pattern&gt; Add a pattern of keys that can be mentioned as part of# commands. For instance ~* allows all the keys. The pattern# is a glob-style pattern like the one of KEYS.# It is possible to specify multiple patterns.# allkeys Alias for ~*# resetkeys Flush the list of allowed keys patterns.# &gt;&lt;password&gt; Add this passowrd to the list of valid password for the user.# For example &gt;mypass will add &quot;mypass&quot; to the list.# This directive clears the &quot;nopass&quot; flag (see later).# &lt;&lt;password&gt; Remove this password from the list of valid passwords.# nopass All the set passwords of the user are removed, and the user# is flagged as requiring no password: it means that every# password will work against this user. If this directive is# used for the default user, every new connection will be# immediately authenticated with the default user without# any explicit AUTH command required. Note that the &quot;resetpass&quot;# directive will clear this condition.# resetpass Flush the list of allowed passwords. Moreover removes the# &quot;nopass&quot; status. After &quot;resetpass&quot; the user has no associated# passwords and there is no way to authenticate without adding# some password (or setting it as &quot;nopass&quot; later).# reset Performs the following actions: resetpass, resetkeys, off,# -@all. The user returns to the same state it has immediately# after its creation.## ACL rules can be specified in any order: for instance you can start with# passwords, then flags, or key patterns. However note that the additive# and subtractive rules will CHANGE MEANING depending on the ordering.# For instance see the following example:## user alice on +@all -DEBUG ~* &gt;somepassword## This will allow &quot;alice&quot; to use all the commands with the exception of the# DEBUG command, since +@all added all the commands to the set of the commands# alice can use, and later DEBUG was removed. However if we invert the order# of two ACL rules the result will be different:## user alice on -DEBUG +@all ~* &gt;somepassword## Now DEBUG was removed when alice had yet no commands in the set of allowed# commands, later all the commands are added, so the user will be able to# execute everything.## Basically ACL rules are processed left-to-right.## For more information about ACL configuration please refer to# the Redis web site at https://redis.io/topics/acl# ACL LOG## The ACL Log tracks failed commands and authentication events associated# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below.acllog-max-len 128# Using an external ACL file## Instead of configuring users here in this file, it is possible to use# a stand-alone file just listing users. The two methods cannot be mixed:# if you configure users here and at the same time you activate the exteranl# ACL file, the server will refuse to start.## The format of the external ACL user file is exactly the same as the# format that is used inside redis.conf to describe users.## aclfile /etc/redis/users.acl# IMPORTANT NOTE: starting with Redis 6 &quot;requirepass&quot; is just a compatiblity# layer on top of the new ACL system. The option effect will be just setting# the password for the default user. Clients will still authenticate using# AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;# if they follow the new protocol: both will work.## requirepass foobared# Command renaming (DEPRECATED).## ------------------------------------------------------------------------# WARNING: avoid using this option if possible. Instead use ACLs to remove# commands from the default user, and put them only in some admin user you# create for administrative purposes.# ------------------------------------------------------------------------## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG &quot;&quot;## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error 'max number of clients reached'.## IMPORTANT: When Redis Cluster is used, the max number of connections is also# shared with the cluster bus: every node in the cluster will use two# connections, one incoming and another outgoing. It is important to size the# limit accordingly in case of very large clusters.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can't remove keys according to the policy, or if the policy is# set to 'noeviction', Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the 'noeviction' policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is 'noeviction').## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select one from the following behaviors:## volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key having an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don't evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, Redis will return an error on write# operations, when there are no suitable keys for eviction.## At the date of writing these commands are: set setnx setex append# incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd# sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby# zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby# getset mset msetnx exec sort## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. For default Redis will check five keys and pick the one that was# used less recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica# to have a different memory setting, and you are sure all the writes performed# to the replica are idempotent, then you may change this default (but be sure# to understand what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory# and so forth). So make sure you monitor your replicas and make sure they# have enough memory to never hit a real out-of-memory condition before the# master hits the configured maxmemory setting.## replica-ignore-maxmemory yes# Redis reclaims expired keys in two ways: upon access when those keys are# found to be expired, and also in background, in what is called the# &quot;active expire key&quot;. The key space is slowly and interactively scanned# looking for expired keys to reclaim, so that it is possible to free memory# of keys that are expired and will never be accessed again in a short time.## The default effort of the expire cycle will try to avoid having more than# ten percent of expired keys still in memory, and will try to avoid consuming# more than 25% of total memory and to add latency to the system. However# it is possible to increase the expire &quot;effort&quot; that is normally set to# &quot;1&quot;, to a greater value, up to the value &quot;10&quot;. At its maximum value the# system will use more CPU, longer cycles (and technically may introduce# more latency), and will tollerate less already expired keys still present# in the system. It's a tradeoff betweeen memory, CPU and latecy.## active-expire-effort 1############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It's up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives.lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no# It is also possible, for the case when to replace the user code DEL calls# with UNLINK calls is not easy, to modify the default behavior of the DEL# command to act exactly like UNLINK, using the following configuration# directive:lazyfree-lazy-user-del no################################ THREADED I/O ################################## Redis is mostly single threaded, however there are certain threaded# operations such as UNLINK, slow I/O accesses and other things that are# performed on side threads.## Now it is also possible to handle Redis clients socket reads and writes# in different I/O threads. Since especially writing is so slow, normally# Redis users use pipelining in order to speedup the Redis performances per# core, and spawn multiple instances in order to scale more. Using I/O# threads it is possible to easily speedup two times Redis without resorting# to pipelining nor sharding of the instance.## By default threading is disabled, we suggest enabling it only in machines# that have at least 4 or more cores, leaving at least one spare core.# Using more than 8 threads is unlikely to help much. We also recommend using# threaded I/O only if you actually have performance problems, with Redis# instances being able to use a quite big percentage of CPU time, otherwise# there is no point in using this feature.## So for instance if you have a four cores boxes, try to use 2 or 3 I/O# threads, if you have a 8 cores, try to use 6 threads. In order to# enable I/O threads use the following configuration directive:## io-threads 4## Setting io-threads to 1 will just use the main thread as usually.# When I/O threads are enabled, we only use threads for writes, that is# to thread the write(2) syscall and transfer the client buffers to the# socket. However it is also possible to enable threading of reads and# protocol parsing using the following configuration directive, by setting# it to yes:## io-threads-do-reads no## Usually threading reads doesn't help much.## NOTE 1: This configuration directive cannot be changed at runtime via# CONFIG SET. Aso this feature currently does not work when SSL is# enabled.## NOTE 2: If you want to test the Redis speedup using redis-benchmark, make# sure you also run the benchmark itself in threaded mode, using the# --threads option to match the number of Redis theads, otherwise you'll not# be able to notice the improvements.############################ KERNEL OOM CONTROL ############################### On Linux, it is possible to hint the kernel OOM killer on what processes# should be killed first when out of memory.## Enabling this feature makes Redis actively control the oom_score_adj value# for all its processes, depending on their role. The default scores will# attempt to have background child processes killed before all others, and# replicas killed before masters.oom-score-adj no# When oom-score-adj is used, this directive controls the specific values used# for master, replica and background child processes. Values range -1000 to# 1000 (higher means more likely to be killed).## Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)# can freely increase their value, but not decrease it below its initial# settings.## Values are used relative to the initial value of oom_score_adj when the server# starts. Because typically the initial value is 0, they will often match the# absolute values.oom-score-adj-values 0 200 800############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check http://redis.io/topics/persistence for more information.appendonly no# The name of the append only file (default: &quot;appendonly.aof&quot;)appendfilename &quot;appendonly.aof&quot;# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is &quot;everysec&quot;, as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# &quot;no&quot; that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use &quot;always&quot; that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use &quot;everysec&quot;.# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it's possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as &quot;appendfsync none&quot;. In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to &quot;yes&quot;. Otherwise leave it as# &quot;no&quot; that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the &quot;redis-check-aof&quot; utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;# string and loads the prefixed RDB file, and continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet called write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn't want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################ Normal Redis instances can't be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its &quot;data age&quot;, so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the &quot;connected&quot; state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point &quot;2&quot; can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they'll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can't be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the &quot;migration barrier&quot;. A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# By default Redis Cluster nodes stop accepting queries if they detect there# is at least an hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the master can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# This option, when set to yes, allows nodes to serve read traffic while the# the cluster is in a down state, as long as it believes it owns the slots. ## This is useful for two cases. The first case is for when an application # doesn't require consistency of data during node failures or network partitions.# One example of this is a cache, where as long as the node has the data it# should be able to serve it. ## The second use case is for configurations that don't meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the# entire cluster without this option set, with it set there is only a write outage.# Without a quorum of masters, slot ownership will not change automatically. ## cluster-allow-reads-when-down no# In order to setup your cluster make sure to read the documentation# available at http://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following two options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-bus-port## Each instruct the node about its address, client port, and cluster message# bus port. The information is then published in the header of the bus packets# so that other nodes will be able to correctly map the address of the node# publishing the information.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usually.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-port 6379# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don't have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# &quot;CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;&quot; if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at http://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key &quot;foo&quot; stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# t Stream commands# m Key-miss events (Note: It is not included in the 'A' class)# A Alias for g$lshzxet, so that the &quot;AKE&quot; string means all the events# (Except key-miss events which are excluded from 'A' due to their# unique nature).## The &quot;notify-keyspace-events&quot; takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don't need# this feature and the feature has some overhead. Note that if you don't# specify at least one of K or E, no events will be delivered.notify-keyspace-events &quot;&quot;############################### GOPHER SERVER ################################## Redis contains an implementation of the Gopher protocol, as specified in# the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).## The Gopher protocol was very popular in the late '90s. It is an alternative# to the web, and the implementation both server and client side is so simple# that the Redis server has just 100 lines of code in order to implement this# support.## What do you do with Gopher nowadays? Well Gopher never *really* died, and# lately there is a movement in order for the Gopher more hierarchical content# composed of just plain text documents to be resurrected. Some want a simpler# internet, others believe that the mainstream internet became too much# controlled, and it's cool to create an alternative space for people that# want a bit of fresh air.## Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol# as a gift.## --- HOW IT WORKS? ---## The Redis Gopher support uses the inline protocol of Redis, and specifically# two kind of inline requests that were anyway illegal: an empty request# or any request that starts with &quot;/&quot; (there are no Redis commands starting# with such a slash). Normal RESP2/RESP3 requests are completely out of the# path of the Gopher protocol implementation and are served as usually as well.## If you open a connection to Redis when Gopher is enabled and send it# a string like &quot;/foo&quot;, if there is a key named &quot;/foo&quot; it is served via the# Gopher protocol.## In order to create a real Gopher &quot;hole&quot; (the name of a Gopher site in Gopher# talking), you likely need a script like the following:## https://github.com/antirez/gopher2redis## --- SECURITY WARNING ---## If you plan to put Redis on the internet in a publicly accessible address# to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.# Once a password is set:## 1. The Gopher server (when enabled, not by default) will still serve# content via Gopher.# 2. However other commands cannot be called before the client will# authenticate.## So use the 'requirepass' option to protect your instance.## To enable Gopher support uncomment the following line and set# the option from no (the default) to yes.## gopher-enabled no############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means &quot;don't start compressing until after 1 node into the list,# going from either the head or tail&quot;# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entires limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing &quot;steps&quot; are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use &quot;activerehashing no&quot; if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use &quot;activerehashing yes&quot; if you don't have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can't consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don't receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited ot 512 mb. However you can change this limit# here, but must be 1mb or greater## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified &quot;hz&quot; value.## By default &quot;hz&quot; is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporary raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A Special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in an &quot;hot&quot; way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don't have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command &quot;CONFIG SET activedefrag yes&quot;.## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag no# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage, to be used when the lower# threshold is reached# active-defrag-cycle-min 1# Maximal effort for defrag in CPU percentage, to be used when the upper# threshold is reached# active-defrag-cycle-max 25# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000# Jemalloc background thread for purging will be enabled by defaultjemalloc-bg-thread yes# It is possible to pin different threads and processes of Redis to specific# CPUs in your system, in order to maximize the performances of the server.# This is useful both in order to pin different Redis threads in different# CPUs, but also in order to make sure that multiple Redis instances running# in the same host will be pinned to different CPUs.## Normally you can do this using the &quot;taskset&quot; command, however it is also# possible to this via Redis configuration directly, both in Linux and FreeBSD.## You can pin the server/IO threads, bio threads, aof rewrite child process, and# the bgsave child process. The syntax to specify the cpu list is the same as# the taskset command:## Set redis server/io threads to cpu affinity 0,2,4,6:# server_cpulist 0-7:2## Set bio threads to cpu affinity 1,3:# bio_cpulist 1,3## Set aof rewrite child process to cpu affinity 8,9,10,11:# aof_rewrite_cpulist 8-11## Set bgsave child process to cpu affinity 1,10,11# bgsave_cpulist 1,10-11 测试 redis-cli连接上来 1docker exec -it 运行着redis服务容器的ID redis-cli 测试持久化文件生成 八、将镜像推送到阿里云 本地镜像发布到阿里云流程 镜像生成方法 1、前面的Dockerfile 2、从容器中创建一个新的镜像 1docker commit [OPTIONS] 容器ID [REPOSITORY[:TAG]] 将本地镜像推送到阿里云 1、本地镜像素材原型 2、阿里云开发者平台 https://promotion.aliyun.com/ntms/act/kubernetes.html 3、创建镜像仓库 命名空间 仓库名称 4、将镜像推送到registry 1234$ sudo docker login --username=white3e registry.cn-shenzhen.aliyuncs.com$ sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号]$ sudo docker push registry.cn-shenzhen.aliyuncs.com/ggccqq/mycentos:[镜像版本号]其中[ImageId][镜像版本]自己填写 5、公有云可以查询得到 6、查看详情 将阿里云上的镜像下载到本地","link":"/2021/01/20/Draft/2021/Docker/"},{"title":"SpringBoot","text":"SpringBoot 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】：尚硅谷学习笔记、springboot官方文档、视频、项目里面做笔记，最后进行综合优化 LEVEL【不是每个都学精】： 路上随便看看 进度：【完】路上看补笔记 综合问题 优点？ SpringBoot分页排序？ 排除自带tomcat等容器？ 测试？ Spring Boot Stater ？ 常用注解？ 一、简介 SpringBoot及其特点 Create stand-alone Spring applications Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) Provide opinionated 'starter' dependencies to simplify your build configuration Automatically configure Spring and 3rd party libraries whenever possible Provide production-ready features such as metrics, health checks, and externalized configuration Absolutely no code generation and no requirement for XML configuration 相关概念 微服务 In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.-- James Lewis and Martin Fowler (2014) 微服务是一种架构风格 一个应用拆分为一组小型服务 每个服务运行在自己的进程内，也就是可独立部署和升级 服务之间使用轻量级HTTP交互 服务围绕业务功能拆分 可以由全自动部署机制独立部署 去中心化，服务自治。服务可以使用不同的语言、不同的存储技术 分布式 Spring Cloud focuses on providing good out of box experience for typical use cases and extensibility mechanism to cover others. Distributed/versioned configuration Service registration and discovery Routing Service-to-service calls Load balancing Circuit Breakers Global locks Leadership election and cluster state Distributed messaging 远程调用 服务发现 负载均衡 服务容错 配置管理 服务监控 链路追踪 日志管理 任务调度 Spring Cloud Alibaba 自动配置 123456789101112131415161718192021222324252627282930313233&lt;!-- Spring Boot 项目启动父依赖 --&gt;&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt;&lt;/parent&gt; &lt;properties&gt; &lt;junit.version&gt;8.0.13&lt;/junit.version&gt; &lt;/properties&gt; &lt;!-- Junit 填写${}会默认父依赖的版本号，父依赖没有必须指定版本，可通过如上&lt;propertites&gt;修改默认版本--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;${junit.version}&lt;/version&gt; &lt;/dependency&gt;&lt;!-- 其父依赖的父依赖 --&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.3.RELEASE&lt;/version&gt; &lt;relativePath&gt;../../spring-boot-dependencies&lt;/relativePath&gt; &lt;/parent&gt;&lt;!--最高层依赖会定义所有当前版本会用到的依赖版本，自动版本仲裁机制--&gt;&lt;properties&gt; &lt;activemq.version&gt;5.15.8&lt;/activemq.version&gt; &lt;antlr2.version&gt;2.7.7&lt;/antlr2.version&gt; &lt;appengine-sdk.version&gt;1.9.71&lt;/appengine-sdk.version&gt; &lt;artemis.version&gt;2.6.4&lt;/artemis.version&gt; &lt;aspectj.version&gt;1.9.2&lt;/aspectj.version&gt; &lt;assertj.version&gt;3.11.1&lt;/assertj.version&gt; 。。。。&lt;/properties&gt; SpringBoot会自动配置以下环境： 1234567891011&lt;!-- 常用starterspring-boot-starter-parent //boot 项目继承的父项目模块.spring-boot-starter-web //boot 项目集成 web 开发模块.spring-boot-starter-tomcat //boot 项目集成 tomcat 内嵌服务器.spring-boot-starter-test //boot 项目集成测试模块.mybatis-spring-boot-starter //boot 项目集成 mybatis 框架.spring-boot-starter-jdbc //boot 项目底层集成 jdbc 实现数据库操作支持.--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; Tomcat、Spring、SpringMVC全套组件以及常用功能【字符编码、dispatcherServlet、viewResolver、MultiparResolver等】 默认包扫描【主程序所在包及其子包】，要扫描上层需要注解上层包 自动配置按需加载【POM引入才会加载】 12345//自定义扫描包//@SpringBootApplication(scanBasePackages = &quot;cn.wmyskxz&quot;)//默认扫描主程序类所在包@SpringBootApplicationpublic class SpringbootApplication {} 项目结构 二、注解 12345678910@ImportResource(&quot;classpath:beans.xml&quot;)//通过xml导入组件@Configuration //声明配置类 @EnableAspectJAutoProxy(exposeProxy = true)//表示通过aop框架暴露该代理对象,AopContext能够访问@Import({User.class})//注入对应类型的组件@EnableConfigurationProperties(StudentProperties.class)//1.开启属性配置功能 2.把组件自动注册到容器@Bean//向容器注册组件，默认单实例@ConditionalOnBean(name = &quot;user02&quot;)//条件成立才装配其注解中的组件@EnableSwagger2Doc //Swagger@Component //注册为组件@ConfigurationProperties(prefix = &quot;student&quot;)//给容器中的组件提供配置绑定 配置类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.lxl.springboot.config;import cn.lxl.springboot.entity.StudentProperties;import cn.lxl.springboot.entity.User;import org.springframework.boot.autoconfigure.condition.ConditionalOnBean;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.*;/*测试均在主程序类*///将配置文件中的bean注入容器@ImportResource(&quot;classpath:beans.xml&quot;)//告诉springboot这是个配置类// proxybeanMethods：代理bean的方法// Full模式：@Configuration(proxybeanMethods=true) 全模式 记录上次用的bean，检查组件是否在容器中有，单实例// Lite模式：@Configuration(proxybeanMethods=false) 轻量级模式 每次加载新的bean，跳过检查@Configuration// 表示通过aop框架暴露该代理对象,AopContext能够访问@EnableAspectJAutoProxy(exposeProxy = true)//给容器中自动创建出对应类型的组件、默认组件名为其全类名、使用其无参构造函数//@Import({User.class})//当不能在原类中添加@component注解时（比如用他人包时），则在使用的地方添加次注解,作用是开启student配置绑定功能且将其注入到容器中//@EnableConfigurationProperties(StudentProperties.class)public class MyConfig {//给容器添加组件，一方法名作为组件的id。返回类型就是组件类型。返回的值就是组建在容器中的实例 @Bean public User user01() { return new User(111111111L, &quot;1&quot;, 1); } //条件装配，有name为user01 的bean才注入注解下的student的bean，反之有 @ConditionalOnMissingBean @ConditionalOnBean(name = &quot;user02&quot;) @Bean public StudentProperties student() { return new StudentProperties(); } @ConditionalOnBean(name = &quot;user01&quot;) @Bean public StudentProperties student1() { return new StudentProperties(); }} 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package cn.lxl.springboot;import cn.lxl.springboot.config.MyConfig;import cn.lxl.springboot.entity.User;import com.spring4all.swagger.EnableSwagger2Doc;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;import org.springframework.context.annotation.ImportResource;/*Spring Boot 项目通常有一个名为 *Application 的入口类，入口类里有一个 main 方法， 这个 main 方法其实就是一个标准的 Javay 应用的入口方法。*///@SpringBootApplication===@Configuration、@EnableAutoConfiguration、@ComponentScan//@EnableAutoConfiguration 让 Spring Boot 根据类路径中的 jar 包依赖为当前项目进行自动配置，// 例如，添加了 spring-boot-starter-web 依赖，会自动添加 Tomcat 和 Spring MVC 的依赖，那么 Spring Boot 会对 Tomcat 和 Spring MVC 进行自动配置。//spring Boot 还会自动扫描 @SpringBootApplication 所在类的同级包以及下级包里的 Bean ，所以入口类建议就配置在 grounpID + arctifactID 组合的包名下（这里为 cn.wmyskxz.springboot 包）//@SpringBootApplication(scanBasePackages = &quot;cn.wmyskxz&quot;) 如需扫描主程序同级外的包则可添加scanBasePackages参数@EnableSwagger2Doc@SpringBootApplicationpublic class SpringbootApplication { public static void main(String[] args) {// SpringApplication.run(SpringbootApplication.class, args);// 返回IOC容器 ConfigurableApplicationContext run = SpringApplication.run(SpringbootApplication.class, args);// 查看所有组件// String[] beanDefinitionNames = run.getBeanDefinitionNames();// for (String name : beanDefinitionNames// ) {// System.out.println(name);// } //配置类本身也是组件// 以类型获取组件 MyConfig bean = run.getBean(MyConfig.class); System.out.println(bean); User user = run.getBean(User.class); System.out.println(user);// 以名称判断是否有组件 boolean user01 = run.containsBean(&quot;user01&quot;); System.out.println(user01);//条件注解测试 boolean student = run.containsBean(&quot;student&quot;); System.out.println(student);//@ImportResource(&quot;classpath:beans.xml&quot;)将配置文件中的bean注入容器 boolean bean1 = run.containsBean(&quot;bean1&quot;); System.out.println(bean1);//配置绑定,需要@Component//@ConfigurationProperties(prefix = &quot;student&quot;)//表示获取所有配置文件中前缀为 sutdent 的配置信息，实体类中使用 //当不能在原类中添加@component注解时（比如用他人包时），则在使用的地方添加此注解,作用是开启student配置绑定功能且将其注入到容器中//@EnableConfigurationProperties(StudentProperties.class) Object student1 = run.getBean(&quot;student1&quot;); System.out.println(student1); }}//运⾏ SpringBoot 有哪⼏种⽅式？//打包⽤命令或者放到容器中运⾏。 ⽤ Maven/Gradle 插件运⾏。//直接执⾏ main ⽅法运⾏。//SpringBoot ⽀持 Java Util Logging，Log4j2，Logback 作为⽇志框架，如果你使⽤ Starters 启动器，SpringBoot 将使⽤ Logback 作为默认框架。//⽤ Maven/Gradle 插件运⾏。可运行thtmeleaf与jsp同时存在的jsp， 1234567891011@Component//表明当前类是一个 Java Bean//配置绑定,需要@Component,或者使用的地方使用//当不能在原类中添加@component注解时（比如用他人包时），则在使用的地方添加此注解,作用是开启student配置绑定功能且将其注入到容器中//@EnableConfigurationProperties(StudentProperties.class)@ConfigurationProperties(prefix = &quot;student&quot;)//表示获取所有配置文件中前缀为 sutdent 的配置信息//student:// name: 真的帅// age: 21public class StudentProperties { private String name; private Integer age; 三、springboot自动装配原理 @SpringBootApplication 1234@SpringBootConfiguration //代表为配置类@ComponentScan //Spring注解，指定扫描哪些包@EnableAutoConfigration //如下public @interface SpringBootApplication {} @EnableAutoConfigration 1234567@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage//如下@Import({AutoConfigurationImportSelector.class})public @interface EnableAutoConfiguration {} @AutoConfigurationPackage 123@Import({Registrar.class}) //给容器中导入一系列组件，默认是主程序所在包下的所有组件 //new AutoConfigurationPackages.PackageImports(metadata)).getPackageNames()public @interface AutoConfigurationPackage {} Registrar 123456789101112 static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { Registrar() { } public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { AutoConfigurationPackages.register(registry, (String[])(new AutoConfigurationPackages.PackageImports(metadata)).getPackageNames().toArray(new String[0]));//获取元注解的包名（com.xxx.xxx）封装到数组中 }public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) { return Collections.singleton(new AutoConfigurationPackages.PackageImports(metadata)); } } @Import({AutoConfigurationImportSelector.class}) 1、利用getAutoConfigurationEntry(annotationMetadata);给容器中批量导入一些组件 2、调用List&lt;-String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes)获取到所有需要导入到容器中的配置类 3、利用工厂加载 Map&lt;String, List&lt;-String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader)；得到所有的组件 4、从META-INF/spring.factories位置来加载一个文件。 默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件 spring-boot-autoconfigure-2.3.4.RELEASE.jar包里面也有META-INF/spring.factories【springboot一启动原始加载的组件】 123456789101112131415161718192021222324public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered { private static final AutoConfigurationImportSelector.AutoConfigurationEntry EMPTY_ENTRY = new AutoConfigurationImportSelector.AutoConfigurationEntry(); private static final String[] NO_IMPORTS = new String[0]; private static final Log logger = LogFactory.getLog(AutoConfigurationImportSelector.class); private static final String PROPERTY_NAME_AUTOCONFIGURE_EXCLUDE = &quot;spring.autoconfigure.exclude&quot;; private ConfigurableListableBeanFactory beanFactory; private Environment environment; private ClassLoader beanClassLoader; private ResourceLoader resourceLoader; private AutoConfigurationImportSelector.ConfigurationClassFilter configurationClassFilter; public AutoConfigurationImportSelector() { } public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return NO_IMPORTS; } else { //1.getAutoConfigurationEntry（）给容器中批量导入一些组件 AutoConfigurationImportSelector.AutoConfigurationEntry autoConfigurationEntry = this.getAutoConfigurationEntry(annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations()); } } 12345678910111213141516protected AutoConfigurationImportSelector.AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } else { AnnotationAttributes attributes = this.getAttributes(annotationMetadata); //2.导入到容器中的所有默认候选组件 List&lt;String&gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); Set&lt;String&gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.getConfigurationClassFilter().filter(configurations); this.fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationImportSelector.AutoConfigurationEntry(configurations, exclusions); }} 123456protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) { //3.利用工厂加载 Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader)；得到所有的组件 List&lt;String&gt; configurations = SpringFactoriesLoader.loadFactoryNames(this.getSpringFactoriesLoaderFactoryClass(), this.getBeanClassLoader()); Assert.notEmpty(configurations, &quot;No auto configuration classes found in META-INF/spring.factories. If you are using a custom packaging, make sure that file is correct.&quot;); return configurations;} 12345678910111213141516171819202122232425262728293031323334353637383940414243public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryType, @Nullable ClassLoader classLoader) { String factoryTypeName = factoryType.getName(); return (List)loadSpringFactories(classLoader).getOrDefault(factoryTypeName, Collections.emptyList());}private static Map&lt;String, List&lt;String&gt;&gt; loadSpringFactories(@Nullable ClassLoader classLoader) { MultiValueMap&lt;String, String&gt; result = (MultiValueMap)cache.get(classLoader); if (result != null) { return result; } else { try { //4、从META-INF/spring.factories位置来加载一个文件。 //默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件 //spring-boot-autoconfigure-2.3.4.RELEASE.jar包里面也有META-INF/spring.factories Enumeration&lt;URL&gt; urls = classLoader != null ? classLoader.getResources(&quot;META-INF/spring.factories&quot;) : ClassLoader.getSystemResources(&quot;META-INF/spring.factories&quot;); LinkedMultiValueMap result = new LinkedMultiValueMap(); while(urls.hasMoreElements()) { URL url = (URL)urls.nextElement(); UrlResource resource = new UrlResource(url); Properties properties = PropertiesLoaderUtils.loadProperties(resource); Iterator var6 = properties.entrySet().iterator(); while(var6.hasNext()) { Entry&lt;?, ?&gt; entry = (Entry)var6.next(); String factoryTypeName = ((String)entry.getKey()).trim(); String[] var9 = StringUtils.commaDelimitedListToStringArray((String)entry.getValue()); int var10 = var9.length; for(int var11 = 0; var11 &lt; var10; ++var11) { String factoryImplementationName = var9[var11]; result.add(factoryTypeName, factoryImplementationName.trim()); } } } cache.put(classLoader, result); return result; } catch (IOException var13) { throw new IllegalArgumentException(&quot;Unable to load factories from location [META-INF/spring.factories]&quot;, var13); } }} ​ spring-boot-autoconfigure-2.3.4.RELEASE.jar包里面也有META-INF/spring.factories 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166# Initializersorg.springframework.context.ApplicationContextInitializer=\\org.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\\org.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener# Application Listenersorg.springframework.context.ApplicationListener=\\org.springframework.boot.autoconfigure.BackgroundPreinitializer# Auto Configuration Import Listenersorg.springframework.boot.autoconfigure.AutoConfigurationImportListener=\\org.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener# Auto Configuration Import Filtersorg.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\\org.springframework.boot.autoconfigure.condition.OnBeanCondition,\\org.springframework.boot.autoconfigure.condition.OnClassCondition,\\org.springframework.boot.autoconfigure.condition.OnWebApplicationCondition# Auto Configureorg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\org.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\org.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\org.springframework.boot.autoconfigure.context.LifecycleAutoConfiguration,\\org.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\org.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\org.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\\org.springframework.boot.autoconfigure.data.cassandra.CassandraDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.cassandra.CassandraRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.couchbase.CouchbaseDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.couchbase.CouchbaseRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.elasticsearch.ReactiveElasticsearchRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.elasticsearch.ReactiveElasticsearchRestClientAutoConfiguration,\\org.springframework.boot.autoconfigure.data.jdbc.JdbcRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.jpa.JpaRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.ldap.LdapRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.mongo.MongoReactiveDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.mongo.MongoReactiveRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.mongo.MongoRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.neo4j.Neo4jDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.neo4j.Neo4jRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.solr.SolrRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.r2dbc.R2dbcDataAutoConfiguration,\\org.springframework.boot.autoconfigure.data.r2dbc.R2dbcRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.r2dbc.R2dbcTransactionManagerAutoConfiguration,\\org.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration,\\org.springframework.boot.autoconfigure.data.redis.RedisReactiveAutoConfiguration,\\org.springframework.boot.autoconfigure.data.redis.RedisRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.rest.RepositoryRestMvcAutoConfiguration,\\org.springframework.boot.autoconfigure.data.web.SpringDataWebAutoConfiguration,\\org.springframework.boot.autoconfigure.elasticsearch.ElasticsearchRestClientAutoConfiguration,\\org.springframework.boot.autoconfigure.flyway.FlywayAutoConfiguration,\\org.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration,\\org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAutoConfiguration,\\org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration,\\org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration,\\org.springframework.boot.autoconfigure.hateoas.HypermediaAutoConfiguration,\\org.springframework.boot.autoconfigure.hazelcast.HazelcastAutoConfiguration,\\org.springframework.boot.autoconfigure.hazelcast.HazelcastJpaDependencyAutoConfiguration,\\org.springframework.boot.autoconfigure.http.HttpMessageConvertersAutoConfiguration,\\org.springframework.boot.autoconfigure.http.codec.CodecsAutoConfiguration,\\org.springframework.boot.autoconfigure.influx.InfluxDbAutoConfiguration,\\org.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration,\\org.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration,\\org.springframework.boot.autoconfigure.jackson.JacksonAutoConfiguration,\\org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration,\\org.springframework.boot.autoconfigure.jdbc.JndiDataSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.jdbc.XADataSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\\org.springframework.boot.autoconfigure.jms.JmsAutoConfiguration,\\org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\\org.springframework.boot.autoconfigure.jms.JndiConnectionFactoryAutoConfiguration,\\org.springframework.boot.autoconfigure.jms.activemq.ActiveMQAutoConfiguration,\\org.springframework.boot.autoconfigure.jms.artemis.ArtemisAutoConfiguration,\\org.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration,\\org.springframework.boot.autoconfigure.jooq.JooqAutoConfiguration,\\org.springframework.boot.autoconfigure.jsonb.JsonbAutoConfiguration,\\org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration,\\org.springframework.boot.autoconfigure.availability.ApplicationAvailabilityAutoConfiguration,\\org.springframework.boot.autoconfigure.ldap.embedded.EmbeddedLdapAutoConfiguration,\\org.springframework.boot.autoconfigure.ldap.LdapAutoConfiguration,\\org.springframework.boot.autoconfigure.liquibase.LiquibaseAutoConfiguration,\\org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration,\\org.springframework.boot.autoconfigure.mail.MailSenderValidatorAutoConfiguration,\\org.springframework.boot.autoconfigure.mongo.embedded.EmbeddedMongoAutoConfiguration,\\org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\org.springframework.boot.autoconfigure.mongo.MongoReactiveAutoConfiguration,\\org.springframework.boot.autoconfigure.mustache.MustacheAutoConfiguration,\\org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\\org.springframework.boot.autoconfigure.quartz.QuartzAutoConfiguration,\\org.springframework.boot.autoconfigure.r2dbc.R2dbcAutoConfiguration,\\org.springframework.boot.autoconfigure.rsocket.RSocketMessagingAutoConfiguration,\\org.springframework.boot.autoconfigure.rsocket.RSocketRequesterAutoConfiguration,\\org.springframework.boot.autoconfigure.rsocket.RSocketServerAutoConfiguration,\\org.springframework.boot.autoconfigure.rsocket.RSocketStrategiesAutoConfiguration,\\org.springframework.boot.autoconfigure.security.servlet.SecurityAutoConfiguration,\\org.springframework.boot.autoconfigure.security.servlet.UserDetailsServiceAutoConfiguration,\\org.springframework.boot.autoconfigure.security.servlet.SecurityFilterAutoConfiguration,\\org.springframework.boot.autoconfigure.security.reactive.ReactiveSecurityAutoConfiguration,\\org.springframework.boot.autoconfigure.security.reactive.ReactiveUserDetailsServiceAutoConfiguration,\\org.springframework.boot.autoconfigure.security.rsocket.RSocketSecurityAutoConfiguration,\\org.springframework.boot.autoconfigure.security.saml2.Saml2RelyingPartyAutoConfiguration,\\org.springframework.boot.autoconfigure.sendgrid.SendGridAutoConfiguration,\\org.springframework.boot.autoconfigure.session.SessionAutoConfiguration,\\org.springframework.boot.autoconfigure.security.oauth2.client.servlet.OAuth2ClientAutoConfiguration,\\org.springframework.boot.autoconfigure.security.oauth2.client.reactive.ReactiveOAuth2ClientAutoConfiguration,\\org.springframework.boot.autoconfigure.security.oauth2.resource.servlet.OAuth2ResourceServerAutoConfiguration,\\org.springframework.boot.autoconfigure.security.oauth2.resource.reactive.ReactiveOAuth2ResourceServerAutoConfiguration,\\org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration,\\org.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration,\\org.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration,\\org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\\org.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration,\\org.springframework.boot.autoconfigure.transaction.jta.JtaAutoConfiguration,\\org.springframework.boot.autoconfigure.validation.ValidationAutoConfiguration,\\org.springframework.boot.autoconfigure.web.client.RestTemplateAutoConfiguration,\\org.springframework.boot.autoconfigure.web.embedded.EmbeddedWebServerFactoryCustomizerAutoConfiguration,\\org.springframework.boot.autoconfigure.web.reactive.HttpHandlerAutoConfiguration,\\org.springframework.boot.autoconfigure.web.reactive.ReactiveWebServerFactoryAutoConfiguration,\\org.springframework.boot.autoconfigure.web.reactive.WebFluxAutoConfiguration,\\org.springframework.boot.autoconfigure.web.reactive.error.ErrorWebFluxAutoConfiguration,\\org.springframework.boot.autoconfigure.web.reactive.function.client.ClientHttpConnectorAutoConfiguration,\\org.springframework.boot.autoconfigure.web.reactive.function.client.WebClientAutoConfiguration,\\org.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\\org.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration,\\org.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration,\\org.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration,\\org.springframework.boot.autoconfigure.web.servlet.MultipartAutoConfiguration,\\org.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration,\\org.springframework.boot.autoconfigure.websocket.reactive.WebSocketReactiveAutoConfiguration,\\org.springframework.boot.autoconfigure.websocket.servlet.WebSocketServletAutoConfiguration,\\org.springframework.boot.autoconfigure.websocket.servlet.WebSocketMessagingAutoConfiguration,\\org.springframework.boot.autoconfigure.webservices.WebServicesAutoConfiguration,\\org.springframework.boot.autoconfigure.webservices.client.WebServiceTemplateAutoConfiguration# Failure analyzersorg.springframework.boot.diagnostics.FailureAnalyzer=\\org.springframework.boot.autoconfigure.data.redis.RedisUrlSyntaxFailureAnalyzer,\\org.springframework.boot.autoconfigure.diagnostics.analyzer.NoSuchBeanDefinitionFailureAnalyzer,\\org.springframework.boot.autoconfigure.flyway.FlywayMigrationScriptMissingFailureAnalyzer,\\org.springframework.boot.autoconfigure.jdbc.DataSourceBeanCreationFailureAnalyzer,\\org.springframework.boot.autoconfigure.jdbc.HikariDriverConfigurationFailureAnalyzer,\\org.springframework.boot.autoconfigure.r2dbc.ConnectionFactoryBeanCreationFailureAnalyzer,\\org.springframework.boot.autoconfigure.session.NonUniqueSessionRepositoryFailureAnalyzer# Template availability providersorg.springframework.boot.autoconfigure.template.TemplateAvailabilityProvider=\\org.springframework.boot.autoconfigure.freemarker.FreeMarkerTemplateAvailabilityProvider,\\org.springframework.boot.autoconfigure.mustache.MustacheTemplateAvailabilityProvider,\\org.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAvailabilityProvider,\\org.springframework.boot.autoconfigure.thymeleaf.ThymeleafTemplateAvailabilityProvider,\\org.springframework.boot.autoconfigure.web.servlet.JspTemplateAvailabilityProvider 按需加载 springboot 通过自动配置文件中类似@ConditionalOnClass({Advice.class})等条件注解按需加载修改默认配置 修改默认配置 @Bean @ConditionalOnBean(MultipartResolver.class) //容器中有这个类型组件 @ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) //容器中没有这个名字 multipartResolver 的组件 public MultipartResolver multipartResolver(MultipartResolver resolver) { //给@Bean标注的方法传入了对象参数，这个参数的值就会从容器中找。 //SpringMVC multipartResolver。防止有些用户配置的文件上传解析器不符合规范 // Detect if the user has created a MultipartResolver but named it incorrectly return resolver; } 给容器中加入了文件上传解析器； SpringBoot默认会在底层配好所有的组件。但是如果用户自己配置了以用户的优先 123@Bean @ConditionalOnMissingBean public CharacterEncodingFilter characterEncodingFilter() {} 总结： SpringBoot先加载所有的自动配置类 xxxxxAutoConfiguration 每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。xxxxProperties里面拿。xxxProperties和配置文件进行了绑定 生效的配置类就会给容器中装配很多组件 只要容器中有这些组件，相当于这些功能就有了 定制化配置 用户直接自己@Bean替换底层的组件 用户去看这个组件是获取的配置文件什么值就去修改。 xxxxxAutoConfiguration ---&gt; 组件 ---&gt; xxxxProperties里面拿值 ----&gt; application.properties SpringBoot开发流程 引入场景依赖 https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html#using-boot-starter 查看自动配置了哪些（选做） 自己分析，引入场景对应的自动配置一般都生效了 配置文件中debug=true开启自动配置报告。console中显示Negative（不生效）\\Positive（生效） 是否需要修改 1.参照文档修改配置项 https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#common-application-properties 自己分析。xxxxProperties绑定了配置文件的哪些。 自定义加入或者替换组件 @Bean、@Component。。。 自定义器 XXXXXCustomizer； 业务代码 四、开发技巧 1.lombok @Data ：代替get set 方法 @ToString ：代替tostring方法 @AllArgsConstructor ：代替全参构造器 @NoArgsConstructor ：代替无参构造器 @EqualsAndHashCode：代替HashCode、equals方法 @Slf4j ：添加 Log.info(“xxx”);打印日志 2.dev-tools ctrl+f9 自动重启 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; 3.Spring Initailizr（项目初始化向导） 自动搭建初始项目环境【相关依赖，项目结构，主程序类】 五、核心功能 1.配置文件 1.1、properties 同以前的properties用法，可用@PropertySource 注解导入配置。 1.2、yaml 1.2.1、简介 YAML 是 &quot;YAML Ain't Markup Language&quot;（YAML 不是一种标记语言）的递归缩写。在开发的这种语言时，YAML 的意思其实是：&quot;Yet Another Markup Language&quot;（仍是一种标记语言）。 非常适合用来做以数据为中心的配置文件。如果properties和ymal都有，则都生效且properties优先。 1.2.2、基本语法 key: value；kv之间有空格 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格 缩进的空格数不重要，只要相同层级的元素左对齐即可 '#'表示注释 字符串无需加引号，如果要加，' ' 与 &quot; &quot; 表示字符串内容 会被 转义/不转义，比如 \\n 单引号回转义成 \\n 字符串，而双引号会换行。 .yml 格式不支持 @PropertySource 注解导入配置。 1.2.3、数据类型 1234567891011121314151617#字面量：单个的、不可再分的值。date、boolean、string、number、nullk: v#对象：键值对的集合。map、hash、set、object 行内写法： k: {k1:v1,k2:v2,k3:v3}#或k: k1: v1 k2: v2 k3: v3#数组：一组按次序排列的值。array、list、queue行内写法： k: [v1,v2,v3]#或者k: - v1 - v2 - v3 1.2.4、示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Datapublic class Person { private String userName; private Boolean boss; private Date birth; private Integer age; private Pet pet; private String[] interests; private List&lt;String&gt; animal; private Map&lt;String, Object&gt; score; private Set&lt;Double&gt; salarys; private Map&lt;String, List&lt;Pet&gt;&gt; allPets;}@Datapublic class Pet { private String name; private Double weight;}# yaml表示以上对象person: userName: zhangsan boss: false birth: 2019/12/12 20:12:33 age: 18 pet: name: tomcat weight: 23.4 interests: [篮球,游泳] animal: - jerry - mario score: english: first: 30 second: 40 third: 50 math: [131,140,148] chinese: {first: 128,second: 136} salarys: [3999,4999.98,5999.99] allPets: sick: - {name: tom} - {name: jerry,weight: 47} health: [{name: mario,weight: 47}] 1.3配置文件自定义类提示 12345678910111213141516171819202122232425&lt;!--让自定义类在配置类中有提示--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;!--打包时去除业务无关依赖processor--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 1.4 bootstrap bootstrap (.yml 或者 .properties)优先于application (.yml 或者 .properties)，两者共用同上下文环境，bootstrap适用于固定属性，加解密等情景。 1.5各个配置文件加载顺序 配置属性加载的顺序如下： 1、开发者工具 Devtools 全局配置参数； 2、单元测试上的 @TestPropertySource 注解指定的参数； 3、单元测试上的 @SpringBootTest 注解指定的参数； 4、命令行指定的参数，如 java -jar springboot.jar --name=&quot;Java技术栈&quot;； 5、命令行中的 SPRING_APPLICATION_JSON 指定参数, 如 java -Dspring.application.json='{&quot;name&quot;:&quot;Java技术栈&quot;}' -jar springboot.jar 6、ServletConfig 初始化参数； 7、ServletContext 初始化参数； 8、JNDI参数（如 java:comp/env/spring.application.json）； 9、Java系统参数（来源：System.getProperties()）； 10、操作系统环境变量参数； 11、RandomValuePropertySource 随机数，仅匹配：ramdom.*； 12、JAR包外面的配置文件参数（application-{profile}.properties（YAML）） 13、JAR包里面的配置文件参数（application-{profile}.properties（YAML）） 14、JAR包外面的配置文件参数（application.properties（YAML）） 15、JAR包里面的配置文件参数（application.properties（YAML）） 16、@Configuration配置文件上 @PropertySource 注解加载的参数； 17、默认参数（通过 SpringApplication.setDefaultProperties 指定）； 2.Web开发 1.静态资源与定制化 2.简单功能分析 2.1、静态资源访问 1、静态资源目录 只要静态资源放在类路径下： called /static (or /public or /resources or /META-INF/resources 访问 ： 当前项目根路径/ + 静态资源名 原理： 静态映射/**。 请求进来，先去找Controller看能不能处理。不能处理的所有请求又都交给静态资源处理器。静态资源也找不到则响应404页面 改变默认的静态资源路径 1234567891011spring: mvc: static-path-pattern: /res/** resources: static-locations: [classpath:/haha/]spring: mvc: static-path-pattern: /res/** resources: static-locations: [classpath:/haha/] 2、静态资源访问前缀 默认无前缀 当前项目 + static-path-pattern + 静态资源名 = 静态资源文件夹下找 123spring: mvc: static-path-pattern: /res/** 3、webjar 自动映射 /webjars/** https://www.webjars.org/ 访问地址：http://localhost:8080/webjars/jquery/3.5.1/jquery.js 后面地址要按照依赖里面的包路径 12345&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt; 2.2、欢迎页支持 静态资源路径下 index.html 可以配置静态资源路径 但是不可以配置静态资源的访问前缀。否则导致 index.html不能被默认访问 controller能处理/index 12345spring:# mvc:# static-path-pattern: /res/** 这个会导致welcome page功能失效 resources: static-locations: [classpath:/haha/] 2.3、自定义 Favicon favicon.ico 放在静态资源目录下即可。 123spring:# mvc:# static-path-pattern: /res/** 这个会导致 Favicon 功能失效 不生效则首页添加 1234&lt;!-- &lt;link rel=&quot;shortcut icon&quot; th:href=&quot;@{/favicon.ico}&quot;/&gt;--&gt;&lt;!-- &lt;link rel=&quot;bookmark&quot; th:href=&quot;@{/favicon.ico}&quot;/&gt;--&gt; &lt;link rel=&quot;shortcut icon&quot; href=&quot;/favicon.ico&quot;/&gt; &lt;link rel=&quot;bookmark&quot; href=&quot;/favicon.ico&quot;/&gt; 停用缓存，谷歌ctrl+F5强制刷新 2.4、静态资源配置原理 SpringBoot启动默认加载 xxxAutoConfiguration 类（自动配置类） SpringMVC功能的自动配置类 WebMvcAutoConfiguration，生效 12345678@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class })@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter({ DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class })public class WebMvcAutoConfiguration {} 给容器中配了什么。 12345@Configuration(proxyBeanMethods = false)@Import(EnableWebMvcConfiguration.class)@EnableConfigurationProperties({ WebMvcProperties.class, ResourceProperties.class })@Order(0)public static class WebMvcAutoConfigurationAdapter implements WebMvcConfigurer {} 配置文件的相关属性和xxx进行了绑定。WebMvcProperties==spring.mvc、ResourceProperties==spring.resources 1、配置类只有一个有参构造器 123456789101112131415161718192021 //有参构造器所有参数的值都会从容器中确定//ResourceProperties resourceProperties；获取和spring.resources绑定的所有的值的对象//WebMvcProperties mvcProperties 获取和spring.mvc绑定的所有的值的对象//ListableBeanFactory beanFactory Spring的beanFactory//HttpMessageConverters 找到所有的HttpMessageConverters//ResourceHandlerRegistrationCustomizer 找到 资源处理器的自定义器。=========//DispatcherServletPath //ServletRegistrationBean 给应用注册Servlet、Filter.... public WebMvcAutoConfigurationAdapter(ResourceProperties resourceProperties, WebMvcProperties mvcProperties, ListableBeanFactory beanFactory, ObjectProvider&lt;HttpMessageConverters&gt; messageConvertersProvider, ObjectProvider&lt;ResourceHandlerRegistrationCustomizer&gt; resourceHandlerRegistrationCustomizerProvider, ObjectProvider&lt;DispatcherServletPath&gt; dispatcherServletPath, ObjectProvider&lt;ServletRegistrationBean&lt;?&gt;&gt; servletRegistrations) { this.resourceProperties = resourceProperties; this.mvcProperties = mvcProperties; this.beanFactory = beanFactory; this.messageConvertersProvider = messageConvertersProvider; this.resourceHandlerRegistrationCustomizer = resourceHandlerRegistrationCustomizerProvider.getIfAvailable(); this.dispatcherServletPath = dispatcherServletPath; this.servletRegistrations = servletRegistrations; } 2、资源处理的默认规则 12345678910111213141516171819202122232425262728293031323334353637383940@Override public void addResourceHandlers(ResourceHandlerRegistry registry) { if (!this.resourceProperties.isAddMappings()) { logger.debug(&quot;Default resource handling disabled&quot;); return; } Duration cachePeriod = this.resourceProperties.getCache().getPeriod(); CacheControl cacheControl = this.resourceProperties.getCache().getCachecontrol().toHttpCacheControl(); //webjars的规则 if (!registry.hasMappingForPattern(&quot;/webjars/**&quot;)) { customizeResourceHandlerRegistration(registry.addResourceHandler(&quot;/webjars/**&quot;) .addResourceLocations(&quot;classpath:/META-INF/resources/webjars/&quot;) .setCachePeriod(getSeconds(cachePeriod)).setCacheControl(cacheControl)); } // String staticPathPattern = this.mvcProperties.getStaticPathPattern(); if (!registry.hasMappingForPattern(staticPathPattern)) { customizeResourceHandlerRegistration(registry.addResourceHandler(staticPathPattern) .addResourceLocations(getResourceLocations(this.resourceProperties.getStaticLocations())) .setCachePeriod(getSeconds(cachePeriod)).setCacheControl(cacheControl)); } }spring:# mvc:# static-path-pattern: /res/** resources: add-mappings: false 禁用所有静态资源规则@ConfigurationProperties(prefix = &quot;spring.resources&quot;, ignoreUnknownFields = false)public class ResourceProperties { private static final String[] CLASSPATH_RESOURCE_LOCATIONS = { &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; }; /** * Locations of static resources. Defaults to classpath:[/META-INF/resources/, * /resources/, /static/, /public/]. */ private String[] staticLocations = CLASSPATH_RESOURCE_LOCATIONS; 3、欢迎页的处理规则 1234567891011121314151617181920212223242526HandlerMapping：处理器映射。保存了每一个Handler能处理哪些请求。 @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping(ApplicationContext applicationContext, FormattingConversionService mvcConversionService, ResourceUrlProvider mvcResourceUrlProvider) { WelcomePageHandlerMapping welcomePageHandlerMapping = new WelcomePageHandlerMapping( new TemplateAvailabilityProviders(applicationContext), applicationContext, getWelcomePage(), this.mvcProperties.getStaticPathPattern()); welcomePageHandlerMapping.setInterceptors(getInterceptors(mvcConversionService, mvcResourceUrlProvider)); welcomePageHandlerMapping.setCorsConfigurations(getCorsConfigurations()); return welcomePageHandlerMapping; }WelcomePageHandlerMapping(TemplateAvailabilityProviders templateAvailabilityProviders, ApplicationContext applicationContext, Optional&lt;Resource&gt; welcomePage, String staticPathPattern) { if (welcomePage.isPresent() &amp;&amp; &quot;/**&quot;.equals(staticPathPattern)) { //要用欢迎页功能，必须是/** logger.info(&quot;Adding welcome page: &quot; + welcomePage.get()); setRootViewName(&quot;forward:index.html&quot;); } else if (welcomeTemplateExists(templateAvailabilityProviders, applicationContext)) { // 调用Controller /index logger.info(&quot;Adding welcome page template: index&quot;); setRootViewName(&quot;index&quot;); }} 4、favicon 3、请求参数处理 1、请求映射 1、rest使用与原理 @xxxMapping； Rest风格支持（使用HTTP请求方式动词来表示对资源的操作） 以前：/getUser 获取用户 /deleteUser 删除用户 /editUser 修改用户 /saveUser 保存用户 现在： /user GET-获取用户 DELETE-删除用户 PUT-修改用户 POST-保存用户 核心Filter；HiddenHttpMethodFilter 用法： 表单method=post，隐藏域 _method=put SpringBoot中手动开启 扩展：如何把_method 这个名字换成我们自己喜欢的。 123456789101112131415161718192021222324252627282930313233343536373839 @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.GET) public String getUser(){ return &quot;GET-张三&quot;; } @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.POST) public String saveUser(){ return &quot;POST-张三&quot;; } @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.PUT) public String putUser(){ return &quot;PUT-张三&quot;; } @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.DELETE) public String deleteUser(){ return &quot;DELETE-张三&quot;; } @Bean @ConditionalOnMissingBean(HiddenHttpMethodFilter.class) @ConditionalOnProperty(prefix = &quot;spring.mvc.hiddenmethod.filter&quot;, name = &quot;enabled&quot;, matchIfMissing = false) public OrderedHiddenHttpMethodFilter hiddenHttpMethodFilter() { return new OrderedHiddenHttpMethodFilter(); }//自定义filter @Bean public HiddenHttpMethodFilter hiddenHttpMethodFilter(){ HiddenHttpMethodFilter methodFilter = new HiddenHttpMethodFilter(); methodFilter.setMethodParam(&quot;_m&quot;); return methodFilter; } Rest原理（表单提交要使用REST的时候） 表单提交会带上**_method=PUT** 请求过来被HiddenHttpMethodFilter拦截 请求是否正常，并且是POST 获取到**_method**的值。 兼容以下请求；PUT.DELETE.PATCH 原生request（post），包装模式requesWrapper重写了getMethod方法，返回的是传入的值。 过滤器链放行的时候用wrapper。以后的方法调用getMethod是调用****requesWrapper的。 Rest使用客户端工具， 如PostMan直接发送Put、delete等方式请求，无需Filter。 12345spring: mvc: hiddenmethod: filter: enabled: true #开启页面表单的Rest功能 2、请求映射原理 SpringMVC功能分析都从 org.springframework.web.servlet.DispatcherServlet-》doDispatch（） 1234567891011121314151617181920protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false;​ WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request);​ try { ModelAndView mv = null; Exception dispatchException = null;​ try { processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request);​ // 找到当前请求使用哪个Handler（Controller的方法）处理 mappedHandler = getHandler(processedRequest); //HandlerMapping：处理器映射。/xxx-&gt;&gt;xxxx RequestMappingHandlerMapping：保存了所有@RequestMapping 和handler的映射规则。 所有的请求映射都在HandlerMapping中。 SpringBoot自动配置欢迎页的 WelcomePageHandlerMapping 。访问 /能访问到index.html； SpringBoot自动配置了默认 的 RequestMappingHandlerMapping 请求进来，挨个尝试所有的HandlerMapping看是否有请求信息。 如果有就找到这个请求对应的handler 如果没有就是下一个 HandlerMapping 我们需要一些自定义的映射处理，我们也可以自己给容器中放HandlerMapping。自定义 HandlerMapping 12345678910111213141516171819202122protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { if (this.handlerMappings != null) { for (HandlerMapping mapping : this.handlerMappings) { HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) { return handler; } } } return null;} protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { if (this.handlerMappings != null) { for (HandlerMapping mapping : this.handlerMappings) { HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) { return handler; } } } return null; } 2、普通参数与基本注解 1.1、注解： @PathVariable（路径）、@RequestHeader（header）、@ModelAttribute（）、@RequestParam（请求参数）、@MatrixVariable（矩阵变量 用；隔开）、@CookieValue（cookie）、@RequestBody（请求体）、@RequestAttribute（请求域） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147@RestControllerpublic class ParameterTestController { // car/2/owner/zhangsan @GetMapping(&quot;/car/{id}/owner/{username}&quot;) public Map&lt;String,Object&gt; getCar(@PathVariable(&quot;id&quot;) Integer id, @PathVariable(&quot;username&quot;) String name, @PathVariable Map&lt;String,String&gt; pv, @RequestHeader(&quot;User-Agent&quot;) String userAgent, @RequestHeader Map&lt;String,String&gt; header, @RequestParam(&quot;age&quot;) Integer age, @RequestParam(&quot;inters&quot;) List&lt;String&gt; inters, @RequestParam Map&lt;String,String&gt; params, @CookieValue(&quot;_ga&quot;) String _ga, @CookieValue(&quot;_ga&quot;) Cookie cookie){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;();// map.put(&quot;id&quot;,id);// map.put(&quot;name&quot;,name);// map.put(&quot;pv&quot;,pv);// map.put(&quot;userAgent&quot;,userAgent);// map.put(&quot;headers&quot;,header); map.put(&quot;age&quot;,age); map.put(&quot;inters&quot;,inters); map.put(&quot;params&quot;,params); map.put(&quot;_ga&quot;,_ga); System.out.println(cookie.getName()+&quot;===&gt;&quot;+cookie.getValue()); return map; } @PostMapping(&quot;/save&quot;) public Map postMethod(@RequestBody String content){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;content&quot;,content); return map; } //1、语法： 请求路径：/cars/sell;low=34;brand=byd,audi,yd //2、SpringBoot默认是禁用了矩阵变量的功能 // 手动开启：原理。对于路径的处理。UrlPathHelper进行解析。 // removeSemicolonContent（移除分号内容）支持矩阵变量的 //3、矩阵变量必须有url路径变量才能被解析 @GetMapping(&quot;/cars/{path}&quot;) public Map carsSell(@MatrixVariable(&quot;low&quot;) Integer low, @MatrixVariable(&quot;brand&quot;) List&lt;String&gt; brand, @PathVariable(&quot;path&quot;) String path){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;low&quot;,low); map.put(&quot;brand&quot;,brand); map.put(&quot;path&quot;,path); return map; } // /boss/1;age=20/2;age=10 @GetMapping(&quot;/boss/{bossId}/{empId}&quot;) public Map boss(@MatrixVariable(value = &quot;age&quot;,pathVar = &quot;bossId&quot;) Integer bossAge, @MatrixVariable(value = &quot;age&quot;,pathVar = &quot;empId&quot;) Integer empAge){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;bossAge&quot;,bossAge); map.put(&quot;empAge&quot;,empAge); return map; }}@RestControllerpublic class ParameterTestController { // car/2/owner/zhangsan @GetMapping(&quot;/car/{id}/owner/{username}&quot;) public Map&lt;String,Object&gt; getCar(@PathVariable(&quot;id&quot;) Integer id, @PathVariable(&quot;username&quot;) String name, @PathVariable Map&lt;String,String&gt; pv, @RequestHeader(&quot;User-Agent&quot;) String userAgent, @RequestHeader Map&lt;String,String&gt; header, @RequestParam(&quot;age&quot;) Integer age, @RequestParam(&quot;inters&quot;) List&lt;String&gt; inters, @RequestParam Map&lt;String,String&gt; params, @CookieValue(&quot;_ga&quot;) String _ga, @CookieValue(&quot;_ga&quot;) Cookie cookie){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;();// map.put(&quot;id&quot;,id);// map.put(&quot;name&quot;,name);// map.put(&quot;pv&quot;,pv);// map.put(&quot;userAgent&quot;,userAgent);// map.put(&quot;headers&quot;,header); map.put(&quot;age&quot;,age); map.put(&quot;inters&quot;,inters); map.put(&quot;params&quot;,params); map.put(&quot;_ga&quot;,_ga); System.out.println(cookie.getName()+&quot;===&gt;&quot;+cookie.getValue()); return map; } @PostMapping(&quot;/save&quot;) public Map postMethod(@RequestBody String content){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;content&quot;,content); return map; } //1、语法： 请求路径：/cars/sell;low=34;brand=byd,audi,yd //2、SpringBoot默认是禁用了矩阵变量的功能 // 手动开启：原理。对于路径的处理。UrlPathHelper进行解析。 // removeSemicolonContent（移除分号内容）支持矩阵变量的 //3、矩阵变量必须有url路径变量才能被解析 @GetMapping(&quot;/cars/{path}&quot;) public Map carsSell(@MatrixVariable(&quot;low&quot;) Integer low, @MatrixVariable(&quot;brand&quot;) List&lt;String&gt; brand, @PathVariable(&quot;path&quot;) String path){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;low&quot;,low); map.put(&quot;brand&quot;,brand); map.put(&quot;path&quot;,path); return map; } // /boss/1;age=20/2;age=10 @GetMapping(&quot;/boss/{bossId}/{empId}&quot;) public Map boss(@MatrixVariable(value = &quot;age&quot;,pathVar = &quot;bossId&quot;) Integer bossAge, @MatrixVariable(value = &quot;age&quot;,pathVar = &quot;empId&quot;) Integer empAge){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;bossAge&quot;,bossAge); map.put(&quot;empAge&quot;,empAge); return map; }} 1.2、Servlet API： WebRequest、ServletRequest、MultipartRequest、 HttpSession、javax.servlet.http.PushBuilder、Principal、InputStream、Reader、HttpMethod、Locale、TimeZone、ZoneId ServletRequestMethodArgumentResolver 以上的部分参数 12345678910111213141516@Override public boolean supportsParameter(MethodParameter parameter) { Class&lt;?&gt; paramType = parameter.getParameterType(); return (WebRequest.class.isAssignableFrom(paramType) || ServletRequest.class.isAssignableFrom(paramType) || MultipartRequest.class.isAssignableFrom(paramType) || HttpSession.class.isAssignableFrom(paramType) || (pushBuilder != null &amp;&amp; pushBuilder.isAssignableFrom(paramType)) || Principal.class.isAssignableFrom(paramType) || InputStream.class.isAssignableFrom(paramType) || Reader.class.isAssignableFrom(paramType) || HttpMethod.class == paramType || Locale.class == paramType || TimeZone.class == paramType || ZoneId.class == paramType); } 1.3、复杂参数： Map、**Model（map、model里面的数据会被放在request的请求域 request.setAttribute）、**Errors/BindingResult、RedirectAttributes（ 重定向携带数据）、ServletResponse（response）、SessionStatus、UriComponentsBuilder、ServletUriComponentsBuilder 12Map&lt;String,Object&gt; map, Model model, HttpServletRequest request 都是可以给request域中放数据，request.getAttribute(); Map、Model类型的参数，会返回 mavContainer.getModel（）；---&gt; BindingAwareModelMap 是Model 也是Map mavContainer.getModel(); 获取到值的 1.4、自定义对象参数： 可以自动类型转换与格式化，可以级联封装。 3、POJO封装过程 ServletModelAttributeMethodProcessor 4、参数处理原理 HandlerMapping中找到能处理请求的Handler（Controller.method()） 为当前Handler 找一个适配器 HandlerAdapter； RequestMappingHandlerAdapter 适配器执行目标方法并确定方法参数的每一个值 1、HandlerAdapter 0 - 支持方法上标注@RequestMapping 1 - 支持函数式编程的 xxxxxx 2、执行目标方法 12345678910// Actually invoke the handler.//DispatcherServlet -- doDispatchmv = ha.handle(processedRequest, response, mappedHandler.getHandler());mav = invokeHandlerMethod(request, response, handlerMethod); //执行目标方法//ServletInvocableHandlerMethodObject returnValue = invokeForRequest(webRequest, mavContainer, providedArgs);//获取方法的参数值Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs); 3、参数解析器-HandlerMethodArgumentResolver 确定将要执行的目标方法的每一个参数的值是什么; SpringMVC目标方法能写多少种参数类型。取决于参数解析器。 当前解析器是否支持解析这种参数 支持就调用 resolveArgument 4、返回值处理器 5、如何确定目标方法每一个参数的值 123456789101112131415161718192021222324252627282930313233343536============InvocableHandlerMethod==========================protected Object[] getMethodArgumentValues(NativeWebRequest request, @Nullable ModelAndViewContainer mavContainer, Object... providedArgs) throws Exception { MethodParameter[] parameters = getMethodParameters(); if (ObjectUtils.isEmpty(parameters)) { return EMPTY_ARGS; } Object[] args = new Object[parameters.length]; for (int i = 0; i &lt; parameters.length; i++) { MethodParameter parameter = parameters[i]; parameter.initParameterNameDiscovery(this.parameterNameDiscoverer); args[i] = findProvidedArgument(parameter, providedArgs); if (args[i] != null) { continue; } if (!this.resolvers.supportsParameter(parameter)) { throw new IllegalStateException(formatArgumentError(parameter, &quot;No suitable resolver&quot;)); } try { args[i] = this.resolvers.resolveArgument(parameter, mavContainer, request, this.dataBinderFactory); } catch (Exception ex) { // Leave stack trace for later, exception may actually be resolved and handled... if (logger.isDebugEnabled()) { String exMsg = ex.getMessage(); if (exMsg != null &amp;&amp; !exMsg.contains(parameter.getExecutable().toGenericString())) { logger.debug(formatArgumentError(parameter, exMsg)); } } throw ex; } } return args; } 5.1、挨个判断所有参数解析器那个支持解析这个参数 1234567891011121314@Nullableprivate HandlerMethodArgumentResolver getArgumentResolver(MethodParameter parameter) { HandlerMethodArgumentResolver result = this.argumentResolverCache.get(parameter); if (result == null) { for (HandlerMethodArgumentResolver resolver : this.argumentResolvers) { if (resolver.supportsParameter(parameter)) { result = resolver; this.argumentResolverCache.put(parameter, result); break; } } } return result;} 5.2、解析这个参数的值 1调用各自 HandlerMethodArgumentResolver 的 resolveArgument 方法即可 5.3、自定义类型参数 封装POJO ServletModelAttributeMethodProcessor 这个参数处理器支持 是否为简单类型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public static boolean isSimpleValueType(Class&lt;?&gt; type) { return (Void.class != type &amp;&amp; void.class != type &amp;&amp; (ClassUtils.isPrimitiveOrWrapper(type) || Enum.class.isAssignableFrom(type) || CharSequence.class.isAssignableFrom(type) || Number.class.isAssignableFrom(type) || Date.class.isAssignableFrom(type) || Temporal.class.isAssignableFrom(type) || URI.class == type || URL.class == type || Locale.class == type || Class.class == type)); }@Override @Nullable public final Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception { Assert.state(mavContainer != null, &quot;ModelAttributeMethodProcessor requires ModelAndViewContainer&quot;); Assert.state(binderFactory != null, &quot;ModelAttributeMethodProcessor requires WebDataBinderFactory&quot;); String name = ModelFactory.getNameForParameter(parameter); ModelAttribute ann = parameter.getParameterAnnotation(ModelAttribute.class); if (ann != null) { mavContainer.setBinding(name, ann.binding()); } Object attribute = null; BindingResult bindingResult = null; if (mavContainer.containsAttribute(name)) { attribute = mavContainer.getModel().get(name); } else { // Create attribute instance try { attribute = createAttribute(name, parameter, binderFactory, webRequest); } catch (BindException ex) { if (isBindExceptionRequired(parameter)) { // No BindingResult parameter -&gt; fail with BindException throw ex; } // Otherwise, expose null/empty value and associated BindingResult if (parameter.getParameterType() == Optional.class) { attribute = Optional.empty(); } bindingResult = ex.getBindingResult(); } } if (bindingResult == null) { // Bean property binding and validation; // skipped in case of binding failure on construction. WebDataBinder binder = binderFactory.createBinder(webRequest, attribute, name); if (binder.getTarget() != null) { if (!mavContainer.isBindingDisabled(name)) { bindRequestParameters(binder, webRequest); } validateIfApplicable(binder, parameter); if (binder.getBindingResult().hasErrors() &amp;&amp; isBindExceptionRequired(binder, parameter)) { throw new BindException(binder.getBindingResult()); } } // Value type adaptation, also covering java.util.Optional if (!parameter.getParameterType().isInstance(attribute)) { attribute = binder.convertIfNecessary(binder.getTarget(), parameter.getParameterType(), parameter); } bindingResult = binder.getBindingResult(); } // Add resolved attribute and BindingResult at the end of the model Map&lt;String, Object&gt; bindingResultModel = bindingResult.getModel(); mavContainer.removeAttributes(bindingResultModel); mavContainer.addAllAttributes(bindingResultModel); return attribute; } WebDataBinder binder = binderFactory.createBinder(webRequest, attribute, name); WebDataBinder :web数据绑定器，将请求参数的值绑定到指定的JavaBean里面 WebDataBinder 利用它里面的 Converters 将请求数据转成指定的数据类型。再次封装到JavaBean中 GenericConversionService：在设置每一个值的时候，找它里面的所有converter那个可以将这个数据类型（request带来参数的字符串）转换到指定的类型（JavaBean -- Integer） byte -- &gt; file @FunctionalInterfacepublic interface Converter&lt;S, T&gt; 未来我们可以给WebDataBinder里面放自己的Converter； private static final class StringToNumber&lt;T extends Number&gt; implements Converter&lt;String, T&gt; 自定义 Converter 1234567891011121314151617181920212223242526272829303132//1、WebMvcConfigurer定制化SpringMVC的功能@Beanpublic WebMvcConfigurer webMvcConfigurer(){ return new WebMvcConfigurer() { @Override public void configurePathMatch(PathMatchConfigurer configurer) { UrlPathHelper urlPathHelper = new UrlPathHelper(); // 不移除；后面的内容。矩阵变量功能就可以生效 urlPathHelper.setRemoveSemicolonContent(false); configurer.setUrlPathHelper(urlPathHelper); } @Override public void addFormatters(FormatterRegistry registry) { registry.addConverter(new Converter&lt;String, Pet&gt;() { @Override public Pet convert(String source) { // 啊猫,3 if(!StringUtils.isEmpty(source)){ Pet pet = new Pet(); String[] split = source.split(&quot;,&quot;); pet.setName(split[0]); pet.setAge(Integer.parseInt(split[1])); return pet; } return null; } }); } };} 6、目标方法执行完成 将所有的数据都放在 ModelAndViewContainer；包含要去的页面地址View。还包含Model数据。 7、处理派发结果 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); renderMergedOutputModel(mergedModel, getRequestToExpose(request), response); 1234567891011121314151617181920212223242526272829303132333435363738InternalResourceView：@Override protected void renderMergedOutputModel( Map&lt;String, Object&gt; model, HttpServletRequest request, HttpServletResponse response) throws Exception { // Expose the model object as request attributes. exposeModelAsRequestAttributes(model, request); // Expose helpers as request attributes, if any. exposeHelpers(request); // Determine the path for the request dispatcher. String dispatcherPath = prepareForRendering(request, response); // Obtain a RequestDispatcher for the target resource (typically a JSP). RequestDispatcher rd = getRequestDispatcher(request, dispatcherPath); if (rd == null) { throw new ServletException(&quot;Could not get RequestDispatcher for [&quot; + getUrl() + &quot;]: Check that the corresponding file exists within your web application archive!&quot;); } // If already included or response already committed, perform include, else forward. if (useInclude(request, response)) { response.setContentType(getContentType()); if (logger.isDebugEnabled()) { logger.debug(&quot;Including [&quot; + getUrl() + &quot;]&quot;); } rd.include(request, response); } else { // Note: The forwarded resource is supposed to determine the content type itself. if (logger.isDebugEnabled()) { logger.debug(&quot;Forwarding to [&quot; + getUrl() + &quot;]&quot;); } rd.forward(request, response); } } 123暴露模型作为请求域属性// Expose the model object as request attributes. exposeModelAsRequestAttributes(model, request); 12345678910111213protected void exposeModelAsRequestAttributes(Map&lt;String, Object&gt; model, HttpServletRequest request) throws Exception { //model中的所有数据遍历挨个放在请求域中 model.forEach((name, value) -&gt; { if (value != null) { request.setAttribute(name, value); } else { request.removeAttribute(name); } }); } 4、数据响应与内容协商 1、响应JSON 1.1、jackson.jar+@ResponseBody 1234567891011 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;web场景自动引入了json场景 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-json&lt;/artifactId&gt; &lt;version&gt;2.3.4.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; 给前端自动返回json数据； 1、返回值解析器 12345678910111213141516171819202122232425262728try { this.returnValueHandlers.handleReturnValue( returnValue, getReturnValueType(returnValue), mavContainer, webRequest); } @Override public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws Exception { HandlerMethodReturnValueHandler handler = selectHandler(returnValue, returnType); if (handler == null) { throw new IllegalArgumentException(&quot;Unknown return value type: &quot; + returnType.getParameterType().getName()); } handler.handleReturnValue(returnValue, returnType, mavContainer, webRequest); }RequestResponseBodyMethodProcessor @Override public void handleReturnValue(@Nullable Object returnValue, MethodParameter returnType, ModelAndViewContainer mavContainer, NativeWebRequest webRequest) throws IOException, HttpMediaTypeNotAcceptableException, HttpMessageNotWritableException { mavContainer.setRequestHandled(true); ServletServerHttpRequest inputMessage = createInputMessage(webRequest); ServletServerHttpResponse outputMessage = createOutputMessage(webRequest); // Try even with null return value. ResponseBodyAdvice could get involved. // 使用消息转换器进行写出操作 writeWithMessageConverters(returnValue, returnType, inputMessage, outputMessage); } 2、返回值解析器原理 1、返回值处理器判断是否支持这种类型返回值 supportsReturnType 2、返回值处理器调用 handleReturnValue 进行处理 3、RequestResponseBodyMethodProcessor 可以处理返回值标了@ResponseBody 注解的。 \\1. 利用 MessageConverters 进行处理 将数据写为json 1、内容协商（浏览器默认会以请求头的方式告诉服务器他能接受什么样的内容类型） 2、服务器最终根据自己自身的能力，决定服务器能生产出什么样内容类型的数据， 3、SpringMVC会挨个遍历所有容器底层的 HttpMessageConverter ，看谁能处理？ 1、得到MappingJackson2HttpMessageConverter可以将对象写为json 2、利用MappingJackson2HttpMessageConverter将对象转为json再写出去。 1.2、SpringMVC到底支持哪些返回值 123456789101112131415ModelAndViewModelViewResponseEntity ResponseBodyEmitterStreamingResponseBodyHttpEntityHttpHeadersCallableDeferredResultListenableFutureCompletionStageWebAsyncTask有 @ModelAttribute 且为对象类型的@ResponseBody 注解 ---&gt; RequestResponseBodyMethodProcessor； 1.3、HTTPMessageConverter原理 1、MessageConverter规范 HttpMessageConverter: 看是否支持将 此 Class类型的对象，转为MediaType类型的数据。 例子：Person对象转为JSON。或者 JSON转为Person 2、默认的MessageConverter 0 - 只支持Byte类型的 1 - String 2 - String 3 - Resource 4 - ResourceRegion 5 - DOMSource.*class * SAXSource.class) \\ StAXSource.**class **StreamSource.**class **Source.class 6 - MultiValueMap 7 - true 8 - true 9 - 支持注解方式xml处理的。 最终 MappingJackson2HttpMessageConverter 把对象转为JSON（利用底层的jackson的objectMapper转换的） 2、内容协商 根据客户端接收能力不同，返回不同媒体类型的数据。 1、引入xml依赖 1234 &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt; &lt;artifactId&gt;jackson-dataformat-xml&lt;/artifactId&gt;&lt;/dependency&gt; 2、postman分别测试返回json和xml 只需要改变请求头中Accept字段。Http协议中规定的，告诉服务器本客户端可以接收的数据类型。 3、开启浏览器参数方式内容协商功能 为了方便内容协商，开启基于请求参数的内容协商功能。 123spring: contentnegotiation: favor-parameter: true #开启请求参数内容协商模式 发请求： http://localhost:8080/test/person?format=json ​ http://localhost:8080/test/person?format=xml 确定客户端接收什么样的内容类型； 1、Parameter策略优先确定是要返回json数据（获取请求头中的format的值） 2、最终进行内容协商返回给客户端json即可。 4、内容协商原理 1、判断当前响应头中是否已经有确定的媒体类型。MediaType 2、获取客户端（PostMan、浏览器）支持接收的内容类型。（获取客户端Accept请求头字段）【application/xml】 contentNegotiationManager 内容协商管理器 默认使用基于请求头的策略 HeaderContentNegotiationStrategy 确定客户端可以接收的内容类型 3、遍历循环所有当前系统的 MessageConverter，看谁支持操作这个对象（Person） 4、找到支持操作Person的converter，把converter支持的媒体类型统计出来。 5、客户端需要【application/xml】。服务端能力【10种、json、xml】 6、进行内容协商的最佳匹配媒体类型 7、用 支持 将对象转为 最佳匹配媒体类型 的converter。调用它进行转化 。 导入了jackson处理xml的包，xml的converter就会自动进来 12345678910WebMvcConfigurationSupportjackson2XmlPresent = ClassUtils.isPresent(&quot;com.fasterxml.jackson.dataformat.xml.XmlMapper&quot;, classLoader);if (jackson2XmlPresent) { Jackson2ObjectMapperBuilder builder = Jackson2ObjectMapperBuilder.xml(); if (this.applicationContext != null) { builder.applicationContext(this.applicationContext); } messageConverters.add(new MappingJackson2XmlHttpMessageConverter(builder.build())); } 5、自定义 MessageConverter 实现多协议数据兼容。json、xml、x-guigu 0、@ResponseBody 响应数据出去 调用 RequestResponseBodyMethodProcessor 处理 1、Processor 处理方法返回值。通过 MessageConverter 处理 2、所有 MessageConverter 合起来可以支持各种媒体类型数据的操作（读、写） 3、内容协商找到最终的 messageConverter； SpringMVC的什么功能。一个入口给容器中添加一个 WebMvcConfigurer 12345678910@Bean public WebMvcConfigurer webMvcConfigurer(){ return new WebMvcConfigurer() { @Override public void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { } } } 有可能我们添加的自定义的功能会覆盖默认很多功能，导致一些默认的功能失效。 大家考虑，上述功能除了我们完全自定义外？SpringBoot有没有为我们提供基于配置文件的快速修改媒体类型功能？怎么配置呢？【提示：参照SpringBoot官方文档web开发内容协商章节】 5、视图解析与模板引擎 视图解析：SpringBoot默认不支持 JSP，需要引入第三方模板引擎技术实现页面渲染。 1、视图解析 1、视图解析原理流程 1、目标方法处理的过程中，所有数据都会被放在 ModelAndViewContainer 里面。包括数据和视图地址 2、方法的参数是一个自定义类型对象（从请求参数中确定的），把他重新放在 ModelAndViewContainer 3、任何目标方法执行完成以后都会返回 ModelAndView（数据和视图地址）。 **4、**processDispatchResult 处理派发结果（页面改如何响应） 1、render(mv, request, response); 进行页面渲染逻辑 1、根据方法的String返回值得到 View 对象【定义了页面的渲染逻辑】 1、所有的视图解析器尝试是否能根据当前返回值得到View对象 2、得到了 redirect:/main.html --&gt; Thymeleaf new RedirectView() 3、ContentNegotiationViewResolver 里面包含了下面所有的视图解析器，内部还是利用下面所有视图解析器得到视图对象。 4、view.render(mv.getModelInternal(), request, response); 视图对象调用自定义的render进行页面渲染工作 RedirectView 如何渲染【重定向到一个页面】 1、获取目标url地址 **2、**response.sendRedirect(encodedURL); 视图解析： 返回值以 forward: 开始： new InternalResourceView(forwardUrl); --&gt; 转发****request.getRequestDispatcher(path).forward(request, response); 返回值以 redirect: 开始： new RedirectView() --》 render就是重定向 返回值是普通字符串： new ThymeleafView（）---&gt; 自定义视图解析器+自定义视图； 大厂学院。 2、模板引擎-Thymeleaf 1、thymeleaf简介 Thymeleaf is a modern server-side Java template engine for both web and standalone environments, capable of processing HTML, XML, JavaScript, CSS and even plain text. 现代化、服务端Java模板引擎 2、基本语法 1、表达式 表达式名字 语法 用途 变量取值 $ 获取请求域、session域、对象等值 选择变量 * 获取上下文对象值 消息 # 获取国际化等值 链接 @ 生成链接 片段表达式 ~ jsp:include 作用，引入公共页面片段 2、字面量 文本值: 'one text' , 'Another one!' **,…**数字: 0 , 34 , 3.0 , 12.3 **,…**布尔值: true , false 空值: null 变量： one，two，.... 变量不能有空格 3、文本操作 字符串拼接: + 变量替换: |The name is ${name}| 4、数学运算 运算符: + , - , * , / , % 5、布尔运算 运算符: and , or 一元运算: ! , not 6、比较运算 比较: &gt; , &lt; , &gt;= , &lt;= ( gt , lt , ge , le **)**等式: == , != ( eq , ne ) 7、条件运算 If-then: (if) ? (then) If-then-else: (if) ? (then) : (else) Default: (value) ?: (defaultvalue) 8、特殊操作 无操作： _ 3、设置属性值-th:attr 设置单个值 123456&lt;form action=&quot;subscribe.html&quot; th:attr=&quot;action=@{/subscribe}&quot;&gt; &lt;fieldset&gt; &lt;input type=&quot;text&quot; name=&quot;email&quot; /&gt; &lt;input type=&quot;submit&quot; value=&quot;Subscribe!&quot; th:attr=&quot;value=#{subscribe.submit}&quot;/&gt; &lt;/fieldset&gt;&lt;/form&gt; 设置多个值 1&lt;img src=&quot;../../images/gtvglogo.png&quot; th:attr=&quot;src=@{/images/gtvglogo.png},title=#{logo},alt=#{logo}&quot; /&gt; 以上两个的代替写法 th:xxxx 12&lt;input type=&quot;submit&quot; value=&quot;Subscribe!&quot; th:value=&quot;#{subscribe.submit}&quot;/&gt;&lt;form action=&quot;subscribe.html&quot; th:action=&quot;@{/subscribe}&quot;&gt; 所有h5兼容的标签写法 https://www.thymeleaf.org/doc/tutorials/3.0/usingthymeleaf.html#setting-value-to-specific-attributes 4、迭代 12345&lt;tr th:each=&quot;prod : ${prods}&quot;&gt; &lt;td th:text=&quot;${prod.name}&quot;&gt;Onions&lt;/td&gt; &lt;td th:text=&quot;${prod.price}&quot;&gt;2.41&lt;/td&gt; &lt;td th:text=&quot;${prod.inStock}? #{true} : #{false}&quot;&gt;yes&lt;/td&gt;&lt;/tr&gt; 12345&lt;tr th:each=&quot;prod,iterStat : ${prods}&quot; th:class=&quot;${iterStat.odd}? 'odd'&quot;&gt; &lt;td th:text=&quot;${prod.name}&quot;&gt;Onions&lt;/td&gt; &lt;td th:text=&quot;${prod.price}&quot;&gt;2.41&lt;/td&gt; &lt;td th:text=&quot;${prod.inStock}? #{true} : #{false}&quot;&gt;yes&lt;/td&gt;&lt;/tr&gt; 5、条件运算 123&lt;a href=&quot;comments.html&quot;th:href=&quot;@{/product/comments(prodId=${prod.id})}&quot;th:if=&quot;${not #lists.isEmpty(prod.comments)}&quot;&gt;view&lt;/a&gt; 12345&lt;div th:switch=&quot;${user.role}&quot;&gt; &lt;p th:case=&quot;'admin'&quot;&gt;User is an administrator&lt;/p&gt; &lt;p th:case=&quot;#{roles.manager}&quot;&gt;User is a manager&lt;/p&gt; &lt;p th:case=&quot;*&quot;&gt;User is some other thing&lt;/p&gt;&lt;/div&gt; 6、属性优先级 3、thymeleaf使用 1、引入Starter 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 2、自动配置好了thymeleaf 12345@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties(ThymeleafProperties.class)@ConditionalOnClass({ TemplateMode.class, SpringTemplateEngine.class })@AutoConfigureAfter({ WebMvcAutoConfiguration.class, WebFluxAutoConfiguration.class })public class ThymeleafAutoConfiguration { } 自动配好的策略 1、所有thymeleaf的配置值都在 ThymeleafProperties 2、配置好了 SpringTemplateEngine 3、配好了 ThymeleafViewResolver 4、我们只需要直接开发页面 123public static final String DEFAULT_PREFIX = &quot;classpath:/templates/&quot;;public static final String DEFAULT_SUFFIX = &quot;.html&quot;; //xxx.html 3、页面开发 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1 th:text=&quot;${msg}&quot;&gt;哈哈&lt;/h1&gt;&lt;h2&gt; &lt;a href=&quot;www.atguigu.com&quot; th:href=&quot;${link}&quot;&gt;去百度&lt;/a&gt; &lt;br/&gt; &lt;a href=&quot;www.atguigu.com&quot; th:href=&quot;@{link}&quot;&gt;去百度2&lt;/a&gt;&lt;/h2&gt;&lt;/body&gt;&lt;/html&gt; 4、构建后台管理系统 1、项目创建 thymeleaf、web-starter、devtools、lombok 2、静态资源处理 自动配置好，我们只需要把所有静态资源放到 static 文件夹下 3、路径构建 th:action=&quot;@{/login}&quot; 4、模板抽取 th:insert/replace/include 5、页面跳转 123456789101112131415@PostMapping(&quot;/login&quot;)public String main(User user, HttpSession session, Model model){ if(StringUtils.hasLength(user.getUserName()) &amp;&amp; &quot;123456&quot;.equals(user.getPassword())){ //把登陆成功的用户保存起来 session.setAttribute(&quot;loginUser&quot;,user); //登录成功重定向到main.html; 重定向防止表单重复提交 return &quot;redirect:/main.html&quot;; }else { model.addAttribute(&quot;msg&quot;,&quot;账号密码错误&quot;); //回到登录页面 return &quot;login&quot;; }} 6、数据渲染 123456789101112131415161718192021222324252627@GetMapping(&quot;/dynamic_table&quot;)public String dynamic_table(Model model){ //表格内容的遍历 List&lt;User&gt; users = Arrays.asList(new User(&quot;zhangsan&quot;, &quot;123456&quot;), new User(&quot;lisi&quot;, &quot;123444&quot;), new User(&quot;haha&quot;, &quot;aaaaa&quot;), new User(&quot;hehe &quot;, &quot;aaddd&quot;)); model.addAttribute(&quot;users&quot;,users); return &quot;table/dynamic_table&quot;;} &lt;table class=&quot;display table table-bordered&quot; id=&quot;hidden-table-info&quot;&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;#&lt;/th&gt; &lt;th&gt;用户名&lt;/th&gt; &lt;th&gt;密码&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr class=&quot;gradeX&quot; th:each=&quot;user,stats:${users}&quot;&gt; &lt;td th:text=&quot;${stats.count}&quot;&gt;Trident&lt;/td&gt; &lt;td th:text=&quot;${user.userName}&quot;&gt;Internet&lt;/td&gt; &lt;td &gt;[[${user.password}]]&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; 6、拦截器 1、HandlerInterceptor 接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * 登录检查 * 1、配置好拦截器要拦截哪些请求 * 2、把这些配置放在容器中 */@Slf4jpublic class LoginInterceptor implements HandlerInterceptor { /** * 目标方法执行之前 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { String requestURI = request.getRequestURI(); log.info(&quot;preHandle拦截的请求路径是{}&quot;,requestURI); //登录检查逻辑 HttpSession session = request.getSession(); Object loginUser = session.getAttribute(&quot;loginUser&quot;); if(loginUser != null){ //放行 return true; } //拦截住。未登录。跳转到登录页 request.setAttribute(&quot;msg&quot;,&quot;请先登录&quot;);// re.sendRedirect(&quot;/&quot;); request.getRequestDispatcher(&quot;/&quot;).forward(request,response); return false; } /** * 目标方法执行完成以后 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { log.info(&quot;postHandle执行{}&quot;,modelAndView); } /** * 页面渲染以后 * @param request * @param response * @param handler * @param ex * @throws Exception */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { log.info(&quot;afterCompletion执行异常{}&quot;,ex); }} 2、配置拦截器 123456789101112131415/** * 1、编写一个拦截器实现HandlerInterceptor接口 * 2、拦截器注册到容器中（实现WebMvcConfigurer的addInterceptors） * 3、指定拦截规则【如果是拦截所有，静态资源也会被拦截】 */@Configurationpublic class AdminWebConfig implements WebMvcConfigurer { @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(new LoginInterceptor()) .addPathPatterns(&quot;/**&quot;) //所有请求都被拦截包括静态资源 .excludePathPatterns(&quot;/&quot;,&quot;/login&quot;,&quot;/css/**&quot;,&quot;/fonts/**&quot;,&quot;/images/**&quot;,&quot;/js/**&quot;); //放行的请求 }} 3、拦截器原理 1、根据当前请求，找到**HandlerExecutionChain【**可以处理请求的handler以及handler的所有 拦截器】 2、先来顺序执行 所有拦截器的 preHandle方法 1、如果当前拦截器prehandler返回为true。则执行下一个拦截器的preHandle 2、如果当前拦截器返回为false。直接 倒序执行所有已经执行了的拦截器的 afterCompletion； 3、如果任何一个拦截器返回false。直接跳出不执行目标方法 4、所有拦截器都返回True。执行目标方法 5、倒序执行所有拦截器的postHandle方法。 6、前面的步骤有任何异常都会直接倒序触发 afterCompletion 7、页面成功渲染完成以后，也会倒序触发 afterCompletion 7、文件上传 1、页面表单 1234&lt;form method=&quot;post&quot; action=&quot;/upload&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;input type=&quot;file&quot; name=&quot;file&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt;&lt;/form&gt; 2、文件上传代码 1234567891011121314151617181920212223242526272829303132333435/** * MultipartFile 自动封装上传过来的文件 * @param email * @param username * @param headerImg * @param photos * @return */@PostMapping(&quot;/upload&quot;)public String upload(@RequestParam(&quot;email&quot;) String email, @RequestParam(&quot;username&quot;) String username, @RequestPart(&quot;headerImg&quot;) MultipartFile headerImg, @RequestPart(&quot;photos&quot;) MultipartFile[] photos) throws IOException { log.info(&quot;上传的信息：email={}，username={}，headerImg={}，photos={}&quot;, email,username,headerImg.getSize(),photos.length); if(!headerImg.isEmpty()){ //保存到文件服务器，OSS服务器 String originalFilename = headerImg.getOriginalFilename(); headerImg.transferTo(new File(&quot;H:\\\\cache\\\\&quot;+originalFilename)); } if(photos.length &gt; 0){ for (MultipartFile photo : photos) { if(!photo.isEmpty()){ String originalFilename = photo.getOriginalFilename(); photo.transferTo(new File(&quot;H:\\\\cache\\\\&quot;+originalFilename)); } } } return &quot;main&quot;;} 3、自动配置原理 **文件上传自动配置类-MultipartAutoConfiguration-**MultipartProperties 自动配置好了 StandardServletMultipartResolver 【文件上传解析器】 原理步骤 1、请求进来使用文件上传解析器判断（isMultipart）并封装（resolveMultipart，返回MultipartHttpServletRequest）文件上传请求 2、参数解析器来解析请求中的文件内容封装成MultipartFile **3、将request中文件信息封装为一个Map；**MultiValueMap&lt;String, MultipartFile&gt; FileCopyUtils。实现文件流的拷贝 12345@PostMapping(&quot;/upload&quot;)public String upload(@RequestParam(&quot;email&quot;) String email, @RequestParam(&quot;username&quot;) String username, @RequestPart(&quot;headerImg&quot;) MultipartFile headerImg, @RequestPart(&quot;photos&quot;) MultipartFile[] photos) 8、异常处理 1、错误处理 1、默认规则 默认情况下，Spring Boot提供/error处理所有错误的映射 对于机器客户端，它将生成JSON响应，其中包含错误，HTTP状态和异常消息的详细信息。对于浏览器客户端，响应一个“ whitelabel”错误视图，以HTML格式呈现相同的数据。 机器客户端（PostMan） 浏览器客户端 要对其进行自定义，添加View解析为error 要完全替换默认行为，可以实现 ErrorController 并注册该类型的Bean定义，或添加ErrorAttributes类型的组件以使用现有机制但替换其内容。 error/下的4xx，5xx页面会被自动解析； 2、定制错误处理逻辑 自定义错误页 error/404.html error/5xx.html；有精确的错误状态码页面就匹配精确，没有就找 4xx.html；如果都没有就触发白页 @ControllerAdvice+@ExceptionHandler处理全局异常；底层是 ExceptionHandlerExceptionResolver 支持的 @ResponseStatus+自定义异常 ；底层是 ResponseStatusExceptionResolver ，把responsestatus注解的信息底层调用 response.sendError(statusCode, resolvedReason)；tomcat发送的/error Spring底层的异常，如 参数类型转换异常；DefaultHandlerExceptionResolver 处理框架底层的异常。 response.sendError(HttpServletResponse.SC_BAD_REQUEST, ex.getMessage()); 自定义实现 HandlerExceptionResolver 处理异常；可以作为默认的全局异常处理规则 ErrorViewResolver 实现自定义处理异常； response.sendError 。error请求就会转给controller 你的异常没有任何人能处理。tomcat底层 response.sendError。error请求就会转给controller basicErrorController 要去的页面地址是 ErrorViewResolver ； 3、异常处理自动配置原理 ErrorMvcAutoConfiguration 自动配置异常处理规则 容器中的组件1：类型：DefaultErrorAttributes -&gt; id：errorAttributes public class DefaultErrorAttributes implements ErrorAttributes, HandlerExceptionResolver DefaultErrorAttributes：定义错误页面中可以包含哪些数据。 **容器中的组件2：类型：**BasicErrorController --&gt; id：basicErrorController（json+白页 适配响应） 处理默认 /error 路径的请求；页面响应 new ModelAndView(&quot;error&quot;, model)； 容器中有组件 View-&gt;id是error；（响应默认错误页） 容器中放组件 BeanNameViewResolver（视图解析器）；按照返回的视图名作为组件的id去容器中找View对象。 **容器中的组件3：**类型：**DefaultErrorViewResolver -&gt; id：**conventionErrorViewResolver 如果发生错误，会以HTTP的状态码 作为视图页地址（viewName），找到真正的页面 error/404、5xx.html 如果想要返回页面；就会找error视图【StaticView】。(默认是一个白页) 写出去json 错误页 4、异常处理步骤流程 1、执行目标方法，目标方法运行期间有任何异常都会被catch、而且标志当前请求结束；并且用 dispatchException 2、进入视图解析流程（页面渲染？） processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); 3、mv = processHandlerException；处理handler发生的异常，处理完成返回ModelAndView； 1、遍历所有的 handlerExceptionResolvers，看谁能处理当前异常【HandlerExceptionResolver处理器异常解析器】 2、系统默认的 异常解析器； 1、DefaultErrorAttributes先来处理异常。把异常信息保存到rrequest域，并且返回null； 2、默认没有任何人能处理异常，所以异常会被抛出 1、如果没有任何人能处理最终底层就会发送 /error 请求。会被底层的BasicErrorController处理 2、解析错误视图；遍历所有的 ErrorViewResolver 看谁能解析。 3、默认的 DefaultErrorViewResolver ,作用是把响应状态码作为错误页的地址，error/500.html 4、模板引擎最终响应这个页面 error/500.html 9、Web原生组件注入（Servlet、Filter、Listener） 1、使用Servlet API @ServletComponentScan(basePackages = &quot;com.atguigu.admin&quot;) :指定原生Servlet组件都放在那里 @WebServlet(urlPatterns = &quot;/my&quot;)：效果：直接响应，没有经过Spring的拦截器？ @WebFilter(urlPatterns={&quot;/css/*&quot;,&quot;/images/*&quot;}) @WebListener 推荐可以这种方式； 扩展：DispatchServlet 如何注册进来 容器中自动配置了 DispatcherServlet 属性绑定到 WebMvcProperties；对应的配置文件配置项是 spring.mvc。 通过 ServletRegistrationBean 把 DispatcherServlet 配置进来。 默认映射的是 / 路径。 Tomcat-Servlet； 多个Servlet都能处理到同一层路径，精确优选原则 A： /my/ B： /my/1 2、使用RegistrationBean 12345678910111213141516171819202122232425262728ServletRegistrationBean`, `FilterRegistrationBean`, and `ServletListenerRegistrationBean@Configurationpublic class MyRegistConfig { @Bean public ServletRegistrationBean myServlet(){ MyServlet myServlet = new MyServlet(); return new ServletRegistrationBean(myServlet,&quot;/my&quot;,&quot;/my02&quot;); } @Bean public FilterRegistrationBean myFilter(){ MyFilter myFilter = new MyFilter();// return new FilterRegistrationBean(myFilter,myServlet()); FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(myFilter); filterRegistrationBean.setUrlPatterns(Arrays.asList(&quot;/my&quot;,&quot;/css/*&quot;)); return filterRegistrationBean; } @Bean public ServletListenerRegistrationBean myListener(){ MySwervletContextListener mySwervletContextListener = new MySwervletContextListener(); return new ServletListenerRegistrationBean(mySwervletContextListener); }} 10、嵌入式Servlet容器 1、切换嵌入式Servlet容器 默认支持的webServer Tomcat, Jetty, or Undertow ServletWebServerApplicationContext 容器启动寻找ServletWebServerFactory 并引导创建服务器 切换服务器 12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 原理 SpringBoot应用启动发现当前是Web应用。web场景包-导入tomcat web应用会创建一个web版的ioc容器 ServletWebServerApplicationContext ServletWebServerApplicationContext 启动的时候寻找 **ServletWebServerFactory**``（Servlet 的web服务器工厂---&gt; Servlet 的web服务器） SpringBoot底层默认有很多的WebServer工厂；TomcatServletWebServerFactory, JettyServletWebServerFactory, or UndertowServletWebServerFactory 底层直接会有一个自动配置类。ServletWebServerFactoryAutoConfiguration ServletWebServerFactoryAutoConfiguration导入了ServletWebServerFactoryConfiguration（配置类） ServletWebServerFactoryConfiguration 配置类 根据动态判断系统中到底导入了那个Web服务器的包。（默认是web-starter导入tomcat包），容器中就有 TomcatServletWebServerFactory TomcatServletWebServerFactory 创建出Tomcat服务器并启动；TomcatWebServer 的构造器拥有初始化方法initialize---this.tomcat.start(); 内嵌服务器，就是手动把启动服务器的代码调用（tomcat核心jar包存在） `` 2、定制Servlet容器 实现 WebServerFactoryCustomizer 把配置文件的值和**ServletWebServerFactory 进行绑定** 修改配置文件 server.xxx 直接自定义 ConfigurableServletWebServerFactory xxxxxCustomizer：定制化器，可以改变xxxx的默认规则 12345678910111213import org.springframework.boot.web.server.WebServerFactoryCustomizer;import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory;import org.springframework.stereotype.Component;@Componentpublic class CustomizationBean implements WebServerFactoryCustomizer&lt;ConfigurableServletWebServerFactory&gt; { @Override public void customize(ConfigurableServletWebServerFactory server) { server.setPort(9000); }} 11、定制化原理 1、定制化的常见方式 修改配置文件； xxxxxCustomizer； 编写自定义的配置类 xxxConfiguration；+ @Bean替换、增加容器中默认组件；视图解析器 Web应用 编写一个配置类实现 WebMvcConfigurer 即可定制化web功能；+ @Bean给容器中再扩展一些组件 12@Configurationpublic class AdminWebConfig implements WebMvcConfigurer @EnableWebMvc + WebMvcConfigurer —— @Bean 可以全面接管SpringMVC，所有规则全部自己重新配置； 实现定制和扩展功能 原理 1、WebMvcAutoConfiguration 默认的SpringMVC的自动配置功能类。静态资源、欢迎页..... 2、一旦使用 @EnableWebMvc 、。会 @Import(DelegatingWebMvcConfiguration.class) 3、DelegatingWebMvcConfiguration 的 作用，只保证SpringMVC最基本的使用 把所有系统中的 WebMvcConfigurer 拿过来。所有功能的定制都是这些 WebMvcConfigurer 合起来一起生效 自动配置了一些非常底层的组件。RequestMappingHandlerMapping、这些组件依赖的组件都是从容器中获取 public class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport 4、WebMvcAutoConfiguration 里面的配置要能生效 必须 @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) 5、@EnableWebMvc 导致了 WebMvcAutoConfiguration 没有生效。 ... ... 2、原理分析套路 场景starter - xxxxAutoConfiguration - 导入xxx组件 - 绑定xxxProperties -- 绑定配置文件项 3.数据访问 1、SQL 1、数据源的自动配置-HikariDataSource 1、导入JDBC场景 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jdbc&lt;/artifactId&gt;&lt;/dependency&gt; 数据库驱动？ 为什么导入JDBC场景，官方不导入驱动？官方不知道我们接下要操作什么数据库。 数据库版本和驱动版本对应 1234567891011121314默认版本：&lt;mysql.version&gt;8.0.22&lt;/mysql.version&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;!-- &lt;version&gt;5.1.49&lt;/version&gt;--&gt; &lt;/dependency&gt;想要修改版本1、直接依赖引入具体版本（maven的就近依赖原则）2、重新声明版本（maven的属性的就近优先原则） &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;mysql.version&gt;5.1.49&lt;/mysql.version&gt; &lt;/properties&gt; 2、分析自动配置 1、自动配置的类 DataSourceAutoConfiguration ： 数据源的自动配置 修改数据源相关的配置：spring.datasource 数据库连接池的配置，是自己容器中没有DataSource才自动配置的 底层配置好的连接池是：HikariDataSource 1234567@Configuration(proxyBeanMethods = false)@Conditional(PooledDataSourceCondition.class)@ConditionalOnMissingBean({ DataSource.class, XADataSource.class })@Import({ DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.OracleUcp.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class })protected static class PooledDataSourceConfiguration DataSourceTransactionManagerAutoConfiguration： 事务管理器的自动配置 JdbcTemplateAutoConfiguration： JdbcTemplate的自动配置，可以来对数据库进行crud 可以修改这个配置项@ConfigurationProperties(prefix = &quot;spring.jdbc&quot;) 来修改JdbcTemplate @Bean@Primary JdbcTemplate；容器中有这个组件 JndiDataSourceAutoConfiguration： jndi的自动配置 XADataSourceAutoConfiguration： 分布式事务相关的 3、修改配置项 123456spring: datasource: url: jdbc:mysql://localhost:3306/db_account username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver 4、测试 123456789101112131415161718@Slf4j@SpringBootTestclass Boot05WebAdminApplicationTests { @Autowired JdbcTemplate jdbcTemplate; @Test void contextLoads() {// jdbcTemplate.queryForObject(&quot;select * from account_tbl&quot;)// jdbcTemplate.queryForList(&quot;select * from account_tbl&quot;,) Long aLong = jdbcTemplate.queryForObject(&quot;select count(*) from account_tbl&quot;, Long.class); log.info(&quot;记录总数：{}&quot;,aLong); }} 2、使用Druid数据源 1、druid官方github地址 https://github.com/alibaba/druid 整合第三方技术的两种方式 自定义 找starter 2、自定义方式 1、创建数据源 12345678910111213141516171819202122 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.17&lt;/version&gt; &lt;/dependency&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;url&quot; value=&quot;${jdbc.url}&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;${jdbc.username}&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;${jdbc.password}&quot; /&gt; &lt;property name=&quot;maxActive&quot; value=&quot;20&quot; /&gt; &lt;property name=&quot;initialSize&quot; value=&quot;1&quot; /&gt; &lt;property name=&quot;maxWait&quot; value=&quot;60000&quot; /&gt; &lt;property name=&quot;minIdle&quot; value=&quot;1&quot; /&gt; &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;60000&quot; /&gt; &lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;300000&quot; /&gt; &lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;testOnBorrow&quot; value=&quot;false&quot; /&gt; &lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt; &lt;property name=&quot;poolPreparedStatements&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;maxOpenPreparedStatements&quot; value=&quot;20&quot; /&gt; 2、StatViewServlet StatViewServlet的用途包括： 提供监控信息展示的html页面 提供监控信息的JSON API 12345678&lt;servlet&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;servlet-class&gt;com.alibaba.druid.support.http.StatViewServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;url-pattern&gt;/druid/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 3、StatFilter 用于统计监控信息；如SQL监控、URI监控 123需要给数据源中配置如下属性；可以允许多个filter，多个用，分割；如：&lt;property name=&quot;filters&quot; value=&quot;stat,slf4j&quot; /&gt; 系统中所有filter： 别名 Filter类名 default com.alibaba.druid.filter.stat.StatFilter stat com.alibaba.druid.filter.stat.StatFilter mergeStat com.alibaba.druid.filter.stat.MergeStatFilter encoding com.alibaba.druid.filter.encoding.EncodingConvertFilter log4j com.alibaba.druid.filter.logging.Log4jFilter log4j2 com.alibaba.druid.filter.logging.Log4j2Filter slf4j com.alibaba.druid.filter.logging.Slf4jLogFilter commonlogging com.alibaba.druid.filter.logging.CommonsLogFilter 慢SQL记录配置 123456&lt;bean id=&quot;stat-filter&quot; class=&quot;com.alibaba.druid.filter.stat.StatFilter&quot;&gt; &lt;property name=&quot;slowSqlMillis&quot; value=&quot;10000&quot; /&gt; &lt;property name=&quot;logSlowSql&quot; value=&quot;true&quot; /&gt;&lt;/bean&gt;使用 slowSqlMillis 定义慢SQL的时长 3、使用官方starter方式 1、引入druid-starter 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.17&lt;/version&gt;&lt;/dependency&gt; 2、分析自动配置 扩展配置项 spring.datasource.druid DruidSpringAopConfiguration.class, 监控SpringBean的；配置项：spring.datasource.druid.aop-patterns DruidStatViewServletConfiguration.class, 监控页的配置：spring.datasource.druid.stat-view-servlet；默认开启 DruidWebStatFilterConfiguration.class, web监控配置；spring.datasource.druid.web-stat-filter；默认开启 DruidFilterConfiguration.class}) 所有Druid自己filter的配置 12345678private static final String FILTER_STAT_PREFIX = &quot;spring.datasource.druid.filter.stat&quot;;private static final String FILTER_CONFIG_PREFIX = &quot;spring.datasource.druid.filter.config&quot;;private static final String FILTER_ENCODING_PREFIX = &quot;spring.datasource.druid.filter.encoding&quot;;private static final String FILTER_SLF4J_PREFIX = &quot;spring.datasource.druid.filter.slf4j&quot;;private static final String FILTER_LOG4J_PREFIX = &quot;spring.datasource.druid.filter.log4j&quot;;private static final String FILTER_LOG4J2_PREFIX = &quot;spring.datasource.druid.filter.log4j2&quot;;private static final String FILTER_COMMONS_LOG_PREFIX = &quot;spring.datasource.druid.filter.commons-log&quot;;private static final String FILTER_WALL_PREFIX = &quot;spring.datasource.druid.filter.wall&quot;; 3、配置示例 1234567891011121314151617181920212223242526272829303132spring: datasource: url: jdbc:mysql://localhost:3306/db_account username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver druid: aop-patterns: com.atguigu.admin.* #监控SpringBean filters: stat,wall # 底层开启功能，stat（sql监控），wall（防火墙） stat-view-servlet: # 配置监控页功能 enabled: true login-username: admin login-password: admin resetEnable: false web-stat-filter: # 监控web enabled: true urlPattern: /* exclusions: '*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*' filter: stat: # 对上面filters里面的stat的详细配置 slow-sql-millis: 1000 logSlowSql: true enabled: true wall: enabled: true config: drop-table-allow: false SpringBoot配置示例 https://github.com/alibaba/druid/tree/master/druid-spring-boot-starter 配置项列表https://github.com/alibaba/druid/wiki/DruidDataSource%E9%85%8D%E7%BD%AE%E5%B1%9E%E6%80%A7%E5%88%97%E8%A1%A8 3、整合MyBatis操作 https://github.com/mybatis starter SpringBoot官方的Starter：spring-boot-starter-* 第三方的： *-spring-boot-starter 12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt;&lt;/dependency&gt; 1、配置模式 全局配置文件 SqlSessionFactory: 自动配置好了 SqlSession：自动配置了 SqlSessionTemplate 组合了SqlSession @Import(AutoConfiguredMapperScannerRegistrar.class）； Mapper： 只要我们写的操作MyBatis的接口标准了 @Mapper 就会被自动扫描进来 123456@EnableConfigurationProperties(MybatisProperties.class) ： MyBatis配置项绑定类。@AutoConfigureAfter({ DataSourceAutoConfiguration.class, MybatisLanguageDriverAutoConfiguration.class })public class MybatisAutoConfiguration{}@ConfigurationProperties(prefix = &quot;mybatis&quot;)public class MybatisProperties 可以修改配置文件中 mybatis 开始的所有； 12345678910111213141516# 配置mybatis规则mybatis: config-location: classpath:mybatis/mybatis-config.xml #全局配置文件位置 mapper-locations: classpath:mybatis/mapper/*.xml #sql映射文件位置 Mapper接口---&gt;绑定Xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.atguigu.admin.mapper.AccountMapper&quot;&gt;&lt;!-- public Account getAcct(Long id); --&gt; &lt;select id=&quot;getAcct&quot; resultType=&quot;com.atguigu.admin.bean.Account&quot;&gt; select * from account_tbl where id=#{id} &lt;/select&gt;&lt;/mapper&gt; 配置 private Configuration configuration; mybatis.configuration下面的所有，就是相当于改mybatis全局配置文件中的值 12345678# 配置mybatis规则mybatis:# config-location: classpath:mybatis/mybatis-config.xml mapper-locations: classpath:mybatis/mapper/*.xml configuration: map-underscore-to-camel-case: true 可以不写全局；配置文件，所有全局配置文件的配置都放在configuration配置项中即可 导入mybatis官方starter 编写mapper接口。标准@Mapper注解 编写sql映射文件并绑定mapper接口 在application.yaml中指定Mapper配置文件的位置，以及指定全局配置文件的信息 （建议；配置在mybatis.configuration） 2、注解模式 123456789@Mapperpublic interface CityMapper { @Select(&quot;select * from city where id=#{id}&quot;) public City getById(Long id); public void insert(City city);} 3、混合模式 123456789@Mapperpublic interface CityMapper { @Select(&quot;select * from city where id=#{id}&quot;) public City getById(Long id); public void insert(City city);} 最佳实战： 引入mybatis-starter 配置application.yaml中，指定mapper-location位置即可 编写Mapper接口并标注@Mapper注解 简单方法直接注解方式 复杂方法编写mapper.xml进行绑定映射 @MapperScan(&quot;com.atguigu.admin.mapper&quot;) 简化，其他的接口就可以不用标注@Mapper注解 4、整合 MyBatis-Plus 完成CRUD 1、什么是MyBatis-Plus MyBatis-Plus（简称 MP）是一个 MyBatis 的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 mybatis plus 官网 建议安装 MybatisX 插件 2、整合MyBatis-Plus 12345&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt;&lt;/dependency&gt; 自动配置 MybatisPlusAutoConfiguration 配置类，MybatisPlusProperties 配置项绑定。mybatis-plus：xxx 就是对****mybatis-plus的定制 SqlSessionFactory 自动配置好。底层是容器中默认的数据源 **mapperLocations 自动配置好的。有默认值。*classpath*:/mapper/*/*.xml；任意包的类路径下的所有mapper文件夹下任意路径下的所有xml都是sql映射文件。 建议以后sql映射文件，放在 mapper下 容器中也自动配置好了 SqlSessionTemplate @Mapper 标注的接口也会被自动扫描；建议直接 @MapperScan(&quot;com.atguigu.admin.mapper&quot;) 批量扫描就行 优点： 只需要我们的Mapper继承 BaseMapper 就可以拥有crud能力 3、CRUD功能 123456789101112131415161718192021222324252627282930313233343536373839404142 @GetMapping(&quot;/user/delete/{id}&quot;) public String deleteUser(@PathVariable(&quot;id&quot;) Long id, @RequestParam(value = &quot;pn&quot;,defaultValue = &quot;1&quot;)Integer pn, RedirectAttributes ra){ userService.removeById(id); ra.addAttribute(&quot;pn&quot;,pn); return &quot;redirect:/dynamic_table&quot;; } @GetMapping(&quot;/dynamic_table&quot;) public String dynamic_table(@RequestParam(value=&quot;pn&quot;,defaultValue = &quot;1&quot;) Integer pn,Model model){ //表格内容的遍历// response.sendError// List&lt;User&gt; users = Arrays.asList(new User(&quot;zhangsan&quot;, &quot;123456&quot;),// new User(&quot;lisi&quot;, &quot;123444&quot;),// new User(&quot;haha&quot;, &quot;aaaaa&quot;),// new User(&quot;hehe &quot;, &quot;aaddd&quot;));// model.addAttribute(&quot;users&quot;,users);//// if(users.size()&gt;3){// throw new UserTooManyException();// } //从数据库中查出user表中的用户进行展示 //构造分页参数 Page&lt;User&gt; page = new Page&lt;&gt;(pn, 2); //调用page进行分页 Page&lt;User&gt; userPage = userService.page(page, null);// userPage.getRecords()// userPage.getCurrent()// userPage.getPages() model.addAttribute(&quot;users&quot;,userPage); return &quot;table/dynamic_table&quot;; } 123456789@Servicepublic class UserServiceImpl extends ServiceImpl&lt;UserMapper,User&gt; implements UserService {}public interface UserService extends IService&lt;User&gt; {} 2、NoSQL Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 1、Redis自动配置 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 自动配置： RedisAutoConfiguration 自动配置类。RedisProperties 属性类 --&gt; spring.redis.xxx是对redis的配置 连接工厂是准备好的。LettuceConnectionConfiguration、JedisConnectionConfiguration 自动注入了RedisTemplate&lt;Object, Object&gt; ： xxxTemplate； 自动注入了StringRedisTemplate；k：v都是String key：value 底层只要我们使用 **StringRedisTemplate、**RedisTemplate就可以操作redis redis环境搭建 1、阿里云按量付费redis。经典网络 2、申请redis的公网连接地址 3、修改白名单 允许0.0.0.0/0 访问 2、RedisTemplate与Lettuce 123456789@Testvoid testRedis(){ ValueOperations&lt;String, String&gt; operations = redisTemplate.opsForValue(); operations.set(&quot;hello&quot;,&quot;world&quot;); String hello = operations.get(&quot;hello&quot;); System.out.println(hello);} 3、切换至jedis 12345678910111213141516171819 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt;&lt;!-- 导入jedis--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;/dependency&gt;spring: redis: host: r-bp1nc7reqesxisgxpipd.redis.rds.aliyuncs.com port: 6379 password: lfy:Lfy123456 client-type: jedis jedis: pool: max-active: 10 4.单元测试 1、JUnit5 的变化 Spring Boot 2.2.0 版本开始引入 JUnit 5 作为单元测试默认库 作为最新版本的JUnit框架，JUnit5与之前版本的Junit框架有很大的不同。由三个不同子项目的几个不同模块组成。 JUnit 5 = JUnit Platform + JUnit Jupiter + JUnit Vintage JUnit Platform: Junit Platform是在JVM上启动测试框架的基础，不仅支持Junit自制的测试引擎，其他测试引擎也都可以接入。 JUnit Jupiter: JUnit Jupiter提供了JUnit5的新的编程模型，是JUnit5新特性的核心。内部 包含了一个测试引擎，用于在Junit Platform上运行。 JUnit Vintage: 由于JUint已经发展多年，为了照顾老的项目，JUnit Vintage提供了兼容JUnit4.x,Junit3.x的测试引擎。 注意： SpringBoot 2.4 以上版本移除了默认对 Vintage 的依赖。如果需要兼容junit4需要自行引入（不能使用junit4的功能 @Test**）** JUnit 5’s Vintage Engine Removed from **spring-boot-starter-test,如果需要继续兼容junit4需要自行引入vintage** 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.hamcrest&lt;/groupId&gt; &lt;artifactId&gt;hamcrest-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 现在版本： 123456789@SpringBootTestclass Boot05WebAdminApplicationTests { @Test void contextLoads() { }} 以前： @SpringBootTest + @RunWith(SpringTest.class) SpringBoot整合Junit以后。 编写测试方法：@Test标注（注意需要使用junit5版本的注解） Junit类具有Spring的功能，@Autowired、比如 @Transactional 标注测试方法，测试完成后自动回滚 2、JUnit5常用注解 JUnit5的注解与JUnit4的注解有所变化 https://junit.org/junit5/docs/current/user-guide/#writing-tests-annotations **@Test 😗*表示方法是测试方法。但是与JUnit4的@Test不同，他的职责非常单一不能声明任何属性，拓展的测试将会由Jupiter提供额外测试 **@ParameterizedTest 😗*表示方法是参数化测试，下方会有详细介绍 **@RepeatedTest 😗*表示方法可重复执行，下方会有详细介绍 **@DisplayName 😗*为测试类或者测试方法设置展示名称 **@BeforeEach 😗*表示在每个单元测试之前执行 **@AfterEach 😗*表示在每个单元测试之后执行 **@BeforeAll 😗*表示在所有单元测试之前执行 **@AfterAll 😗*表示在所有单元测试之后执行 **@Tag 😗*表示单元测试类别，类似于JUnit4中的@Categories **@Disabled 😗*表示测试类或测试方法不执行，类似于JUnit4中的@Ignore **@Timeout 😗*表示测试方法运行如果超过了指定时间将会返回错误 **@ExtendWith 😗*为测试类或测试方法提供扩展类引用 12345678910import org.junit.jupiter.api.Test; //注意这里使用的是jupiter的Test注解！！public class TestDemo { @Test @DisplayName(&quot;第一次测试&quot;) public void firstTest() { System.out.println(&quot;hello world&quot;); } 3、断言（assertions） 断言（assertions）是测试方法中的核心部分，用来对测试需要满足的条件进行验证。这些断言方法都是 org.junit.jupiter.api.Assertions 的静态方法。JUnit 5 内置的断言可以分成如下几个类别： 检查业务逻辑返回的数据是否合理。 所有的测试运行结束以后，会有一个详细的测试报告； 1、简单断言 用来对单个值进行简单的验证。如： 方法 说明 assertEquals 判断两个对象或两个原始类型是否相等 assertNotEquals 判断两个对象或两个原始类型是否不相等 assertSame 判断两个对象引用是否指向同一个对象 assertNotSame 判断两个对象引用是否指向不同的对象 assertTrue 判断给定的布尔值是否为 true assertFalse 判断给定的布尔值是否为 false assertNull 判断给定的对象引用是否为 null assertNotNull 判断给定的对象引用是否不为 null 12345678910111213141516@Test@DisplayName(&quot;simple assertion&quot;)public void simple() { assertEquals(3, 1 + 2, &quot;simple math&quot;); assertNotEquals(3, 1 + 1); assertNotSame(new Object(), new Object()); Object obj = new Object(); assertSame(obj, obj); assertFalse(1 &gt; 2); assertTrue(1 &lt; 2); assertNull(null); assertNotNull(new Object());} 2、数组断言 通过 assertArrayEquals 方法来判断两个对象或原始类型的数组是否相等 12345@Test@DisplayName(&quot;array assertion&quot;)public void array() { assertArrayEquals(new int[]{1, 2}, new int[] {1, 2});} 3、组合断言 assertAll 方法接受多个 org.junit.jupiter.api.Executable 函数式接口的实例作为要验证的断言，可以通过 lambda 表达式很容易的提供这些断言 12345678@Test@DisplayName(&quot;assert all&quot;)public void all() { assertAll(&quot;Math&quot;, () -&gt; assertEquals(2, 1 + 1), () -&gt; assertTrue(1 &gt; 0) );} 4、异常断言 在JUnit4时期，想要测试方法的异常情况时，需要用**@Rule注解的ExpectedException变量还是比较麻烦的。而JUnit5提供了一种新的断言方式Assertions.assertThrows()** ,配合函数式编程就可以进行使用。 12345678@Test@DisplayName(&quot;异常测试&quot;)public void exceptionTest() { ArithmeticException exception = Assertions.assertThrows( //扔出断言异常 ArithmeticException.class, () -&gt; System.out.println(1 % 0));} 5、超时断言 Junit5还提供了Assertions.assertTimeout() 为测试方法设置了超时时间 123456@Test@DisplayName(&quot;超时测试&quot;)public void timeoutTest() { //如果测试方法时间超过1s将会异常 Assertions.assertTimeout(Duration.ofMillis(1000), () -&gt; Thread.sleep(500));} 6、快速失败 通过 fail 方法直接使得测试失败 12345@Test@DisplayName(&quot;fail&quot;)public void shouldFail() { fail(&quot;This should fail&quot;);} 4、前置条件（assumptions） JUnit 5 中的前置条件（assumptions【假设】）类似于断言，不同之处在于不满足的断言会使得测试方法失败，而不满足的前置条件只会使得测试方法的执行终止。前置条件可以看成是测试方法执行的前提，当该前提不满足时，就没有继续执行的必要。 1234567891011121314151617181920@DisplayName(&quot;前置条件&quot;)public class AssumptionsTest { private final String environment = &quot;DEV&quot;; @Test @DisplayName(&quot;simple&quot;) public void simpleAssume() { assumeTrue(Objects.equals(this.environment, &quot;DEV&quot;)); assumeFalse(() -&gt; Objects.equals(this.environment, &quot;PROD&quot;)); } @Test @DisplayName(&quot;assume then do&quot;) public void assumeThenDo() { assumingThat( Objects.equals(this.environment, &quot;DEV&quot;), () -&gt; System.out.println(&quot;In DEV&quot;) ); }} assumeTrue 和 assumFalse 确保给定的条件为 true 或 false，不满足条件会使得测试执行终止。assumingThat 的参数是表示条件的布尔值和对应的 Executable 接口的实现对象。只有条件满足时，Executable 对象才会被执行；当条件不满足时，测试执行并不会终止。 5、嵌套测试 JUnit 5 可以通过 Java 中的内部类和@Nested 注解实现嵌套测试，从而可以更好的把相关的测试方法组织在一起。在内部类中可以使用@BeforeEach 和@AfterEach 注解，而且嵌套的层次没有限制。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@DisplayName(&quot;A stack&quot;)class TestingAStackDemo { Stack&lt;Object&gt; stack; @Test @DisplayName(&quot;is instantiated with new Stack()&quot;) void isInstantiatedWithNew() { new Stack&lt;&gt;(); } @Nested @DisplayName(&quot;when new&quot;) class WhenNew { @BeforeEach void createNewStack() { stack = new Stack&lt;&gt;(); } @Test @DisplayName(&quot;is empty&quot;) void isEmpty() { assertTrue(stack.isEmpty()); } @Test @DisplayName(&quot;throws EmptyStackException when popped&quot;) void throwsExceptionWhenPopped() { assertThrows(EmptyStackException.class, stack::pop); } @Test @DisplayName(&quot;throws EmptyStackException when peeked&quot;) void throwsExceptionWhenPeeked() { assertThrows(EmptyStackException.class, stack::peek); } @Nested @DisplayName(&quot;after pushing an element&quot;) class AfterPushing { String anElement = &quot;an element&quot;; @BeforeEach void pushAnElement() { stack.push(anElement); } @Test @DisplayName(&quot;it is no longer empty&quot;) void isNotEmpty() { assertFalse(stack.isEmpty()); } @Test @DisplayName(&quot;returns the element when popped and is empty&quot;) void returnElementWhenPopped() { assertEquals(anElement, stack.pop()); assertTrue(stack.isEmpty()); } @Test @DisplayName(&quot;returns the element when peeked but remains not empty&quot;) void returnElementWhenPeeked() { assertEquals(anElement, stack.peek()); assertFalse(stack.isEmpty()); } } }} 6、参数化测试 参数化测试是JUnit5很重要的一个新特性，它使得用不同的参数多次运行测试成为了可能，也为我们的单元测试带来许多便利。 利用**@ValueSource**等注解，指定入参，我们将可以使用不同的参数进行多次单元测试，而不需要每新增一个参数就新增一个单元测试，省去了很多冗余代码。 @ValueSource: 为参数化测试指定入参来源，支持八大基础类以及String类型,Class类型 @NullSource: 表示为参数化测试提供一个null的入参 @EnumSource: 表示为参数化测试提供一个枚举入参 @CsvFileSource：表示读取指定CSV文件内容作为参数化测试入参 @MethodSource：表示读取指定方法的返回值作为参数化测试入参(注意方法返回需要是一个流) 当然如果参数化测试仅仅只能做到指定普通的入参还达不到让我觉得惊艳的地步。让我真正感到他的强大之处的地方在于他可以支持外部的各类入参。如:CSV,YML,JSON 文件甚至方法的返回值也可以作为入参。只需要去实现ArgumentsProvider接口，任何外部文件都可以作为它的入参。 1234567891011121314151617181920@ParameterizedTest@ValueSource(strings = {&quot;one&quot;, &quot;two&quot;, &quot;three&quot;})@DisplayName(&quot;参数化测试1&quot;)public void parameterizedTest1(String string) { System.out.println(string); Assertions.assertTrue(StringUtils.isNotBlank(string));}@ParameterizedTest@MethodSource(&quot;method&quot;) //指定方法名@DisplayName(&quot;方法来源参数&quot;)public void testWithExplicitLocalMethodSource(String name) { System.out.println(name); Assertions.assertNotNull(name);}static Stream&lt;String&gt; method() { return Stream.of(&quot;apple&quot;, &quot;banana&quot;);} 7、迁移指南 在进行迁移的时候需要注意如下的变化： 注解在 org.junit.jupiter.api 包中，断言在 org.junit.jupiter.api.Assertions 类中，前置条件在 org.junit.jupiter.api.Assumptions 类中。 把@Before 和@After 替换成@BeforeEach 和@AfterEach。 把@BeforeClass 和@AfterClass 替换成@BeforeAll 和@AfterAll。 把@Ignore 替换成@Disabled。 把@Category 替换成@Tag。 把@RunWith、@Rule 和@ClassRule 替换成@ExtendWith。 5.指标监控 1、SpringBoot Actuator 1、简介 未来每一个微服务在云上部署以后，我们都需要对其进行监控、追踪、审计、控制等。SpringBoot就抽取了Actuator场景，使得我们每个微服务快速引用即可获得生产级别的应用监控、审计等功能。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 2、1.x与2.x的不同 3、如何使用 引入场景 访问 http://localhost:8080/actuator/** 暴露所有监控信息为HTTP 123456management: endpoints: enabled-by-default: true #暴露所有端点信息 web: exposure: include: '*' #以web方式暴露 测试 http://localhost:8080/actuator/beans http://localhost:8080/actuator/configprops http://localhost:8080/actuator/metrics http://localhost:8080/actuator/metrics/jvm.gc.pause http://localhost:8080/actuator/endpointName/detailPath 。。。。。。 4、可视化 https://github.com/codecentric/spring-boot-admin 2、Actuator Endpoint 1、最常使用的端点 ID 描述 auditevents 暴露当前应用程序的审核事件信息。需要一个AuditEventRepository组件。 beans 显示应用程序中所有Spring Bean的完整列表。 caches 暴露可用的缓存。 conditions 显示自动配置的所有条件信息，包括匹配或不匹配的原因。 configprops 显示所有@ConfigurationProperties。 env 暴露Spring的属性ConfigurableEnvironment flyway 显示已应用的所有Flyway数据库迁移。 需要一个或多个Flyway组件。 health 显示应用程序运行状况信息。 httptrace 显示HTTP跟踪信息（默认情况下，最近100个HTTP请求-响应）。需要一个HttpTraceRepository组件。 info 显示应用程序信息。 integrationgraph 显示Spring integrationgraph 。需要依赖spring-integration-core。 loggers 显示和修改应用程序中日志的配置。 liquibase 显示已应用的所有Liquibase数据库迁移。需要一个或多个Liquibase组件。 metrics 显示当前应用程序的“指标”信息。 mappings 显示所有@RequestMapping路径列表。 scheduledtasks 显示应用程序中的计划任务。 sessions 允许从Spring Session支持的会话存储中检索和删除用户会话。需要使用Spring Session的基于Servlet的Web应用程序。 shutdown 使应用程序正常关闭。默认禁用。 startup 显示由ApplicationStartup收集的启动步骤数据。需要使用SpringApplication进行配置BufferingApplicationStartup。 threaddump 执行线程转储。 如果您的应用程序是Web应用程序（Spring MVC，Spring WebFlux或Jersey），则可以使用以下附加端点： ID 描述 heapdump 返回hprof堆转储文件。 jolokia 通过HTTP暴露JMX bean（需要引入Jolokia，不适用于WebFlux）。需要引入依赖jolokia-core。 logfile 返回日志文件的内容（如果已设置logging.file.name或logging.file.path属性）。支持使用HTTPRange标头来检索部分日志文件的内容。 prometheus 以Prometheus服务器可以抓取的格式公开指标。需要依赖micrometer-registry-prometheus。 最常用的Endpoint Health：监控状况 Metrics：运行时指标 Loggers：日志记录 2、Health Endpoint 健康检查端点，我们一般用于在云平台，平台会定时的检查应用的健康状况，我们就需要Health Endpoint可以为平台返回当前应用的一系列组件健康状况的集合。 重要的几点： health endpoint返回的结果，应该是一系列健康检查后的一个汇总报告 很多的健康检查默认已经自动配置好了，比如：数据库、redis等 可以很容易的添加自定义的健康检查机制 3、Metrics Endpoint 提供详细的、层级的、空间指标信息，这些信息可以被pull（主动推送）或者push（被动获取）方式得到； 通过Metrics对接多种监控系统 简化核心Metrics开发 添加自定义Metrics或者扩展已有Metrics 4、管理Endpoints 1、开启与禁用Endpoints 默认所有的Endpoint除过shutdown都是开启的。 需要开启或者禁用某个Endpoint。配置模式为 management.endpoint..enabled = true 1234management: endpoint: beans: enabled: true 或者禁用所有的Endpoint然后手动开启指定的Endpoint 12345678management: endpoints: enabled-by-default: false endpoint: beans: enabled: true health: enabled: true 2、暴露Endpoints 支持的暴露方式 HTTP：默认只暴露health和info Endpoint JMX：默认暴露所有Endpoint 除过health和info，剩下的Endpoint都应该进行保护访问。如果引入SpringSecurity，则会默认配置安全访问规则 ID JMX Web auditevents Yes No beans Yes No caches Yes No conditions Yes No configprops Yes No env Yes No flyway Yes No health Yes Yes heapdump N/A No httptrace Yes No info Yes Yes integrationgraph Yes No jolokia N/A No logfile N/A No loggers Yes No liquibase Yes No metrics Yes No mappings Yes No prometheus N/A No scheduledtasks Yes No sessions Yes No shutdown Yes No startup Yes No threaddump Yes No 3、定制 Endpoint 1、定制 Health 信息 12345678910111213141516171819202122232425262728import org.springframework.boot.actuate.health.Health;import org.springframework.boot.actuate.health.HealthIndicator;import org.springframework.stereotype.Component;@Componentpublic class MyHealthIndicator implements HealthIndicator { @Override public Health health() { int errorCode = check(); // perform some specific health check if (errorCode != 0) { return Health.down().withDetail(&quot;Error Code&quot;, errorCode).build(); } return Health.up().build(); }}构建HealthHealth build = Health.down() .withDetail(&quot;msg&quot;, &quot;error service&quot;) .withDetail(&quot;code&quot;, &quot;500&quot;) .withException(new RuntimeException()) .build();management: health: enabled: true show-details: always #总是显示详细信息。可显示每个模块的状态信息 12345678910111213141516171819202122232425262728293031@Componentpublic class MyComHealthIndicator extends AbstractHealthIndicator { /** * 真实的检查方法 * @param builder * @throws Exception */ @Override protected void doHealthCheck(Health.Builder builder) throws Exception { //mongodb。 获取连接进行测试 Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 检查完成 if(1 == 2){// builder.up(); //健康 builder.status(Status.UP); map.put(&quot;count&quot;,1); map.put(&quot;ms&quot;,100); }else {// builder.down(); builder.status(Status.OUT_OF_SERVICE); map.put(&quot;err&quot;,&quot;连接超时&quot;); map.put(&quot;ms&quot;,3000); } builder.withDetail(&quot;code&quot;,100) .withDetails(map); }} 2、定制info信息 常用两种方式 1、编写配置文件 12345info: appName: boot-admin version: 2.0.1 mavenProjectName: @project.artifactId@ #使用@@可以获取maven的pom文件值 mavenProjectVersion: @project.version@ 2、编写InfoContributor 12345678910111213141516import java.util.Collections;import org.springframework.boot.actuate.info.Info;import org.springframework.boot.actuate.info.InfoContributor;import org.springframework.stereotype.Component;@Componentpublic class ExampleInfoContributor implements InfoContributor { @Override public void contribute(Info.Builder builder) { builder.withDetail(&quot;example&quot;, Collections.singletonMap(&quot;key&quot;, &quot;value&quot;)); }} http://localhost:8080/actuator/info 会输出以上方式返回的所有info信息 3、定制Metrics信息 1、SpringBoot支持自动适配的Metrics JVM metrics, report utilization of: Various memory and buffer pools Statistics related to garbage collection Threads utilization Number of classes loaded/unloaded CPU metrics File descriptor metrics Kafka consumer and producer metrics Log4j2 metrics: record the number of events logged to Log4j2 at each level Logback metrics: record the number of events logged to Logback at each level Uptime metrics: report a gauge for uptime and a fixed gauge representing the application’s absolute start time Tomcat metrics (server.tomcat.mbeanregistry.enabled must be set to true for all Tomcat metrics to be registered) Spring Integration metrics 2、增加定制Metrics 1234567891011121314151617class MyService{ Counter counter; public MyService(MeterRegistry meterRegistry){ counter = meterRegistry.counter(&quot;myservice.method.running.counter&quot;); } public void hello() { counter.increment(); }}//也可以使用下面的方式@BeanMeterBinder queueSize(Queue queue) { return (registry) -&gt; Gauge.builder(&quot;queueSize&quot;, queue::size).register(registry);} 4、定制Endpoint 12345678910111213141516@Component@Endpoint(id = &quot;container&quot;)public class DockerEndpoint { @ReadOperation public Map getDockerInfo(){ return Collections.singletonMap(&quot;info&quot;,&quot;docker started...&quot;); } @WriteOperation private void restartDocker(){ System.out.println(&quot;docker restarted....&quot;); }} 场景：开发ReadinessEndpoint来管理程序是否就绪，或者Liveness****Endpoint来管理程序是否存活； 当然，这个也可以直接使用 https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-features.html#production-ready-kubernetes-probes 6.原理解析 1、Profile功能 为了方便多环境适配，springboot简化了profile功能。 1、application-profile功能 默认配置文件 application.yaml；任何时候都会加载 指定环境配置文件 application-{env}.yaml 激活指定环境 配置文件激活 命令行激活：java -jar xxx.jar --spring.profiles.active=prod --person.name=haha 修改配置文件的任意值，命令行优先 默认配置与环境配置同时生效 同名配置项，profile配置优先 2、@Profile条件装配功能 1234567@Configuration(proxyBeanMethods = false)@Profile(&quot;production&quot;)public class ProductionConfiguration { // ...} 3、profile分组 1234spring.profiles.group.production[0]=proddbspring.profiles.group.production[1]=prodmq使用：--spring.profiles.active=production 激活 2、外部化配置 https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-external-config Default properties (specified by setting SpringApplication.setDefaultProperties). @PropertySource annotations on your @Configuration classes. Please note that such property sources are not added to the Environment until the application context is being refreshed. This is too late to configure certain properties such as logging.* and spring.main.* which are read before refresh begins. Config data (such as **application.properties** files) A RandomValuePropertySource that has properties only in random.*. OS environment variables. Java System properties (System.getProperties()). JNDI attributes from java:comp/env. ServletContext init parameters. ServletConfig init parameters. Properties from SPRING_APPLICATION_JSON (inline JSON embedded in an environment variable or system property). Command line arguments. properties attribute on your tests. Available on @SpringBootTest and the test annotations for testing a particular slice of your application. @TestPropertySource annotations on your tests. Devtools global settings properties in the $HOME/.config/spring-boot directory when devtools is active. 1、外部配置源 常用：Java属性文件、YAML文件、环境变量、命令行参数； 2、配置文件查找位置 (1) classpath 根路径 (2) classpath 根路径下config目录 (3) jar包当前目录 (4) jar包当前目录的config目录 (5) /config子目录的直接子目录 3、配置文件加载顺序： 当前jar包内部的application.properties和application.yml 当前jar包内部的application-{profile}.properties 和 application-{profile}.yml 引用的外部jar包的application.properties和application.yml 引用的外部jar包的application-{profile}.properties 和 application-{profile}.yml 4、指定环境优先，外部优先，后面的可以覆盖前面的同名配置项 3、自定义starter 1、starter启动原理 starter-pom引入 autoconfigurer 包 autoconfigure包中配置使用 META-INF/spring.factories 中 EnableAutoConfiguration 的值，使得项目启动加载指定的自动配置类 编写自动配置类 xxxAutoConfiguration -&gt; xxxxProperties @Configuration @Conditional @EnableConfigurationProperties @Bean ...... 引入starter --- xxxAutoConfiguration --- 容器中放入组件 ---- 绑定xxxProperties ---- 配置项 2、自定义starter atguigu-hello-spring-boot-starter（启动器） atguigu-hello-spring-boot-starter-autoconfigure（自动配置包） 4、SpringBoot原理 Spring原理【Spring注解】、SpringMVC原理、自动配置原理、SpringBoot原理 1、SpringBoot启动过程 创建 SpringApplication 保存一些信息。 判定当前应用的类型。ClassUtils。Servlet bootstrappers**：初始启动引导器（List）：去spring.factories文件中找** org.springframework.boot.Bootstrapper 找 ApplicationContextInitializer；去spring.factories****找 ApplicationContextInitializer List&lt;ApplicationContextInitializer&lt;?&gt;&gt; initializers 找 ApplicationListener ；应用监听器。去spring.factories****找 ApplicationListener List&lt;ApplicationListener&lt;?&gt;&gt; listeners 运行 SpringApplication StopWatch 记录应用的启动时间 **创建引导上下文（Context环境）**createBootstrapContext() 获取到所有之前的 bootstrappers 挨个执行 intitialize() 来完成对引导启动器上下文环境设置 让当前应用进入headless模式。java.awt.headless 获取所有 RunListener**（运行监听器）【为了方便所有Listener进行事件感知】** getSpringFactoriesInstances 去spring.factories****找 SpringApplicationRunListener. 遍历 SpringApplicationRunListener 调用 starting 方法； 相当于通知所有感兴趣系统正在启动过程的人，项目正在 starting。 保存命令行参数；ApplicationArguments 准备环境 prepareEnvironment（）; 返回或者创建基础环境信息对象。StandardServletEnvironment 配置环境信息对象。 读取所有的配置源的配置属性值。 绑定环境信息 监听器调用 listener.environmentPrepared()；通知所有的监听器当前环境准备完成 创建IOC容器（createApplicationContext（）） 根据项目类型（Servlet）创建容器， 当前会创建 AnnotationConfigServletWebServerApplicationContext 准备ApplicationContext IOC容器的基本信息 prepareContext() 保存环境信息 IOC容器的后置处理流程。 应用初始化器；applyInitializers； 遍历所有的 ApplicationContextInitializer 。调用 initialize.。来对ioc容器进行初始化扩展功能 遍历所有的 listener 调用 contextPrepared。EventPublishRunListenr；通知所有的监听器****contextPrepared 所有的监听器 调用 contextLoaded。通知所有的监听器 contextLoaded； **刷新IOC容器。**refreshContext 创建容器中的所有组件（Spring注解） 容器刷新完成后工作？afterRefresh 所有监听 器 调用 listeners.started(context); 通知所有的监听器 started **调用所有runners；**callRunners() 获取容器中的 ApplicationRunner 获取容器中的 CommandLineRunner 合并所有runner并且按照@Order进行排序 遍历所有的runner。调用 run 方法 如果以上有异常， 调用Listener 的 failed 调用所有监听器的 running 方法 listeners.running(context); 通知所有的监听器 running **running如果有问题。继续通知 failed 。**调用所有 Listener 的 **failed；**通知所有的监听器 failed 123456789public interface Bootstrapper { /** * Initialize the given {@link BootstrapRegistry} with any required registrations. * @param registry the registry to initialize */ void intitialize(BootstrapRegistry registry);} 12345678910111213141516171819202122@FunctionalInterfacepublic interface ApplicationRunner { /** * Callback used to run the bean. * @param args incoming application arguments * @throws Exception on error */ void run(ApplicationArguments args) throws Exception;}@FunctionalInterfacepublic interface CommandLineRunner { /** * Callback used to run the bean. * @param args incoming main method arguments * @throws Exception on error */ void run(String... args) throws Exception;} 2、Application Events and Listeners https://docs.spring.io/spring-boot/docs/current/reference/html/spring-boot-features.html#boot-features-application-events-and-listeners ApplicationContextInitializer ApplicationListener SpringApplicationRunListener 3、ApplicationRunner 与 CommandLineRunner 六、基本组件 Spring Profiles 不同环境不同配置文件 12345678application.properties（默认，配置选择其他#spring.profiles.active=test（其他配置文件的profile）指定不同环境配置application-{profile}.properties）这样每次部署需要修改配置文件，也可通过jar包运行时的命令如下：java -jar target/spring-boot-helloworld-0.0.1.jar --spring.profiles.active=prodapplication-dev.propertiesapplication-prod.propertiesapplication-test.properties spring batch[TODO] Swagger 接口文档，接口测试 启动类添加注解@EnableSwagger2Doc 123456789101112131415161718192021222324252627282930313233343536@Api：用在请求的类上，表示对类的说明 tags=&quot;说明该类的作用，可以在UI界面上看到的注解&quot; value=&quot;该参数没什么意义，在UI界面上也看到，所以不需要配置&quot; @ApiOperation：用在请求的方法上，说明方法的用途、作用 value=&quot;说明方法的用途、作用&quot; notes=&quot;方法的备注说明&quot; @ApiImplicitParams：用在请求的方法上，表示一组参数说明 @ApiImplicitParam：用在@ApiImplicitParams注解中，指定一个请求参数的各个方面 name：参数名 value：参数的汉字说明、解释 required：参数是否必须传 paramType：参数放在哪个地方 · header --&gt; 请求参数的获取：@RequestHeader · query --&gt; 请求参数的获取：@RequestParam · path（用于restful接口）--&gt; 请求参数的获取：@PathVariable · body（不常用） · form（不常用） dataType：参数类型，默认String，其它值dataType=&quot;Integer&quot; defaultValue：参数的默认值 @ApiResponses：用在请求的方法上，表示一组响应 @ApiResponse：用在@ApiResponses中，一般用于表达一个错误的响应信息 code：数字，例如400 message：信息，例如&quot;请求参数没填好&quot; response：抛出异常的类 @ApiModel：用于响应类上，表示一个返回响应数据的信息 （这种一般用在post创建的时候，使用@RequestBody这样的场景， 请求参数无法使用@ApiImplicitParam注解进行描述的时候） @ApiModelProperty：用在属性上，描述响应类的属性 七、实际应用 1.springboot启动缓慢 问题：卡在 Mapped URL path [/v2/api-docs] onto method [public org.springframework.http.ResponseEntity&lt;springfox.documentation.spring.web.json.Json&gt; springfox.documentation.swagger2.web.Swagger2Controller.getDocumentation(java.lang.String,javax.servlet.http.HttpServletRequest)] 解决：把项目下的 .idea 文件删除，然后关闭idea，重新导入 2.解决Long 型ID传给前端 js精度缺失问题 12@JsonFormat(shape = JsonFormat.Shape.STRING)Long id； Maven出错 1.检查所有JDK版本配置是否为正确版本 1）IDEA setting中 设置完后清楚IDEA缓存重启","link":"/2021/01/20/Draft/2021/SpringBoot/"},{"title":"谷粒商城","text":"介绍 新学四问 WHY【与前代优化了什么，弥补了什么空白】：学习实践分布式组件，高并发等业务 WHAT【框架，思维导图，主题框架】：通过项目扎实技术，SpringBoot+SpringCloud+Docker+VUE+Element-UI HOW【如何记忆，学习资源（官方文档、视频资源、项目地址）】：谷粒商城源码和软件、在线教程 LEVEL【不是每个都学精】：清晰明了 进度： 【85 -5.25】 =分布式基础篇= 一、项目介绍 1.1 项目背景 项目在线接口文档：1、分页请求参数 - 谷粒商城 (easydoc.net) 1.2 电商模式 市面上有5种常见的电商模式 B2B 、B2C、C2B 、C2C、O2O 1.2.1 B2B 模式 B2B （ Business to Business） 是指商家和商家建立的商业关系，如阿里巴巴。 1.2.2 B2C 模式 B2C(Business to Consumer) 就是我们经常看到的供应商直接把商品买个用户，即 “商对客” 模式，也就是我们呢说的商业零售，直接面向消费销售产品和服务，如苏宁易购，京东，天猫，小米商城 1.2.3 C2B 模式 C2B (Customer to Business),即消费者对企业，先有消费者需求产生而后有企业生产，即先有消费者提出需求，后又生产企业按需求组织生产 1.2.4 C2C 模式 C2C (Customer to Consumer) 客户之间吧自己的东西放到网上去卖 如 淘宝 咸鱼 1.2.5 O2O O2O 即 Online To Offline，也即将线下商务的机会与互联网结合在一起，让互联网成为线下交易前台，线上快速支付，线下优质服务，如：饿了么，美团，淘票票，京东到家 1.3 谷粒商城 谷粒商城是一个 B2C 模式的电商平台，销售自营商品给客户 1.4 项目架构图 项目微服务架构图 微服务划分图 1.5 项目技术和特色 ● 前后分离技术、并开发基于 vue 的后台管理系统。 ●SpringCloud 全新的解决方案 ●应用监控、限流、网关、熔断降级、等分布式方案，全方位涉及 ●透彻讲解分布式事务，分布式锁等分布式系统的难点 ●压力测试与性能优化 ●各种集群技术的区别以及使用 ●CI/CD的使用 1.6 项目前置要求 学习项目的前置知识 ●熟悉 Springboot 以及常见的整合方案 ●了解 SpringCloud ●熟悉 git maven ●熟悉 Linux 、 redis docker 基本操作 ●了解html、css、js 、vue ●熟悉使用 idea 开发项目 二、分布式基础概念 2.1 微服务 微服务架构风格，就像是把一个单独的应用程序开发成一套小服务，每个小服务运行在自己的进程中，并使用轻量级机制通信，通常是HTTP API 这些服务围绕业务能力来构建，并通过完全自动化部署机制来独立部署，这些服务使用不同的编程语言来写，以及不同的数据存储技术，并保持最低限度的集中式管理。 简而言之，拒绝大型单体应用，基于业务边界进行服务拆分，每个服务独立部署运行。 2.2 集群 &amp; 分布式 &amp; 节点 集群是个物理状态，分布式是个工作方式。 只要是一对机器，也可以叫做集群，他不是一起协作干活，这谁也不知道。 《分布式系统原理与范型》定义： “分布式系统是若干独立计算机的集合，这些计算机对于用户来说像但各相关系统” 分布式系统 (distributed system) 是建立网络之上的软件系统 分布式是指讲不通的业务分布在不同的地方 集群实质是将几台服务器集中在一起，实现同一业务 例如：京东是一个分布式系统，众多业务运行在不同的机器上，所有业务构成一个大型的业务集群，每一个小的业务，比如用户系统，访问压力大的时候一台服务器是不够的，我们就应该将用户系统部署到多个服务器，也就是每个一业务系统也可以做集群化 分布式中的每一个节点，都可以做集群，而集群并不一定就是分布式的 节点：集群中的一个服务器 2.3 远程调用 在分布式系统中，各个服务可能处于不同主机，但是服务之间不可避免的需要互相调用，我们称之为远程调用 SpringCloud 中使用 HTTP + JSON 的方式来完成远程调用。 HTTP+JSON 2.4 负载均衡 分布式系统中，A 服务需要调用 B 服务，B 服务在多台机器中都存在，A 调用任意一个服务器均可完成功能。 为了使每一个服务器都不要太忙后者太闲，我们可以负载均衡调用每一个服务器，提升网站的健壮性。 常见的负载均算法： 轮询： 为第一个请求选择健康池中的第一个后端服务器，然后按顺序往后依次选择，直到最后一个，然后循环。 最小连接： 优先选择连接数最少，也就是压力最小的后端服务器，在会话较长的情况下可以考虑采取这种方式。 2.5 服务注册/发现 &amp; 注册中心 A 服务调用 B 服务， A 服务 不知道 B 服务当前在哪几台服务器上有，那些正常的，那些服务已经下线，解决这个问题可以引入注册中心。 如果某些服务下线，我们其他人可以实时的感知到其他服务的状态，从而避免调用不可用的服务。 2.6 配置中心 每一个服务最终都有大量配置，并且每个服务都可能部署在多个服务器上，我们经常需要变更配置，我们可以让每个服务在配置中心获取自己的配置，配置中心用来集中管理微服务的配置信息。 2.7 服务熔断 &amp; 服务降级 在微服务架构中，微服务之间通过网络来进行通信，存在相互依赖，当其中一个服务不可用时，有可能会造成雪崩效应，要防止这种情况，必须要有容错机制来保证服务。 服务熔断 ●设置服务的超时，当被调用的服务经常失败达到某个阈值，我们可以开启短路保护机制，后来的请求不再去调用这个服务，本地直接返回默认的数据。 服务降级 ●在运维期间，当系统处于高峰期，系统资源紧张，我们可以让非核心业务降级运行，降级某些服务不处理，或者简单处理【抛异常，返回NULL，调用MOCk数据，调用FallBack处理逻辑】 2.8 Api网关 在微服务架构中， Api Gateway 作为整体架构的重要组件，它抽象服务中需要的公共功能，同时它提供给了客户端负载均衡、服务自动熔断、灰度发布、统一认证、限流监控、日志统计 等 丰富功能，帮助我们解决了很多 API 管理的难题 。 三、搭建环境 3.1 安装 Linux 虚拟机 下载&amp;安装 VirtualBox https://www.virtualbox.org/ 要开启 CPU 虚拟化 下载&amp; 安装 Vagrant https://app.vagrantup.com/boxes/search Vagrant 官方镜像仓库 https://www.vagrantup.com/downloads.html Vagrant下载 打开window cmd窗口，运行Vagrant init centos/7 ,即可初始化一个centos系统 运行vagrant up即可启动虚拟机。系统root用户的密码是vagrant vagrant其他常用命令 1234567vagrant ssh 自动使用 vagrant 用户连接虚拟机vagrant upload source [destination] [name|id] 上传文件https://www.vagrantup.com/docs/cli/init.html Vagrant 命令行vagrant reload 重启并重新加载Vagrantfile 默认虚拟机的ip 地址不是固定ip 开发不方便 Vagrant 和 VirtualBox 版本有对应问题 都安装最新版本 则安装成功 修改 Vagrantfile config.vm.network &quot;private_network&quot;, ip: &quot;192.168.56.10&quot; 这里 ip 需要在 物理机下使用 ipconfig 命令找到 3.2 安装 docker Docker 参考 Docker 3.2.1 卸载系统之前安装的docker Uninstall old versions. 一步一步往下执行就行 12345678sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine SET UP THE REPOSITORY Install the yum-utils package (which provides the yum-config-manager utility) and set up the stable repository. 1234sudo yum install -y yum-utilssudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo INSTALL DOCKER ENGINE 1.Install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: 1sudo yum install docker-ce docker-ce-cli containerd.io Start Docker 1sudo systemctl start docker Verify that Docker Engine is installed correctly by running the hello-world image. 1sudo docker run hello-world 设置 docker 开机自启动 1sudo systemctl enable docker 查看Docker的镜像 1sudo docker images 设置 Docker 镜像加速 登录 aliyun.com 在控制台找到容器镜像服务 docker 进入 对应容器 123docker ps #查看主机有哪些容器正在运行docker exec -it 容器ID/NAME /bin/bash #进入对应容器exit; #退出容器[快捷键 ctrl + p/q] 3.3 docker 安装 mysql 3.3.1 下载镜像文件 12sudo docker pull mysql:5.7sudo docker images 3.3.2 创建实例并启动 12345678910111213# --name指定容器名字 -v目录挂载 -p指定端口映射 -e设置mysql参数 -d后台运行sudo docker run -p 3306:3306 --name mysql \\-v /mydata/mysql/log:/var/log/mysql \\-v /mydata/mysql/data:/var/lib/mysql \\-v /mydata/mysql/conf:/etc/mysql \\-e MYSQL_ROOT_PASSWORD=root \\-d mysql:5.7####-v 将对应文件挂载到主机【类似快捷方式，两者内容同步】，比如-v /mydata/mysql/log【主机快捷方式】:/var/log/mysql【容器中文件夹】-e 初始化对应MYSQL参数-p 容器端口映射到主机的端口su rootdocker ps #查看主机有哪些容器正在运行 MySQL 配置 vi /mydata/mysql/conf/my.cnf 创建&amp;修改该文件 1234567891011[client]default-character-set=utf8[mysql]default-character-set=utf8[mysqld]init_connect='SET collation_connection = utf8_unicode_ci'init_connect='SET NAMES utf8'character-set-server=utf8collation-server=utf8_unicode_ciskip-character-set-client-handshakeskip-name-resolve 12#重启mysqldocker restart mysql 3.3.3 通过容器的mysql 命令工具连接 3.4 docker 安装 redis 3.4.1 下载镜像文件 1docker pull redis 3.4.2 创建实例并启动 1234567891011#先创建文件夹和文件再挂载，否则会将其文件名创建为文件夹mkdir -p /mydata/redis/conftouch /mydata/redis/conf/redis.conf# 启动 同时 映射到对应文件夹# 后面 \\ 代表换行docker run -p 6379:6379 --name redis \\-v /mydata/redis/data:/data \\-v /mydata/redis/conf/redis.conf:/etc/redis/redis.conf \\-d redis redis-server /etc/redis/redis.confdocker ps 3.4.3 使用 redis 镜像执行 redis-cli 命令连接 进入docker cli docker exec -it redis redis-cli 持久化 默认 appendonly on 没有开启 1234vi /mydata/redis/conf/redis.conf# 插入下面内容appendonly yesdocker restart redis 3.5 开发环境统一 3.5.1 Maven版本与配置 1.配置阿里云镜像 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 配置 jdk 1.8 编译项目 12345678910111213&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; 3.5.2 idea &amp; vscode VSCode插件 Vetur ——语法高亮，智能感知 包含格式化功能，Alt+Shift+F (格式化全文) ，Ctrl+K Ctrl+F (格式化选中代码，两个Ctrl需要同时按着) EsLint 一一 语法纠错 Auto Close Tag 一一 自动闭合HTML/XML标签 Auto Rename Tag 一一 自动完成另-侧标签的同步修改 JavaScript(ES6) code snippets 一一 ES6 语法智能提示以及快速输入，除j外还支持.ts, .jsx， .tsx， .html, .vue;省去了配置其支持各种包含is代码文件的时间 3.5.3 MybatisPlus整合 4.1 配置环境 4.1.1、导入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; 4.2.2、配置数据源 配置数据源 导入数据库驱动 https://mvnrepository.com/artifact/mysql/mysql-connector-java 123456&lt;!--导入mysql驱动--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.17&lt;/version&gt; &lt;/dependency&gt; 在application.yml配置数据源相关信息 123456789101112spring: datasource: username: root password: root url: jdbc:mysql://192.168.56.10:3306/gulimall_pms driver-class-name: com.mysql.jdbc.Drivermybatis-plus: # mapper文件扫描 mapper-locations: classpath*:/mapper/**/*.xml global-config: db-config: id-type: auto # 数据库主键自增 配置MyBatis-Plus包扫描： 使用@MapperScanner 告诉MyBatis-Plus,Sql映射文件位置 1234567@MapperScan(&quot;com.atguigu.gulimall.product.dao&quot;)@SpringBootApplicationpublic class GulimallProductApplication { public static void main(String[] args) { SpringApplication.run(GulimallProductApplication.class, args); }} 具体过程参考官网： https://baomidou.com/guide/install.html#release 4.2 分页配置 12345678910111213141516171819202122@Configuration // 声明配置类@EnableTransactionManagement // 开启注解@MapperScan(&quot;com.atguigu.gulimall.product.dao&quot;) // 指定扫描包public class MyBatisConfig { /** * 引入分页插件 拦截器 * @return */ @Bean public PaginationInterceptor paginationInterceptor() { PaginationInterceptor paginationInterceptor = new PaginationInterceptor(); // 设置请求的页面大于最大页后操作， true调回到首页，false 继续请求 默认false paginationInterceptor.setOverflow(true); // 设置最大单页限制数量，默认 500 条，-1 不受限制 paginationInterceptor.setLimit(1000); // 开启 count 的 join 优化,只针对部分 left join paginationInterceptor.setCountSqlParser(new JsqlParserCountOptimize(true)); return paginationInterceptor; }} 4.3 逻辑删除 说明: 只对自动注入的sql起效: 插入: 不作限制 查找: 追加where条件过滤掉已删除数据,且使用 wrapper.entity 生成的where条件会忽略该字段 更新: 追加where条件防止更新到已删除数据,且使用 wrapper.entity 生成的where条件会忽略该字段 删除: 转变为 更新 例如: 删除: update user set deleted=1 where id = 1 and deleted=0 查找: select id,name,deleted from user where deleted=0 步骤1：配置 application.yml 1234567mybatis-plus: mapper-locations: classpath*:/mapper/**/*.xml global-config: db-config: id-type: auto # 数据库主键自增 logic-delete-value: 1 # 逻辑已删除值(默认为 1) logic-not-delete-value: 0 # 逻辑未删除值(默认为 0) 步骤2: 实体类字段上加上@TableLogic注解 12345/** * 是否显示[0-不显示，1显示] */@TableLogic(value = &quot;1&quot;,delval = &quot;0&quot;)private Integer showStatus; 3.5.4 安装配置git 下载git https://git-scm.com/ 配置 git 进入 git bash 1234# 配置用户名git config --global user.name &quot;user.name&quot;# 配置邮箱git config --global user.email &quot;username@email.com&quot; # 注册账号使用的邮箱 配置 ssh 免密码登录 https://github.com/settings/keys 12345678git bash 使用 ssh-keygen -t rsa -C &quot;XXX@xxx.com&quot; 命令 连续三次回车一般用户目录下都会有id_rsa 文件id_rsa.pub 文件或者 cat ~/.ssh/id_rsa.pub登录进 github/gitee 设置 SSH KEY使用 ssh -T git@gitee.com 测试是否成功具体过程参考百度 项目结构创建 3.5.5 逆向工程的使用 1.下载人人开源项目 https://gitee.com/renrenio 选择 renren -generator 项目 1.下载之后，导入项目，修改数据库，修改工程shiro依赖为SpringSecurity 2.修改对应数据库配置，模块名与数据库表名前缀 下载 renren-fast-vue项目 1.vscode 导入前端项目 2.前后端联调测试基本功能 2.创建业务模块 3.配置公共环境 4.使用人人代码生成器生成基础业务代码 修改相关配置 3.6 前后端联调 3.6.1 启动前端项目 npm install 安装依赖 通过 npm run dev 启动 3.6.2 启动后端项目 这时，你使用浏览器直接该接口，发现访问没事，但是用前端项目访问却报错，什么原因？ 这其实是浏览器的同源策略造成的跨域问题。 3.6.3 跨域问题 跨域：浏览器对于javascript的同源策略的限制 。 以下情况都属于跨域： 跨域原因说明 示例 域名不同 www.jd.com 与 www.taobao.com 域名相同，端口不同 www.jd.com:8080 与 www.jd.com:8081 二级域名不同 item.jd.com 与 miaosha.jd.com 如果域名和端口都相同，但是请求路径不同，不属于跨域，如： www.jd.com/item www.jd.com/goods http和https也属于跨域 而我们刚才是从manager.gmall.com去访问api.gmall.com，这属于端口不同，跨域了。 3.6.3.1 为什么会有跨域问题 跨域不一定都会有跨域问题。 因为跨域问题是浏览器对于ajax请求的一种安全限制：一个页面发起的ajax请求，只能是与当前页域名相同的路径，这能有效的阻止跨站攻击。 因此：跨域问题 是针对ajax的一种限制。 但是这却给我们的开发带来了不便，而且在实际生产环境中，肯定会有很多台服务器之间交互，地址和端口都可能不同，怎么办？ 3.6.3.2 为什么会有跨域问题 目前比较常用的跨域解决方案有3种： ●Jsonp最早的解决方案，利用script标签可以跨域的原理实现。限制： ○需要服务的支持 ○只能发起GET请求 ●nginx反向代理 思路是：利用nginx把跨域反向代理为不跨域，支持各种请求方式 缺点：需要在nginx进行额外配置，语义不清晰 ●CORS规范化的跨域请求解决方案，安全可靠。优势： ○在服务端进行控制是否允许跨域，可自定义规则 ○支持各种请求方式 ●缺点： ○会产生额外的请求 我们这里会采用cors的跨域方案。 3.6.4 跨域解决 什么是cors CORS是一个W3C标准，全称是&quot;跨域资源共享&quot;（Cross-origin resource sharing）。 它允许浏览器向跨源服务器，发出XMLHttpRequest请求，从而克服了AJAX只能同源使用的限制。 CORS需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE浏览器不能低于IE10。 ●浏览器端： 目前，所有浏览器都支持该功能（IE10以下不行）。整个CORS通信过程，都是浏览器自动完成，不需要用户参与。 ●服务端： CORS通信与AJAX没有任何差别，因此你不需要改变以前的业务逻辑。只不过，浏览器会在请求中携带一些头信息，我们需要以此判断是否允许其跨域，然后在响应头中加入一些信息即可。这一般通过过滤器完成即可。 原理 预检请求 跨域请求会在正式通信之前，增加一次HTTP查询请求，称为&quot;预检&quot;请求（preflight）。 浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 一个“预检”请求的样板： OPTIONS /cors HTTP/1.1 Origin: http://localhost:1000 Access-Control-Request-Method: GET Access-Control-Request-Headers: X-Custom-Header User-Agent: Mozilla/5.0... ●Origin：会指出当前请求属于哪个域（协议+域名+端口）。服务会根据这个值决定是否允许其跨域。 ●Access-Control-Request-Method：接下来会用到的请求方式，比如PUT ●Access-Control-Request-Headers：会额外用到的头信息 预检请求的响应 服务的收到预检请求，如果许可跨域，会发出响应： HTTP/1.1 200 OK Date: Mon, 01 Dec 2008 01:15:39 GMT Server: Apache/2.0.61 (Unix) Access-Control-Allow-Origin: http://miaosha.jd.com Access-Control-Allow-Credentials: true Access-Control-Allow-Methods: GET, POST, PUT Access-Control-Allow-Headers: X-Custom-Header Access-Control-Max-Age: 1728000 Content-Type: text/html; charset=utf-8 Content-Encoding: gzip Content-Length: 0 Keep-Alive: timeout=2, max=100 Connection: Keep-Alive Content-Type: text/plain 如果服务器允许跨域，需要在返回的响应头中携带下面信息： ●Access-Control-Allow-Origin：可接受的域，是一个具体域名或者*（代表任意域名） ●Access-Control-Allow-Credentials：是否允许携带cookie，默认情况下，cors不会携带cookie，除非这个值是true ●Access-Control-Allow-Methods：允许访问的方式 ●Access-Control-Allow-Headers：允许携带的头 ●Access-Control-Max-Age：本次许可的有效时长，单位是秒，过期之前的ajax请求就无需再次进行预检了 有关cookie： 要想操作cookie，需要满足3个条件： ●服务的响应头中需要携带Access-Control-Allow-Credentials并且为true。 ●浏览器发起ajax需要指定withCredentials 为true ●响应头中的Access-Control-Allow-Origin一定不能为*，必须是指定的域名 实现方式 虽然原理比较复杂，但是前面说过： ●浏览器端都有浏览器自动完成，我们无需操心 ●服务端可以通过拦截器统一实现，不必每次都去进行跨域判定的编写。 事实上，Spring已经帮我们写好了CORS的跨域过滤器，内部已经实现了刚才所讲的判定逻辑。 12spring-webmvc：CorsFilterspring-webflux：CorsWebFilter springcloud-gateway集成的是webflux，所以这里使用的是CorsWebFilter 在gmall-gateway中编写一个配置类，并且注册CorsWebFilter： Plain Text 123456789101112131415161718192021@Configurationpublic class CorsConfig { @Bean public CorsWebFilter corsWebFilter() { // 初始化CORS配置对象 CorsConfiguration config = new CorsConfiguration(); // 允许的域,不要写*，否则cookie就无法使用了 config.addAllowedOrigin(&quot;http://manager.gmall.com&quot;); config.addAllowedOrigin(&quot;http://www.gmall.com&quot;); // 允许的头信息 config.addAllowedHeader(&quot;*&quot;); // 允许的请求方式 config.addAllowedMethod(&quot;*&quot;); // 是否允许携带Cookie信息 config.setAllowCredentials(true); // 添加映射路径，我们拦截一切请求 UrlBasedCorsConfigurationSource corsConfigurationSource = new UrlBasedCorsConfigurationSource(); corsConfigurationSource.registerCorsConfiguration(&quot;/**&quot;, config); return new CorsWebFilter(corsConfigurationSource); }} 四、分布式组件 SpringCloud Alibaba 请查看cloud笔记或官方文档或中文文档 SpringClouid的几大痛点： SpringCloud部分组件停止维护和更新，给开发带来不便; SpringCloud部分环境搭建复杂，没有完善的可视化界面，我们需要大量的二次开发和定制 SpringCloud配置复杂，难以上手，部分配置差别难以区分和合理应用 SpringCloud Alibaba的优势： 阿里使用过的组件经历了考验，性能强悍，设计合理，现在开源出来大家用 成套的产品搭配完善的可视化界面给开发运维带来极大的便利 搭建简单，学习曲线低。 技术搭配方案 SpringCloud Alibaba Nacos : 注册中心 (服务发现/注册) Nacos: 配置中心 (动态配置管理) Sentinel: 服务容错(限流、降级、熔断) Seata: 原Fescar, 即分布式事务解决方案 SpringCloud Ribbon: 负载均衡 Feign: 声明式HTTP客户端(调用远程服务) Gateway: API 网关 (webflux 编程模式) Sleuth:调用链监控 项目架构 组件技术方案 Nacos【注册中心】 注册中心【服务发现/注册】 1.公共依赖引进 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 2.下载Nacos Server Releases · alibaba/nacos (github.com)并点击文件夹下的startup.bat启动服务,启动不了可能未配JAVA_HOME环境变量 3.配置 1、首先，修改 pom.xml 文件，引入 Nacos Discovery Starter 1234&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 2、在应用的 /resource /application.properties 中配置 Nacos Server地址 1spring.cloud.nacos.discovery.server-addr=127.0.0.1:8848 4.启动类添加注解开启服务与注解功能 1@EnableDiscoveryClient 5.配置模块名 1spring.application.name=service-provider 6.启动模块访问 Nacos Nacos【配置中心】 1、pom.xml 引入 Nacos Config Starter 12345 &lt;!--配置中心来做配置管理--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 5.3.2、在应用的 resource 下 bootstrap.properties 12spring.application.name=gulimall-couponspring.cloud.nacos.config.server-addr=127.0.0.1:8848 5.3.3、在 nacos 中添加配置 选择右上角添加配置 Data ID 改成 gulimall-coupon.properties 默认规则 用户名.应用名.properties 5.3.4、在应用中使用@Value 和 @RefreshScope 1234567891011121314151617@RefreshScope // 刷新对应controller@RestController@RequestMapping(&quot;coupon/coupon&quot;)public class CouponController { @Autowired private CouponService couponService; @Value(&quot;${coupon.user.name}&quot;) private String name; @Value(&quot;${coupon.user.age}&quot;) private Integer age; @RequestMapping(&quot;/test&quot;) public R test() { return R.ok().put(&quot;name&quot;,name).put(&quot;age&quot;,age); } 5.3.5、进阶 1、核心概念 命名空间: 用于进行租户粒度的配置隔离。不同的命名空间下，可以存在相同的 Group 或 DatalD 的配置。Namespace 的常用场景之一是不同环境的配置的区分隔离，例如开发测试环境和生产环境的资源(如配置、服务)隔离等。 配置集 一组相关或者不相关的配置项的集合称为配置集。在系统中，一个配置文件通常就是一个配置集，包含了系统各个方面的配置。例如，一个配置集可能包含了数据源、线程池、日志级别等配置项。 配置集ID: Nacos 中的某个配置集的 ID，配置集 ID 是组织划分配置的维度之一，Data ID 通常用于组织划分系统的配置集，一个系统或者应用可以包含多个配置集，一个系统应用可以包含多个配置集，每个配置集都可以被一个有意义的名称标识，Data ID 通常采用类 Java 包 如 ( com.taobao.tc.refund.log.level ) 的命名规则保证全局唯一性，此命名规则非强制 配置分组： Nacos 中的一组配置集，是组织配置的维度之一，通过一个有意义的字符串，(如 Buy 或 Trade ) 对配置集进行分组，从而区分 Data ID 相同的配置集，当您在 Nacos 上创建一个配置时，如果未填写配置分组的名称，则配置分组的名称默认采用，DEFAULT_GROUP 配置分组的常见场景，不同的应用或组件采用了相同的配置类型，如 database_url 配置和 MQ_topic 配置 bootstrap.properties 配置 12345678910spring.application.name=gulimall-coupon # 服务的名称spring.cloud.nacos.config.server-addr=127.0.0.1:8848 # 服务注册地址spring.cloud.nacos.config.namespace=ae34901c-9215-4409-ae61-c6b8d6c8f9b0 # 命名空间地址#spring.cloud.nacos.config.group=111 # 对应分组spring.cloud.nacos.config.ext-config[0].data-id=datasource.yml # 配置集指定data_idspring.cloud.nacos.config.ext-config[0].group=dev # 配置集指定 group 分组spring.cloud.nacos.config.ext-config[0].refresh=true # 是否动态刷新 在配置中心修改后 微服务自动刷新 相关解释 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 1、如何使用 Nacos作为配置中心统一管理配置 * 1.1 引入依赖 * &lt;dependency&gt; * &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; * &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; * &lt;/dependency&gt; * 1.2 创建一个bootstrap.properties * spring.application.name=gulimall-coupon * spring.cloud.nacos.config.server-addr=127.0.0.1:8848 * 1.3 需要给配置中心默认添加一个叫 数据集 (Data Id) gulimall-coupon.properties 默认规则 用户名.应用名.properties * 1.4 给 应用名.properties 添加任何配置 * 1.5 动态获取配置 * @RefreshScope: 动态获取并刷新配置 * @Value$(&quot;${配置项的名字}&quot;) * 如果配置中心和当前应用的配置文件中都配置了相同的配置文件，优先使用配置中心的文件 * * 2、细节 * 1、命名空间：配置隔离： * 默认：public(保留空间)；默认新增的所有配置都在 public 空间 * 1、开发 测试 生产 利用命名空间来做环境隔离 * 注意: 在bootstrap.properties配置上 需要使用哪个命名空间下的配置 * spring.cloud.nacos.config.namespace=b1e89ce0-784f-4a80-9de0-e9b080eeaca5 * 2、每一个微服务之间互相隔离配置，每一个微服务都创建自己的命名空间，只加载自己命名空间下的所有配置 * * * 2、配置集：所有配置的集合 * * 3、配置集ID：类似文件名 * Data ID：类型文件名 * * 4、配置分组： * 默认所有的配置都属于 DEFAULT_GROUP * * 每个微服务创建自己的命名空间，使用配置分组区分环境，dev、test、prod * * 3、同时加载多个配置集 * 1、微服务任何配置信息，任何配置文件多可以放在配置中心中 * 2、只要在 bootstrap.properties 说明加载配置中心中哪些配置文件即可 * 3、@Value，@ConfigurationProperties... * 以前SpringBoot的任何方式从配置文件中获取值，都能使用 * 配置中心有的优先使用配置中心的 * @author gcq * @Create 2020-10-16 */ Sentinel 1、简介 1、服务降级限流 什么是熔断： ​ A 服务调用 B 服务的某个功能，由于网络不稳定问题，或者 B 服务卡机，导致功能时间超长，如果这样子的次数太多，我们可以直接将 B 断路了，（A 不在请求 B 接口）凡是调用 B 服务的直接返回降级数据，不必等待 B 的 超时执行，这样 B 的故障问题，就不会级联影响到 A。 什么是降级： ​ 整个网站处于流量高峰期服务器压力剧增，根据当前自身业务情况以及流量，对一些服务和页面进行有策略的降级/停止服务，所有的调用直接返回降级数据以此缓解服务器资源的压力，以保证核心业务的正常运行，同时也保持了客户和大部分客户等到正确的响应 异同： ​ 相同点 ​ 1、为了保证集群大部分服务的可用性和可靠性，防止崩溃，牺牲小我 ​ 2、用户最终都是体验到某个功能不可用 ​ 不同点： ​ 1、熔断是被调用方的故障，触发系统的主动规则 ​ 2、降级是基于全局的考虑停止一些正常服务，释放资源 什么是限流 ​ 对打入的服务的请求流量进行控制，使服务能够承担不超过自己能力的流量压力 2、Sentinel 简介 官方文档：https://github.com/alibaba/Sentinel/wiki/介绍 项目文档：https://github.com/alibaba/Sentinel 随着微服务的流行，服务和服务之间的稳定性变得越来越重要。Sentinel 以流量为切入点，从流量控制、熔断降级、系统负载保护等多个维度保护服务的稳定性。 Sentinel 具有以下特征: 丰富的应用场景：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。 完备的实时监控：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。 广泛的开源生态：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。 完善的 SPI 扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。 Sentinel 的主要特性： Sentinel 分为两个部分: 核心库（Java 客户端）不依赖任何框架/库，能够运行于所有 Java 运行时环境，同时对 Dubbo / Spring Cloud 等框架也有较好的支持。 控制台（Dashboard）基于 Spring Boot 开发，打包后可以直接运行，不需要额外的 Tomcat 等应用容器。 Sentinel 基本概念: 资源 资源是 Sentinel 的关键概念。它可以是 Java 应用程序中的任何内容，例如，由应用程序提供的服务，或由应用程序调用的其它应用提供的服务，甚至可以是一段代码。在接下来的文档中，我们都会用资源来描述代码块。 只要通过 Sentinel API 定义的代码，就是资源，能够被 Sentinel 保护起来。大部分情况下，可以使用方法签名，URL，甚至服务名称作为资源名来标示资源。 规则 围绕资源的实时状态设定的规则，可以包括流量控制规则、熔断降级规则以及系统保护规则。所有规则可以动态实时调整。 2、Hystrix 与 Sentinel 比较 3、整合 Feign 和 Sentinel 测试熔断降级 熔断降级官网解释：https://github.com/alibaba/Sentinel/wiki/熔断降级 Spring- Cloud整合Sentinel和Feign：https://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel pom.xml 12345678910&lt;!-- 导入sentinel依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--导入openFeign --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; application.properties 12345678## sentinel与项目间的通信端口spring.cloud.sentinel.transport.port=8719## sentinel端口设置spring.cloud.sentinel.transport.dashboard=localhost:8333## 暴露信息management.endpoints.web.exposure.exclude=*## 配置文件打开 Sentinel 对 Feign 的支持feign.sentinel.enabled=true 开启后，在微服务中调用远程服务，Sentinel 就会记录微服务之间的调用，从而对远程调用进行设置熔断降级等。 请求设置 设置流控规则 Feign设置 结果 4、整合 Sentinel 测试限流 官网Spring-Cloud 整合：https://github.com/alibaba/spring-cloud-alibaba/wiki/Sentinel Pom.xml 参考 3、整合 Feign 和 Sentinel 测试熔断降级 控制台： 超过单继阈值，返回自定义请求结果 实现方式： 12345678910/** 1、代码 * try (Entry entry = SphU.entry(&quot;resourceName&quot;)) { * }(BlockedException e){} * 2、基于注解 * @SentinelResource(value = &quot;getCurrentSeckillSkusSource&quot;,blockHandler = &quot;BlockHandler&quot;) * 无论1/2方式一定要配置限流以后的默认返回 * url可以设置统一返回**/ 具体实现方式参考官网给出文档：https://github.com/alibaba/Sentinel/wiki/如何使用 5、Sentinel网关限流 官网文档：https://github.com/alibaba/Sentinel/wiki/网关限流 pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-sentinel-gateway&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;&lt;/dependency&gt; 启动Sentinle1.7.1 后比原先的1.6.1多个一个功能 您可以在 GatewayCallbackManager 注册回调进行定制： setBlockHandler：注册函数用于实现自定义的逻辑处理被限流的请求，对应接口为 BlockRequestHandler。默认实现为 DefaultBlockRequestHandler，当被限流时会返回类似于下面的错误信息：Blocked by Sentinel: FlowException。 12345678910111213141516@Configurationpublic class SentinelGatewayConfig { public SentinelGatewayConfig() { GatewayCallbackManager.setBlockHandler(new BlockRequestHandler() { // 网关限流了 就会回调 @Override public Mono&lt;ServerResponse&gt; handleRequest(ServerWebExchange serverWebExchange, Throwable throwable) { R error = R.error(BizCodeEnume.TO_MANY_REQUEST.getCode(), BizCodeEnume.TO_MANY_REQUEST.getMsg()); String errorJson = JSON.toJSONString(error); Mono&lt;ServerResponse&gt; body = ServerResponse.ok().body(Mono.just(errorJson), String.class); return body; } }); }} Seata 1、简介 Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 AT、TCC、SAGA 和 XA 事务模式，为用户打造一站式的分布式解决方案。 https://seata.io/zh-cn/index.html 2、核心概念 TC (Transaction Coordinator) - 事务协调者 维护全局和分支事务的状态，驱动全局事务提交或回滚。 TM (Transaction Manager) - 事务管理器 定义全局事务的范围：开始全局事务、提交或回滚全局事务。 RM (Resource Manager) - 资源管理器 管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。 OSS 5.6.1、简介 对象存储服务 阿里云对象存储服务（Object Storage Service，简称 OSS），是阿里云提供的海量、安全、低成本、高可靠的云存储服务。您可以在任何应用、任何时间、任何地点存储和访问任意类型的数据。 5.6.2、使用步骤 使用 Java 上传文件 https://help.aliyun.com/document_detail/32011.html?spm=a2c4g.11186623.6.915.56196d39rr96Ll 来自官网实例 - 上传文件流 12345678910111213141516171819@Test public void testUpload() throws FileNotFoundException { // Endpoint以杭州为例，其它Region请按实际情况填写。 String endpoint = &quot;http://oss-cn-hangzhou.aliyuncs.com&quot;; // 云账号AccessKey有所有API访问权限，建议遵循阿里云安全最佳实践，创建并使用RAM子账号进行API访问或日常运维，请登录 //https://ram.console.aliyun.com 创建。 String accessKeyId = &quot;&lt;yourAccessKeyId&gt;&quot;; String accessKeySecret = &quot;&lt;yourAccessKeySecret&gt;&quot;; // 创建OSSClient实例。 OSS ossClient = new OSSClientBuilder().build(endpoint, accessKeyId, accessKeySecret); // 上传文件流。 InputStream inputStream = new FileInputStream(&quot;&lt;yourlocalFile&gt;&quot;); ossClient.putObject(&quot;&lt;yourBucketName&gt;&quot;, &quot;&lt;yourObjectName&gt;&quot;, inputStream); // 关闭OSSClient。 ossClient.shutdown(); System.out.println(&quot;上传成功&quot;); } 需要填写 AccesskeyId 以及 Secret 同时也要指定文件位置 以及文件名字 最终上传结果 5.6.3、 抽取成一个单独的第三方服务？OSS 官网文档 https://github.com/alibaba/aliyun-spring-boot/blob/master/aliyun-spring-boot-samples/aliyun-oss-spring-boot-sample/README-zh.md 新建项目 gulimall-third-party pom 配置 123456789101112131415161718192021222324&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alicloud-oss&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;${spring-cloud.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- springCloudAlibaba --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; application.yml 配置 accessKey、secretKey、endpoint 1234567891011121314151617spring: cloud: nacos: discovery: server-addr: 127.0.0.1:8848 alicloud: access-key: LTAI4GE22ckocpNBfd36zkxJ secret-key: qDFrQ6cxZqc4DwxoWx5K2aosXXj0Go oss: endpoint: oss-cn-shenzhen.aliyuncs.com ## 地域节点 bucket: gulimall-oss01 # 桶列表 application: name: gulimall-third-partyserver: port: 30000 5.6.4、Web端上传介绍 阿里云对象存储-普通上传方式： 用户提交文件到服务器，服务器将文件提交到 OSS 对象存储 和数据直传到OSS相比，以上方法有三个缺点： 上传慢：用户数据需先上传到应用服务器，之后再上传到OSS。网络传输时间比直传到OSS多一倍。如果用户数据不通过应用服务器中转，而是直传到OSS，速度将大大提升。而且OSS采用BGP带宽，能保证各地各运营商之间的传输速度。 扩展性差：如果后续用户多了，应用服务器会成为瓶颈。 费用高：需要准备多台应用服务器。由于OSS上传流量是免费的，如果数据直传到OSS，不通过应用服务器，那么将能省下几台应用服务器。 阿里云对象存储-服务端签名后直传： 流程介绍 背景 采用JavaScript客户端直接签名（参见JavaScript客户端签名直传）时，AccessKey ID和AcessKey Secret会暴露在前端页面，因此存在严重的安全隐患。因此，OSS提供了服务端签名后直传的方案。 Web端向服务端请求签名，然后直接上传，不会对服务端产生压力，而且安全可靠。服务端签后直传 OpenFeign 简介 Feign 是一个声明式的 HTTP 客户端，他的目的就是让远程调用更加简单，Feign提供了 HTTP请求的模板，通过编写简单的接口和插入注解，就可以定义好的 HTTP 请求参数、格式、地址等信息 Feign 整合了 Ribbon（负载均衡）和 Hystrix（服务熔断），可以让我们不再需要显示地使用这两个组件 SpringCloud - Feign，在 NetflixFeign 的基础上扩展了对 SpringMVC 注解的支持，在其实现下，我们只需创建一个接口并用注解的方式来配置它，即可完成对服务提供方的接口绑定。简化了 SpringCloud - Ribbon 自行封装服务调用客户端的开发量。 1.引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 2.创建服务接口 12345@FeignClient(&quot;gulimall-coupon&quot;)public interface CouponFeignService { @RequestMapping(&quot;/coupon/coupon/member/list&quot;) public R membercoupons();} 3.开启远程调用功能 客户端主启动类注解开启远程调用功能，调用接口使用服务 1@EnableFeignClients(basePackages = &quot;com.atguigu.gulimall.member.feign&quot;) Gateway 简介 网关作为流浪入口，常用功能包括路由转发，权限效验，限流控制等，而 SpringCloud GateWay作为 SpringCloud 官方推出的第二代网关框架，取代了 Zull 网关 网上测试 三种网关对应请求数 网关提供 API 全托管服务，丰富的 API 管理功能，辅助企业管理大规模的 API，以降低管理成本和安全风险、包括协议适配、协议转发、安全策略、防刷、流量、监控日志等功能 Spring Cloud GateWay 旨在提供一种简单有效的方式来对 API 进行路由，并为他们提供切面，列如、安全性、监控/指标 和弹性等 Sleuth+Zipkin 服务链路追踪 为什么用 微服务架构是一个分布式架构，它按业务划分服务单元，一个分布式系统往往有很多个服务单元。由于服务单元数量众多，业务的复杂性，如果出现了错误和异常，很难去定位。主要体现在，一个请求可能需要调用很多个服务，而内部服务的调用复杂性，决定了问题难以定位。所以微服务架构中，必须实现分布式链路追踪，去跟进一个请 求到底有哪些服务参与，参与的顺序又是怎样的，从而达到每个请求的步骤清晰可见，出了问题，很快定位。 链路追踪组件有Google 的Dapper, Twitter 的Zipkin,以及阿里的Eagleeye(鹰眼)等，它们都是非常优秀的链路追踪开源组件。 五、前端开发基础知识 这个等以后深入学习在进行记录~~~~ 7.1 VSCode 使用 Get 1234567891011&quot;http-get请求&quot;: { &quot;prefix&quot;: &quot;httpget&quot;, &quot;body&quot;: [ &quot;this.\\\\$http({&quot;, &quot;url: this.\\\\$http.adornUrl(''),&quot;, &quot;method: 'get',&quot;, &quot;params: this.\\\\$http.adornParams({})&quot;, &quot;}).then(({ data }) =&gt; {&quot;, &quot;})&quot;], &quot;description&quot;: &quot;httpGET请求&quot;}, POST 12345678910&quot;http-post请求&quot;: { &quot;prefix&quot;: &quot;httppost&quot;, &quot;body&quot;: [ &quot;this.\\\\$http({&quot;, &quot;url: this.\\\\$http.adornUrl(''),&quot;, &quot;method: 'post',&quot;, &quot;data: this.\\\\$http.adornData(data, false)&quot;, &quot;}).then(({ data }) =&gt; { });&quot; ], &quot;description&quot;: &quot;httpPOST请求&quot;} 7.2 ES6 7.3 Node.js 7.4 Vue 7.4.5 整合 Element UI 1、安装 1npm i element-ui -S 2、引入 Element main.js 中引入以下内容 1234567891011import Vue from 'vue';import ElementUI from 'element-ui';import 'element-ui/lib/theme-chalk/index.css'; // 引入静态资源文件import App from './App.vue';Vue.use(ElementUI);new Vue({ el: '#app', render: h =&gt; h(App)}); 7.5 Babel 7.6 Webpack 六、商品服务&amp;三级分类 8.1 基础概念 8.1.1、三级分类 一级分类查出二级分类数据，二级分类中查询出三级分类数据 数据库设计 8.1.2、SPU 和 SKU SPU：Standard Product Unit （标准化产品单元） 是商品信息聚合的最小单位，是一组可复用，易检索的标准化信息的组合，该集合描述了一个产品的特性 IPhoneX 是 SPU,MI8 是 SPU IPhoneX 64G 黑曜石 是 SKU MIX8 + 64G 是 SKU SKU: Stock KeepingUnit (库存量单位) 8.1.3、基本属性 【规格参数】与 销售属性 每个分共下的商共享规格参数、与销售属性，只是有些商品不一定更用这个分类下全部的属性： 属性是以三级分类组织起来的 规格参数中有些是可以提供检索的 规格参数也是基本属性，他们具有自己的分组 属性的分组也是以三级分类组织起来的 属性名确定的，但是值是每一个商品不同来决定的 【属性分组-规格参数-销售属性-三级分类】关联关系 SPU-SKU-属性表 8.1.4、接口文档位置 https://easydoc.xyz/s/78237135/ZUqEdvA4/hKJTcbfd 8.1.5、 Object 划分 VO （View Object）视图对象 用于展示层，它的作用是把某个指定页面（或组件）的所有数据封装起来。通常用于业务层之间的数据传递，和 PO 一样也是仅仅包含数据而已，但应是抽象出的业务对象，可以和表对应，也可以不，这根据业务的需要，用 new 关键字创建，由 GC 回收 view Object 试图对象 接受页面传递来的数据，封装对象，封装页面需要用的数据 DTO（Data Transfer Object）数据传输对象 主要用于展示层与服务层之间的数据传输对象。这个概念来源于 J2EE 的设计模式，原来的目的是为了 EJB的分布式应用提供粗粒度的数据实体，以减少分布式调用的次数，从而提高分数调用的性能和降低网络负载，但在这里，泛指用于展示层与服务层之间的数据传输对象 DO（Domain Object）领域对象 就是从现实世界中抽象出来的有形或无形的业务实体。 PO（Persistent Object）：持久化对象 它跟持久层（通常是关系型数据库）的数据结构形成一一对应的映射关系，如果持久层是关系型数据库，那么，数据表中的每个字段就对应PO的一个属性。po 就是对应数据库中某一个表的一条记录，多个记录可以用 PO 的集合，PO 中应该不包含任何对数据库到操作 TO (Transfer Object) 数据传输对象 不同的应用程序之间传输的对象 BO(business object) 业务对象 从业务模型的角度看，见 UML 原件领域模型中的领域对象，封装业务逻辑的， java 对象，通过调用 DAO 方法，结合 PO VO,进行业务操作，business object 业务对象，主要作用是把业务逻辑封装成一个对象，这个对象包括一个或多个对象，比如一个简历，有教育经历，工作经历，社会关系等等，我们可以把教育经历对应一个 PO 、工作经验对应一个 PO、 社会关系对应一个 PO, 建立一个对应简历的的 BO 对象处理简历，每 个 BO 包含这些 PO ,这样处理业务逻辑时，我们就可以针对 BO 去处理 POJO ( plain ordinary java object) 简单无规则 java 对象 传统意义的 java 对象，就是说一些 Object/Relation Mapping 工具中，能够做到维护数据库表记录的 persisent object 完全是一个符合 Java Bean 规范的 纯 java 对象，没有增加别的属性和方法，我们的理解就是最基本的 Java bean 只有属性字段 setter 和 getter 方法 POJO 时是 DO/DTO/BO/VO 的统称 DAO（data access object） 数据访问对象 是一个 sun 的一个标准 j2ee 设计模式，这个模式有个接口就是 DAO ，他负持久层的操作，为业务层提供接口，此对象用于访问数据库，通常和 PO 结合使用，DAO 中包含了各种数据库的操作方法，通过它的方法，结合 PO 对数据库进行相关操作，夹在业务逻辑与数据库资源中间，配合VO 提供数据库的 CRUD 功能 VO 与 DTO 的区别 DTO 和 VO 的属性值基本是一致的，而且他们通常都是 POJO【简单的 Java 对象（Plain Old Java Object）】,但两者存在本质上的区别；DTO 代表服务层需要接收的数据和返回的数据，而VO 代表展示层需要显示的数据。 DTO 与 DO 的区别 首先是概念上的区别，DTO 是展示层和服务层之间的数据传输对象（可以认为是两者之间的协议），而 DO是对现实世界各种业务角色的抽象，这就引出了两者在数据上的区别。 DO 与 PO 的区别 DO 和 PO 在绝大部分情况下是一一对应的，PO是只含有 get/set 方法的POJO，但某些场景还是能反映出两者在概念上存在本质区别： DO在某些场景下不需要进行显式的持久化，例如利用策略模式设计的商品折扣策略，会衍生出折扣策略的接口和不同折扣策略实现类，这些折扣策略实现类可以算是DO，但它们只会驻留在静态内存池，不需要持久化到持久层，因此，这类 DO 是不存在对应的 PO的。 同样的道理，某些场景下，PO也没有对应的DO，例如老师Teacher和学生Student存在多对多的关系，在关系数据库中，这种关系需要表现为一个中间表，也就对应有一个TeacherAndStudentPO的PO，但这个PO在业务领域没有任何现实的意义，它完全不能与任何DO对应上。 8.2 三级分类接口编写 12// 返回查询所有分类以及子子分类，以树形结构组装起来 List&lt;CategoryEntity&gt; listWithTree(); 实现类： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Override public List&lt;CategoryEntity&gt; listWithTree() { // 1、查出所有分类 设置为null查询全部 List&lt;CategoryEntity&gt; entities = baseMapper.selectList(null); // 2、组装成父子的树形结构 List&lt;CategoryEntity&gt; levelList = entities.stream().filter(categoryEntity -&gt; { // parentCid ==0 为父目录默认0 return categoryEntity.getParentCid() == 0; }).map(menu -&gt; { // 设置二三级分类 递归 menu.setChildren(getChildrens(menu,entities)); return menu; }).sorted((menu1, menu2) -&gt; { // 排序 Sort字段排序 return (menu1.getSort() == null ? 0 : menu1.getSort()) - (menu2.getSort() == null ? 0 : menu2.getSort()); }).collect(Collectors.toList()); return levelList; }/** * 递归查询子分类 * @param root 当前category对象 * @param all 全部分类数据 * @return */ private List&lt;CategoryEntity&gt; getChildrens(CategoryEntity root, List&lt;CategoryEntity&gt; all) { List&lt;CategoryEntity&gt; collect = all.stream().filter(categoryEntity -&gt; { // 遍历所有的category对象的父类id = 等于root的分类id 说明是他的子类 return categoryEntity.getParentCid() == root.getCatId(); }).map(menu -&gt; { // 1、递归遍历菜单 menu.setChildren(getChildrens(menu, all)); return menu; }).sorted((menu1, menu2) -&gt; { // 2、菜单排序 return (menu1.getSort() == null ? 0 : menu1.getSort()) - (menu2.getSort() == null ? 0 : menu2.getSort()); }).collect(Collectors.toList()); return collect; } 跨域 跨域：指的是浏览器不能执行其他网站的脚本。它是由浏览器的同源策略造成的，是浏览器对javascript施加的安全限制。 同源策略：是指协议，域名，端口都要相同，其中有一个不同都会产生跨域； 下图详细说明了 URL 的改变导致是否允许通信 跨域流程 浏览器发请求都要实现发送一个请求询问是否可以进行通信 ，我直接给你返回可以通信不就可以了吗？ 相关资料参考：https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Access_control_CORS 解决跨越（ 一 ） 使用nginx部署为同一域 开发过于麻烦，上线在使用 解决跨域 （ 二 ）配置当次请求允许跨域 1、添加响应头 Access-Control-Allow-Origin: 支持哪些来源的请求跨域 Access-Control-Allow-Methods: 支持哪些方法跨域 Access-Control-Allow-Credentials: 跨域请求默认不包含cookie,设置为true可以包含cookie Access-Control-Expose-Headers: 跨域请求暴露的字段 ​ CORS请求时, XML .HttpRequest对象的getResponseHeader()方法只能拿到6个基本字段: CacheControl、Content-L anguage、Content Type、Expires、 Last-Modified、 Pragma。 如果想拿到其他字段，就必须在Access-Control-Expose-Headers里面指定。 Access-Control-Max- Age: 表明该响应的有效时间为多少秒。在有效时间内，浏览器无须为同一-请求再次发起预检请求。请注意，浏览器自身维护了一个最大有效时间，如果该首部字段的值超过了最大有效时间，将不会生效。 网关配置文件 12345678910111213- id: product_route uri: lb://gulimall-product predicates: - Path=/api/product/** filters: - RewritePath=/api/(?&lt;segment&gt;/?.*),/$\\{segment}- id: admin_route uri: lb://renren-fast # lb负载均衡 predicates: - Path=/api/** # path指定对应路径 filters: # 重写路径 - RewritePath=/api/(?&lt;segment&gt;/?.*), /renren-fast/$\\{segment} 跨越设置 请求先发送到网关，网关在转发给其他服务 事先都要注册到注册中心 123456789101112131415161718192021@Configurationpublic class GulimallCorsConfiguration { @Bean public CorsWebFilter corsWebFilter() { UrlBasedCorsConfigurationSource source = new UrlBasedCorsConfigurationSource(); CorsConfiguration corsConfiguration = new CorsConfiguration(); // 配置跨越 corsConfiguration.addAllowedHeader(&quot;*&quot;); // 允许那些头 corsConfiguration.addAllowedMethod(&quot;*&quot;); // 允许那些请求方式 corsConfiguration.addAllowedOrigin(&quot;*&quot;); // 允许请求来源 corsConfiguration.setAllowCredentials(true); // 是否允许携带cookie跨越 // 注册跨越配置 source.registerCorsConfiguration(&quot;/**&quot;,corsConfiguration); return new CorsWebFilter(source); }} 去掉renren-fast默认跨域设置 8.2.1 树形展示三级分类数据 1、用到的前端组件 Tree 树形控件 12345678910&lt;el-tree :data=&quot;data&quot; :props=&quot;defaultProps&quot; @node-click=&quot;handleNodeClick&quot;&gt;&lt;/el-tree&gt;&lt;!--data 展示数据props 配置选项children 指定子树为节点对象的某个属性值label 指定节点标签为节点对象的某个属性值disabled 节点选择框是否禁用为节点对象的某个属性值@node-click 节点被点击时的回调--&gt; 配置静态数据就能显示出对应的效果 2、编写方法获取全部菜单数据 12345678910getMenus() { this.$http({ // 请求接口见上面 url: this.$http.adornUrl(&quot;/product/category/list/tree&quot;), method: &quot;get&quot; }).then(({ data }) =&gt; { console.log(&quot;返回的菜单数据&quot; + data.data); this.menus = data.data; }); } 3、最终展示结果 ( append ,edit 会在后面介绍) 8.2.2 逻辑删除&amp;删除效果细化 效果图 1、节点的内容支持自定义，可以在节点区添加按钮或图标等内容 123456789101112 &lt;el-button v-if=&quot;node.childNodes.length == 0&quot; type=&quot;text&quot; size=&quot;mini&quot; @click=&quot;() =&gt; remove(node, data)&quot; &gt; Delete&lt;/el-button&gt; &lt;!-- v-if=&quot;node.childNodes.length == 0&quot; 没有子节点可以删除 type 对应类型 size 大小 @click 点击后出发的方法 此处使用了 箭头函数--&gt; 2、前端remove方法进行删除 1234567891011121314151617181920212223242526272829303132remove(node, data) { this.$confirm(`是否删除【${data.name}】菜单 ? `, &quot;提示&quot;, { confirmButtonText: &quot;确定&quot;, cancelButtonText: &quot;取消&quot;, type: &quot;warning&quot; }) .then(() =&gt; { // 拿到当前节点的catId var ids = [data.catId]; this.$http({ url: this.$http.adornUrl(&quot;/product/category/delete&quot;), method: &quot;post&quot;, data: this.$http.adornData(ids, false) }).then(({ data }) =&gt; { this.$message({ message: &quot;菜单删除成功&quot;, type: &quot;success&quot; }); // 刷新菜单 this.getMenus(); // 设置默认需要展开的菜单 /** default-expanded-keys 默认展开节点的 key 数组 */ this.expandedKey = [node.parent.data.catId]; console.log(node.parent.data.catId); }); }) .catch(() =&gt; {}); console.log(&quot;remove&quot;, node, data); } 3、后端接口 -- 逻辑删除配置 3.1 第一种方式 mybatisplus 逻辑删除参考官网：https://baomidou.com/guide/logic-delete.html 在appliction.yml 中配置 myabtisplus 逻辑删除 1234567mybatis-plus: mapper-locations: classpath*:/mapper/**/*.xml global-config: db-config: id-type: auto # 数据库主键自增 logic-delete-value: 1 # 逻辑已删除值(默认为 1) logic-not-delete-value: 0 # 逻辑未删除值(默认为 0) 3.2 第二种方式 在 Entity中 使用注解 1234567/** 是否为数据库表字段 默认 true 存在，false 不存在 标记为false 说明 该字段不在数据库 **/@TableField(exist = false)private List&lt;CategoryEntity&gt; children; 4、Controller 实现 使用了代码生成器 12345678910111213141516171819 /** * 删除 * @RequestBody:获取请求体，必须发送post请求才有 get请求没有 * SpringMvc 自动将请求体的数据 ( json ) 转为对应的对象 */ @RequestMapping(&quot;/delete&quot;) public R delete(@RequestBody Long[] catIds){ // 1、检查当前删除的菜单，是否被别的地方应用 categoryService.removeMenuByIds(Arrays.asList(catIds)); return R.ok(); }//Service 实现 @Override public void removeMenuByIds(List&lt;Long&gt; asList) { // 1、逻辑删除 baseMapper.deleteBatchIds(asList); } 8.2.3 新增效果&amp;基本修改 1、前端组件 button 组件 Dialog 对话框 123456789101112131415161718192021222324252627282930313233343536373839 &lt;!-- 层级小于2 才能新增 --&gt; &lt;el-button v-if=&quot;node.level &lt;= 2&quot; type=&quot;text&quot; size=&quot;mini&quot; @click=&quot;() =&gt; append(data)&quot; &gt;Append &lt;/el-button&gt; &lt;el-button type=&quot;text&quot; size=&quot;mini&quot; @click=&quot;() =&gt; edit(data)&quot; &gt;edit &lt;/el-button&gt;&lt;!-- 上面组件在tree中--&gt;&lt;el-dialog :title=&quot;title&quot; :visible.sync=&quot;dialogVisible&quot; width=&quot;30%&quot; :before-close=&quot;handleClose&quot; :close-on-click-modal=&quot;false&quot; &gt; &lt;el-form :model=&quot;category&quot;&gt; &lt;el-form-item label=&quot;分类名称&quot;&gt; &lt;el-input v-model=&quot;category.name&quot; autocomplete=&quot;off&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;图标&quot;&gt; &lt;el-input v-model=&quot;category.icon&quot; autocomplete=&quot;off&quot;&gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;el-form-item label=&quot;计量单位&quot;&gt; &lt;el-input v-model=&quot;category.productUnit&quot; autocomplete=&quot;off&quot; &gt;&lt;/el-input&gt; &lt;/el-form-item&gt; &lt;/el-form&gt; &lt;span slot=&quot;footer&quot; class=&quot;dialog-footer&quot;&gt; &lt;el-button @click=&quot;dialogVisible = false&quot;&gt;取 消&lt;/el-button&gt; &lt;el-button type=&quot;primary&quot; @click=&quot;submitData&quot;&gt;确 定&lt;/el-button&gt; &lt;/span&gt; &lt;/el-dialog&gt; 2、新增&amp;修改 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 append(data) { this.dialogVisible = true; console.log(&quot;append&quot;, data); this.category.name = &quot;&quot;; this.dialogType = &quot;add&quot;; this.title = &quot;添加分类&quot;; this.category.parentCid = data.catId; this.category.catLevel = data.catLevel * 1 + 1; this.category.name = &quot;&quot;; this.category.catId = null; this.category.icon = &quot;&quot;; this.category.productUnit = &quot;&quot;; }, // 要修改的数据 edit(data) { console.log(&quot;要修改的数据&quot;, data); this.dialogType = &quot;edit&quot;; this.title = &quot;修改分类&quot;; this.dialogVisible = true; // 发送请求获取最新的数据 this.$http({ url: this.$http.adornUrl(`/product/category/info/${data.catId}`), method: &quot;get&quot; }).then(({ data }) =&gt; { // 请求成功 console.log(&quot;要回显的数据&quot;, data); this.category.name = data.data.name; this.category.catId = data.data.catId; this.category.icon = data.data.icon; this.category.productUnit = data.data.productUnit; this.category.parentCid = data.data.parentCid; }); },submitData() { if (this.dialogType == &quot;add&quot;) { // 进行新增 this.addCategory(); } if (this.dialogType == &quot;edit&quot;) { // 进行修改 this.editCategory(); } }, // 添加三级分类 addCategory() { console.log(&quot;添加三级分类的数据&quot;, this.category); this.$http({ url: this.$http.adornUrl(&quot;/product/category/save&quot;), method: &quot;post&quot;, data: this.$http.adornData(this.category, false) }).then(({ data }) =&gt; { this.$message({ message: &quot;菜单保存成功&quot;, type: &quot;success&quot; }); // 关闭对话框 this.dialogVisible = false; // 重新刷新数据 this.getMenus(); // 默认展开的菜单 this.expandedKey = [this.category.parentCid]; }); }, // 修改三级分类数据 editCategory() { // 解构出单独的几个对象 用来提交 var { catId, name, icon, productUnit } = this.category; this.$http({ url: this.$http.adornUrl(&quot;/product/category/update&quot;), method: &quot;post&quot;, data: this.$http.adornData({ catId, name, icon, productUnit }, false) }).then(({ data }) =&gt; { this.$message({ message: &quot;菜单修改成功&quot;, type: &quot;success&quot; }); // 关闭对话框 this.dialogVisible = false; // 重新刷新数据 this.getMenus(); // 默认展开的菜单 this.expandedKey = [this.category.parentCid]; }); }, 3、要考虑到的点 以及细节 添加完成后要把表单的数据进行清除，否则第二次打开任然会有上次表单提交剩下的数据 this.category.name = &quot;&quot;; 修改和新增用的是同一个表单，因此在方法对话框中 动态的绑定了 :title=&quot;title&quot; 标题 用于显示是新增还是修改 一个表单都是一个提交方法 因此在提交方法的时候进行了判断，根据变量赋值决定调用那个方法 this.dialogType = &quot;add&quot;; this.dialogType = &quot;edit&quot;; 8.2.4 拖拽功能&amp;数据收集&amp;批量删除 效果演示 1、前端用的组件 Tree 树形控件 可拖拽节点 通过 draggable 属性可让节点变为可拖拽。 1234567891011121314151617181920212223 &lt;el-tree :expand-on-click-node=&quot;false&quot; :data=&quot;menus&quot; :props=&quot;defaultProps&quot; show-checkbox node-key=&quot;catId&quot; :default-expanded-keys=&quot;expandedKey&quot; :draggable=&quot;draggable&quot; :allow-drop=&quot;allowDrop&quot; @node-drop=&quot;handleDrop&quot; ref=&quot;menuTree&quot; &gt;&lt;!-- :expand-on-click-node 是否在点击节点的时候展开或者收缩节点 默认 true 则只有点箭头图标的时候才会展开或者收缩节点。show-checkbox 节点是否可被选择node-key 每个树节点用来做唯一标识的属性 default-expanded-keys 默认展开节点的节点draggable 表示是否可以被拖拽 true&amp;falseallow-drop 拖拽时判定目标节点能否被放置node-drop 拖拽成功完成出发的事件ref 该组件tree的引用详细解释参考官网 https://element.eleme.cn/#/zh-CN/component/tree--&gt; 2、主要业务逻辑 TODO：暂时不懂 回头再来看 allowDrop： 拖拽时判定目标节点能否被放置。type 参数有三种情况：'prev'、'inner' 和 'next'，分别表示放置在目标节点前、插入至目标节点和放置在目标节点后 @node-drop 拽成功完成时触发的事件 共四个参数，依次为：被拖拽节点对应的 Node、结束拖拽时最后进入的节点、被拖拽节点的放置位置（before、after、inner）、event 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485allowDrop(draggingNode, dropNode, type) { // 1、被拖动的当前节点以及所在的父节点总层次不能大于3 // 1) 被拖动节点的总层数 console.log(&quot;allowDrop&quot;, draggingNode, dropNode, type); this.countNodeLevel(draggingNode.data); // 当前正在拖动的节点 + 父节点所在的深度不大于3即可 let deep = Math.abs(this.maxLevel - draggingNode.level) + 1; console.log(&quot;深度&quot;, deep); if (type == &quot;inner&quot;) { return deep + dropNode.level &lt;= 3; } else { return deep + dropNode.parent.level &lt;= 3; }}, countNodeLevel(node) { // 找到所有子节点，求出最大深度 if (node.children != null &amp;&amp; node.children.length &gt; 0) { for (let i = 0; i &lt; node.children.length; i++) { if (node.children[i].catLevel &gt; this.maxLevel) { this.maxLevel = node.children[i].catLevel; } // 递归查找 this.countNodeLevel(node.children[i]); } } }, handleDrop(draggingNode, dropNode, dropType, ev) { console.log(&quot;handleDrop: &quot;, draggingNode, dropNode, dropType); // 1、当前节点最新的父节点 let pCid = 0; let siblings = null; if (dropType == &quot;before&quot; || dropType == &quot;after&quot;) { pCid = dropNode.parent.data.catId == undefined ? 0 : dropNode.parent.data.catId; siblings = dropNode.parent.childNodes; } else { pCid = dropNode.data.catId; siblings = dropNode.childNodes; } // this.PCid = pCid this.pCid.push(pCid); // 2、当前拖拽节点的最新顺序 for (let i = 0; i &lt; siblings.length; i++) { if (siblings[i].data.catId == draggingNode.data.catId) { // 如果遍历的是当前正在拖拽的节点 let catLevel = draggingNode.level; if (siblings[i].level != draggingNode.level) { // 当前结点的层级发生变化 catLevel = siblings[i].level; // 修改它子节点的层级 this.updateChildNodeLevel(siblings[i]); } // 如果遍历当前正在拖拽的节点 this.updateNodes.push({ catId: siblings[i].data.catId, sort: i, parentCid: pCid }); } else { this.updateNodes.push({ catId: siblings[i].data.catId, sort: i }); } } // 3、当前拖拽节点的最新层级 console.log(&quot;updateNodes&quot;, this.updateNodes); }, updateChildNodeLevel() { if (node.childNodes.length &gt; 0) { for (let i = 0; i &lt; node.childNodes.length; i++) { var cNode = node.childNodes[i].data; this.updateNodes.push({ catId: cNode.catId, catLevel: node.childNodes[i].level }); this.updateChildNodeLevel(node.childNodes[i]); } } }, 3、批量删除 1&lt;el-button type=&quot;danger&quot; @click=&quot;batchDelete&quot;&gt;批量删除&lt;/el-button&gt; batchDelete方法 123456789101112131415161718192021222324252627282930batchDelete() { let catIds = []; let names = []; // 返回目前被选中的节点所组成的数组 let checkedNodes = this.$refs.menuTree.getCheckedNodes(); console.log(&quot;被选中的元素&quot;, checkedNodes); for (let i = 0; i &lt; checkedNodes.length; i++) { // 遍历节点数组 拿到需要的值 catIds.push(checkedNodes[i].catId); names.push(checkedNodes[i].name); } this.$confirm(`是否批量删除【${names}】菜单 ? `, &quot;提示&quot;, { confirmButtonText: &quot;确定&quot;, cancelButtonText: &quot;取消&quot;, type: &quot;warning&quot; }).then(() =&gt; { this.$http({ url: this.$http.adornUrl(&quot;/product/category/delete&quot;), method: &quot;post&quot;, data: this.$http.adornData(catIds, false) }).then(({ data }) =&gt; { this.$message({ message: &quot;删除成功&quot;, type: &quot;success&quot; }); // 刷新菜单 this.getMenus(); }); }); }, 后端接口也是逻辑批量删除 1void removeMenuByIds(List&lt;Long&gt; asList); //接收的是一个id数组 总结： 前端用到的组件 Dialog 对话框、可拖拽节点、Switch 开关、Button 按钮、Tree组件（属性较多） 细节点： 没开启拖拽 开启拖拽： 通过 draggable 属性可让节点变为可拖拽。 七、商品服务&amp;品牌管理 9.1、效果显示优化与快速显示开关 vue代码 123456789101112131415 &lt;el-switch v-model=&quot;scope.row.showStatus&quot; active-color=&quot;#13ce66&quot; inactive-color=&quot;#ff4949&quot; :active-value=&quot;1&quot; :inactive-value=&quot;0&quot; @change=&quot;updateBrandStatus(scope.row)&quot; &gt; &lt;!-- scope.row 拿到整行的数据active-color switch 打开时的背景色inactive-color switch 关闭时的背景色active-value switch 打开时的值inactive-value switch 关闭时的值--&gt; 组件地址：https://element.eleme.cn/#/zh-CN/component/switch 用户点击 switch 开关就会调用后台的接口更改对应数据库的字段 ( 决定是否显示)，定义了 @change 事件 只要修后就会触发对应方法 123456789101112131415updateBrandStatus(data) { console.log(&quot;整行数据&quot;,data); // 单独就封装两个字段 let {brandId,showStatus} = data this.$http({ url: this.$http.adornUrl('/product/brand/update'), method: 'post', data: this.$http.adornData({brandId,showStatus}, false) }).then(({ data }) =&gt; { this.$message({ type:&quot;success&quot;, message:&quot;状态更新成功&quot; }) }); }, 9.2、表单效验&amp;自定义效验规则 Form 组件提供了表单验证的功能，只需要通过 rules 属性传入约定的验证规则，并将 Form-Item 的 prop 属性设置为需校验的字段名即可 1234567&lt;el-form :model=&quot;dataForm&quot; :rules=&quot;dataRule&quot; ref=&quot;dataForm&quot; @keyup.enter.native=&quot;dataFormSubmit()&quot; label-width=&quot;140px&quot; &gt; data中 自定义的规则 ，用来对数据进行判断 123456789101112131415161718192021222324dataRule: { name: [{ required: true, message: &quot;品牌名不能为空&quot;, trigger: &quot;blur&quot; }], // 自定义的规则 firstLetter: [ { validator: (rule, value, callback) =&gt; { if(value == '') { callback(new Error(&quot;首字母必须填写&quot;)) } else if(! /^[a-zA-Z]$/.test(value)) { callback(new Error(&quot;首字母必须a-z或者A-Z之间&quot;)) } else { callback() } },trigger:'blur'} ], sort: [{ validator: (rule, value, callback) =&gt; { if(value == '') { callback(new Error(&quot;排序字段必须填写&quot;)); } else if(!Number.isInteger(value) || value &lt; 0) { callback(new Error(&quot;排序必须是一个大于等于0的整数&quot;)); } else { callback(); } }, trigger: &quot;blur&quot; }] } 9.3、JSR303 数据效验 &amp; 统一异常处理 前端数据效验成功了，就会把json数据传递到后端，但是有人利用接口 比如 postman 乱发送请求 那会怎么办，于是后端也会利用 JSR303进行数据效验 9.3.1、给Bean添加效验注解 javax.validation.constraints包下 并定义自己的的message提示 123@NotEmpty(messsage = &quot;logo不能为空&quot;)@URL(message = &quot;logo必须是一个合法的url地址&quot;)private String logo; 9.3.2、开启效验功能 @Valid 效果：效验错误以后有默认的响应 Controller代码： 12345@RequestMapping(&quot;/save&quot;)public R save(@Valid@RequestBody BrandEntity brand){ brandService.save(brand); return R.ok();} 9.3.3、给效验的bean后紧跟一个BindingResult 就可以获取到效验的结果 12345678910111213141516public R save(@Valid @RequestBody BrandEntity brand,BindingResult result){ if(result.hasErrors()){ Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); //1、获取校验的错误结果 result.getFieldErrors().forEach((item)-&gt;{ //FieldError 获取到错误提示 String message = item.getDefaultMessage(); //获取错误的属性的名字 String field = item.getField(); map.put(field,message); }); return R.error(400,&quot;提交的数据不合法&quot;).put(&quot;data&quot;,map); }else { } 9.3.4、分组效验 (多场景复杂效验) 添加一个组 &amp; 修改一个组 1、@NotBlank(message = &quot;品牌名必须提交&quot;,groups = {AddGroup.class,UpdateGroup.class}) 给效验注解标注什么情况需要进行效验 @Validated({AddGroup.class}) 在对应方法上进行标注 默认没有指定分组的效验注解 @NotBlank 在分组效验情况@Validated({AddGroup.class})不生效，只会在@Validated生效 12345// 标记使用修改分组public R update(@Validated(UpdateGroup.class) @RequestBody BrandEntity brand){ brandService.updateById(brand); return R.ok(); } Entity 12345678910111213141516171819202122232425262728293031323334353637383940/** * 品牌id */@Null(message = &quot;新增不能指定Id&quot;,groups = {AddGroup.class})@NotNull(message = &quot;修改必须指定品牌id&quot;,groups = {UpdateGroup.class})@TableIdprivate Long brandId;/** * 品牌名 */@NotBlank(message = &quot;品牌名不能为空&quot;,groups = {AddGroup.class,UpdateGroup.class})private String name;/** * 品牌logo地址 */@NotEmpty(groups = {AddGroup.class})@URL(message = &quot;logo必须是一个合法的url地址&quot;,groups = {AddGroup.class,UpdateGroup.class})private String logo;/** * 介绍 */private String descript;/** * 显示状态[0-不显示；1-显示] */@NotNull(groups = {AddGroup.class, UpdateStatusGroup.class})@ListValue(vals={0,1},groups = {AddGroup.class,UpdateStatusGroup.class})private Integer showStatus;/** * 检索首字母 */@NotEmpty(groups = {AddGroup.class})@Pattern(regexp = &quot;^[a-zA-Z]$&quot;,message = &quot;检索首字母必须是一个字母&quot;,groups = {AddGroup.class,UpdateGroup.class})private String firstLetter;/** * 排序 */@NotNull(groups = {AddGroup.class})@Min(value=0,message = &quot;排序必须大于等于0&quot;,groups = {AddGroup.class,UpdateGroup.class})private Integer sort; 9.3.5、自定义效验 编写一个自定义的效验注解 编写一个自定义的效验器何自定义的效验注解 12345678910111213141516@Documented@Constraint(validatedBy = {ListValueConstraintValidator.class})//【可以指定多个不同的效验器，适配不同类型的效验】@Target({ METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER, TYPE_USE })@Retention(RUNTIME)public @interface ListValue { // 三要素不能丢 String message() default &quot;{com.atguigu.gulimall.product.valid.ListValue.message}&quot;; Class&lt;?&gt;[] groups() default { }; Class&lt;? extends Payload&gt;[] payload() default { }; int[] vals() default { };} 实现约束 123456789101112131415161718192021222324public class ListValueConstraintValidator implements ConstraintValidator&lt;ListValue,Integer&gt; { private Set&lt;Integer&gt; set = new HashSet&lt;&gt;(); // 初始化方法 @Override public void initialize(ListValue constraintAnnotation) { int[] vals = constraintAnnotation.vals(); for(int val : vals) { // 将结果添加到set集合 set.add(val); } } /** * 判断效验是否成功 * @param value 需要效验的值 * @param context * @return */ @Override public boolean isValid(Integer value, ConstraintValidatorContext context) { // 判断是包含该值 return set.contains(value); }} 9.3.6、异常处理 这里使用到了 SpringMVC 的注解 @ControllerAdvice 1、编写异常处理类使用SpringMvc的@ControllerAdvice 2、使用@ExceptionHandler标记方法可以处理异常 12345678910111213141516171819202122232425262728293031@Slf4j@RestControllerAdvice(basePackages = &quot;com.atguigu.gulimall.product.controller&quot;)public class GulimallExceptionControllerAdvice { /** * 捕获定义的异常 * @param e * @return */ @ExceptionHandler(value = MethodArgumentNotValidException.class) public R handleVaildException(MethodArgumentNotValidException e) { log.error(&quot;数据效验出现问题{},异常类型:{}&quot;,e.getMessage(),e.getClass()); Map&lt;String,String&gt; errorMap = new HashMap&lt;&gt;(); BindingResult bindingResult = e.getBindingResult(); bindingResult.getFieldErrors().forEach(fieldError -&gt; { errorMap.put(fieldError.getField(),fieldError.getDefaultMessage()); }); return R.error(BizCodeEnume.VAILD_EXCEPTION.getCode(),BizCodeEnume.VAILD_EXCEPTION.getMsg()).put(&quot;data&quot;,errorMap); } /** * 兜底异常 * @param throwable * @return */ @ExceptionHandler(value = Throwable.class) public R handleException(Throwable throwable) { return R.error(); }} 异常错误码定义 （重点） 后端将定义的错误码写入到开发手册，前端出现对于的错误，就可以通过手册查询到对应的异常 12345678910111213141516171819202122232425262728293031323334/*** * 错误码和错误信息定义类 * 1. 错误码定义规则为5为数字 * 2. 前两位表示业务场景，最后三位表示错误码。例如：100001。10:通用 001:系统未知异常 * 3. 维护错误码后需要维护错误描述，将他们定义为枚举形式 * 错误码列表： * 10: 通用 * 001：参数格式校验 * 11: 商品 * 12: 订单 * 13: 购物车 * 14: 物流 * * */public enum BizCodeEnume { UNKNOW_EXCEPTION(10000,&quot;系统未知异常&quot;), VAILD_EXCEPTION(10001,&quot;参数格式校验失败&quot;); private int code; private String msg; BizCodeEnume(int code,String msg){ this.code = code; this.msg = msg; } public int getCode() { return code; } public String getMsg() { return msg; }} 八、商品服务&amp;属性分组 10.1 、前端组件抽取 &amp; 父子组件交互 10.1.1 属性分组 - 效果 左边这个树形空间我们已经写过了，在三级分类的时候编写过，相对应的功能，拖拽，添加，删除等等都已经实现，那我们为什么不把他抽取出一个公共的组件便于其他组件利用？ 说干就干！ 在 modules 中新建 common 文件夹，在common中 新建 category.vue category.vue 核心代码 抽取需要的结果，多余的结果进行删除 1234&lt;template&gt; &lt;el-tree :data=&quot;menus&quot; :props=&quot;defaultProps&quot; node-key=&quot;catId&quot; ref=&quot;menuTree&quot; @node-click=&quot;nodeClick&quot;&gt; &lt;/el-tree&gt;&lt;/template&gt; 10.1.2 Layout 布局 通过基础的 24 分栏，迅速简便地创建布局、类似之前学过的bootstrap 1234&lt;el-row :gutter=&quot;20&quot;&gt; &lt;el-col :span=&quot;6&quot;&gt;表格&lt;/el-col&gt; &lt;el-col :span=&quot;18&quot;&gt;菜单&lt;/el-col&gt;&lt;/el-row&gt; 左边放表格，右边放菜单 引入 category 组件 1import category from &quot;../common/category&quot;; 并且声明 1234//import引入的组件需要注入到对象中才能使用 components: { category }, 最后在组件中使用 1234&lt;el-col :span=&quot;6&quot;&gt; &lt;!-- 直接使用--&gt; &lt;category @tree-node-click=&quot;treenodeclick&quot;&gt;&lt;/category&gt; &lt;/el-col&gt; 右边菜单使用代码生成器的代码直接引入即可 10.1.3 父子组件如何进行交互 子组件中的 Tree 组件的一个事件 node-click 节点被点击时的回调 12 // 向父组件发送事件，后面是需要传递的对象，参数等this.$emit(&quot;tree-node-click&quot;,data,node,component) 父组件需要定义相同方法接收 12&lt;category @tree-node-click=&quot;treenodeclick&quot;&gt;&lt;/category&gt; 123456789// 感知到节点被点击 treenodeclick(node, data, component) { console.log(&quot;attrgroup感知到category节点被点击&quot;); console.log(&quot;刚才被点击的菜单id:&quot; + data.catId); if (node.level == 3) { this.catId = data.catId; this.getDataList(); } }, 10.2 、获取属性分类分组 查看提供的接口文档 请求参数需要我们带对应的分页参数，所以我们在请求的时候得把参数带上 那就定义接口 go 1234567/** * * @param params 分页请求相关参数 * @param catelogId 三级分类id * @return */PageUtils queryPage(Map&lt;String, Object&gt; params, Long catelogId); 实现类 123456789101112131415161718192021222324252627@Overridepublic PageUtils queryPage(Map&lt;String, Object&gt; params, Long catelogId) { // 分类id 等于01 查询全部 if (catelogId == 0) { IPage&lt;AttrGroupEntity&gt; page = this.page(new Query&lt;AttrGroupEntity&gt;().getPage(params), new QueryWrapper&lt;AttrGroupEntity&gt;()); return new PageUtils(page); } else { // 拿到参数中的 key String key = (String) params.get(&quot;key&quot;); // 先根据分类id进行查询 QueryWrapper&lt;AttrGroupEntity&gt; wrapper = new QueryWrapper&lt;AttrGroupEntity&gt;() .eq(&quot;catelog_id&quot;,catelogId); // selecet * from attrgroup where catelog_id = ? and (attr_group_id = key or like attr_group_name = key) // 有时候查询也不会带上key 所以得判断 if (!StringUtils.isEmpty(key)) { // where条件后加上 and wrapper.and((obj) -&gt; { obj.eq(&quot;attr_group_id&quot;,key).or().like(&quot;attr_group_name&quot;,key); }); } // 组装条件进行查询 IPage&lt;AttrGroupEntity&gt; page = this.page(new Query&lt;AttrGroupEntity&gt;().getPage(params), wrapper); return new PageUtils(page); }} 10.3 、分类新增 &amp; 级联选择器 级联选择器是啥？？ 没接触过 官方解释 Cascader 级联选择器 当一个数据集合有清晰的层级结构时，可通过级联选择器逐级查看并选择。 对应效果 attrgroup-add-or-update.vue 中 加入该组件 12345678910111213 &lt;el-cascader v-model=&quot;dataForm.catelogPath&quot; placeholder=&quot;试试搜索：手机&quot; :options=&quot;categorys&quot; :props=&quot;props&quot; filterable &gt;&lt;/el-cascader&gt;&lt;!--placeholder=&quot;试试搜索：手机&quot;默认的搜索提示:options=&quot;categorys&quot; 可选项数据源，键名可通过 Props 属性配置:props=&quot;props&quot; 配置选项 filterable 是否可搜索选项--&gt; 那么问题来了？ 我怎样把数据加载到这个组件里面？ 在你组件加载完成后，我在调用方法 ( getCategorys() ) 获取菜单数据，在设置到options不就行了吗？ 12345678getCategorys(){ this.$http({ url: this.$http.adornUrl(&quot;/product/category/list/tree&quot;), method: &quot;get&quot; }).then(({ data }) =&gt; { this.categorys = data.data; }); }, 10.4 、分类修改 &amp; 回显级联选择器 修改和新增用的是一个添加组件 那么我们再点击修改后，你如何把 级联显示的数据再次显示出来？ 在 AttrGroup点击修改后，会触发addOrUpdateHandle方法，他会通过引用 vue 文件里的 addOrUpdate 并调用他的 init初始化方法 12345678910// 新增 / 修改 addOrUpdateHandle(id) { // 对话框显示 this.addOrUpdateVisible = true; // 要渲染的组件完成选然后 调用该方法 this.$nextTick(() =&gt; { // this 当前 refs 当前所有组件 this.$refs.addOrUpdate.init(id); }); }, init执行完成会回调 1234567891011.then(({ data }) =&gt; { if (data &amp;&amp; data.code === 0) { this.dataForm.attrGroupName = data.attrGroup.attrGroupName; this.dataForm.sort = data.attrGroup.sort; this.dataForm.descript = data.attrGroup.descript; this.dataForm.icon = data.attrGroup.icon; this.dataForm.catelogId = data.attrGroup.catelogId; // 设置 级联的路径 从数据中取 this.dataForm.catelogPath = data.attrGroup.catelogPath; } }); 后端如何根据分组id 查询出 对应的分类？定义接口 123456/** * 找到catelogId的完整路径 * 【父/子/孙】 * @param catelogId */Long[] findCatelogPath(Long catelogId); 实现 12345678910111213141516171819202122232425262728@Overridepublic Long[] findCatelogPath(Long catelogId) { List&lt;Long&gt; paths = new ArrayList&lt;&gt;(); List&lt;Long&gt; parentPath = findParentPath(catelogId, paths); // 反转 Collections.reverse(parentPath); //转成Long[] 数组返回 return parentPath.toArray(new Long[parentPath.size()]);}/** * 递归查找 * @param catelogId 三级分类的id * @param paths 路径 * @return */private List&lt;Long&gt; findParentPath(Long catelogId,List&lt;Long&gt; paths) { // 1、收集当前节点id paths.add(catelogId); // 2、通过分类id拿到 Category 对象 CategoryEntity byId = this.getById(catelogId); // 3、如果不是根节点 就一直递归下去查找 if (byId.getParentCid() != 0) { findParentPath(byId.getParentCid(),paths); } return paths;} 在 AttrGroupEntity 中 添加了一个新属性 12345/** * 用于存放 级联显示的 父子孙的地址 */@TableField(exist = false) // 标注为false 表是不是数据库字段private Long[] catelogPath; Controller 具体设置返回数据 123456789101112131415/** * 信息 */ @RequestMapping(&quot;/info/{attrGroupId}&quot;) public R info(@PathVariable(&quot;attrGroupId&quot;) Long attrGroupId){ // 根据id查询出 分组group对象AttrGroupEntity attrGroup = attrGroupService.getById(attrGroupId);// 拿到分类id Long catelogId = attrGroup.getCatelogId(); // 根据分类id查询出 他的 父 子 孙 对应的数据,并且设置到 attrGroup对象 Long[] catelogPath = categoryService.findCatelogPath(catelogId); attrGroup.setCatelogPath(catelogPath); return R.ok().put(&quot;attrGroup&quot;, attrGroup); } 10.5、品牌分类关联与级联更新 10.5.1、实现品牌管理搜索 在原本查询中加入新功能 12345678910111213141516@Override public PageUtils queryPage(Map&lt;String, Object&gt; params) { // 1、获取key String key = (String) params.get(&quot;key&quot;); QueryWrapper&lt;BrandEntity&gt; wrapper = new QueryWrapper&lt;&gt;(); // key不为空 brand_id 和 name 进行值匹配 if (!StringUtils.isEmpty(key)) { wrapper.eq(&quot;brand_id&quot;,key).or().like(&quot;name&quot;,key); } IPage&lt;BrandEntity&gt; page = this.page( new Query&lt;BrandEntity&gt;().getPage(params), wrapper ); return new PageUtils(page); } 我们是直接把数据表存进了中间表,如果在真正的品牌名和分类名进行了修改，那么此时中间表的数据就是不对的，这时候数据就不是一致性 解决 在进行修改的时候，也要把中间表的数据进行更改 Brand 1234567891011121314151617181920212223242526272829303132// BrandController /** /** * 修改 */ @RequestMapping(&quot;/update&quot;) public R update(@Validated(UpdateGroup.class) @RequestBody BrandEntity brand){ brandService.updateDetail(brand); return R.ok(); }// Service@Overridepublic void updateDetail(BrandEntity brand) { // 保证字段一致 // 根据id更改 this.updateById(brand); if (!StringUtils.isEmpty(brand.getName())) { // 同步更新其他关联表中的数据 categoryBrandRelationService.updateBrand(brand.getBrandId(),brand.getName()); // TODO 更新其他关联 }}// Service实现类 @Override public void updateBrand(Long brandId, String name) { CategoryBrandRelationEntity relationEntity = new CategoryBrandRelationEntity(); relationEntity.setBrandName(name); relationEntity.setBrandId(brandId); this.update(relationEntity,new UpdateWrapper&lt;CategoryBrandRelationEntity&gt;().eq(&quot;brand_id&quot;,brandId)); } Category 12345678910111213@Overridepublic void updateCascate(CategoryEntity category) { this.updateById(category); categoryBrandRelationService.updateCategory(category.getCatId(),category.getName());}// Service @Override public void updateCascate(CategoryEntity category) { // 更新自己表对象 this.updateById(category); // 更新关联表对象 categoryBrandRelationService.updateCategory(category.getCatId(),category.getName()); } 九、商品服务&amp;平台属性 Object划分 1、PO (persistant object) 持久化对象 po 就是对应数据库中某一个表的一条记录，多个记录可以用 PO 的集合，PO 中应该不包含任何对数据库到操作 2、DO ( Domain Object) 领域对象 就是从现实世界抽象出来的有形或无形的业务实体 3、TO (Transfer Object) 数据传输对象 不同的应用程序之间传输的对象 4、DTO (Data Transfer Object) 数据传输对象 这个概念来源于 J2EE 的设计模式，原来的目的是为了 EJB的分布式应用提供粗粒度的数据实体，以减少分布式调用的次数，从而提高分数调用的性能和降低网络负载，但在这里，泛指用于展示层与服务层之间的数据传输对象 5、VO(value object) 值对象 通常用于业务层之间的数据传递，和 PO 一样也是仅仅包含数据而已，但应是抽象出的业务对象，可以和表对应，也可以不，这根据业务的需要，用 new 关键字创建，由 GC 回收 view Object 试图对象 接受页面传递来的数据，封装对象，封装页面需要用的数据 6、BO(business object) 业务对象 从业务模型的角度看，见 UML 原件领域模型中的领域对象，封装业务逻辑的， java 对象，通过调用 DAO 方法，结合 PO VO,进行业务操作，business object 业务对象，主要作用是把业务逻辑封装成一个对象，这个对象包括一个或多个对象，比如一个简历，有教育经历，工作经历，社会关系等等，我们可以把教育经历对应一个 PO 、工作经验对应一个 PO、 社会关系对应一个 PO, 建立一个对应简历的的 BO 对象处理简历，每 个 BO 包含这些 PO ,这样处理业务逻辑时，我们就可以针对 BO 去处理 7、POJO ( plain ordinary java object) 简单无规则 java 对象 传统意义的 java 对象，就是说一些 Object/Relation Mapping 工具中，能够做到维护数据库表记录的 persisent object 完全是一个符合 Java Bean 规范的 纯 java 对象，没有增加别的属性和方法，我们的理解就是最基本的 Java bean 只有属性字段 setter 和 getter 方法 POJO 时是 DO/DTO/BO/VO 的统称 8、DAO（data access object） 数据访问对象 是一个 sun 的一个标准 j2ee 设计模式，这个模式有个接口就是 DAO ，他负持久层的操作，为业务层提供接口，此对象用于访问数据库，通常和 PO 结合使用，DAO 中包含了各种数据库的操作方法，通过它的方法，结合 PO 对数据库进行相关操作，夹在业务逻辑与数据库资源中间，配合VO 提供数据库的 CRUD 功能 11.1 规格参数新增与VO 11.1.1 获取分类规格参数 具体需求： 查询的数据没有分类和分组 属性分类 和 所属分组居然没有数据？ 先查看返回的数据 发现没有 catelogName 和 AttrGroupName 的数据！！ 原因是 AttrEntity没有这两个属性 但是他关联了 pms_attr_attrgroup_relation 表 这个表里面有 attr_Group_id 那么思路来了？ 1、我先用 attr_id 去 pms_attr_attrgroup_relation中查询出 attr_Group_id 然后通过 attr_Group_id去pms_attr_group 表中查询出 分组的名称 2、自身 attr表中有 catelog_id 那我根据这个id去 category表中查询到 分类的姓名 不就行了吗？ 上代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344@Overridepublic PageUtils queryBaseAttrPage(Map&lt;String, Object&gt; params, Long catelogId) { QueryWrapper&lt;AttrEntity&gt; wrapper = new QueryWrapper&lt;&gt;(); if (catelogId != 0) { wrapper.eq(&quot;catelog_id&quot;,catelogId); } String key = (String) params.get(&quot;key&quot;); if (!StringUtils.isEmpty(key)) { wrapper.and((wrapper1) -&gt; { wrapper1.eq(&quot;attr_id&quot;,key).or().like(&quot;attr_name&quot;,key); }); } IPage&lt;AttrEntity&gt; page = this.page( new Query&lt;AttrEntity&gt;().getPage(params), wrapper ); PageUtils pageUtils = new PageUtils(page); // 拿到分页记录 List&lt;AttrEntity&gt; records = page.getRecords(); List&lt;AttrRespVo&gt; respVo = records.stream().map((attrEntity) -&gt; { AttrRespVo attrRespVo = new AttrRespVo(); BeanUtils.copyProperties(attrEntity, attrRespVo); // 1、设置分类和分组的名字 AttrAttrgroupRelationEntity attr_idEntity = relationDao.selectOne(new QueryWrapper&lt;AttrAttrgroupRelationEntity&gt;() .eq(&quot;attr_id&quot;, attrEntity.getAttrId())); if (attr_idEntity != null) { AttrGroupEntity attrGroupEntity = attrGroupDao.selectById(attr_idEntity.getAttrGroupId()); attrRespVo.setGroupName(attrGroupEntity.getAttrGroupName()); } CategoryEntity categoryEntity = categoryDao.selectById(attrEntity.getCatelogId()); if (categoryEntity != null) { attrRespVo.setCatelogName(categoryEntity.getName()); } return attrRespVo; }).collect(Collectors.toList()); pageUtils.setList(respVo); return pageUtils;} 但是原先的 AttrEntity 对象里面任然没有这两个属性 ! 怎么解决 1、我直接在 AttrEntity中新建对应的字段不就行了吗？ 然后设置成不是数据库的字段 缺点：这样子是不是太乱了？ 2、新建一个 VO 抽取出这两个属性 不就行了吗？ 123456789@Datapublic class AttrRespVo extends AttrVo { /** * catelogName 手机数码，所属分类名字 * groupName 主体 所属分组名字 */ private String catelogName; private String groupName;} 11.2 规格参数列表&amp;规格修改 11.2.1、获取分类规格参数 123456789101112131415/** * 获取分类规格参数 attrType 和 catelogId 通用 * @param params * @param catelogId * @param type sale 销售属性 base 规格参数 * @return */@GetMapping(&quot;/{attrType}/list/{catelogId}&quot;)public R baseAttrList(@RequestParam Map&lt;String,Object&gt; params ,@PathVariable(&quot;catelogId&quot;) Long catelogId, @PathVariable(&quot;attrType&quot;) String type) { PageUtils page = attrService.queryBaseAttrPage(params,catelogId,type); return R.ok().put(&quot;page&quot;,page);} 实现类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Override public PageUtils queryBaseAttrPage(Map&lt;String, Object&gt; params, Long catelogId, String type) { // 先根据 attr_type字段进行查询 该字段表示 1是基本属性 0是销售属性，基本属性和销售属性我们放在一张表内 QueryWrapper&lt;AttrEntity&gt; wrapper = new QueryWrapper&lt;AttrEntity&gt;() .eq(&quot;attr_type&quot;,&quot;base&quot;.equalsIgnoreCase(type)? ProductConstant.AttrEnum.ATTR_TYPE_BASE.getCode(): ProductConstant.AttrEnum.ATTR_TYPE_SALE.getCode()); // 分类id不等于0 按照分类id进行查询 if (catelogId != 0) { wrapper.eq(&quot;catelog_id&quot;,catelogId); } // 取出参数 key,进行条件查询 String key = (String) params.get(&quot;key&quot;); if (!StringUtils.isEmpty(key)) { wrapper.and((wrapper1) -&gt; { wrapper1.eq(&quot;attr_id&quot;,key).or().like(&quot;attr_name&quot;,key); }); } // 封装分页数据 IPage&lt;AttrEntity&gt; page = this.page( new Query&lt;AttrEntity&gt;().getPage(params), wrapper ); PageUtils pageUtils = new PageUtils(page); // 拿到全部的分页记录 List&lt;AttrEntity&gt; records = page.getRecords(); // 查询所属分类以及所属分组 List&lt;AttrRespVo&gt; respVo = records.stream().map((attrEntity) -&gt; { // 实例化数据传输对象 将 attrEntity的分类名字以及分组名字 复制到该对象 AttrRespVo attrRespVo = new AttrRespVo(); BeanUtils.copyProperties(attrEntity, attrRespVo); // Controller地址如果是 基础 才进行查询分裂 if(&quot;base&quot;.equalsIgnoreCase(type)){ // 1、根据attr_id 查询到 attr和 attrGroup的关系表 AttrAttrgroupRelationEntity attr_idEntity = relationDao.selectOne(new QueryWrapper&lt;AttrAttrgroupRelationEntity&gt;() .eq(&quot;attr_id&quot;, attrEntity.getAttrId())); //查询到的关系对象以及分组id不为空 if (attr_idEntity != null &amp;&amp; attr_idEntity.getAttrGroupId()!=null) { // 根据分组id查询到分组信息 AttrGroupEntity attrGroupEntity = attrGroupDao.selectById(attr_idEntity.getAttrGroupId()); attrRespVo.setGroupName(attrGroupEntity.getAttrGroupName()); } } // 根据attrEntity的分类id 查询到分类对象并设置分类姓名 CategoryEntity categoryEntity = categoryDao.selectById(attrEntity.getCatelogId()); if (categoryEntity != null) { attrRespVo.setCatelogName(categoryEntity.getName()); } return attrRespVo; }).collect(Collectors.toList()); //封装到分页工具类 pageUtils.setList(respVo); return pageUtils; } 先是根据 catelog_id查询到AttrEntity 对象 该对象里拥有 attr_id 然后根据attr_id 去 pms_attr_attrgroup_relation表进行查询 11.2.2、查询属性详情 Controller 123456789/** * 信息 */ @RequestMapping(&quot;/info/{attrId}&quot;) public R info(@PathVariable(&quot;attrId&quot;) Long attrId){AttrRespVo attr = attrService.getAttrInfo(attrId); return R.ok().put(&quot;attr&quot;, attr); } Service 实现 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic AttrRespVo getAttrInfo(Long attrId) { // 实例化封装VO数据对象 AttrRespVo attrRespVo = new AttrRespVo(); // 根据attrid查询出商品属性 AttrEntity attrEntity = this.getById(attrId); // 将商品属性对象的属性复制到 VO数据对象 BeanUtils.copyProperties(attrEntity, attrRespVo); // attrType == 1为基本类型才进行查询分组信息 if (attrEntity.getAttrType() == ProductConstant.AttrEnum.ATTR_TYPE_BASE.getCode()) { // 根据 attr_id 查询到 商品属性以及商品分组关系表 AttrAttrgroupRelationEntity relationEntity = relationDao.selectOne(new QueryWrapper&lt;AttrAttrgroupRelationEntity&gt;() .eq(&quot;attr_id&quot;, attrId)); if (relationEntity != null) { // 设置分组id attrRespVo.setAttrGroupId(relationEntity.getAttrGroupId()); // 分组&amp;商品信息关系表拿到分组id 查询到分组对象 AttrGroupEntity attrGroupEntity = attrGroupDao.selectById(relationEntity.getAttrGroupId()); if (attrGroupEntity != null) { //不为空设置分组姓名 attrRespVo.setGroupName(attrGroupEntity.getAttrGroupName()); } } } // 从当前商品属性对象中拿到分类id进行设置分类信息 Long catelogId = attrEntity.getCatelogId(); // 根据 catelogId 查询到分类 父 子 孙 路径 Long[] catelogPath = categoryService.findCatelogPath(catelogId); attrRespVo.setCatelogPath(catelogPath); //最后根据分类id 查询到分类对象 并进行设置分类姓名 CategoryEntity categoryEntity = categoryService.getById(catelogId); if (categoryEntity != null) { attrRespVo.setCatelogName(categoryEntity.getName()); } return attrRespVo;} 主要理解： 先是根据 attrId 查询到商品属性 然后根据 attrId 查询到分类关系表 该表中有分组id attrGroup_id 通过这个查询到 分组对象信息， ，同时在 商品属性表中拿到 分类id 通过分类id 查询到 查询到分类对象 同时根据 分类id查询对应层级关系，并设置分类姓名 11.3.3、销售属性 销售属性 和 基本属性 通过一个字段来区分，对应的修改，查询 都用的同一个方法，所以在方法的中 也进行了对应的判断 保存 1234567891011121314151617@Overridepublic void saveAttr(AttrVo attr) { AttrEntity attrEntity = new AttrEntity(); BeanUtils.copyProperties(attr, attrEntity); // 保存基本数据 this.save(attrEntity); // 2、保存关联关系 // 等于1 说明基本属性 才进行保存分组关系 if (attr.getAttrType() == ProductConstant.AttrEnum.ATTR_TYPE_BASE.getCode() &amp;&amp; attr.getAttrGroupId() != null) { AttrAttrgroupRelationEntity relationEntity = new AttrAttrgroupRelationEntity(); relationEntity.setAttrGroupId(attr.getAttrGroupId()); relationEntity.setAttrId(attrEntity.getAttrId()); relationDao.insert(relationEntity); }} 修改 1234567891011121314151617181920212223242526272829303132333435@Transactional @Override public void updateAttr(AttrVo attr) { // Vo数据传输对象属性拷贝到 attrEntity对象 AttrEntity attrEntity = new AttrEntity(); BeanUtils.copyProperties(attr, attrEntity); this.updateById(attrEntity); // attrType 等于 1 也就是基本属性 if (attrEntity.getAttrType() == ProductConstant.AttrEnum.ATTR_TYPE_BASE.getCode()) { //1、修改分组关联 AttrAttrgroupRelationEntity relationEntity = new AttrAttrgroupRelationEntity(); // 设置attrid 以及 分组id relationEntity.setAttrId(attr.getAttrId()); relationEntity.setAttrGroupId(attr.getAttrGroupId()); // 关系表中根据attr_id 进行更新 relationDao.update(relationEntity, new UpdateWrapper&lt;AttrAttrgroupRelationEntity&gt;(). eq(&quot;attr_id&quot;, attr.getAttrId())); // 根据 attr_id 是否可以查询到结果 Integer count = relationDao.selectCount(new QueryWrapper&lt;AttrAttrgroupRelationEntity&gt;(). eq(&quot;attr_id&quot;, attr.getAttrId())); // 查得到 if (count &gt; 0) { // 进行更新 relationDao.update(relationEntity, new UpdateWrapper&lt;AttrAttrgroupRelationEntity&gt;(). eq(&quot;attr_id&quot;, attr.getAttrId())); } else { // 查不到意味着 没有该记录 则进行插入 relationDao.insert(relationEntity); } } } 11.3 查询分类关联属性&amp;删除关联&amp;查询分组未关联属性 11.3.1、获取属性分组的关联的所有属性 1234567891011/** * 获取属性分组的关联的所有属性 * @param attrgroupId * @return */@GetMapping(&quot;/{attrgroupId}/attr/relation&quot;)public R attrRelation(@PathVariable(&quot;attrgroupId&quot;) Long attrgroupId) { List&lt;AttrEntity&gt; entityList = attrService.getRelationAttr(attrgroupId); return R.ok().put(&quot;data&quot;, entityList);} Service 实现 1234567891011121314151617181920212223/** * 根据分组id查找关联的所有基本属性 * @param attrgroupId * @return */@Overridepublic List&lt;AttrEntity&gt; getRelationAttr(Long attrgroupId) { // 根据attr_group_id 查询到 关系表 List&lt;AttrAttrgroupRelationEntity&gt; entities = relationDao.selectList(new QueryWrapper&lt;AttrAttrgroupRelationEntity&gt;() .eq(&quot;attr_group_id&quot;, attrgroupId)); // 从关系表中 取到 属性id List&lt;Long&gt; attrIds = entities.stream().map((attr) -&gt; { return attr.getAttrId(); }).collect(Collectors.toList()); if (attrIds == null || attrIds.size() == 0) { return null; } // 按照属性id进行查询 Collection&lt;AttrEntity&gt; attrEntities = this.listByIds(attrIds); return (List&lt;AttrEntity&gt;) attrEntities;} 已知有参数 attr_group_id 进行查询 能查到 attr_id,在利用 attr_id 查询到 attr相关信息 11.3.2、删除属性与分组的关联关系 12345678910/** * 删除属性与分组的关联关系 * @param relationVo * @return */@PostMapping(&quot;/attr/relation/delete&quot;)public R deleteRelation(@RequestBody AttrGroupRelationVo[] relationVo) { attrService.deleteRelation(relationVo); return R.ok();} Service 实现 1234567891011121314@Override public void deleteRelation(AttrGroupRelationVo[] relationVo) { // 转成 list 进行stream流处理 List&lt;AttrAttrgroupRelationEntity&gt; entities = Arrays.asList(relationVo).stream().map((item) -&gt; { // 将 item对应属性拷贝到 relationEntity对象中 AttrAttrgroupRelationEntity relationEntity = new AttrAttrgroupRelationEntity(); BeanUtils.copyProperties(item, relationEntity); return relationEntity; }).collect(Collectors.toList()); // id批量删除 relationDao.deleteBatchRelation(entities); } deleteBatchRelation 方法 1234567&lt;delete id=&quot;deleteBatchRelation&quot;&gt; DELETE FROM `pms_attr_attrgroup_relation` WHERE &lt;!- 循环遍历进行删除 使用的是 or--&gt; &lt;foreach collection=&quot;entities&quot; item=&quot;item&quot; separator=&quot; OR &quot;&gt; ( attr_id=#{item.attrId} AND attr_group_id=#{item.attrGroupId} ) &lt;/foreach&gt;&lt;/delete&gt; 11.3.3、查询分组未关联属性 123456@GetMapping(&quot;/{attrgroupId}/noattr/relation&quot;)public R attrNoRelation(@PathVariable(&quot;attrgroupId&quot;) Long attrgroupId, @RequestParam Map&lt;String, Object&gt; params) { PageUtils page = attrService.getNoRelationAttr(params,attrgroupId); return R.ok().put(&quot;page&quot;,page);} Service 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic PageUtils getNoRelationAttr(Map&lt;String, Object&gt; params, Long attrgroupId) { // 1、当前分组只能关联自己所属分类里面的所有属性 AttrGroupEntity attrGroupEntity = attrGroupDao.selectById(attrgroupId); Long catelogId = attrGroupEntity.getCatelogId(); //2、当前分组只能关联别的分组没有引用的属性 //2.1 当前分类下的 **其他分组** 根据分类id进行查询 List&lt;AttrGroupEntity&gt; group = attrGroupDao.selectList(new QueryWrapper&lt;AttrGroupEntity&gt;() .eq(&quot;catelog_id&quot;, catelogId)); // 拿到分组id List&lt;Long&gt; collect = group.stream().map(item -&gt; { return item.getAttrGroupId(); }).collect(Collectors.toList()); //2.2 这些分组关联的属性 根据分组id查询出关联表 List&lt;AttrAttrgroupRelationEntity&gt; groupId = relationDao.selectList(new QueryWrapper&lt;AttrAttrgroupRelationEntity&gt;() .in(&quot;attr_group_id&quot;, collect)); // 拿到所有的属性id List&lt;Long&gt; attrIds = groupId.stream().map((item) -&gt; { return item.getAttrId(); }).collect(Collectors.toList()); //2.3 从当前分类的所有属性中移除这些属性 QueryWrapper&lt;AttrEntity&gt; wrapper = new QueryWrapper&lt;AttrEntity&gt;() .eq(&quot;catelog_id&quot;, catelogId) .eq(&quot;attr_type&quot;, ProductConstant.AttrEnum.ATTR_TYPE_BASE.getCode()); // attrIds 属性id数组不为空 if (attrIds != null &amp;&amp; attrIds.size() &gt; 0) { // 在attrids 数组中得id不用进行查询 wrapper.notIn(&quot;attr_id&quot;, attrIds); } //取出参数进行 对应条件查询 String key = (String) params.get(&quot;key&quot;); if (!StringUtils.isEmpty(key)) { wrapper.and((w) -&gt; { w.eq(&quot;attr_id&quot;, key).or().like(&quot;attr_name&quot;, key); }); } // 根据分页数据 以及 wrapper进行查询 IPage&lt;AttrEntity&gt; page = this.page(new Query&lt;AttrEntity&gt;().getPage(params), wrapper); PageUtils pageUtils = new PageUtils(page); return pageUtils;} 11.4.4、新增分组与属性关联 12345@PostMapping(&quot;/attr/relation&quot;)public R addRelation(@RequestBody List&lt;AttrGroupRelationVo&gt; attrGroupRelationVo) { attrAttrgroupRelationService.saveBatch(attrGroupRelationVo); return R.ok();} Service 12345678910@Overridepublic void saveBatch(List&lt;AttrGroupRelationVo&gt; attrGroupRelationVo) { List&lt;AttrAttrgroupRelationEntity&gt; collect = attrGroupRelationVo.stream().map((item) -&gt; { // 复制属性 返回 AttrAttrgroupRelationEntity relationEntity = new AttrAttrgroupRelationEntity(); BeanUtils.copyProperties(item, relationEntity); return relationEntity; }).collect(Collectors.toList()); this.saveBatch(collect);} 业务之间需要多种测试 各种判断 十、商品服务&amp;新增商品 12.1 获取分类关联的品牌 前端发送请求 后端编写： Controller 123456789101112131415161718/** * 获取分类关联的品牌 * @param catId * @return */@GetMapping(&quot;/brands/list&quot;)public R relationBrandList(@RequestParam(value = &quot;catId&quot;,required = true) Long catId) { List&lt;BrandEntity&gt; vos = categoryBrandRelationService.getBrandCatId(catId); //拿到 品牌对象数据后 从中抽取除 品牌姓名和id List&lt;BrandVo&gt; collect = vos.stream().map(item -&gt; { BrandVo brandVo = new BrandVo(); brandVo.setBrandId(item.getBrandId()); brandVo.setBrandName(item.getName()); return brandVo; }).collect(Collectors.toList()); return R.ok().put(&quot;data&quot;,collect);} Service实现 12345678910111213@Overridepublic List&lt;BrandEntity&gt; getBrandCatId(Long catId) { // 根据分类id查询出 分类和品牌的关系表 List&lt;CategoryBrandRelationEntity&gt; catelogId = categoryBrandRelationDao.selectList(new QueryWrapper&lt;CategoryBrandRelationEntity&gt;().eq(&quot;catelog_id&quot;, catId)); List&lt;BrandEntity&gt; collect = catelogId.stream().map(item -&gt; { //根据品牌id查询出品牌对象 BrandEntity brandEntity = brandDao.selectById(item.getBrandId()); return brandEntity; }).collect(Collectors.toList()); return collect;} 总结： 业务操作设计两个表 pms_brand pms_category_brand_relation 请求参数是 CatId Long类型 先根据 CatId 在 pms_category_brand_relation表中查询到品牌 Id 拿到 brand_id 查询出 brand 的相关信息 组装成 BrandVo 后 返回 12.2 获取分类下所有分组以及属性 基本信息输入成功后，就会跳转到规格参数， 并根据分类id查询出对应数据 Controller 12345678910/** * 获取分类下所有分组&amp;关联属性 * @param catelogId * @return */@RequestMapping(&quot;/{catelogId}/withattr&quot;)public R getAttrGroupWithAttrs(@PathVariable(&quot;catelogId&quot;) Long catelogId) { List&lt;AttrGroupWithAttrsVo&gt; vos = attrGroupService.getAttrGroupWithAttrsByCatelogId(catelogId); return R.ok().put(&quot;data&quot;,vos);} Service 实现 12345678910111213141516171819@Overridepublic List&lt;AttrGroupWithAttrsVo&gt; getAttrGroupWithAttrsByCatelogId(Long catelogId) { // 1、根据分类id查询出 查询分组关系 List&lt;AttrGroupEntity&gt; attrgroupEntites = this.list(new QueryWrapper&lt;AttrGroupEntity&gt;().eq(&quot;catelog_id&quot;, catelogId)); List&lt;AttrGroupWithAttrsVo&gt; collect = attrgroupEntites.stream().map(group -&gt; { AttrGroupWithAttrsVo attrsVo = new AttrGroupWithAttrsVo(); // 2、将分组属性拷贝到 VO中 BeanUtils.copyProperties(group, attrsVo); // 3、通过分组id查询出 商品属性信息 // 调用 getRelationAttr方法先根据 分组id去 中间关系表查询到商品属性id 然后根据商品属性id查询到商品信息 List&lt;AttrEntity&gt; relationAttr = attrService.getRelationAttr(attrsVo.getAttrGroupId()); attrsVo.setAttrs(relationAttr); return attrsVo; }).collect(Collectors.toList()); return collect;} 12.3 商品 VO 抽取&amp;商品新增业务流程 商品属性、销售属性、规格参数、基本信息都填好了后就会生成一串 JSON 我们将 json 放到 json解析网站上 并生成对应得实体类 12.3.4 封装 Vo 中，更改对应得属性 有些参与计算的属性 如 int price 将类型更改为 BigDecimal 12.3.5 分析业务流程 业务流程: 12.3.6 主要编码！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127@Transactional@Overridepublic void saveSpuInfo(SpuSaveVo vo) { // 1、保存Spu基本信息 pms_spu_info SpuInfoEntity spuInfoEntity = new SpuInfoEntity(); BeanUtils.copyProperties(vo,spuInfoEntity); spuInfoEntity.setCreateTime(new Date()); spuInfoEntity.setUpdateTime(new Date()); this.saveBaseSpuInfo(spuInfoEntity); // 2、保存Spu的描述信息 pms_spu_info_desc List&lt;String&gt; decript = vo.getDecript(); SpuInfoDescEntity spuInfoDescEntity = new SpuInfoDescEntity(); // SpuInfoEntity保存到取得 spuId 设置到 Desc中 spuInfoDescEntity.setSpuId(spuInfoEntity.getId()); // 以逗号来拆分 spuInfoDescEntity.setDecript(String.join(&quot;,&quot;,decript)); spuInfoDescService.saveSpuInfoDesc(spuInfoDescEntity); // 3、保存Spu的图片集 pms_spu_images List&lt;String&gt; imageList = vo.getImages(); spuImagesService.saveImages(spuInfoEntity.getId(),imageList); // 4、保存spu的规格参数 pms_product_attr_value List&lt;BaseAttrs&gt; baseAttrs = vo.getBaseAttrs(); List&lt;ProductAttrValueEntity&gt; collect = baseAttrs.stream().map(attr -&gt; { // 设置 spu 属性值 ProductAttrValueEntity valueEntity = new ProductAttrValueEntity(); valueEntity.setAttrId(attr.getAttrId()); AttrEntity attrEntity = attrService.getById(attr.getAttrId()); valueEntity.setAttrName(attrEntity.getAttrName()); valueEntity.setSpuId(spuInfoEntity.getId()); valueEntity.setQuickShow(attr.getShowDesc()); valueEntity.setAttrValue(attr.getAttrValues()); return valueEntity; }).collect(Collectors.toList()); attrValueService.saveProductAttr(collect); // 5、保存SPU的积分信息 gulimall_sms sms =&gt; sms_spu_bounds Bounds bounds = vo.getBounds(); SpuBoundTo spuBoundTo = new SpuBoundTo(); BeanUtils.copyProperties(bounds,spuBoundTo); spuBoundTo.setSpuId(spuInfoEntity.getId()); // 远程服务调用 R r = couponFeignService.saveSpuBounds(spuBoundTo); if (r.getCode() != 0) { log.error(&quot;远程保存优惠信息失败&quot;); } // 5、保存当前Spu对应的所有SKU信息 List&lt;Skus&gt; skus = vo.getSkus(); if (skus != null &amp;&amp; skus.size() &gt; 0) { // 遍历 skus 集合 skus.forEach(item -&gt;{ String defaultImage = &quot;&quot;; // 遍历 skus 集合中的图片 for (Images image : item.getImages()) { // 默认图片等于 1 该记录则是默认图片 if (image.getDefaultImg() == 1) { defaultImage = image.getImgUrl(); } } // private String skuName; // private String price; // private String skuTitle; // private String skuSubtitle; // 只有上面4个属性相同 SkuInfoEntity skuInfoEntity = new SkuInfoEntity(); BeanUtils.copyProperties(item,skuInfoEntity); // 其他属性需要自己赋值 skuInfoEntity.setBrandId(spuInfoEntity.getBrandId()); skuInfoEntity.setCatalogId(spuInfoEntity.getCatalogId()); skuInfoEntity.setSaleCount(0L); skuInfoEntity.setSpuId(spuInfoDescEntity.getSpuId()); skuInfoEntity.setSkuDefaultImg(defaultImage); //5.1、SKU的基本信息 pms_sku_info skuInfoService.saveSkuInfo(skuInfoEntity); Long skuId = skuInfoEntity.getSkuId(); // 保存 sku 图片信息 List&lt;SkuImagesEntity&gt; imagesEntities = item.getImages().stream().map(img -&gt; { SkuImagesEntity skuImagesEntity = new SkuImagesEntity(); skuImagesEntity.setSkuId(skuId); skuImagesEntity.setImgUrl(img.getImgUrl()); skuImagesEntity.setDefaultImg(img.getDefaultImg()); return skuImagesEntity; }).filter(entity -&gt;{ //返回 true 需要 false 过滤 return !StringUtils.isEmpty(entity.getImgUrl()); }).collect(Collectors.toList()); // TODO 没有图片路径的无需保存 //5.2、SKU的图片信息 pms_sku_images skuImagesService.saveBatch(imagesEntities); List&lt;Attr&gt; attr = item.getAttr(); // 保存 sku 销售属性 List&lt;SkuSaleAttrValueEntity&gt; skuSaleAttrValueEntities = attr.stream().map(a -&gt; { SkuSaleAttrValueEntity skuSaleAttrValueEntity = new SkuSaleAttrValueEntity(); BeanUtils.copyProperties(a, skuSaleAttrValueEntity); skuSaleAttrValueEntity.setSkuId(skuId); return skuSaleAttrValueEntity; }).collect(Collectors.toList()); //5.3、SKU的销售属性信息 pms_sku_sale_attr_value saleAttrValueService.saveBatch(skuSaleAttrValueEntities); //5.4、SKU的优惠、满减等信息 gulimall_sms -&gt;sms_sku_ladder \\sms_sku_full_reduction\\sms_member_price SkuReductionTo skuReductionTo = new SkuReductionTo(); BeanUtils.copyProperties(item,skuReductionTo); skuReductionTo.setSkuId(skuId); if (skuReductionTo.getFullCount() &gt; 0 || skuReductionTo.getFullPrice().compareTo(new BigDecimal(&quot;0&quot;)) == 1) { R r1 = couponFeignService.saveSkuReduction(skuReductionTo); if (r1.getCode() != 0) { log.error(&quot;远程保存sku优惠信息失败&quot;); } } }); }} saveImages 1234567891011121314151617@Overridepublic void saveImages(Long id, List&lt;String&gt; imageList) { if (imageList == null || imageList.size() &lt;=0) { log.error(&quot;图片为空！！！！！！&quot;); } else { List&lt;SpuImagesEntity&gt; collect = imageList.stream().map(img -&gt; { SpuImagesEntity entity = new SpuImagesEntity(); // 设置主要属性 entity.setSpuId(id); entity.setImgUrl(img); return entity; }).collect(Collectors.toList()); this.saveBatch(collect ); }} 远程服务调用 对应方法 保存了 商品阶梯价格、商品满减信息、商品会员价格 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Overridepublic void saveSkuReduction(SkuReductionTo skuReductionTo) { //5.4、SKU的优惠、满减等信息 gulimall_sms -&gt;sms_sku_ladder \\sms_sku_full_reduction\\sms_member_price //sms_sku_ladder SkuLadderEntity skuLadderEntity = new SkuLadderEntity(); skuLadderEntity.setSkuId(skuReductionTo.getSkuId()); skuLadderEntity.setFullCount(skuReductionTo.getFullCount()); skuLadderEntity.setAddOther(skuReductionTo.getCountStatus()); skuLadderEntity.setDiscount(skuReductionTo.getDiscount()); if (skuLadderEntity.getFullCount() &gt; 0) { skuLadderService.save(skuLadderEntity); } //sms_sku_full_reduction SkuFullReductionEntity skuFullReductionEntity = new SkuFullReductionEntity(); BeanUtils.copyProperties(skuReductionTo,skuFullReductionEntity); // BigDecimal 用 compareTo来比较 if (skuFullReductionEntity.getFullPrice().compareTo(new BigDecimal(&quot;0&quot;)) == 1) { this.save(skuFullReductionEntity); } //sms_member_price List&lt;MemberPrice&gt; memberPrice = skuReductionTo.getMemberPrice(); List&lt;MemberPriceEntity&gt; collect = memberPrice.stream().map(item -&gt; { MemberPriceEntity priceEntity = new MemberPriceEntity(); priceEntity.setSkuId(skuReductionTo.getSkuId()); priceEntity.setMemberPrice(item.getPrice()); priceEntity.setMemberLevelName(item.getName()); priceEntity.setMemberLevelId(item.getId()); priceEntity.setAddOther(1); return priceEntity; }).filter(item -&gt; { // 会员对应价格等于0 过滤掉 return item.getMemberPrice().compareTo(new BigDecimal(&quot;0&quot;)) == 1; }).collect(Collectors.toList()); memberPriceService.saveBatch(collect);} 12.3.7 总结 电商系统中大表数据不做关联 宁可一点一点查询 商品新增业务，眨眼一看很多代码，但是如果把他们划分成几个功能点一一完成，业务也就不会变得很庞大 相关操作的表 12.3.8 商品保存后 Debug 调试 很少有一次写的代码能一次通过，所以我们要 一个功能点一个断点来调试程序是否正确 12# 将当前会话等级设置成读为提交，当前窗口就能查看到没有提交的数据SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; 具体 Debug 过程不在此叙述 12.3.9 商品保存其他问题处理 1、 sku_images 表中 img_url 字段为空 sku_images 中有很多图片都是为空，因此我们需要在程序中处理这个数据，空数据不写入到数据库中 解决思路： skuImages 保存部分代码、如果 ImgUrl 为空则进行过滤 1234}).filter(entity -&gt;{ //返回 true 需要 false 过滤 return !StringUtils.isEmpty(entity.getImgUrl());}).collect(Collectors.toList()); 2、sku 满减以及打折信息 数据出现错误 有部分数据 为0 解决思路： 在代码中过滤对应为0的数据 部分修改代码 1234567// 满几件 大于0 可以添加 满多少钱 至少要大于0if (skuReductionTo.getFullCount() &gt; 0 || skuReductionTo.getFullPrice().compareTo(new BigDecimal(&quot;0&quot;)) == 1) { R r1 = couponFeignService.saveSkuReduction(skuReductionTo); if (r1.getCode() != 0) { log.error(&quot;远程保存sku优惠信息失败&quot;); }} 远程服务中也进行对应修改 1234567891011121314151617181920212223/**保存 商品阶梯价格件数 大于0才能进行修改**/if (skuLadderEntity.getFullCount() &gt; 0) { skuLadderService.save(skuLadderEntity);}/**保存商品满减信息**/ // BigDecimal 用 compareTo来比较if (skuFullReductionEntity.getFullPrice().compareTo(new BigDecimal(&quot;0&quot;)) == 1) { this.save(skuFullReductionEntity);}/**保存商品会员价格也进行了过滤数据**/ }).filter(item -&gt; { // 会员对应价格等于0 过滤掉 return item.getMemberPrice().compareTo(new BigDecimal(&quot;0&quot;)) == 1; }).collect(Collectors.toList()); 3、程序中其他的异常 程序中总会出现一些其他的异常的，这个留到高级篇进行讲解 十一、 商品服务&amp;商品管理 SPU SKU 11.1 商品管理 SPU 检索 功能概述： 查询刚刚发布的商品，并能进行对应的条件查询 Controller 12345678910/** * 列表 */ @RequestMapping(&quot;/list&quot;) //@RequiresPermissions(&quot;product:spuinfo:list&quot;) public R list(@RequestParam Map&lt;String, Object&gt; params){ PageUtils page = spuInfoService.queryPageByCondition(params); return R.ok().put(&quot;page&quot;, page); } Service 实现 1234567891011121314151617181920212223242526272829303132333435363738/** * 根据条件进行查询 spu相关信息 * @param params 分页参数 status上架状态，catelogId 分类id brandId 品牌id * @return */@Overridepublic PageUtils queryPageByCondition(Map&lt;String, Object&gt; params) { QueryWrapper&lt;SpuInfoEntity&gt; wrapper = new QueryWrapper&lt;&gt;(); // 取出参数 key 进行查询 String key = (String) params.get(&quot;key&quot;); if (!StringUtils.isEmpty(key)) { wrapper.and((w) -&gt;{ w.eq(&quot;id&quot;,&quot;key&quot;).or().like(&quot;spu_name&quot;,key); }); } // 验证不为空 取出参数进行 查询 String status = (String) params.get(&quot;status&quot;); if (!StringUtils.isEmpty(status)) { wrapper.eq(&quot;publish_status&quot;,status); } String brandId = (String) params.get(&quot;brandId&quot;); if (!StringUtils.isEmpty(brandId) &amp;&amp; ! &quot;0&quot;.equalsIgnoreCase(brandId)) { wrapper.eq(&quot;brand_id&quot;,brandId); } String catelogId = (String) params.get(&quot;catelogId&quot;); if (!StringUtils.isEmpty(catelogId) &amp;&amp; ! &quot;0&quot;.equalsIgnoreCase(catelogId)) { wrapper.eq(&quot;catalog_id&quot;,catelogId); } IPage&lt;SpuInfoEntity&gt; page = this.page( new Query&lt;SpuInfoEntity&gt;().getPage(params), wrapper); return new PageUtils(page);} 总结业务流程 主要查询的是 pms_spu_info 这张表 前端传递过来的参数 主要有分页，以及 品牌id 分类id 我们需要判断这些值是否正确，然后进行查询数据 11.2 商品管理 SkU 检索 功能概述： 查询具体的商品管理 库存 Controller 12345678910/** * 列表 */@RequestMapping(&quot;/list&quot;)//@RequiresPermissions(&quot;product:skuinfo:list&quot;)public R list(@RequestParam Map&lt;String, Object&gt; params){ PageUtils page = skuInfoService.queryPageByCondition(params); return R.ok().put(&quot;page&quot;, page);} Service 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Overridepublic PageUtils queryPageByCondition(Map&lt;String, Object&gt; params) { QueryWrapper&lt;SkuInfoEntity&gt; wrapper = new QueryWrapper&lt;&gt;(); // 取出参数 进行查询 String key = (String) params.get(&quot;key&quot;); if (!StringUtils.isEmpty(key)) { wrapper.and((w) -&gt;{ w.eq(&quot;sku_id&quot;,key).or().like(&quot;sku_name&quot;,key); }); } // 验证 id 是否为0 否则进行匹配 String catelogId = (String) params.get(&quot;catelogId&quot;); if (!StringUtils.isEmpty(catelogId) &amp;&amp; !&quot;0&quot;.equalsIgnoreCase(catelogId)) { wrapper.eq(&quot;catalog_id&quot;,catelogId); } String brandId = (String) params.get(&quot;brandId&quot;); if (!StringUtils.isEmpty(brandId) &amp;&amp; !&quot;0&quot;.equalsIgnoreCase(brandId)) { wrapper.eq(&quot;brand_id&quot;,brandId); } String min = (String) params.get(&quot;min&quot;); if (!StringUtils.isEmpty(min)) { wrapper.ge(&quot;price&quot;,min); } String max = (String) params.get(&quot;max&quot;); if (!StringUtils.isEmpty(max) ) { // 怕前端传递的数据是 abc 等等 所以要抛出异常 try { BigDecimal bigDecimal = new BigDecimal(max); if ( bigDecimal.compareTo(new BigDecimal(&quot;0&quot;)) == 1) { wrapper.le(&quot;price&quot;,max); } } catch (Exception e) { e.printStackTrace(); } } IPage&lt;SkuInfoEntity&gt; page = this.page( new Query&lt;SkuInfoEntity&gt;().getPage(params), wrapper ); return new PageUtils(page);} 总结业务流程： 主要查询是 pms_sku_info 这张表，我出现的问题是发布商品时候，实体类的 价格 与 VO属性不一致，出现了数据没有拷贝，修改后，数据查询正常 十二、仓储服务&amp;仓库管理 14.1 整合ware服务&amp;获取仓库列表 14.1.1整合 ware 服务 1、首先服务需要先注册到 Nacos 中，需要自己配置文件 配置对应的 nacos注册地址， 2、启动类需要开启 服务注册发现，开启远程服务调用 1234@EnableFeignClients // 开启openfeign 远程服务调用@EnableTransactionManagement //开启事务@MapperScan(&quot;com.atguigu.gulimall.ware.dao&quot;) //mapper包扫描@EnableDiscoveryClient //服务注册发现 14.1.2 获取仓库列表 具体需求： 根据参数名查询出 仓库相关的记录 ，具体对于操作的是wms_ware_info 表 Controller 12345678910/** * 列表 */@RequestMapping(&quot;/list&quot;)//@RequiresPermissions(&quot;ware:wareinfo:list&quot;)public R list(@RequestParam Map&lt;String, Object&gt; params){ PageUtils page = wareInfoService.queryPage(params); return R.ok().put(&quot;page&quot;, page);} Service 1234567891011121314151617181920@Overridepublic PageUtils queryPage(Map&lt;String, Object&gt; params) { QueryWrapper&lt;WareInfoEntity&gt; wrapper = new QueryWrapper&lt;&gt;(); // 取出参数 String key = (String)params.get(&quot;key&quot;); // 拼接条件 if (!StringUtils.isEmpty(key)) { wrapper.eq(&quot;id&quot;,key).or() .like(&quot;name&quot;,key) .eq(&quot;address&quot;,key) .eq(&quot;areacode&quot;,key); } IPage&lt;WareInfoEntity&gt; page = this.page( new Query&lt;WareInfoEntity&gt;().getPage(params), wrapper ); return new PageUtils(page);} 总结业务流程 仓库维护对应的是 wms_ware_info 根据前端传递过来的参数进行拼接然后进行查询 14.2 查询库存&amp;创建采购需求 14.2.1 查询库存 具体需求： 根据 仓库、skuId 进行查询对应的表是 wms_ware_sku Controller 12345678910/** * 列表 */@RequestMapping(&quot;/list&quot;)//@RequiresPermissions(&quot;ware:waresku:list&quot;)public R list(@RequestParam Map&lt;String, Object&gt; params){ PageUtils page = wareSkuService.queryPage(params); return R.ok().put(&quot;page&quot;, page);} Service 123456789101112131415161718192021@Overridepublic PageUtils queryPage(Map&lt;String, Object&gt; params) { QueryWrapper&lt;WareSkuEntity&gt; queryWrapper = new QueryWrapper&lt;&gt;(); // 取出请求的参数 组装条件进行查询 String skuId = (String) params.get(&quot;skuId&quot;); if (!StringUtils.isEmpty(skuId)) { queryWrapper.eq(&quot;sku_id&quot;,skuId); } String wareId = (String) params.get(&quot;wareId&quot;); if (!StringUtils.isEmpty(wareId)) { queryWrapper.eq(&quot;ware_id&quot;,wareId); } IPage&lt;WareSkuEntity&gt; page = this.page( new Query&lt;WareSkuEntity&gt;().getPage(params), queryWrapper ); return new PageUtils(page);} 总结业务流程： wms_ware_sku 主要的操作的就是该表 一样的封装条件，然后进行查询 14.2.2 创建采购需求 具体需求： 选择 仓库、状态、 以及其他关键字 查询出对应的数据 那么查询的是哪张表？ wms_purchase_detail Controller 如往常一样 调用 Service 返回结果 组装 返回~~~ 12345678910/** * 列表 */@RequestMapping(&quot;/list&quot;)//@RequiresPermissions(&quot;ware:purchasedetail:list&quot;)public R list(@RequestParam Map&lt;String, Object&gt; params){ PageUtils page = purchaseDetailService.queryPage(params); return R.ok().put(&quot;page&quot;, page);} Service 123456789101112131415161718192021222324252627282930313233343536373839/** * // status: 0,//状态 * // wareId: 1,//仓库id * @param params * @return */@Overridepublic PageUtils queryPage(Map&lt;String, Object&gt; params) { QueryWrapper&lt;PurchaseDetailEntity&gt; wrapper = new QueryWrapper&lt;&gt;(); // 取出key String key = (String) params.get(&quot;key&quot;); // key 主要查询条件 if (!StringUtils.isEmpty(key)) { wrapper.and((w) -&gt;{ w.eq(&quot;purchase_id&quot;,key).or().eq(&quot;sku_id&quot;,key); }); } String status = (String) params.get(&quot;status&quot;); if (!StringUtils.isEmpty(status)) { wrapper.and((w) -&gt;{ w.eq(&quot;status&quot;,status); }); } String wareId = (String) params.get(&quot;wareId&quot;); if (!StringUtils.isEmpty(wareId)) { wrapper.and((w) -&gt;{ w.eq(&quot;ware_id&quot;,wareId); }); } IPage&lt;PurchaseDetailEntity&gt; page = this.page( new Query&lt;PurchaseDetailEntity&gt;().getPage(params), wrapper ); return new PageUtils(page);} 总结业务流程： 对 wms_purchase_detail 表进行操作 拼装条件 查询 嗯 就这样~~~ 14.3 合并采购需求&amp;领取采购单&amp;完成采购&amp;Spu规格维护 14.3.1 合并采购需求 具体需求： 选中 点击批量操作 请求参数分享 1234{ purchaseId: 1, //整单id items:[1,2,3,4] //合并项集合} 那就建立对应的 Vo 用来接收请求参数 Controller 123456789101112/** * 合并采购 * @param mergeVo * @return *////ware/purchase/merge@PostMapping(&quot;/merge&quot;)public R merge(@RequestBody MergeVo mergeVo) { purchaseService.mrgePurchase(mergeVo); return R.ok();} Service 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 合并采购需求 * @param mergeVo */@Transactional@Overridepublic void mrgePurchase(MergeVo mergeVo) { // 拿到采购单id Long purchaseId = mergeVo.getPurchaseId(); // 采购单 id为空 新建 if (purchaseId == null ) { PurchaseEntity purchaseEntity = new PurchaseEntity(); // 状态设置为新建 purchaseEntity.setStatus(WareConstant.PurchaseStatusEnum.CREATED.getCode()); purchaseEntity.setCreateTime(new Date()); purchaseEntity.setUpdateTime(new Date()); this.save(purchaseEntity); // 拿到最新的采购单id purchaseId = purchaseEntity.getId(); } //TODO 确认采购是 0 或 1 才可以合并 // 拿到合并项 **采购需求的id** List&lt;Long&gt; items = mergeVo.getItems(); Long finalPurchaseId = purchaseId; List&lt;PurchaseDetailEntity&gt; collect = items.stream().map(i -&gt; { // 采购需求 PurchaseDetailEntity detailEntity = new PurchaseDetailEntity(); // 通过采购单id 查询到 采购信息对象 PurchaseEntity byId = this.getById(finalPurchaseId); // 状态如果是正在采购 if (! (byId.getStatus() == WareConstant.PurchaseDetailStatusEnum.BUYING.getCode())) { // 设置为已分配 detailEntity.setStatus(WareConstant.PurchaseDetailStatusEnum.HASERROR.getCode()); } detailEntity.setId(i); // 设置采购单id detailEntity.setPurchaseId(finalPurchaseId); return detailEntity; }).collect(Collectors.toList()); // id批量更新 purchaseDetailService.updateBatchById(collect); // 再次合并的话 更新修改时间 PurchaseEntity purchaseEntity = new PurchaseEntity(); purchaseEntity.setId(purchaseId); purchaseEntity.setUpdateTime(new Date()); this.updateById(purchaseEntity);} 总结业务流程: 从 参数 mergeVo中取出 purchaseId 和 items 进行相关操作 主要是用来操作两张表 wms_purchase 和 wms_purchase_detail wms_purchase 表是 采购信息 wms_purchase_detail 表是 采购需求 我的理解是将 wms_purchase 的id插入到 wms_purchase_detail 表中 也就是 purchase_id 字段，中间通过这个字段关联起来，同时 wms_purchase 表对 status 状态有比较多的选择，视频里面也是定义了两个常量 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class WareConstant { public enum PurchaseStatusEnum{ CREATED(0,&quot;新建&quot;),ASSIGNED(1,&quot;已分配&quot;), RECEIVE(2,&quot;已领取&quot;),FINISH(3,&quot;已完成&quot;), HASERROR(4,&quot;有异常&quot;); private int code; private String msg; PurchaseStatusEnum(int code,String msg) { this.code = code; this.msg = msg; } public int getCode() { return code; } public void setCode(int code) { this.code = code; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } } public enum PurchaseDetailStatusEnum{ CREATED(0,&quot;新建&quot;),ASSIGNED(1,&quot;已分配&quot;), BUYING(2,&quot;正在采购&quot;),FINISH(3,&quot;已完成&quot;), HASERROR(4,&quot;采购失败&quot;); private int code; private String msg; PurchaseDetailStatusEnum(int code,String msg) { this.code = code; this.msg = msg; } public int getCode() { return code; } public void setCode(int code) { this.code = code; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } }} 将常量抽取出来，修改更加方便 14.3.2 领取采购单 具体需求： 合并采购需求成功后，具体这个功能有啥用啊？ 你总得需要有人去采购吧？ 所以就会有一个采购APP 工作人员点击采购，然后就去采购，这里就没有实现采购 APP 就用接口来实现，通过 JSON 的参数 来请求 Controller 1234567891011/** * * @param ids * @return */@PostMapping(&quot;/received&quot;)public R received(@RequestBody List&lt;Long&gt; ids) { purchaseService.received(ids); return R.ok();} Service 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic void received(List&lt;Long&gt; ids) { // 1、确认当前采购单是 新建或者 已分配状态 才能进行采购 List&lt;PurchaseEntity&gt; collect = ids.stream().map(id -&gt; { // 根据采购id查询出采购信息 PurchaseEntity byId = this.getById(id); return byId; }).filter(item -&gt; { // 新建或者已分配留下 if (item.getStatus() == WareConstant.PurchaseStatusEnum.CREATED.getCode() || item.getStatus() == WareConstant.PurchaseStatusEnum.ASSIGNED.getCode()) { return true; } return false; }).map(item -&gt; { // 设置为已领取 item.setStatus(WareConstant.PurchaseStatusEnum.RECEIVE.getCode()); item.setUpdateTime(new Date()); return item; }).collect(Collectors.toList()); // 2、改变采购单状态 this.updateBatchById(collect); // 3、改变采购项的状态 collect.forEach((item) -&gt; { // 根据 purchase_id 查询出采购需求 List&lt;PurchaseDetailEntity&gt; entities = purchaseDetailService.listDetailByPurchaseId(item.getId()); // List&lt;PurchaseDetailEntity&gt; detailEntites = entities.stream().map(entity -&gt; { PurchaseDetailEntity detailEntity = new PurchaseDetailEntity(); detailEntity.setId(entity.getId()); // 设置状态正在采购 detailEntity.setStatus(WareConstant.PurchaseDetailStatusEnum.BUYING.getCode()); return detailEntity; }).collect(Collectors.toList()); // id批量更新 purchaseDetailService.updateBatchById(detailEntites); });} 总结业务流程： 业务分析 采购人员通过 APP 点击采购 完成对应的采购需求，这里使用的是 PostMan 来发送请求，发送请求 带的参数是什么？ 参数就是 采购Id 通过采购 Id 查询出采购相关信息，然后设置采购表的状态，设置成采购成功，同时通过这个 id 在 wms_purchase_detail 表中 对应的是 purchase_id 查询采购需求表的数据， 查询到后将他的状态设置成 “正在采购“ 14.3.3 完成采购 具体需求： 采购人员参与采购后，采购就会有他的结果，采购成功、采购失败， 有了请求参数，如果比较多，那么底考虑设计一个 VO 哦 123456@PostMapping(&quot;/done&quot;)public R finish(@RequestBody PurchaseDoneVo doneVo) { purchaseService.done(doneVo); return R.ok();} Vo 12345678910@Datapublic class PurchaseDoneVo { /** * 采购单id */ @NotNull private Long id; public List&lt;PurchaseItemDoneVo&gt; items;} 1234567891011121314@Datapublic class PurchaseItemDoneVo { /** * 完成/失败的需求详情 */ private Long itemId; /** * 状态 */ private Integer status; private String reason;} Service 123456789101112131415161718192021222324252627282930313233343536373839@Transactional@Overridepublic void done(PurchaseDoneVo doneVo) { // 采购单id Long id = doneVo.getId(); // 2、改变采购项目的状态 Boolean flag = true; List&lt;PurchaseItemDoneVo&gt; items = doneVo.getItems(); List&lt;PurchaseDetailEntity&gt; updates = new ArrayList&lt;&gt;(); for (PurchaseItemDoneVo item : items) { PurchaseDetailEntity detailEntity = new PurchaseDetailEntity(); // 如果采购失败 if (item.getStatus() == WareConstant.PurchaseDetailStatusEnum.HASERROR.getCode()) { flag = false; detailEntity.setStatus(item.getStatus()); } else { // 3、将成功采购的进行入库 PurchaseDetailEntity entity = purchaseDetailService.getById(item.getItemId()); wareSkuService.addStock(entity.getSkuId(),entity.getWareId(),entity.getSkuNum()); detailEntity.setStatus(WareConstant.PurchaseDetailStatusEnum.FINISH.getCode()); } detailEntity.setId(item.getItemId()); updates.add(detailEntity); } // 批量更新 purchaseDetailService.updateBatchById(updates); // 1、改变采购单状态 PurchaseEntity purchaseEntity = new PurchaseEntity(); purchaseEntity.setId(id); // 设置状态根据变量判断 purchaseEntity.setStatus(flag?WareConstant.PurchaseStatusEnum.FINISH.getCode():WareConstant.PurchaseStatusEnum.HASERROR.getCode()); purchaseEntity.setUpdateTime(new Date()); this.updateById(purchaseEntity);} addStock方法 123456789101112131415161718192021222324252627282930@Overridepublic void addStock(Long skuId, Long wareId, Integer skuNum) { // 先根据 skuId 和 ware_id 查询 是否拥有这个用户 List&lt;WareSkuEntity&gt; wareSkuEntities = wareSkuDao.selectList(new QueryWrapper&lt;WareSkuEntity&gt;().eq(&quot;sku_id&quot;, skuId).eq(&quot;ware_id&quot;, wareId)); //没有这个用户 那就新增 if(wareSkuEntities == null || wareSkuEntities.size() == 0) { WareSkuEntity wareSkuEntity = new WareSkuEntity(); // 根据属性值设置 wareSkuEntity.setSkuId(skuId); wareSkuEntity.setStock(skuNum); wareSkuEntity.setWareId(wareId); wareSkuEntity.setStockLocked(0); // TODO 远程查询sku的名字 如果失败整个事务不需要回滚 try { // 远程调用 根据 skuid进行查询 R info = productFeignService.info(skuId); Map&lt;String,Object&gt; map = (Map&lt;String, Object&gt;) info.get(&quot;skuInfo&quot;); if (info.getCode() == 0) { wareSkuEntity.setSkuName((String) map.get(&quot;skuName&quot;)); } } catch (Exception e) { e.printStackTrace(); } wareSkuDao.insert(wareSkuEntity); }else { // 有该记录那就进行更新 wareSkuDao.addStock(skuId,wareId,skuNum); }} addStock 具体实现 123&lt;insert id=&quot;addStock&quot;&gt;UPDATE `wms_ware_sku` SET stock=stock+#{skuNum} WHERE sku_id=#{skuId} AND ware_id=#{wareId}&lt;/insert&gt; 14.3.4 Spu规格维护 具体需求： 前端项目遇到的问题： 需要自己去 router目录下找到 index.js 增加改行配置，主要是配置规格维护的路径 在 Spu管理上 点击规格后查询出相关规格信息 Controller 12345@GetMapping(&quot;/base/listforspu/{spuId}&quot;)public R baseAttrListforspu(@PathVariable(&quot;spuId&quot;) Long spuId) { List&lt;ProductAttrValueEntity&gt; productAttrValueEntity = productAttrValueService.baseAttrListforspu(spuId); return R.ok().put(&quot;data&quot;,productAttrValueEntity);} Service 123456@Overridepublic List&lt;ProductAttrValueEntity&gt; baseAttrListforspu(Long spuId) { // 1、根据spuid进行查询 List&lt;ProductAttrValueEntity&gt; attrValueEntities = this.baseMapper.selectList(new QueryWrapper&lt;ProductAttrValueEntity&gt;().eq(&quot;spu_id&quot;, spuId)); return attrValueEntities;} 总结业务流程： 单表操作，根据 spu_id 查询出 pms_product_attr_value 表的信息 14.3.5 Spu更新操作 具体需求： 根据spu_id 查询出规格信息后 ，修改对应信息 提交后会发送一个post请求，并同时带上请求参数 12345678910111213141516[{ &quot;attrId&quot;: 7, &quot;attrName&quot;: &quot;入网型号&quot;, &quot;attrValue&quot;: &quot;LIO-AL00&quot;, &quot;quickShow&quot;: 1}, { &quot;attrId&quot;: 14, &quot;attrName&quot;: &quot;机身材质工艺&quot;, &quot;attrValue&quot;: &quot;玻璃&quot;, &quot;quickShow&quot;: 0}, { &quot;attrId&quot;: 16, &quot;attrName&quot;: &quot;CPU型号&quot;, &quot;attrValue&quot;: &quot;HUAWEI Kirin 980&quot;, &quot;quickShow&quot;: 1}] 刚好这四个参数和实体类的一致，就不需要创建 Vo来接收 Controller 12345678@RequestMapping(&quot;/update/{spuId}&quot;)//@RequiresPermissions(&quot;product:attr:update&quot;)public R updateSpuAttr(@PathVariable(&quot;spuId&quot;) Long spuId, @RequestBody List&lt;ProductAttrValueEntity&gt; productAttrValueEntityList){ productAttrValueService.updateSpuAttr(spuId,productAttrValueEntityList); return R.ok();} Service 123456789101112131415161718/** * 先删除 后更新 * @param spuId * @param productAttrValueEntityList */@Overridepublic void updateSpuAttr(Long spuId, List&lt;ProductAttrValueEntity&gt; productAttrValueEntityList) { // 1、根据spuid删除记录 this.baseMapper.delete(new QueryWrapper&lt;ProductAttrValueEntity&gt;().eq(&quot;spu_id&quot;,spuId)); // 2、遍历传递过来的记录 设置 spuId List&lt;ProductAttrValueEntity&gt; collect = productAttrValueEntityList.stream().map(item -&gt; { item.setSpuId(spuId); return item; }).collect(Collectors.toList()); // 3、批量保存 this.saveBatch(collect);} 总结业务流程： 更新操作，根据前端传递过来的参数来进行更新，前端传递了一个 spu_id 和多个 spu属性值 一个 spu_id对多个 spu 属性值 先根据 spu_id 删除存在 pms_product_attr_value 表的记录 然后对多个 ProductAttrValueEntity 对象设置 sup_id ，最后进行批量保存 分布式基础篇总结 1、分布式基础概念 微服务、注册中心（Nacos）、配置中心（Nacos Cofig）、远程调用、Feign、网关 2、基础开发 SpringBoot2.0、SpringCloud、Mybatis-Plus、Vue组件化、阿里云对象存储 3、环境 Vagrant、Linux、Docker、MySQL、Redis、逆向工程&amp;人人开源 4、开发规范 数据效验JSR303、全局异常处理、全局统一返回、全家跨越处理 枚举状态、业务状态、VO与TO与PO划分、逻辑删除 Lombok:@Data、@Slf4j =分布式高级篇= 1、Elasticsearch - 全文检索 简介 https://www.elastic.co/cn/what-is/elasticsearch/ 全文搜索属于最常见的需求，开源的 Elasticsearch 是目前全文搜索引擎的首选。 他可以快速地存储、搜索和分析海量数据。维基百科、Stack Overflow、Github 都采用他 Elatic 的底层是开源库吧Lucene。但是，你没法直接用，必须自己写代码调用它的接口，Elastic 是 Lunce 的封装，提供了 REST API 的操作接口，开箱即用 REST API：天然的跨平台 官网文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 官网中文：https://www.elastic.co/guide/cn/elasticsearch/guide/current/index.html 社区中文： http://doc.codingdict.com/elasticsearch/ 1.1、基本概念 1.1.1 index(索引) 动词，相当于 MySQL 中的 insert； 名词，相当于MySQL 中的 DataBase 1.1.2 Type(类型) 在 Index（索引）中，可以定义一个或多个类型 类似于 MySQL 中的 Table，每一种类型的数据放在一起 1.1.3 Document(文档) 保存在某个索引（index）下，某种类型（Type）的一个数据（Document）,文档是 JSON 格式的，Document 就像是 MySQL 中某个 Table 里面的内容 1.1.4 倒排索引机制 1.2 Docker 安装 ES 1.2.1 下载镜像 docker pull elasticsearch:7.4.2 存储和检索数据 docker pull kibana:7.4.2 可视化检索数据 1.2.2 创建实例 1、ElasticSearch 配置 1234mkdir -p /mydata/elasticsearch/config # 用来存放配置文件mkdir -p /mydata/elasticsearch/data # 数据echo &quot;http.host: 0.0.0.0&quot; &gt;/mydata/elasticsearch/config/elasticsearch.yml # 允许任何机器访问chmod -R 777 /mydata/elasticsearch/ ## 设置elasticsearch文件可读写权限 启动 1234567docker run --name elasticsearch -p 9200:9200 -p 9300:9300 \\-e &quot;discovery.type=single-node&quot; \\-e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; \\-v /mydata/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml \\-v /mydata/elasticsearch/data:/usr/share/elasticsearch/data \\-v /mydata/elasticsearch/plugins:/usr/share/elasticsearch/plugins \\-d elasticsearch:7.4.2 开机启动 elasticsearch 1docker update elasticsearch --restart=always 以后再外面装好插件重启就可 特别注意： -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx128m&quot; \\ 测试环境下，设置 ES 的初始内存和最大内存，否则导致过大启动不了ES 2、Kibana 123docker run --name kibana -e ELASTICSEARCH_HOSTS=http://192.168.56.10:9200 -p 5601:5601 -d kibana:7.4.2http://192.168.56.10:9200 改成自己Elasticsearch上的地址 3、安装nginx 随便启动一个 nginx 实例，只是为了复制出配置 1docker run -p80:80 --name nginx -d nginx:1.10 将容器内的配置文件拷贝到当前目录 （注意后面有个小点 ） 1docker container cp nginx:/etc/nginx . 创建nginx文件夹 123456mkdir -p /mydata/nginx/htmlmkdir -p /mydata/nginx/logs #由于拷贝完成后会在config中存在一个nginx文件夹，所以需要将它的内容移动到conf中 #conf 文件夹下就是原先nginx的配置mv /mydata/nginx/conf/nginx/* /mydata/nginx/conf/rm -rf /mydata/nginx/conf/nginx 别忘了后面的点 修改文件名称：mv nginx.conf 把这个conf 移动到 /mydata/nginx 下 终止原容器， docker stop nginx 执行命令删除容器：docker rm $Containerid 创建新的 nginx 执行以下命令 12345docker run -p 80:80 --name nginx \\-v /mydata/nginx/html:/usr/share/nginx/html \\-v /mydata/nginx/logs:/var/log/nginx \\-v /mydata/nginx/conf/:/etc/nginx \\-d nginx:1.10 1.3 初步检索 1.3.1、_cat GET /_cat/nodes：查看所有节点 GET /_cat/health：查看 es 健康状况 GET /_cat/master：查看主节点 GET /_cat/incices：查看所有索引 show databases; 1.3.2 索引一个文档（保存） 保存一个数据，保存在哪个索引的哪个类型下，指定用哪个唯一标识 PUT customer/external/1; 在 customer 索引下的 external 类型下保存 1号数据为 1PUT customer/external/1 1.3.3、查询文档 1GET custome/external/1 结果： 123456789101112{ &quot;_index&quot;: &quot;customer&quot;, // 在那个索引 &quot;_type&quot;: &quot;external&quot;, // 在那个类型 &quot;_id&quot;: &quot;1&quot;, // 记录id &quot;_version&quot;: 1, 。// 版本号 &quot;_seq_no&quot;: 0, // 并发控制字段，每次更新就会+1，用来做乐观锁 &quot;_primary_term&quot;: 1, //同上，主分片重新分配，如重启，就会变化 &quot;found&quot;: true, &quot;_source&quot;: { &quot;name&quot;: &quot;John Doe&quot; // 真正的内容 }} 1更新携带：?if_seq_no=4&amp;if_primary_term=1 1.3.4 更新文档 123456789101112131415161718192021222324252627POST customer/external/1/_update{ &quot;doc&quot;:{ &quot;name&quot;:&quot;John Doew&quot; }}或者POST customer/external/1{ &quot;name&quot;:&quot;John Doe2&quot;}或者PUT customer/external/1{ &quot;name&quot;:&quot;jack&quot;}不同：Post操作会对比源文档数据，如果相同不会有什么操作，文档 version 不增加 PUT操作总会将数据重新保存并增加 version 版本带 _update 对比元数据如果一样就不进行任何操作看场景对于大并发更新，不带update对于大并发查询并偶尔更新，带update 对比更新，重新计算分配规则更新同时增加属性POS customer/external/1/_update{ &quot;doc&quot;:{&quot;name&quot;:&quot;Jane Doe&quot;,&quot;age&quot;:20}}PUT 和 POST 不带_update也可以 1.3.5 删除文档&amp;索引 12DELETE customer/external/1DELETE customer 1.3.6 bulk 批量 API 12345POST customer/external/_bulk{&quot;index&quot;:{&quot;_id&quot;:&quot;1&quot;}}{&quot;name&quot;:&quot;John Doe&quot;}{&quot;index&quot;:{&quot;_id&quot;:&quot;2&quot;}}{&quot;name&quot;:&quot;John Doe&quot;} 语法格式 1234{action:{metadata}}\\n{requeestBody}\\n{action:{metadata}}\\n{requesetbod }\\n 复杂实例： 12345678POST /_bulk{&quot;delete&quot;:{&quot;_index&quot;:&quot;website&quot;,&quot;_type&quot;:&quot;blog&quot;,&quot;_id&quot;:&quot;123&quot;}}{&quot;create&quot;:{&quot;_index&quot;:&quot;website&quot;,&quot;_type&quot;:&quot;blog&quot;,&quot;_id&quot;:&quot;123&quot;}}{&quot;title&quot;:&quot;my first blog post&quot;}{&quot;index&quot;:{&quot;_index&quot;:&quot;website&quot;,&quot;_type&quot;:&quot;blog&quot;}}{&quot;title&quot;:&quot;my second blog post&quot;}{&quot;update&quot;:{&quot;_index&quot;:&quot;website&quot;,&quot;_type&quot;:&quot;blog&quot;,&quot;_id&quot;:&quot;123&quot;}}{&quot;doc&quot;:{&quot;title&quot;:&quot;my updated blog post&quot;}} bulk API以此按顺序执行所有的action (动作)。如果一一个单个的动作因任何原因而失败，它将继续处理它后面剩余的动作。当bulkAPI 返回时，它将提供每个动作的状态(与发送的顺序相同)，所以您可以检查是否一个指定的动作是不是失败了。 1.3.7 样本测试数据 我准备了一份顾客银行账户信息虚构的 JSON 文档样本，每个用户都有下列的 schema （模式）： 12345678910111213{ &quot;account_number&quot;: 1, &quot;balance&quot;: 39225, &quot;firstname&quot;: &quot;Amber&quot;, &quot;lastname&quot;: &quot;Duke&quot;, &quot;age&quot;: 32, &quot;gender&quot;: &quot;M&quot;, &quot;address&quot;: &quot;880 Holmes Lane&quot;, &quot;employer&quot;: &quot;Pyrami&quot;, &quot;email&quot;: &quot;amberduke@pyrami.com&quot;, &quot;city&quot;: &quot;Brogan&quot;, &quot;state&quot;: &quot;IL&quot;} https://github.com/elastic/elasticsearch/edit/master/docs/src/test/resources/accounts.json 导入测试数据 POST bank/account/_bank 测试数据 1.4 进阶检索 1.4.1 SearchAPI ES 支持两种基本方式检索: 一个是通过使用 REST request URL,发送搜索参数，(uri + 检索参数) 另一个是通过使用 REST request bod 来发送他们，(uri + 请求体) 1、检索信息 一切检索从_search开始 GET /bank/_search 检索 bank 下的所有信息，包括 type 和 docs GET /bank/_search?q=*&amp;sort=account_number:asc 请求参数方式检索 响应结果解释 took - Elasticearch执行搜索的时间(毫秒) time_ out - 告诉我们搜索是否超时 _shards - 告诉我们多少个分片被搜索了，以及统计了成功/失败的搜索分片 hit - 搜索结果 hits.total - 搜索结果 hits.hits - 实际的搜索结果数组(默认为前10的文档) sort - 结果的排序key (键) (没有则按 score 排序) score 和 max score - 相关性得分和最高得分(全文检索用) uri + 请求体进行检查 1234567GET /bank/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;account_number&quot;: &quot;asc&quot; } ]} HTTP 客户端工具（POSTMAN）,get请求不能携带请求体，我们变为 post也是一样的 我们 POST 一个 JSON风格的查询请求体到 _search API 需要了解，一旦搜索结果被返回，ES 就完成了这次请求的搜索，并且不会维护任何服务端的资源或者结果的 cursor（游标） 1.4.2、QueryDSL 1、基本语法格式 ES 提供了一个可以执行查询的 Json 风格的 DSL （domain-specifig langurage 领域特定语言），这个被成为 Query DSL ，该查询语言非常全面，并且刚开始的时候优点复杂，真正学好对他的方法是从一些基础的示例开始的 一个查询语句 的典型结构 123456{ QUERY_NAME:{ ARGUMENT:VALUE, ARGUMENT:VALUE..... }} 如果针对某个字段，那么它的结构如下 12345678{ QUERY_NAME:{ FIELD_NAME:{ ARGUMENT:VALUE, ARGUMENT:VALUE..... } }} 123456789GET /bank/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;account_number&quot;: &quot;asc&quot; } ], &quot;from&quot;: 10, &quot;size&quot;: 10} query 定义如何查询 match_all 查询类型【代表查询所有的所有】，es中可以在 query中 组合非常多的查询类型完成复杂查询 除了 query 参数之外，我们也可以传递其他的参数改变查询结构，如 sort，size from + size 限定，完成分页功能 sort排序，多字段排序，会在前序字段相等时后续字段内部排序，否则以前序为准 2、返回部分字段 12345678910111213141516GET bank/_search{ &quot;query&quot;:{ &quot;match_all&quot;: {} }, &quot;sort&quot;: [ { &quot;balance&quot;: { &quot;order&quot;: &quot;desc&quot; } } ], &quot;from&quot;: 5, &quot;size&quot;: 5, &quot;_source&quot;: [&quot;firstname&quot;,&quot;lastname&quot;]} 3、match【匹配查询】 基本类型(非字符串)，精准匹配 12345678GET bank/_search{ &quot;query&quot;:{ &quot;match&quot;: { &quot;address&quot;: &quot;mill lane&quot; } }} 全文检索按照评分进行排序，会对检索条件进行分词匹配 4、match_phrase【短语匹配】 将需要匹配的值当成一个整体单词（不分词）进行检索 1234GET /bank/_search{ &quot;query&quot;: { &quot;match_phrase&quot;: { &quot;address&quot;: &quot;mill lane&quot; } }} 5、multi_match【多字段匹配】 123456789GET bank/_search{ &quot;query&quot;:{ &quot;multi_match&quot;: { &quot;query&quot;: &quot;mill&quot;, &quot;fields&quot;: [&quot;address&quot;,&quot;city&quot;] } }} 6、bool 【复合查询】 bool 用来做复合查询 复合语句可以合并 任何 其他嵌套语句，包括复合语句，了解这一点是很重要的，这就意味着，复合语句之间可以互相嵌套，可以表达式非常复杂的逻辑 must：必须达到 must 列举的所有条件 1234567891011121314151617181920212223242526272829GET /bank/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;gender&quot;: &quot;M&quot; } }, { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } } ], &quot;must_not&quot;: [ {&quot;match&quot;:{ &quot;age&quot;:&quot;18&quot; }} ], &quot;should&quot;: [ {&quot;match&quot;: { &quot;lastname&quot;: &quot;Wallace&quot; }} ] } }} should:应该达到 should 列举的条件，如果达到会增加相关文档的评分，并不会改变查询的结果，如果 query 中只有 should 且只有一种匹配规则，那么 should的条件就会被作为默认匹配条件而区改变查询结果 12345&quot;should&quot;: [ {&quot;match&quot;: { &quot;lastname&quot;: &quot;Wallace&quot; }} ] must_not 必须不是指定的情况 12345&quot;must_not&quot;: [ {&quot;match&quot;:{ &quot;age&quot;:&quot;18&quot; }} ], 7、filter【结果过滤】 并不是所有的查询都需要产生分数，特别是那些仅仅用于 filtering（过滤）的文档，为了不计算分数 ES 会自动检查场景并且优化查询的执行 12345678910111213141516GET /bank/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;match_all&quot;: {} }, &quot;filter&quot;: { &quot;range&quot;: { &quot;balance&quot;: { &quot;gte&quot;: 20000, &quot;lte&quot;: 30000 } } } } }} 8、term 和 match 一样，匹配某个属性的值，全文检索字段用 match，其他非text字段匹配用 term 12345678GET bank/_search{ &quot;query&quot;:{ &quot;match_phrase&quot;: { &quot;address&quot;: &quot;789 Madison Street&quot; } }} 9、aggregations（执行聚合） 聚合提供了从数据分组和提取数据的能力，最简单的聚合方法大致等于 SQL GROUP BY 和 SQL 聚合函数，在 ES 中，你有执行搜索返回 hits （命中结果） 并且同时返回聚合结果，把一个响应中的所有 hits（命中结果）分隔开的能力，这是非常强大有效的，你可以执行查询和多个聚合，并且在一个使用中得到各自的（任何一个的）返回结果，使用一次简洁简化的 API 来避免网络往返 搜索address中包含mill的所有人的年龄分布以及平均年龄 12345678910111213141516171819202122232425262728GET bank/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } }, &quot;aggs&quot;: { &quot;ageAgg&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;age&quot;, &quot;size&quot;: 10 } }, &quot;ageAvg&quot;:{ &quot;avg&quot;: { &quot;field&quot;: &quot;age&quot; } }, &quot;balanceAvg&quot;:{ &quot;avg&quot;: { &quot;field&quot;: &quot;balance&quot; } } }, &quot;size&quot;:0} 按照年龄聚合，并且请求这些年龄段的这些人的平均薪资 123456789101112131415161718192021GET bank/_search{ &quot;query&quot;:{ &quot;match_all&quot;: {} }, &quot;aggs&quot;: { &quot;ageAgg&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;age&quot;, &quot;size&quot;: 10 }, &quot;aggs&quot;: { &quot;ageAvg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;balance&quot; } } } } }} 查出所有年龄分布，并且这些年龄段中M的平均薪资和 F 的平均薪资以及这个年龄段的总体平均薪资 1234567891011121314151617181920212223242526272829GET /bank/_search{ &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;aggs&quot;: { &quot;ageAgg&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;age&quot;, &quot;size&quot;: 100 }, &quot;aggs&quot;: { &quot;genderAgg&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;gender.keyword&quot;, &quot;size&quot;: 10 }, &quot;aggs&quot;: { &quot;balanceAvg&quot;: { &quot;avg&quot;: { &quot;field&quot;: &quot;balance&quot; } } } } } } }} 10、nested 嵌套查询 数据类型概览 参考博客：https://elastic.blog.csdn.net/article/details/82950393 1.4.3 Mapping 1、字段类型 2、映射 Mapping（映射） Mapping 是用来定义一个文档（document）,以及他所包含的属性（field）是如何存储索引的，比如使用 mapping来定义的： 哪些字符串属性应该被看做全文本属性（full text fields） 那些属性包含数字，日期或者地理位置 文档中的所有属性是能被索引（_all 配置） 日期的格式 自定义映射规则来执行动态添加属性 查看 mapping 信息 GET bank/_mapping 修改 mapping 信息 https://www.elastic.co/guide/en/elasticsearch/reference/7.10/mapping-types.html 自动猜测的映射类型 3、新版本改变 关系型数据库中两个数据表示是独立的，即使他们里面有相同名称的列也不影响使用，但ES中不是这样的。elasticsearch 是基于Lucene开发的搜索引擎，而ES中不同type下名称相同的filed 最终在Lucene,中的处理方式是一样的。 两个不同 type下的两个user_ name, 在ES同-个索引下其实被认为是同一一个filed,你必须在两个不同的type中定义相同的filed映射。否则，不同typpe中的相同字段称就会在处理中出现神突的情况，导致Lucene处理效率下降。 去掉type就是为了提高ES处理数据的效率。 ES 7.x URL 中的 type 参数 可选，比如索引一个文档不再要求提供文档类型 ES 8.X 不在支持 URL 中的 type 参数 解决： 1、将索引从多类型迁移到单类型，每种类型文档一个独立的索引 2、将已存在的索引下的类型数据，全部迁移到指定位置即可，详见数据迁移 1、创建映射 123456789PUT /my_index{ &quot;mappings&quot;:{ &quot;properties&quot;: { &quot;age&quot;:{&quot;type&quot;:&quot;integer&quot;}, &quot;email&quot;:{&quot;type&quot;:&quot;keyword&quot;} } }} 2、添加新的字段映射 123456789PUT /my_index/_mapping{ &quot;properties&quot;:{ &quot;employeeid&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;:false } }} 3、更新映射 对于已经存在的映射字段，我们不能更新，更新必须创建新的索引进行数据迁移 4、数据迁移 先创 new_twitter 的正确映射，然乎使用如下方式进行数据迁移 1234567891011121314151617181920POST _reindex [固定写法]{ &quot;source&quot;:{ &quot;index&quot;:&quot;twitter&quot; }, &quot;dest&quot;:{ &quot;index&quot;:&quot;new_twitter&quot; }}## 将旧索引的 type 下的数据进行迁移POST _reindex{ &quot;source&quot;: { &quot;index&quot;:&quot;twitter&quot;, &quot;type&quot;:&quot;tweet&quot; }, &quot;dest&quot;:{ &quot;index&quot;:&quot;twweets&quot; }} 参考官网：https://www.elastic.co/guide/en/elasticsearch/reference/7.10/mapping-types.html 参数映射规则：https://www.elastic.co/guide/en/elasticsearch/reference/7.10/mapping-params.html#mapping-params 1.4.4 分词 一个 tokenizer（分词器）接收一个字符流，将之分割为独立的 tokens（词元，通常是独立的单词），然后输出 token 流 列如，witespace tokenizer 遇到的空白字符时分割文本，它会将文本 “Quick brown fox” 分割为 【Quick brown fox】 该 tokenizer (分词器)还负责记录各个term (词条)的顺序或 position 位置(用于phrase短语和word proximity词近邻查询)，以及 term (词条)所代表的原始 word (单词)的start(起始)和end (结束)的 character offsets (字符偏移量) (用于 高亮显示搜索的内容)。 Elasticsearch 提供了很多内置的分词器，可以用来构建custom analyzers(自定义分词器) 1、安装 ik 分词器 注意:不能用默认的 elasticsearch-plugin.install xxx.zip 进行自动安装 https://github.com/medcl/elasticsearch-analysis-ik/releases 下载与 es对应的版本 安装后拷贝到 plugins 目录下 2、测试 分词器 3、自定义词库 修改 /usr/share/elasticsearch/plugins/ik/config/中的 IKAnalyzer.cfg.xml /usr/share/elasticsearch/plugins/ik/config 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;entry key=&quot;remote_ext_dict&quot;&gt;http://192.168.56.10/es/fenci.txt&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展停止词字典--&gt; &lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;&lt;/properties&gt; 1.5 Elasticsearch - Rest - client 1、9300：TCP Spring-data-elasticsearch:transport-api.jar SpringBoot版本不同，transport-api.jar 不同，不能适配 es 版本 7.x 已经不在适合使用，8 以后就要废弃 2、9200：HTTP JestClient 非官方，更新慢 RestTemplate:默认发送 HTTP 请求，ES很多操作都需要自己封装、麻烦 HttpClient：同上 Elasticsearch - Rest - Client：官方RestClient，封装了 ES 操作，API层次分明 最终选择 Elasticsearch - Rest - Client （elasticsearch - rest - high - level - client） 1.5.1 SpringBoot 整合 1、Pom.xml 123456&lt;!-- 导入es的 rest-high-level-client --&gt;&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.4.2&lt;/version&gt;&lt;/dependency&gt; 为什么要导入这个？这个配置那里来的？ 官网：https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-getting-started-maven.html 1.5.2 Config配置 1234567891011121314151617181920212223242526272829303132333435363738/** * @author gcq * @Create 2020-10-26 * * 1、导入配置 * 2、编写配置，给容器注入一个RestHighLevelClient * 3、参照API 官网进行开发 */@Configurationpublic class GulimallElasticsearchConfig { public static final RequestOptions COMMON_OPTIONS; static { RequestOptions.Builder builder = RequestOptions.DEFAULT.toBuilder();// builder.addHeader(&quot;Authorization&quot;, &quot;Bearer &quot; + TOKEN);// builder.setHttpAsyncResponseConsumerFactory(// new HttpAsyncResponseConsumerFactory// .HeapBufferedResponseConsumerFactory(30 * 1024 * 1024 * 1024)); COMMON_OPTIONS = builder.build(); } @Bean public RestHighLevelClient esRestClient() { RestClientBuilder builder = null; builder = RestClient.builder(new HttpHost(&quot;192.168.56.10&quot;, 9200, &quot;http&quot;)); RestHighLevelClient client = new RestHighLevelClient(builder);// RestHighLevelClient client = new RestHighLevelClient(// RestClient.builder(// new HttpHost(&quot;localhost&quot;, 9200, &quot;http&quot;),// new HttpHost(&quot;localhost&quot;, 9201, &quot;http&quot;))); return client; }} 官网：https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-getting-started-initialization.html 1.5.3 使用 测试是否注入成功 1234567@Autowiredprivate RestHighLevelClient client;@Testpublic void contextLoads() { System.out.println(client);} 测试是否能 添加 或更新数据 1234567891011121314151617181920/** * 添加或者更新 * @throws IOException */@Testpublic void indexData() throws IOException { IndexRequest indexRequest = new IndexRequest(&quot;users&quot;); User user = new User(); user.setAge(19); user.setGender(&quot;男&quot;); user.setUserName(&quot;张三&quot;); String jsonString = JSON.toJSONString(user); indexRequest.source(jsonString,XContentType.JSON); // 执行操作 IndexResponse index = client.index(indexRequest, GulimallElasticsearchConfig.COMMON_OPTIONS); // 提取有用的响应数据 System.out.println(index);} 测试复杂检索 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 @Test public void searchTest() throws IOException { // 1、创建检索请求 SearchRequest searchRequest = new SearchRequest(); // 指定索引 searchRequest.indices(&quot;bank&quot;); // 指定 DSL，检索条件 SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.matchQuery(&quot;address&quot;, &quot;mill&quot;)); //1、2 按照年龄值分布进行聚合 TermsAggregationBuilder aggAvg = AggregationBuilders.terms(&quot;ageAgg&quot;).field(&quot;age&quot;).size(10); sourceBuilder.aggregation(aggAvg); //1、3 计算平均薪资 AvgAggregationBuilder balanceAvg = AggregationBuilders.avg(&quot;balanceAvg&quot;).field(&quot;balance&quot;); sourceBuilder.aggregation(balanceAvg); System.out.println(&quot;检索条件&quot; + sourceBuilder.toString()); searchRequest.source(sourceBuilder); // 2、执行检索 SearchResponse searchResponse = client.search(searchRequest, GulimallElasticsearchConfig.COMMON_OPTIONS); // 3、分析结果 System.out.println(searchResponse.toString()); // 4、拿到命中得结果 SearchHits hits = searchResponse.getHits(); // 5、搜索请求的匹配 SearchHit[] searchHits = hits.getHits(); // 6、进行遍历 for (SearchHit hit : searchHits) { // 7、拿到完整结果字符串 String sourceAsString = hit.getSourceAsString(); // 8、转换成实体类 Accout accout = JSON.parseObject(sourceAsString, Accout.class); System.out.println(&quot;account:&quot; + accout ); } // 9、拿到聚合 Aggregations aggregations = searchResponse.getAggregations();// for (Aggregation aggregation : aggregations) {//// } // 10、通过先前名字拿到对应聚合 Terms ageAgg1 = aggregations.get(&quot;ageAgg&quot;); for (Terms.Bucket bucket : ageAgg1.getBuckets()) { // 11、拿到结果 String keyAsString = bucket.getKeyAsString(); System.out.println(&quot;年龄:&quot; + keyAsString); long docCount = bucket.getDocCount(); System.out.println(&quot;个数：&quot; + docCount); } Avg balanceAvg1 = aggregations.get(&quot;balanceAvg&quot;); System.out.println(&quot;平均薪资：&quot; + balanceAvg1.getValue()); System.out.println(searchResponse.toString()); } 结果： 1234567accout:GulimallSearchApplicationTests.Accout(account_number=970, balance=19648, firstname=Forbes, lastname=Wallace, age=28, gender=M, address=990 Mill Road, employer=Pheast, email=forbeswallace@pheast.com, city=Lopezo, state=AK)accout:GulimallSearchApplicationTests.Accout(account_number=136, balance=45801, firstname=Winnie, lastname=Holland, age=38, gender=M, address=198 Mill Lane, employer=Neteria, email=winnieholland@neteria.com, city=Urie, state=IL)accout:GulimallSearchApplicationTests.Accout(account_number=345, balance=9812, firstname=Parker, lastname=Hines, age=38, gender=M, address=715 Mill Avenue, employer=Baluba, email=parkerhines@baluba.com, city=Blackgum, state=KY)accout:GulimallSearchApplicationTests.Accout(account_number=472, balance=25571, firstname=Lee, lastname=Long, age=32, gender=F, address=288 Mill Street, employer=Comverges, email=leelong@comverges.com, city=Movico, state=MT)年龄:38个数:2年龄:28个数:1年龄:32个数:1平均薪水:25208.0 总结：参考官网的API 和对应在 kibana 中发送的请求，在代码中通过调用对应API实现效果 官网：https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/java-rest-high-search.html#java-rest-high-search-request-optional ELK Elasticsearch 用于检索数据 logstach：存储数据 Kiban:视图化查看数据 2、商城业务 &amp; 商品上架 上架的商品才可以在网站展示 上架的商品需要可以检索 2.1 商品Mapping 分析：商品上架在 es 中是存入 sku 还是 spu ？ 1、检索的时候输入名字，是需要按照 sku 的 title进行全文检索的 2、检索使用商品规格，规格是 spu 的公共属性，每个 spu 是一样的 3、按照分类 id 进去的 都是直接列出 spu的，还可以切换 4、我们如果将 sku 的 全量信息 保存在 es 中 （包括 spu 属性），就太多量字段了 5、如果我们将 spu 以及他包含的 sku 信息保存到 es 中，也可以方 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970PUT product{ &quot;mappings&quot;:{ &quot;properties&quot;:{ &quot;skuId&quot;:{ &quot;type&quot;:&quot;long&quot; }, &quot;spuId&quot;:{ &quot;type&quot;:&quot;keyword&quot; }, &quot;skuTitle&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; }, &quot;skuPrice&quot;:{ &quot;type&quot;:&quot;keyword&quot; }, &quot;skuImg&quot;:{ &quot;type&quot;:&quot;text&quot;, &quot;analyzer&quot;: &quot;ik_smart&quot; }, &quot;saleCount&quot;:{ &quot;type&quot;:&quot;long&quot; }, &quot;hasStock&quot;:{ &quot;type&quot;:&quot;boolean&quot; }, &quot;hotScore&quot;:{ &quot;type&quot;:&quot;long&quot; }, &quot;brandId&quot;:{ &quot;type&quot;:&quot;long&quot; }, &quot;catelogId&quot;:{ &quot;type&quot;:&quot;long&quot; }, &quot;brandName&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;: false, &quot;doc_values&quot;: false }, &quot;brandImg&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;: false, &quot;doc_values&quot;: false }, &quot;catalogName&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;: false, &quot;doc_values&quot;: false }, &quot;attrs&quot;:{ &quot;type&quot;:&quot;nested&quot;, &quot;properties&quot;: { &quot;attrId&quot;:{ &quot;type&quot;:&quot;long&quot; }, &quot;attrName&quot;:{ &quot;type&quot;:&quot;keyword&quot;, &quot;index&quot;:false, &quot;doc_values&quot;:false }, &quot;attrValue&quot;: { &quot;type&quot;:&quot;keyword&quot; } } } } }} 2.2 商品上架接口编写 需求分析： 上架商品、将该商品相关属性上传到 Es中 为搜索服务做铺垫 Controller 1234567891011/** * 商品上架 * @param spuId * @return */@RequestMapping(&quot;/{spuId}/up&quot;)public R list(@PathVariable(&quot;spuId&quot;) Long spuId){ spuInfoService.up(spuId); return R.ok();} Service 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108@Overridepublic void up(Long spuId) { // 1、查出当前 spuid 对应的所有skuid信息，品牌的名字 List&lt;SkuInfoEntity&gt; skus = skuInfoService.getSkuBySpuId(spuId); // 取出 Skuid 组成集合 List&lt;Long&gt; skuIds = skus.stream().map(SkuInfoEntity::getSkuId).collect(Collectors.toList()); //TODO 4、查询当前 sku 的所有可以用来被检索的规格属性 List&lt;ProductAttrValueEntity&gt; attrValueEntities = attrValueService.baseAttrListforspu(spuId); // 返回所有 attrId List&lt;Long&gt; attrIds = attrValueEntities.stream().map(attr -&gt; { return attr.getAttrId(); }).collect(Collectors.toList()); // 根据 attrIds 查询出 检索属性，pms_attr表中 search_type为一 则是检索属性 List&lt;Long&gt; searchAttrIds = attrService.selectSearchAttrs(attrIds); // 将查询出来的 attr_id 放到 set 集合中 用来判断 attrValueEntities 是否包含 attrId Set&lt;Long&gt; idSet = new HashSet&lt;&gt;(searchAttrIds); List&lt;SkuEsModel.Attrs&gt; attrList = attrValueEntities.stream().filter(item -&gt; { // 过滤掉不包含在 searchAttrIds集合中的元素 return idSet.contains(item.getAttrId()); }).map(item -&gt; { SkuEsModel.Attrs attrs1 = new SkuEsModel.Attrs(); // 属性对拷 将 ProductAttrValueEntity对象与 SkuEsModel.Attrs相同的属性进行拷贝 BeanUtils.copyProperties(item, attrs1); return attrs1; }).collect(Collectors.toList()); //TODO 1、发送远程调用，库存系统查询是否有库存 Map&lt;Long, Boolean&gt; stockMap = null; try { R r = wareFeignService.getSkuHasStock(skuIds); // key 是 SkuHasStockVo的 skuid value是 item的hasStock 是否拥有库存 TypeReference&lt;List&lt;SkuHasStockVo&gt;&gt; typeReference = new TypeReference&lt;List&lt;SkuHasStockVo&gt;&gt;() { }; stockMap = r.getData(typeReference).stream().collect(Collectors.toMap(SkuHasStockVo::getSkuId, item -&gt; item.getHasStock())); } catch (Exception e) { log.error(&quot;库存服务异常：原因：{}&quot;,e); e.printStackTrace(); } // 2、封装每个 sku 的信息 Map&lt;Long, Boolean&gt; finalStockMap = stockMap; List&lt;SkuEsModel&gt; upProducts = skus.stream().map(sku -&gt; { // 组装需要查询的数据 SkuEsModel skuEsModel = new SkuEsModel(); BeanUtils.copyProperties(sku, skuEsModel); skuEsModel.setSkuPrice(sku.getPrice()); skuEsModel.setSkuImg(sku.getSkuDefaultImg()); // 设置属性 skuEsModel.setAttrs(attrList); // 设置库存信息 if (finalStockMap == null) { // 远程服务出现问题，任然设置为null skuEsModel.setHasStock(true); } else { skuEsModel.setHasStock(finalStockMap.get(sku.getSkuId())); } //TODO 2、热度频分 0 skuEsModel.setHotScore(0L); //TODO 3、查询品牌名字和分类的信息 BrandEntity brandEntity = brandService.getById(skuEsModel.getBrandId()); skuEsModel.setBrandName(brandEntity.getName()); skuEsModel.setBrandImg(brandEntity.getLogo()); CategoryEntity categoryEntity = categoryService.getById(skuEsModel.getCatalogId()); skuEsModel.setCatalogName(categoryEntity.getName()); return skuEsModel; }).collect(Collectors.toList()); //TODO 5、将数据发送给 es保存 ，直接发送给 search服务 R r = esFeignClient.productStatusUp(upProducts); if (r.getCode() == 0) { // 远程调用成功 // TODO 6、修改当前 spu 的状态 baseMapper.updateSpuStatus(spuId, ProductConstant.StatusEnum.SPU_UP.getCode()); } else { // 远程调用失败 //TODO 7、重复调用？ 接口冥等性 重试机制 /** * 1、构造请求数据，将对象转成json * RequestTemplate template = buildTemplateFromArgs.create(argv); * 2、发送请求进行执行(执行成功进行解码) * executeAndDecode(template); * 3、执行请求会有重试机制 * while (true) { * try { * return executeAndDecode(template); * } catch (RetryableException e) { * try { * retryer.continueOrPropagate(e); * } catch (RetryableException th) { * throw cause; * } * continute */ }} 2.2.1、发送远程调用，库存系统查询是否有库存 12345678910111213/** * 根据 skuIds 查询是否有库存 * @param skuIds * @return */@PostMapping(&quot;/hasstock&quot;)public R getSkuHasStock(@RequestBody List&lt;Long&gt; skuIds) { List&lt;SkuHasStockVo&gt; vos = wareSkuService.getSkusStock(skuIds); R ok = R.ok(); ok.setData(vos); return ok;} Service 123456789101112131415@Overridepublic List&lt;SkuHasStockVo&gt; getSkusStock(List&lt;Long&gt; skuIds) { List&lt;SkuHasStockVo&gt; collect = skuIds.stream().map(skuId -&gt; { SkuHasStockVo vo = new SkuHasStockVo(); // 查询当前 sku 的总库存良 // SELECT SUM(stock-stock_locked) FROM `wms_ware_sku` WHERE sku_id = 1 Long count = baseMapper.getSkuStock(skuId); vo.setSkuId(skuId); vo.setHasStock(count == null?false:count&gt;0); return vo; }).collect(Collectors.toList()); return collect;} 2.2.2 将数据发送给 es保存 ，直接发送给 search服务 controller 123456789101112131415161718192021/** * 上架商品 * @param skuEsModelList * @return */ @PostMapping(&quot;/product&quot;) public R productStatusUp(@RequestBody List&lt;SkuEsModel&gt; skuEsModelList){ boolean b = false; try { b = productSaveService.productStatusUp(skuEsModelList); } catch (Exception e) { log.error(&quot;ElasticSaveController商品上架错误：{}&quot;,e); return R.error(BizCodeEnume.PRODUCT_UP_EXCEPTION.getCode(),BizCodeEnume.PRODUCT_UP_EXCEPTION.getMsg()); } if (!b) { return R.ok(); } else { return R.error(BizCodeEnume.PRODUCT_UP_EXCEPTION.getCode(),BizCodeEnume.PRODUCT_UP_EXCEPTION.getMsg()); } } service 123456789101112131415161718192021222324252627282930@Overridepublic boolean productStatusUp(List&lt;SkuEsModel&gt; skuEsModelList) throws IOException { // 保存到es // 批量保存 BulkRequest bulkRequest = new BulkRequest(); for (SkuEsModel model : skuEsModelList) { // 1、构造请求 指定es索引 IndexRequest indexRequest = new IndexRequest(EsConstant.PRODUCT_INDEX); // 1.1 指定id indexRequest.id(model.getSkuId().toString()); // 1.2 将对象esmodel对象转换成 json 进行存储 String s = JSON.toJSONString(model); // 1.3 设置文档源 indexRequest.source(s, XContentType.JSON); bulkRequest.add(indexRequest); } BulkResponse bulk = restHighLevelClient.bulk(bulkRequest, GulimallElasticsearchConfig.COMMON_OPTIONS); // TODO 1、 如果批量错误 boolean b = bulk.hasFailures(); List&lt;String&gt; collect = Arrays.stream(bulk.getItems()).map(item -&gt; { return item.getId(); }).collect(Collectors.toList()); log.error(&quot;商品上架完成：{}&quot;,collect); return b;} 业务流程总结： 前端点击上架后，发送请求并带上参数 spuid 根据 spuid 查询 pms_sku_info 表得到商品相关属性 根据 spuid 查询 pms_product_attr_value 表得到可以用来检索的规格属性 从 ProductAttrValueEntity 中拿到所有的 attrId，根据 attrId 查询 pms_attr 查询检索的属性 x根据 pms_attr 查询到检索属性后，用检索属性和 原先根据 spuid 查询 pms_sku_info 表得到商品相关属性进行比较，pms_sku_info 包含 从 pms_attr 字段attr_id 则数据保存否则过滤 根据 skuIds 去查询远程仓库中是否有库存 SELECT SUM(stock-stock_locked) FROM wms_ware_sku WHERE sku_id = 1 组装数据 设置 SkuEsModel 发送给 es 保存 3、商城业务 &amp; 首页整合 3.1 整合 thymeleaf 渲染首页 需求分析： 开发传统Java WEB工程时，我们可以使用JSP页面模板语言，但是在SpringBoot中已经不推荐使用了。SpringBoot支持如下页面模板语言 Thymeleaf FreeMarker Velocity Groovy JSP thymeleaf 官网：https://www.thymeleaf.org/ 官网文档给出了，语法、相关标签如何使用的步骤，由于官网文档都是英文，英文文档阅读能力好的同学可以选择阅读，英文不好的同学可以选择中文文档进行学习，为此我在网上找到了相关的中文文档：文档：thymeleaf 链接：http://note.youdao.com/noteshare?id=7771a96e9031b30b91ed55c50528e918 3.1.2 SpringBoot 整合 thymeleaf Pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;!-- 版本后由SpringBoot进行管理--&gt;&lt;/dependency&gt; application.yml 123Spring: thymeleaf: cache: false # 开发过程建议关闭缓存 resource 目录介绍： index.html中使用 123&lt;!DOCTYPE html&gt;&lt;!--使用thymeleaf中必须声明加上该行代码--&gt;&lt;html lang=&quot;en&quot; xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt; 相关语法使用 1234567&lt;!--和jsp相关表达式有点相似 具体使用过程参考文档--&gt;&lt;ul&gt; &lt;li th:each=&quot;category : ${categorys}&quot;&gt; &lt;a href=&quot;#&quot; class=&quot;header_main_left_a&quot; th:attr=&quot;ctg-data=${category.catId}&quot;&gt; &lt;b th:text=&quot;${category.name}&quot;&gt;家用电器&lt;/b&gt;&lt;/a&gt; &lt;/li&gt; 默认SpringBoot会直接去找 templates 下的 index.html 3.2 整合dev-tools 渲染分类数据 需求分析： 我们需要在页面的侧边查询出分类的数据，并且选中一级分类数据后显示二级和三级分类数据 先获取一级分类数据 用户选中后在查询二级分类数据 Controller 12345678910111213141516/** * 查询所有一级分类 * @param model * @return */@GetMapping({&quot;/&quot;,&quot;/index.html&quot;})public String indexPage(Model model){ // select * from category where parent_id = 0 //TODO 1、查询所有的一级分类 List&lt;CategoryEntity&gt; categoryEntityList = categoryService.getLevel1Categorys(); model.addAttribute(&quot;categorys&quot;,categoryEntityList); // 查询所有的一级分类 return &quot;index&quot;;} Service 123456@Overridepublic List&lt;CategoryEntity&gt; getLevel1Categorys() { // parent_cid为0则是一级目录 return baseMapper.selectList(new QueryWrapper&lt;CategoryEntity&gt;().eq(&quot;parent_cid&quot;,0));} 一级分类数据能显示后，着手处理二级分类数据获取 Controller 12345678910/** * 查询完整分类数据 * @return */@ResponseBody@RequestMapping(&quot;/index/catalog.json&quot;)public Map&lt;String, List&lt;Catelog2Vo&gt;&gt; getCatelogJson() { Map&lt;String, List&lt;Catelog2Vo&gt;&gt; catelogJson = categoryService.getCatelogJson(); return catelogJson;} Service 123456789101112131415161718192021222324252627282930313233@Overridepublic Map&lt;String, List&lt;Catelog2Vo&gt;&gt; getCatelogJson() { // 1、查询所有1级分类 List&lt;CategoryEntity&gt; level1Categorys = getLevel1Categorys(); // 2、封装数据封装成 map类型 key为 catId,value List&lt;Catelog2Vo&gt; Map&lt;String, List&lt;Catelog2Vo&gt;&gt; categoryList = level1Categorys.stream().collect(Collectors.toMap(k -&gt; k.getCatId().toString(), v -&gt; { // 1、每一个的一级分类，查询到这个一级分类的二级分类 List&lt;CategoryEntity&gt; categoryEntities = baseMapper.selectList(new QueryWrapper&lt;CategoryEntity&gt;().eq(&quot;parent_cid&quot;, v.getCatId())); // 2、封装上面的结果 List&lt;Catelog2Vo&gt; catelog2Vos = null; if (categoryEntities != null) { catelog2Vos = categoryEntities.stream().map(l2 -&gt; { Catelog2Vo catelog2Vo = new Catelog2Vo(v.getCatId().toString(), null, l2.getCatId().toString(), l2.getName()); // 1、查询当前二级分类的三级分类vo List&lt;CategoryEntity&gt; categoryEntities1 = baseMapper.selectList(new QueryWrapper&lt;CategoryEntity&gt;().eq(&quot;parent_cid&quot;, l2.getCatId())); if (categoryEntities1 != null ){ // 2、分装成指定格式 List&lt;Catelog2Vo.catelog3Vo&gt; catelog3VoList = categoryEntities1.stream().map(l3 -&gt; { Catelog2Vo.catelog3Vo catelog3Vo = new Catelog2Vo.catelog3Vo(l2.getCatId().toString(), l3.getCatId().toString(), l3.getName()); return catelog3Vo; }).collect(Collectors.toList()); // 3、设置分类数据 catelog2Vo.setCatalog3List(catelog3VoList); } return catelog2Vo; }).collect(Collectors.toList()); } return catelog2Vos; })); // 2、封装数据 return categoryList;} 上方代码具体业务逻辑： 查询到一级分类，根据一级分类查询出二级分类并设置对应Vo对象，以此类推 4、商城业务 &amp; Nginx 域名访问 4.1 Nginx 搭建域名环境一（反向代理配置） 什么是 反向代理? vi nginx.conf 文件后在底部有该条语句： 引入nginx下的 conf.d 下面的conf文件 那么我们开始在该目录下增加关于 谷粒商城的 nginx 拷贝原先默认的 conf 修改 4.2 Nginx 搭建域名环境二 （负载均衡到网关） 配置 UpStream 使用nginx实现负载平衡的最简单配置如下,官网地址：https://nginx.org/en/docs/http/load_balancing.html 123456789101112131415http { upstream myapp1 { server srv1.example.com; server srv2.example.com; server srv3.example.com; } server { listen 80; location / { proxy_pass http://myapp1; } }} 同时在 本机上 hosts 文件上那个配置 域名映射 将请求转接给网关后，需要在网关配置 1234- id: gulimall_host_route uri: lb://gulimall-product predicates: - Host=**.gulimall.com 最后放几张图方便理解哈 5、性能压测 &amp; 压力测试 简介 压力测试考察当前软硬件环境下系统所能承受住的最大负荷并帮助找出系统的瓶颈所在，压测都是为了系统 在线上的处理能力和稳定性维持在一个标准范围内，做到心中有数 使用压力测试，我们有希望找到很多种用其他测试方法更难发现的错误，有两种错误类型是： 内存泄漏、并发与同步 有效的压力测试系统将应用以下这些关键条件：重复、并发、量级、随机变化 5.1 性能指标 5.1.1 Jvm 内存模型 1、Jvm内存模型 5.1.2 堆 所有的对象实例以及数组都要在堆上分配，堆时垃圾收集器管理的主要区域，也被称为 &quot;GC堆&quot;，也是我们优化最多考虑的地方 堆可以细分为： 新生代 Eden空间 From Survivor 空间 To Survivor 空间 老年代 永久代/原空间 Java8 以前永久代、受 JVM 管理、Java8 以后原空间，直接使用物理内存，因此默认情况下，原空间的大小仅受本地内存限制 垃圾回收 从 Java8 开始,HotSpot 已经完全将永久代（Permanent Generation）移除，取而代之的是一个新的区域 - 元空间（MetaSpac) 5.1.3 jconsole 与 jvisualvm jdk 的两个小工具 jconsole、jvisualvm（升级版本的 jconsole）。通过命令行启动、可监控本地和远程应用、远程应用需要配置 1、jvisualvm 能干什么 监控内存泄漏、跟踪垃圾回收、执行时内存、cpu分析、线程分析..... 运行：正在运行的线程 休眠：sleep 等待：wait 驻留：线程池里面的空闲线程 监视：组赛的线程、正在等待锁 2、安装插件方便查看 gc cmd 启动 jvisualvm 工具-&gt;插件 如果503 错误解决 打开网址： https://visualvm.github.io/pluginscenters.html cmd 查看自己的jdk版本，找到对应的 docker stats 查看相关命令 5.1.4 监控指标 SQL 耗时越小越好、一般情况下微妙级别 命中率越高越好、一般情况下不能低于95% 锁等待次数越低越好、等待时间越短越好 1、中间件指标 毫秒 压测内容 压测线程数 吞吐量/ms 90%响应时间/ms 99%响应时间/ms Nginx 50 1834 11 40 GateWay 50b 16577 5 19 简单服务 50 17965 5 10 首页一级菜单渲染 50 400（db,thymeleaf） 149 230 首页渲染(开缓存) 50 467 128 209 首页渲染（开缓存、优化数据库，关日志） 50 1300 60 107 三级分类数据获取 50 4.4（db）/18（优化后） 16622 16832 三级分类数据获取（优化业务） 50 163 410 585 三级分类数据获取（redis缓存） 50 553 113 182 首页全量数据获取 50 10（静态资源）/12 6183 12358 Nginx+Gateway 50 Gateway+简单服务 50 2685 7 10 全链路 50 900 73 971 中间件越多，性能损失越大，大多都损失在了网络交互 2、数据库指标 响应时间（Response Time:RT） 响应时间指用户从客户端发起一个请求开始，到客户端接收到服务器端返回的响应结束，整个过程所耗费的时间 HPS（Hits Per Second） ：每秒点击次数，单位是次/秒 TPS（Transaction per Second）：系统每秒处理交易数，单位是笔/秒 QPS (Query perSecond) :系统每秒处理查询次数，单位是次/秒。对于互联网业务中，如果某些业务有且仅有一个请求连接，那么TPS=QPS=HPS，一般情况下用TPS来衡量整个业务流程，用QPS来衡量接口查询次数，用HPS来表示对服务器单击请求。 无论TPS、QPS、HPS,此指标是衡量系统处理能力非常重要的指标，越大越好，根据经验，一般情况下: 金融行业: 1000TPS~50000TPS, 不包括互联网化的活动 保险行业: 1007P-00000PS， 不包括互联网化的活动 制造行业: 10TPS~5000TPS 互联网电子商务: 10000TPS~-100000TPS 互联网中型网站: 1000TPS~50000TPS 互联网小型网站: 5007PS~10000TPS 最大响应时间(Max Response Time) 指用户发出请求或者指令到系统做出反应(响应)的最大时间。 最少响应时间 （Mininum ResponseTime）指用户发出请求或者指令到系统做出反应（响应）的最少时间 90%响应时间（90% Response Time） 是指所有用户的响应时间进行排序、第90%的响应时间 从外部看、性能测试主要关注如下三个指 吞吐量：每秒钟系统能够处理的请求数、任务数 响应时间：服务处理一个请求或一个任务的耗时 错误率：一批请求中结果出错的请求所占比例 吞吐量大:系统支持高并发， 响应时间：越短说明接口性能越好 5.2 JMeter 5.2.1 JMeter 安装 jmeter官网：https://jmeter.apache.org/ 5.2.2 JMeter 压测示例 1、添加线程组 2、添加 HTTP 请求 3、添加监听器 4、启动压测&amp;查看 汇总图 察看结果树 汇总报告 聚合报告 5.2.3 JMeter Address Already in use 错误解决 windows本身提供的端口访问机制的问题。 Windows提供给TCP/IP 链接的端口为1024-5000，并且要四分钟来循环回收他们。就导致 我们在短时间内跑大量的请求时将端口占满了。 1.cmd中，用regedit命令打开注册表 2.在HKEY_ LOCAL MACHINE\\SYSTEMCurrentControlSet\\Services Tcpip\\Parameters下， ​ 1.右击parameters,添加一个新的DWORD,名字为MaxUserPort ​ 2.然后双击 MaxUserPort,输入数值数据为65534,基数选择十进制(如果是分布式运行的话，控制机器和负载机器都需要这样操作哦) 3.修改配置完毕之后记得重启机器才会生效 TCPTimedWaitDelay:30 6、缓存与分布式锁 6.1 缓存 6.1.2 缓存使用 为了系统性能的提升，我们一般都会将部分数据放入缓存中，加速访问，而 db 承担数据落盘工作 哪些数据适合放入缓存？ 即时性、数据一致性要求不高的 访问量大且更新频率不高的数据（读多、写少） 举例：电商类应用、商品分类，商品列表等适合缓存并加一个失效时间（根据数据更新频率来定）后台如果发布一个商品、买家需要 5 分钟才能看到新商品一般还是可以接受的 伪代码 123456data = cche.load(b); //从缓存中加载数据if(data == null) { data = db.load(id); // 从数据库加载数据 cache.put(id,data); // 保存到 cache中}return data; 注意：在开发中，凡是放到缓存中的数据我们都应该制定过期时间，使其可以在系统即使没有主动更新数据也能自动触发数据加载的流程，避免业务奔溃导致的数据永久不一致的问题 6.1.3 整合 redis 作为缓存 1、引入依赖 SpringBoot 整合 redis，查看SpringBoot提供的 starts 官网：https://docs.spring.io/spring-boot/docs/2.1.18.RELEASE/reference/html/using-boot-build-systems.html#using-boot-starter pom.xml 123456789101112131415161718&lt;!--引入redis--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;!--不加载自身的 lettuce--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;io.lettuce&lt;/groupId&gt; &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!--jedis--&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;/dependency&gt; 2、配置 application.yaml 1234Spring: redis: host: 192.168.56.10 port: 6379 RedisAutoConfig.java 3、测试 12345678910@AutowiredStringRedisTemplate stringRedisTemplate;@Testpublic void testStringRedisTemplate() { stringRedisTemplate.opsForValue().set(&quot;hello&quot;,&quot;world_&quot; + UUID.randomUUID().toString()); String hello = stringRedisTemplate.opsForValue().get(&quot;hello&quot;); System.out.println(&quot;之前保存的数据是：&quot; + hello);} 4、优化三级分类数据获取 1234567891011121314151617181920212223242526272829303132/** * TODO 产生堆外内存溢出 OutOfDirectMemoryError * 1、SpringBoot2.0以后默认使用 Lettuce作为操作redis的客户端，它使用 netty进行网络通信 * 2、lettuce 的bug导致netty堆外内存溢出，-Xmx300m netty 如果没有指定堆内存移除，默认使用 -Xmx300m * 可以通过-Dio.netty.maxDirectMemory 进行设置 * 解决方案 不能使用 -Dio.netty.maxDirectMemory调大内存 * 1、升级 lettuce客户端，2、 切换使用jedis * redisTemplate: * lettuce、jedis 操作redis的底层客户端，Spring再次封装 * @return */@Overridepublic Map&lt;String, List&lt;Catelog2Vo&gt;&gt; getCatelogJson() { // 给缓存中放 json 字符串、拿出的是 json 字符串，还要逆转为能用的对象类型【序列化和反序列化】 // 1、加入缓存逻辑，缓存中放的数据是 json 字符串 // JSON 跨语言，跨平台兼容 String catelogJSON = redisTemplate.opsForValue().get(&quot;catelogJSON&quot;); if (StringUtils.isEmpty(catelogJSON)) { // 2、缓存没有，从数据库中查询 Map&lt;String, List&lt;Catelog2Vo&gt;&gt; catelogJsonFromDb = getCatelogJsonFromDb(); // 3、查询到数据，将数据转成 JSON 后放入缓存中 String s = JSON.toJSONString(catelogJsonFromDb); redisTemplate.opsForValue().set(&quot;catelogJSON&quot;,s); return catelogJsonFromDb; } // 转换为我们指定的对象 Map&lt;String, List&lt;Catelog2Vo&gt;&gt; result = JSON.parseObject(catelogJSON, new TypeReference&lt;Map&lt;String, List&lt;Catelog2Vo&gt;&gt;&gt;() {}); return result;} 6.2 缓存失效 高并发下缓存失效问题 缓存失效 缓存雪崩 缓存击穿 分布式下如何加锁 6.4 分布式锁 分布式锁原理与应用 分布式锁基本原理 **理解：**就先当1000个人去占一个厕所，厕所只能有一个人占到这个坑，占到这个坑其他人就只能在外面等待，等待一段时间后可以再次来占坑，业务执行后，释放锁，那么其他人就可以来占这个坑 分布式锁演进 - 阶段一 代码： 1234567891011Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;, &quot;0&quot;); if (lock) { // 加锁成功..执行业务 Map&lt;String,List&lt;Catelog2Vo&gt;&gt; dataFromDb = getDataFromDB(); redisTemplate.delete(&quot;lock&quot;); // 删除锁 return dataFromDb; } else { // 加锁失败，重试 synchronized() // 休眠100ms重试 return getCatelogJsonFromDbWithRedisLock(); } 分布式锁演进 - 阶段二 代码： 1234567891011121314Boolean lock = redisTemplate.opsForValue().setIfAbsent() if (lock) { // 加锁成功..执行业务 // 设置过期时间 redisTemplate.expire(&quot;lock&quot;,30,TimeUnit.SECONDS); Map&lt;String,List&lt;Catelog2Vo&gt;&gt; dataFromDb = getDataFromDB(); redisTemplate.delete(&quot;lock&quot;); // 删除锁 return dataFromDb; } else { // 加锁失败，重试 synchronized() // 休眠100ms重试 return getCatelogJsonFromDbWithRedisLock(); } 分布式锁演进 - 阶段三 代码： 1234567891011121314// 设置值同时设置过期时间Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;,&quot;111&quot;,300,TimeUnit.SECONDS);if (lock) { // 加锁成功..执行业务 // 设置过期时间,必须和加锁是同步的，原子的 redisTemplate.expire(&quot;lock&quot;,30,TimeUnit.SECONDS); Map&lt;String,List&lt;Catelog2Vo&gt;&gt; dataFromDb = getDataFromDB(); redisTemplate.delete(&quot;lock&quot;); // 删除锁 return dataFromDb;} else { // 加锁失败，重试 synchronized() // 休眠100ms重试 return getCatelogJsonFromDbWithRedisLock();} 分布式锁演进 - 阶段四 图解： 代码： 12345678910111213141516171819202122232425 String uuid = UUID.randomUUID().toString(); // 设置值同时设置过期时间 Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;,uuid,300,TimeUnit.SECONDS); if (lock) { // 加锁成功..执行业务 // 设置过期时间,必须和加锁是同步的，原子的// redisTemplate.expire(&quot;lock&quot;,30,TimeUnit.SECONDS); Map&lt;String,List&lt;Catelog2Vo&gt;&gt; dataFromDb = getDataFromDB();// String lockValue = redisTemplate.opsForValue().get(&quot;lock&quot;);// if (lockValue.equals(uuid)) {// // 删除我自己的锁// redisTemplate.delete(&quot;lock&quot;); // 删除锁// }// 通过使用lua脚本进行原子性删除 String script = &quot;if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end&quot;; //删除锁 Long lock1 = redisTemplate.execute(new DefaultRedisScript&lt;Long&gt;(script, Long.class), Arrays.asList(&quot;lock&quot;), uuid); return dataFromDb; } else { // 加锁失败，重试 synchronized() // 休眠100ms重试 return getCatelogJsonFromDbWithRedisLock(); } 分布式锁演进 - 阶段五 最终模式 代码： 1234567891011121314151617181920212223242526272829 String uuid = UUID.randomUUID().toString(); // 设置值同时设置过期时间 Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;,uuid,300,TimeUnit.SECONDS); if (lock) { System.out.println(&quot;获取分布式锁成功&quot;); // 加锁成功..执行业务 // 设置过期时间,必须和加锁是同步的，原子的// redisTemplate.expire(&quot;lock&quot;,30,TimeUnit.SECONDS); Map&lt;String,List&lt;Catelog2Vo&gt;&gt; dataFromDb;// String lockValue = redisTemplate.opsForValue().get(&quot;lock&quot;);// if (lockValue.equals(uuid)) {// // 删除我自己的锁// redisTemplate.delete(&quot;lock&quot;); // 删除锁// } try { dataFromDb = getDataFromDB(); } finally { String script = &quot;if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end&quot;; //删除锁 Long lock1 = redisTemplate.execute(new DefaultRedisScript&lt;Long&gt;(script, Long.class), Arrays.asList(&quot;lock&quot;), uuid); } return dataFromDb; } else { // 加锁失败，重试 synchronized() // 休眠200ms重试 System.out.println(&quot;获取分布式锁失败，等待重试&quot;); try { TimeUnit.MILLISECONDS.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } return getCatelogJsonFromDbWithRedisLock(); } 问题： 分布式加锁解锁都是这两套代码，可以封装成工具类 分布式锁有更专业的框架 分布式锁 - Redisson 1、简介&amp;整合 官网文档上详细说明了 不推荐使用 setnx来实现分布式锁，应该参考 the Redlock algorithm 的实现 the Redlock algorithm：https://redis.io/topics/distlock 在Java 语言环境下使用 Redisson github：https://github.com/redisson/redisson 有对应的 中文文档 在 Maven 仓库中搜索也能搜索出 Redisson Pom 123456&lt;!--以后使用 redisson 作为分布锁，分布式对象等功能--&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.12.0&lt;/version&gt;&lt;/dependency&gt; 2、Redisson - Lock 锁测试 &amp; Redisson - Lock 看门狗原理 - Redisson 如何解决死锁 12345678910111213141516171819202122232425262728293031323334@RequestMapping(&quot;/hello&quot;)@ResponseBodypublic String hello(){ // 1、获取一把锁，只要锁得名字一样，就是同一把锁 RLock lock = redission.getLock(&quot;my-lock&quot;); // 2、加锁 lock.lock(); // 阻塞式等待，默认加的锁都是30s时间 // 1、锁的自动续期，如果业务超长，运行期间自动给锁续上新的30s，不用担心业务时间长，锁自动过期后被删掉 // 2、加锁的业务只要运行完成，就不会给当前锁续期，即使不手动解锁，锁默认会在30s以后自动删除 lock.lock(10, TimeUnit.SECONDS); //10s 后自动删除 //问题 lock.lock(10, TimeUnit.SECONDS) 在锁时间到了后，不会自动续期 // 1、如果我们传递了锁的超时时间，就发送给 redis 执行脚本，进行占锁，默认超时就是我们指定的时间 // 2、如果我们为指定锁的超时时间，就是用 30 * 1000 LockWatchchdogTimeout看门狗的默认时间、 // 只要占锁成功，就会启动一个定时任务，【重新给锁设置过期时间，新的过期时间就是看门狗的默认时间】,每隔10s就自动续期 // internalLockLeaseTime【看门狗时间】 /3,10s //最佳实践 // 1、lock.lock(10, TimeUnit.SECONDS);省掉了整个续期操作，手动解锁 try { System.out.println(&quot;加锁成功，执行业务...&quot; + Thread.currentThread().getId()); Thread.sleep(3000); } catch (Exception e) { } finally { // 解锁 将设解锁代码没有运行，reidsson会不会出现死锁 System.out.println(&quot;释放锁....&quot; + Thread.currentThread().getId()); lock.unlock(); } return &quot;hello&quot;;} 进入到 Redisson Lock 源码 1、进入 Lock 的实现 发现 他调用的也是 lock 方法参数 时间为 -1 2、再次进入 lock 方法 发现他调用了 tryAcquire 3、进入 tryAcquire 4、里头调用了 tryAcquireAsync 这里判断 laseTime != -1 就与刚刚的第一步传入的值有关系 5、进入到 tryLockInnerAsync 方法 6、internalLockLeaseTime 这个变量是锁的默认时间 这个变量在构造的时候就赋初始值 7、最后查看 lockWatchdogTimeout 变量 也就是30秒的时间 3、Reidsson - 读写锁 二话不说，上代码！！！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 保证一定能读取到最新数据，修改期间，写锁是一个排他锁（互斥锁，独享锁）读锁是一个共享锁 * 写锁没释放读锁就必须等待 * 读 + 读 相当于无锁，并发读，只会在 reids中记录好，所有当前的读锁，他们都会同时加锁成功 * 写 + 读 等待写锁释放 * 写 + 写 阻塞方式 * 读 + 写 有读锁，写也需要等待 * 只要有写的存在，都必须等待 * @return String */ @RequestMapping(&quot;/write&quot;) @ResponseBody public String writeValue() { RReadWriteLock lock = redission.getReadWriteLock(&quot;rw_lock&quot;); String s = &quot;&quot;; RLock rLock = lock.writeLock(); try { // 1、改数据加写锁，读数据加读锁 rLock.lock(); System.out.println(&quot;写锁加锁成功...&quot; + Thread.currentThread().getId()); s = UUID.randomUUID().toString(); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } redisTemplate.opsForValue().set(&quot;writeValue&quot;,s); } catch (Exception e) { e.printStackTrace(); } finally { rLock.unlock(); System.out.println(&quot;写锁释放...&quot; + Thread.currentThread().getId()); } return s; } @RequestMapping(&quot;/read&quot;) @ResponseBody public String readValue() { RReadWriteLock lock = redission.getReadWriteLock(&quot;rw_lock&quot;); RLock rLock = lock.readLock(); String s = &quot;&quot;; rLock.lock(); try { System.out.println(&quot;读锁加锁成功...&quot; + Thread.currentThread().getId()); s = (String) redisTemplate.opsForValue().get(&quot;writeValue&quot;); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } } catch (Exception e) { e.printStackTrace(); } finally { rLock.unlock(); System.out.println(&quot;读锁释放...&quot; + Thread.currentThread().getId()); } return s; } 来看下官网的解释 4、Redisson - 闭锁测试 官网！！！ 上代码 1234567891011121314151617181920212223/** * 放假锁门 * 1班没人了 * 5个班级走完，我们可以锁们了 * @return */@GetMapping(&quot;/lockDoor&quot;)@ResponseBodypublic String lockDoor() throws InterruptedException { RCountDownLatch door = redission.getCountDownLatch(&quot;door&quot;); door.trySetCount(5); door.await();//等待闭锁都完成 return &quot;放假了....&quot;;}@GetMapping(&quot;/gogogo/{id}&quot;)@ResponseBodypublic String gogogo(@PathVariable(&quot;id&quot;) Long id) { RCountDownLatch door = redission.getCountDownLatch(&quot;door&quot;); door.countDown();// 计数器减一 return id + &quot;班的人走完了.....&quot;;} 和 JUC 的 CountDownLatch 一致 await()等待闭锁完成 countDown() 把计数器减掉后 await就会放行 5、Redisson - 信号量测试 官网！！！ 1234567891011121314151617181920212223/** * 车库停车 * 3车位 * @return */@GetMapping(&quot;/park&quot;)@ResponseBodypublic String park() throws InterruptedException { RSemaphore park = redission.getSemaphore(&quot;park&quot;); boolean b = park.tryAcquire();//获取一个信号，获取一个值，占用一个车位 return &quot;ok=&quot; + b;}@GetMapping(&quot;/go&quot;)@ResponseBodypublic String go() { RSemaphore park = redission.getSemaphore(&quot;park&quot;); park.release(); //释放一个车位 return &quot;ok&quot;;} 类似 JUC 中的 Semaphore 6、Redission - 缓存一致性解决 6.3 缓存数据一致性 缓存数据一致性 - 双写模式 两个线程写 最终只有一个线程写成功，后写成功的会把之前写的数据给覆盖，这就会造成脏数据 缓存数据一致性 - 失效模式 三个连接 一号连接 写数据库 然后删缓存 二号连接 写数据库时网络连接慢，还没有写入成功 三号链接 直接读取数据，读到的是一号连接写入的数据，此时 二号链接写入数据成功并删除了缓存，三号开始更新缓存发现更新的是二号的缓存 缓存数据一致性解决方案 无论是双写模式还是失效模式，都会到这缓存不一致的问题，即多个实力同时更新会出事，怎么办？ 1、如果是用户纯度数据（订单数据、用户数据），这并发几率很小，几乎不用考虑这个问题，缓存数据加上过期时间，每隔一段时间触发读的主动更新即可 2、如果是菜单，商品介绍等基础数据，也可以去使用 canal 订阅，binlog 的方式 3、缓存数据 + 过期时间也足够解决大部分业务对缓存的要求 4、通过加锁保证并发读写，写写的时候按照顺序排好队，读读无所谓，所以适合读写锁，（业务不关心脏数据，允许临时脏数据可忽略） 总结: 我们能放入缓存的数据本来就不应该是实时性、一致性要求超高的。所以缓存数据的时候加上过期时间，保证每天拿到当前的最新值即可 我们不应该过度设计，增加系统的复杂性 遇到实时性、一致性要求高的数据，就应该查数据库，即使慢点 ![](D:\\java\\gitee\\java-learning-note\\Evan Guo\\项目笔记\\谷粒商城项目开发文档\\分布式高级篇\\image\\image-20201101054937769.png) 最后符上 三级分类数据 加上分布式锁 6.5 Spring Cache 1、简介 Spring 从3.1开始定义了 org.springframework.cache.Cache 和 org.sprngframework.cache.CacheManager 接口睐统一不同的缓存技术 并支持使用 JCache（JSR-107）注解简化我们的开发 Cache 接口为缓存的组件规范定义，包含缓存的各种操作集合 Cache 接口下 Spring 提供了各种 XXXCache的实现，如 RedisCache、EhCache,ConcrrentMapCache等等， 每次调用需要缓存功能实现方法的时候，Spring 会检查检查指定参数的马努表犯法是否已经被嗲用过，如果有就直接从缓存中获取方法调用后的结果，如果没有就调用方法并缓存结果后返回给用户，下次直接调用从缓存中获取 使用 Sprng 缓存抽象时我们需要关注的点有以下两点 1、确定方法需要被缓存以及他们的的缓存策略 2、从缓存中读取之前缓存存储的数据 官网地址：https://docs.spring.io/spring-framework/docs/5.2.10.RELEASE/spring-framework-reference/integration.html#cache-strategie 缓存注解配置 2、基础概念 从3.1版本开始，Spring 框架就支持透明地向现有 Spring 应用程序添加缓存。与事务支持类似，缓存抽象允许在对代码影响最小的情况下一致地使用各种缓存解决方案。从 Spring 4.1 开始，缓存抽象在JSR-107注释和更多定制选项的支持下得到了显著扩展。 123456789101112131415161718/*** 8、整合SpringCache简化缓存开发* 1、引入依赖* spring-boot-starter-cache* 2、写配置* 1、自动配置了那些* CacheAutoConfiguration会导入 RedisCacheConfiguration* 自动配置好了缓存管理器，RedisCacheManager* 2、配置使用redis作为缓存* Spring.cache.type=redis** 4、原理* CacheAutoConfiguration -&gt;RedisCacheConfiguration -&gt;* 自动配置了 RedisCacheManager -&gt;初始化所有的缓存 -&gt; 每个缓存决定使用什么配置* -&gt;如果redisCacheConfiguration有就用已有的，没有就用默认的* -&gt;想改缓存的配置，只需要把容器中放一个 RedisCacheConfiguration 即可* -&gt;就会应用到当前 RedisCacheManager管理所有缓存分区中*/ 3、注解 对于缓存声明，Spring的缓存抽象提供了一组Java注解 1234567/**@Cacheable: Triggers cache population:触发将数据保存到缓存的操作@CacheEvict: Triggers cache eviction: 触发将数据从缓存删除的操作@CachePut: Updates the cache without interfering with the method execution:不影响方法执行更新缓存@Caching: Regroups multiple cache operations to be applied on a method:组合以上多个操作@CacheConfig: Shares some common cache-related settings at class-level:在类级别共享缓存的相同配置**/ 注解使用 12345678910111213141516171819202122232425262728/** * 1、每一个需要缓存的数据我们都需要指定放到那个名字的缓存【缓存分区的划分【按照业务类型划分】】 * 2、@Cacheable({&quot;category&quot;}) * 代表当前方法的结果需要缓存，如果缓存中有，方法不调用 * 如果缓存中没有，调用方法，最后将方法的结果放入缓存 * 3、默认行为: * 1、如果缓存中有，方法不用调用 * 2、key默自动生成，缓存的名字:SimpleKey[](自动生成的key值) * 3、缓存中value的值，默认使用jdk序列化，将序列化后的数据存到redis * 3、默认的过期时间，-1 * * 自定义操作 * 1、指定缓存使用的key key属性指定，接收一个SpEl * 2、指定缓存数据的存活时间 配置文件中修改ttl * 3、将数据保存为json格式 * @return */ //value 缓存的别名 // key redis中key的名称，默认是方法名称 @Cacheable(value = {&quot;category&quot;},key = &quot;#root.method.name&quot;) @Override public List&lt;CategoryEntity&gt; getLevel1Categorys() { long l = System.currentTimeMillis(); // parent_cid为0则是一级目录 List&lt;CategoryEntity&gt; categoryEntities = baseMapper.selectList(new QueryWrapper&lt;CategoryEntity&gt;().eq(&quot;parent_cid&quot;, 0)); System.out.println(&quot;耗费时间：&quot; + (System.currentTimeMillis() - l)); return categoryEntities; } 4、表达式语法 配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.atguigu.gulimall.product.config;import org.springframework.boot.autoconfigure.cache.CacheProperties;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.cache.annotation.EnableCaching;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.cache.RedisCacheConfiguration;import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.RedisSerializationContext;import org.springframework.data.redis.serializer.StringRedisSerializer;/** * @author gcq * @Create 2020-11-01 */@EnableConfigurationProperties(CacheProperties.class)@EnableCaching@Configurationpublic class MyCacheConfig { /** * 配置文件中的东西没有用上 * 1、原来的配置吻技安绑定的配置类是这样子的 * @ConfigurationProperties(prefix = &quot;Spring.cache&quot;) * 2、要让他生效 * @EnableConfigurationProperties(CacheProperties.class) * @param cacheProperties * @return */ @Bean RedisCacheConfiguration redisCacheConfiguration(CacheProperties cacheProperties) { RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig(); // 设置key的序列化 config = config.serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(new StringRedisSerializer())); // 设置value序列化 -&gt;JackSon config = config.serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(new GenericJackson2JsonRedisSerializer())); CacheProperties.Redis redisProperties = cacheProperties.getRedis(); if (redisProperties.getTimeToLive() != null) { config = config.entryTtl(redisProperties.getTimeToLive()); } if (redisProperties.getKeyPrefix() != null) { config = config.prefixKeysWith(redisProperties.getKeyPrefix()); } if (!redisProperties.isCacheNullValues()) { config = config.disableCachingNullValues(); } if (!redisProperties.isUseKeyPrefix()) { config = config.disableKeyPrefix(); } return config; }} yaml 12345678Spring: cache: type: redis redis: time-to-live: 3600000 # 过期时间 key-prefix: CACHE_ # key前缀 use-key-prefix: true # 是否使用写入redis前缀 cache-null-values: true # 是否允许缓存空值 5、缓存穿透问题解决 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 1、每一个需要缓存的数据我们都需要指定放到那个名字的缓存【缓存分区的划分【按照业务类型划分】】 * 2、@Cacheable({&quot;category&quot;}) * 代表当前方法的结果需要缓存，如果缓存中有，方法不调用 * 如果缓存中没有，调用方法，最后将方法的结果放入缓存 * 3、默认行为: * 1、如果缓存中有，方法不用调用 * 2、key默自动生成，缓存的名字:SimpleKey[](自动生成的key值) * 3、缓存中value的值，默认使用jdk序列化，将序列化后的数据存到redis * 3、默认的过期时间，-1 * * 自定义操作 * 1、指定缓存使用的key key属性指定，接收一个SpEl * 2、指定缓存数据的存活时间 配置文件中修改ttl * 3、将数据保存为json格式 * 4、Spring-Cache的不足： * 1、读模式： * 缓存穿透:查询一个null数据，解决 缓存空数据：ache-null-values=true * 缓存击穿:大量并发进来同时查询一个正好过期的数据，解决:加锁 ？ 默认是无加锁 * 缓存雪崩:大量的key同时过期，解决：加上随机时间，Spring-cache-redis-time-to-live * 2、写模式：（缓存与数据库库不一致） * 1、读写加锁 * 2、引入canal，感知到MySQL的更新去更新数据库 * 3、读多写多，直接去数据库查询就行 * * 总结： * 常规数据（读多写少，即时性，一致性要求不高的数据）完全可以使用SpringCache 写模式（ 只要缓存数据有过期时间就足够了） * * 特殊数据：特殊设计 * 原理： * CacheManager(RedisManager) -&gt; Cache(RedisCache) -&gt;Cache负责缓存的读写 * @return */@Cacheable(value = {&quot;category&quot;},key = &quot;#root.method.name&quot;,sync = true)@Overridepublic List&lt;CategoryEntity&gt; getLevel1Categorys() { long l = System.currentTimeMillis(); // parent_cid为0则是一级目录 List&lt;CategoryEntity&gt; categoryEntities = baseMapper.selectList(new QueryWrapper&lt;CategoryEntity&gt;().eq(&quot;parent_cid&quot;, 0)); System.out.println(&quot;耗费时间：&quot; + (System.currentTimeMillis() - l)); return categoryEntities;} 6、缓存更新 12345678910111213141516171819202122/** * 级联更新所有的关联数据 * @CacheEvict 失效模式 * 1、同时进行多种缓存操作 @Caching * 2、指定删除某个分区下的所有数据 @CacheEvict(value = {&quot;category&quot;},allEntries = true) * 3、存储同一类型的数据，都可以指定成同一分区，分区名默认就是缓存的前缀 * * @param category */ @Caching(evict = { @CacheEvict(value = {&quot;category&quot;},key = &quot;'getLevel1Categorys'&quot;), @CacheEvict(value = {&quot;category&quot;},key = &quot;'getCatelogJson'&quot;) })// @CacheEvict(value = {&quot;category&quot;},allEntries = true) @Transactional @Override public void updateCascate(CategoryEntity category) { // 更新自己表对象 this.updateById(category); // 更新关联表对象 categoryBrandRelationService.updateCategory(category.getCatId(), category.getName()); } 总结业务流程： 如果忘了这个技术点看下做的笔记的例子，然后去官网看下文档，温故而知新 流程图 7、商城业务 &amp; 商品检索 7.1 检索业务分析 7.1.1 检索查询参数模型分析抽取 打个比例吧 你肯定上过京东、淘宝买过东西吧？ 那么你想要购买什么东西，你需要在搜索框中搜索你想要购买的物品，那么系统就会给你响应 我在京东搜索 Iphone 他会显示出相对应的产品 那么我们开始对业务条件进行分析，并创建对应的VO类 好的创建出来了........ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 封装页面所有可能传递过来的查询条件 * @author gcq * @Create 2020-11-02 */@Datapublic class SearchParam { /** * 页面传递过来的全文匹配关键字 */ private String keyword; /** * 三级分类id */ private Long catalog3Id; /** * sort=saleCout_asc/desc * sort=skuPrice_asc/desc * sort=hotScore_asc/desc * 排序条件 */ private String sort; /** * hasStock(是否有货) skuPrice区间，brandId、catalog3Id、attrs */ /** * 是否显示有货 */ private Integer hasStock = 0; /** * 价格区间查询 */ private String skuPrice; /** * 按照品牌进行查询，可以多选 */ private List&lt;Long&gt; brandId; /** * 按照属性进行筛选 */ private List&lt;String&gt; attrs; /** * 页码 */ private Integer pageNum = 1;} 7.1.2 检索返回结果模型分析抽取 那么返回的数据我们是不是也要创建一个 VO 用来返回页面的数据？ 借鉴京东的实例来做参考 那么抽取实体类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * 查询结果返回 * @author gcq * @Create 2020-11-02 */@Datapublic class SearchResult { /** * 查询到所有商品的商品信息 */ private List&lt;SkuEsModel&gt; products; /** * 以下是分页信息 * 当前页码 */ private Integer pageNum; /** * 总共记录数 */ private Long total; /** * 总页码 */ private Integer totalPages; /** * 当前查询到的结果，所有设计的品牌 */ private List&lt;BrandVo&gt; brands; /** * 当前查询结果，所有涉及到的分类 */ private List&lt;CatalogVo&gt; catalogs; /** * 当前查询到的结果，所有涉及到的所有属性 */ private List&lt;AttrVo&gt; attrs; /** * 页码 */ private List&lt;Integer&gt; pageNavs; //==================以上是要返回给页面的所有信息 @Data public static class BrandVo { /** * 品牌id */ private Long brandId; /** * 品牌名字 */ private String brandName; /** * 品牌图片 */ private String brandImg; } @Data public static class CatalogVo { /** * 分类id */ private Long catalogId; /** * 品牌名字 */ private String CatalogName; } @Data public static class AttrVo { /** * 属性id */ private Long attrId; /** * 属性名字 */ private String attrName; /** * 属性值 */ private List&lt;String&gt; attrValue; }} 7.2 检索语句构建 7.2.1 查询部分 &amp; 聚合部分 那么这个 DSL 编写我们就在 Kibana 中测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149GET gulimall_product/_search{ &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;skuTitle&quot;: &quot;华为&quot; // 按照关键字查询 } } ], &quot;filter&quot;: [ { &quot;term&quot;: { &quot;catalogId&quot;: &quot;225&quot; // 根据分类id过滤 } }, { &quot;terms&quot;: { &quot;brandId&quot;: [ // 品牌id &quot;1&quot;, &quot;5&quot;, &quot;9&quot; ] } }, { &quot;nested&quot;: { // 根据属性id 以及属性值进行过滤 &quot;path&quot;: &quot;attrs&quot;, &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;term&quot;: { &quot;attrs.attrId&quot;: { &quot;value&quot;: &quot;8&quot; } } }, { &quot;terms&quot;: { &quot;attrs.attrValue&quot;: [ &quot;2019&quot; ] } } ] } } } }, { &quot;term&quot;: { // 是否有库存 &quot;hasStock&quot;: { &quot;value&quot;: &quot;false&quot; } } }, { &quot;range&quot;: { // 价格区间 &quot;skuPrice&quot;: { &quot;gte&quot;: 0, &quot;lte&quot;: 7000 } } } ] } }, &quot;sort&quot;: [ //排序 { &quot;skuPrice&quot;: { &quot;order&quot;: &quot;desc&quot; } } ], &quot;from&quot;: 0, &quot;size&quot;:4, &quot;highlight&quot;: { // 对搜索田间进行高亮 &quot;fields&quot;: {&quot;skuTitle&quot;: {}}, &quot;pre_tags&quot;: &quot;&lt;b style=color:red&gt;&quot;, &quot;post_tags&quot;: &quot;&lt;/b&gt;&quot; }, &quot;aggs&quot;: { &quot;brand_agg&quot;: { //品牌进行聚合 &quot;terms&quot;: { &quot;field&quot;: &quot;brandId&quot;, &quot;size&quot;: 10 }, &quot;aggs&quot;: { &quot;brand_name_agg&quot;: { // 品牌名字 &quot;terms&quot;: { &quot;field&quot;: &quot;brandName&quot;, &quot;size&quot;: 10 } }, &quot;brand_img_agg&quot;: { //品牌图片 &quot;terms&quot;: { &quot;field&quot;: &quot;brandImg&quot;, &quot;size&quot;: 10 } } } }, &quot;catalog_agg&quot;: { // 分类 &quot;terms&quot;: { &quot;field&quot;: &quot;catalogId&quot;, &quot;size&quot;: 10 }, &quot;aggs&quot;: { &quot;catalog_name_agg&quot;: { //分类名字 &quot;terms&quot;: { &quot;field&quot;: &quot;catalogName&quot;, &quot;size&quot;: 10 } } } }, &quot;attr_agg&quot;:{ &quot;nested&quot;: { &quot;path&quot;: &quot;attrs&quot; }, &quot;aggs&quot;: { //属性聚合 &quot;attr_id_agg&quot;: { &quot;terms&quot;: { &quot;field&quot;: &quot;attrs.attrId&quot;, &quot;size&quot;: 10 }, &quot;aggs&quot;: { &quot;attr_name_agg&quot;: { //属性名字 &quot;terms&quot;: { &quot;field&quot;: &quot;attrs.attrName&quot;, &quot;size&quot;: 10 } }, &quot;attr_value_agg&quot;:{ //属性的值 &quot;terms&quot;: { &quot;field&quot;: &quot;attrs.attrValue&quot;, &quot;size&quot;: 10 } } } } } } }} 用代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125/** * 准备检索请求 * #模糊匹配、过滤（按照属性、分类、品牌、价格区间、库存）、排序、分页、高亮、聚合分析 * * @return */ private SearchRequest buildSearchRequest(SearchParam param) { SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); //构建DSL语句 /** * 模糊匹配 过滤（按照属性、分类、品牌、价格区间、库存） */ // 1、构建bool - query BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 1.1 must - 模糊匹配 if (!StringUtils.isEmpty(param.getKeyword())) { boolQuery.must(QueryBuilders.matchQuery(&quot;skuTitle&quot;, param.getKeyword())); } // 1.2 bool - filter 按照三级分类id来查询 if (param.getCatalog3Id() != null) { boolQuery.filter(QueryBuilders.termQuery(&quot;catalogId&quot;, param.getCatalog3Id())); } // 1.2 bool - filter 按照品牌id来查询 if (param.getBrandId() != null &amp;&amp; param.getBrandId().size() &gt; 0) { boolQuery.filter(QueryBuilders.termsQuery(&quot;brandId&quot;, param.getBrandId())); } // 1.2 bool - filter 按照所有指定的属性来进行查询 *******不理解这个attr=1_5寸:8寸这样的设计 if (param.getAttrs() != null &amp;&amp; param.getAttrs().size() &gt; 0) { for (String attr : param.getAttrs()) { // attr=1_5寸:8寸&amp;attrs=2_16G:8G BoolQueryBuilder nestedboolQuery = QueryBuilders.boolQuery(); String[] s = attr.split(&quot;_&quot;); String attrId = s[0];// 检索的属性id String[] attrValues = s[1].split(&quot;:&quot;); nestedboolQuery.must(QueryBuilders.termQuery(&quot;attrs.attrId&quot;, attrId)); nestedboolQuery.must(QueryBuilders.termsQuery(&quot;attrs.attrValue&quot;, attrValues)); // 每一个必须都生成一个nested查询 NestedQueryBuilder nestedQuery = QueryBuilders.nestedQuery(&quot;attrs&quot;, nestedboolQuery, ScoreMode.None); boolQuery.filter(nestedQuery); } } // 1.2 bool - filter 按照库存是否存在 boolQuery.filter(QueryBuilders.termQuery(&quot;hasStock&quot;, param.getHasStock() == 1 ? true : false)); // 1.2 bool - filter 按照价格区间 /** * 1_500/_500/500_ */ if (!StringUtils.isEmpty(param.getSkuPrice())) { RangeQueryBuilder rangeQuery = QueryBuilders.rangeQuery(&quot;skuPrice&quot;); String[] s = param.getSkuPrice().split(&quot;_&quot;); if (s.length == 2) { // 区间 rangeQuery.gte(s[0]).lte(s[1]); } else if (s.length == 1) { if (param.getSkuPrice().startsWith(&quot;_&quot;)) { rangeQuery.lte(s[0]); } if (param.getSkuPrice().endsWith(&quot;_&quot;)) { rangeQuery.gte(s[0]); } } boolQuery.filter(rangeQuery); } //把以前所有条件都拿来进行封装 sourceBuilder.query(boolQuery); /** * 排序、分页、高亮 */ //2.1、排序 if (!StringUtils.isEmpty(param.getSort())) { String sort = param.getSort(); //sort=hotScore_asc/desc String[] s = sort.split(&quot;_&quot;); SortOrder order = s[1].equalsIgnoreCase(&quot;asc&quot;) ? SortOrder.ASC : SortOrder.DESC; sourceBuilder.sort(s[0], order); } //2.2 分页 pageSize:5 // pageNum:1 from 0 size:5 [0,1,2,3,4] // pageNum:2 from 5 size:5 // from (pageNum - 1)*size sourceBuilder.from((param.getPageNum() - 1) * EsConstant.PRODUCT_PAGESIZE); sourceBuilder.size(EsConstant.PRODUCT_PAGESIZE); //2.3、高亮 if (!StringUtils.isEmpty(param.getKeyword())) { HighlightBuilder builder = new HighlightBuilder(); builder.field(&quot;skuTitle&quot;); builder.preTags(&quot;&lt;b style='color:red'&gt;&quot;); builder.postTags(&quot;&lt;/b&gt;&quot;); sourceBuilder.highlighter(builder); } /** * 聚合分析 */ //1、品牌聚合 TermsAggregationBuilder brand_agg = AggregationBuilders.terms(&quot;brand_agg&quot;); brand_agg.field(&quot;brandId&quot;).size(50); //品牌聚合的子聚合 brand_agg.subAggregation(AggregationBuilders.terms(&quot;brand_name_agg&quot;).field(&quot;brandName&quot;).size(2)); brand_agg.subAggregation(AggregationBuilders.terms(&quot;brand_img_agg&quot;).field(&quot;brandImg&quot;).size(2)); // TODO 1、聚合brand sourceBuilder.aggregation(brand_agg); //2、分类聚合 TermsAggregationBuilder catalog_agg = AggregationBuilders.terms(&quot;catalog_agg&quot;).field(&quot;catalogId&quot;).size(20); catalog_agg.subAggregation(AggregationBuilders.terms(&quot;catalog_name_agg&quot;).field(&quot;catalogName&quot;).size(1)); // TODO 2、聚合catalog sourceBuilder.aggregation(catalog_agg); //3、属性聚合 attr_agg NestedAggregationBuilder attr_agg = AggregationBuilders.nested(&quot;attr_agg&quot;, &quot;attrs&quot;); // 聚合出当前所有的attrId TermsAggregationBuilder attr_id_agg = AggregationBuilders.terms(&quot;attr_id_agg&quot;).field(&quot;attrs.attrId&quot;); //聚合分析出当前attr_id对应的名字 attr_id_agg.subAggregation(AggregationBuilders.terms(&quot;attr_name_agg&quot;).field(&quot;attrs.attrName&quot;).size(1)); // 聚合分析出当前attr_id对应的可能的属性值attractValue attr_id_agg.subAggregation(AggregationBuilders.terms(&quot;attr_value_agg&quot;).field(&quot;attrs.attrValue&quot;).size(50)); attr_agg.subAggregation(attr_id_agg); // TODO 3、聚合attr sourceBuilder.aggregation(attr_agg); String s = sourceBuilder.toString(); System.out.println(&quot;构建的DSL:&quot; + s); SearchRequest searchRequest = new SearchRequest(new String[]{EsConstant.PRODUCT_INDEX}, sourceBuilder); return searchRequest; } 代码实现基本上是 根据 json 来写 调用对应的 API term 和 terms 不要调用错误 7.3 结果提取封装 Controller 12345678910111213/** * 自动将页面提交过来的所有请求查询参数自动封装成指定的对象 * @param param * @return */ @GetMapping(&quot;/list.html&quot;) public String listPage(SearchParam param, Model model){ //1、根据传递来的页面参数，去es中检索商品 SearchResult result = mallSearchService.search(param); model.addAttribute(&quot;result&quot;,result); return &quot;list&quot;; } Service 123456789101112131415161718@Overridepublic SearchResult search(SearchParam param) { // 1、动态构建出查询需要的DSL语句 SearchResult result = null; //1、准备检索请求 SearchRequest searchRequest = buildSearchRequest(param); try { // 2、执行检索请求 SearchResponse response = client.search(searchRequest, GulimallElasticsearchConfig.COMMON_OPTIONS); // 3、分析响应数据封装成我们需要的格式 result = buildSearchResult(response, param); } catch (Exception e) { e.printStackTrace(); } return result;} 具体业务执行 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * 构建结果数据 * * @param response * @return */private SearchResult buildSearchResult(SearchResponse response, SearchParam param) { SearchResult result = new SearchResult(); SearchHits hits = response.getHits(); List&lt;SkuEsModel&gt; esModels = new ArrayList&lt;&gt;(); if (hits.getHits() != null &amp;&amp; hits.getHits().length &gt; 0) { for (SearchHit hit : hits.getHits()) { String sourceAsString = hit.getSourceAsString(); SkuEsModel skuEsModel = JSON.parseObject(sourceAsString, SkuEsModel.class); if (!StringUtils.isEmpty(param.getKeyword())) { HighlightField skuTitle = hit.getHighlightFields().get(&quot;skuTitle&quot;); String string = skuTitle.getFragments()[0].string(); skuEsModel.setSkuTitle(string); } esModels.add(skuEsModel); } } //1、返回所有查询到的商品 result.setProducts(esModels); //2、当前所有商品设计到的所有属性信息 List&lt;SearchResult.AttrVo&gt; attrVos = new ArrayList&lt;&gt;(); ParsedNested attr_agg = response.getAggregations().get(&quot;attr_agg&quot;); ParsedLongTerms attr_id_agg = attr_agg.getAggregations().get(&quot;attr_id_agg&quot;); for (Terms.Bucket bucket : attr_id_agg.getBuckets()) { SearchResult.AttrVo attrVo = new SearchResult.AttrVo(); // 1、得到属性的id Long attrId = bucket.getKeyAsNumber().longValue(); // 2、得到属性的名字 String attrName = ((ParsedStringTerms) bucket.getAggregations().get(&quot;attr_name_agg&quot;)).getBuckets().get(0).getKeyAsString(); // 3、得到属性的所有值 List&lt;String&gt; attrValue = ((ParsedStringTerms) bucket.getAggregations().get(&quot;attr_value_agg&quot;)).getBuckets().stream().map(item -&gt; { String keyAsString = item.getKeyAsString(); return keyAsString; }).collect(Collectors.toList()); attrVo.setAttrId(attrId); attrVo.setAttrName(attrName); attrVo.setAttrValue(attrValue); attrVos.add(attrVo); } result.setAttrs(attrVos); //3、当前所有商品的分类信息 ParsedLongTerms Catalog_agg = response.getAggregations().get(&quot;catalog_agg&quot;); List&lt;SearchResult.CatalogVo&gt; catalogVos = new ArrayList&lt;&gt;(); List&lt;? extends Terms.Bucket&gt; buckets = Catalog_agg.getBuckets(); for (Terms.Bucket bucket : buckets) { SearchResult.CatalogVo catalogVo = new SearchResult.CatalogVo(); // 得到分类id String keyAsString = bucket.getKeyAsString(); catalogVo.setCatalogId(Long.parseLong(keyAsString)); // 得到分类名 ParsedStringTerms catalog_name_agg = bucket.getAggregations().get(&quot;catalog_name_agg&quot;); String catalog_name = catalog_name_agg.getBuckets().get(0).getKeyAsString(); catalogVo.setCatalogName(catalog_name); catalogVos.add(catalogVo); } result.setCatalogs(catalogVos); //4、当前所有商品的品牌信息 List&lt;SearchResult.BrandVo&gt; brandVos = new ArrayList&lt;&gt;(); ParsedLongTerms brand_agg = response.getAggregations().get(&quot;brand_agg&quot;); for (Terms.Bucket bucket : brand_agg.getBuckets()) { SearchResult.BrandVo brandVo = new SearchResult.BrandVo(); // 1、得到品牌的id long brandId = bucket.getKeyAsNumber().longValue(); // 2、得到品牌的图片 String brandImg = ((ParsedStringTerms) bucket.getAggregations().get(&quot;brand_img_agg&quot;)).getBuckets().get(0).getKeyAsString(); // 3、得到品牌的姓名 String brandname = ((ParsedStringTerms) bucket.getAggregations().get(&quot;brand_name_agg&quot;)).getBuckets().get(0).getKeyAsString(); brandVo.setBrandName(brandname); brandVo.setBrandId(brandId); brandVo.setBrandImg(brandImg); brandVos.add(brandVo); } result.setBrands(brandVos); //5、分页信息 - 总记录数 long total = hits.getTotalHits().value; result.setTotal(total); //6、分页信息 - 页码 result.setPageNum(param.getPageNum()); //7、分页信息 - 总页码 int totalPages = (int) total % EsConstant.PRODUCT_PAGESIZE == 0 ? (int) total / EsConstant.PRODUCT_PAGESIZE : ((int) total / EsConstant.PRODUCT_PAGESIZE + 1); result.setTotalPages(totalPages); List&lt;Integer&gt; pageNavs = new ArrayList&lt;&gt;(); for(int i = 1; i &lt;= totalPages; i++) { pageNavs.add(i); } result.setPageNavs(pageNavs); return result;} 7.4 页面数据渲染 7.4.1基本数据渲染 遍历后显示结果 7.4.3 筛选条件渲染 品牌条件筛选 分类 属性筛选 12345678910111213141516171819202122232425262728293031323334353637 function searchProducts(name, value) { // 跳转对应地址 location.href = replaceAndParamVal(location.href,name,value) } /** * 正则表达式替换 * **/ function replaceAndParamVal(url, paramName, replaceVal,forceAdd) { var oUrl = url.toString(); // 1、如果没有就添加，有就替换 if (oUrl.indexOf(paramName) != -1) { if(forceAdd) { var nUrl = &quot;&quot;; if (oUrl.indexOf(&quot;?&quot;) != -1) { nUrl = oUrl + &quot;&amp;&quot; + paramName + &quot;=&quot; + replaceVal; } else { nUrl = oUrl + &quot;?&quot; + paramName + &quot;=&quot; + replaceVal; } return nUrl; } else { var re = eval('/(' + paramName + '=)([^&amp;]*)/gi'); var nUrl = oUrl.replace(re, paramName + '=' + replaceVal) return nUrl; } } else { var nUrl = &quot;&quot;; if (oUrl.indexOf(&quot;?&quot;) != -1) { nUrl = oUrl + &quot;&amp;&quot; + paramName + &quot;=&quot; + replaceVal; } else { nUrl = oUrl + &quot;?&quot; + paramName + &quot;=&quot; + replaceVal; } return nUrl; } } 7.4.4 分页数据筛选 7.4.5 页面排序功能 7.4.6 页面价格筛选 JS 123456789$(&quot;#skuPriceSearchBtn&quot;).click(function() { // 1、拼上价格区间的查询条件 var from = $(&quot;#skuPriceFrom&quot;).val(); var to = $(&quot;#skuPriceTo&quot;).val(); // 2、拼接语句 var query = from + &quot;_&quot; + to; // 3、替换 skuPrice location.href = replaceAndParamVal(location.href,&quot;skuPrice&quot;,query);}) 7.4.7 面包屑导航（遇到问题！） 前端页面： 在返回Vo类中 新增了 Controller中 的解析方法中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 8、构建面包屑导航功能 if (param.getAttrs()!=null &amp;&amp; param.getAttrs().size() &gt;0){ List&lt;SearchResult.NavVo&gt; collect = param.getAttrs().stream().map(attr -&gt; { // 1、分析每个 attrs传递过来的值 SearchResult.NavVo navo = new SearchResult.NavVo(); // attrs=2_5寸：6寸 String[] s = attr.split(&quot;_&quot;); navo.setNavValue(s[1]); R r = productFeignService.getAttrInfo(Long.parseLong(s[0])); result.getAttrIds().add(Long.parseLong(s[0])); if (r.getCode() == 0) { AttrResponseVo data = r.getData(&quot;attr&quot;, new TypeReference&lt;AttrResponseVo&gt;() { }); navo.setNavName(data.getAttrName()); } else { navo.setNavName(s[0]); } // 取消这个面包屑导航以后，我们要跳转到那个地方，将请求地址的url里面的当前置空 //拿到所有的查询条件后，去掉当前 // attrs=15_海思(Hisilicon) String replace = replaceQueryString(param, attr,&quot;attrs&quot;); navo.setLink(&quot;http://search.gulimall.com/list.html?&quot; + replace); return navo; }).collect(Collectors.toList()); result.setNavs(collect); } // 品牌、分类 if(param.getBrandId() != null &amp;&amp; param.getBrandId().size() &gt; 0) { List&lt;SearchResult.NavVo&gt; navs = result.getNavs(); SearchResult.NavVo navVo = new SearchResult.NavVo(); navVo.setNavName(&quot;品牌&quot;); //TODO 远程查询所有品牌 R info = productFeignService.info(param.getBrandId()); if (info.getCode() == 0) { List&lt;BrandVo&gt; brand = info.getData(&quot;brand&quot;, new TypeReference&lt;List&lt;BrandVo&gt;&gt;() { }); StringBuffer buffer = new StringBuffer(); String replace = &quot;&quot;; for (BrandVo brandVo : brand) { buffer.append(brandVo.getBrandName() + &quot;;&quot;); replace = replaceQueryString(param,brandVo.getBrandId() + &quot;&quot;,&quot;brandId&quot;); } navVo.setNavValue(buffer.toString()); navVo.setLink(&quot;http://search.gulimall.com/list.html?&quot; + replace); } navs.add(navVo); } //TODO 分类：不需要导航 return result; 8、异步 &amp; 线程池 8.1 线程回顾 8.1.1 初始化线程的 4 种方式 1、继承 Thread 2、实现 Runnable 3、实现 Callable 接口 + FutureTask（可以拿到返回结果，可以处理异常） 4、线程池 方式一和方式二 主进程无法获取线程的运算结果，不适合当前场景 方式三：主进程可以获取当前线程的运算结果，但是不利于控制服务器种的线程资源，可以导致服务器资源耗尽 方式四：通过如下两种方式初始化线程池 123Executors.newFixedThreadPool(3);//或者new ThreadPollExecutor(corePoolSize,maximumPoolSize,keepAliveTime,TimeUnit,unit,workQueue,threadFactory,handler); 通过线程池性能稳定，也可以获取执行结果，并捕获异常，但是，在业务复杂情况下，一个异步调用可能会依赖另一个异步调用的执行结果 8.1.2 线程池的 7 大参数 运行流程： 1、线程池创建，准备好 core 数量 的核心线程，准备接受任务 2、新的任务进来，用 core 准备好的空闲线程执行 core 满了，就将再进来的任务放入阻塞队列中，空闲的 core 就会自己去阻塞队列获取任务执行 阻塞队列也满了，就直接开新线程去执行，最大只能开到 max 指定的数量 max 都执行好了，Max-core 数量空闲的线程会在 keepAliveTime 指定的时间后自动销毁，终保持到 core 大小 如果线程数开到了 max 数量，还有新的任务进来，就会使用 reject 指定的拒绝策略进行处理 3、所有的线程创建都是由指定的 factory 创建的 面试; 一个线程池 core 7、max 20 ，queue 50 100 并发进来怎么分配的 ? 先有 7 个能直接得到运行，接下来 50 个进入队列排队，再多开 13 个继续执行，线程70个被安排上了，剩下30个默认拒绝策略 8.1.3 常见的 4 种线程池 newCacheThreadPool 创建一个可缓存的线程池，如果线程池长度超过需要，可灵活回收空闲线程，若无可回收，则新建线程 newFixedThreadPool 创建一个指定长度的线程池，可控制线程最大并发数，超出的线程会再队列中等待 newScheduleThreadPool 创建一个定长线程池，支持定时及周期性任务执行 newSingleThreadExecutor 创建一个单线程化的线程池，她只会用唯一的工作线程来执行任务，保证所有任务 8.1.4 开发中为什么使用线程池 降低资源的消耗 通过重复利用已创建好的线程降低线程的创建和销毁带来的损耗 提高响应速度 因为线程池中的线程没有超过线程池的最大上限时，有的线程处于等待分配任务的状态，当任务来时无需创建新的线程就能执行 提高线程的客观理性 线程池会根据当前系统的特点对池内的线程进行优化处理，减少创建和销毁线程带来的系统开销，无限的创建和销毁线程不仅消耗系统资源，还降低系统的稳定性，使用线程池进行统一分配 8.2 CompletableFuture 异步编排 业务场景： 查询商品详情页逻辑比较复杂，有些数据还需要远程调用，必然需要花费更多的时间 假如商品详情页的每个查询，需要如下标注时间才能完成 那么，用户需要5.5s后才能看到商品相详情页的内容，很显然是不能接受的 如果有多个线程同时完成这 6 步操作，也许只需要 1.5s 即可完成响应 8.2.1 创建异步对象 CompletableFuture 提供了四个静态方法来创建一个异步操作 1、runXxx 都是没有返回结果的，supplyXxxx都是可以获取返回结果的 2、可以传入自定义的线程池，否则就是用默认的线程池 3、根据方法的返回类型来判断是否该方法是否有返回类型 代码实现： 123456789101112131415161718public static void main(String[] args) throws ExecutionException, InterruptedException { System.out.println(&quot;main....start.....&quot;); CompletableFuture&lt;Void&gt; completableFuture = CompletableFuture.runAsync(() -&gt; { System.out.println(&quot;当前线程：&quot; + Thread.currentThread().getId()); int i = 10 / 2; System.out.println(&quot;运行结果：&quot; + i); }, executor); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;当前线程：&quot; + Thread.currentThread().getId()); int i = 10 / 2; System.out.println(&quot;运行结果：&quot; + i); return i; }, executor); Integer integer = future.get(); System.out.println(&quot;main....stop.....&quot; + integer); } 8.2.2 计算完成时回调方法 whenComplete 可以处理正常和异常的计算结果，exceptionally 处理异常情况 whenComplete 和 whenCompleteAsync 的区别 ​ whenComplete ：是执行当前任务的线程继续执行 whencomplete 的任务 ​ whenCompleteAsync： 是执行把 whenCompleteAsync 这个任务继续提交给线程池来进行执行 方法不以 Async 结尾，意味着 Action 使用相同的线程执行，而 Async 可能会使用其他线程执行（如果是使用相同的线程池，也可能会被同一个线程选中执行） 123456789101112CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;当前线程：&quot; + Thread.currentThread().getId()); int i = 10 / 0; System.out.println(&quot;运行结果：&quot; + i); return i;}, executor).whenComplete((res,exception) -&gt;{ // 虽然能得到异常信息，但是没法修改返回的数据 System.out.println(&quot;异步任务成功完成了...结果是：&quot; +res + &quot;异常是：&quot; + exception);}).exceptionally(throwable -&gt; { // 可以感知到异常，同时返回默认值 return 10;}); 8.2.3 handle 方法 和 complete 一样，可以对结果做最后的处理（可处理异常），可改变返回值 1234567891011121314CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;当前线程：&quot; + Thread.currentThread().getId()); int i = 10 / 2; System.out.println(&quot;运行结果：&quot; + i); return i;}, executor).handle((res,thr) -&gt;{ if (res != null ) { return res * 2; } if (thr != null) { return 0; } return 0;}); 8.2.4 线程串行方法 thenApply 方法：当一个线程依赖另一个线程时，获取上一个任务返回的结果，并返回当前任物的返回值 thenAccept方法：消费处理结果，接受任务处理结果，并消费处理，无返回结果 thenRun 方法：只要上面任务执行完成，就开始执行 thenRun ,只是处理完任务后，执行 thenRun的后续操作 带有 Async 默认是异步执行的，同之前， 以上都要前置任务完成 12345678910111213141516171819202122/** * 线程串行化， * 1、thenRun:不能获取到上一步的执行结果，无返回值 * .thenRunAsync(() -&gt;{ * System.out.println(&quot;任务2启动了....&quot;); * },executor); * 2、能接受上一步结果，但是无返回值 thenAcceptAsync * 3、thenApplyAsync 能收受上一步结果，有返回值 * */ CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;当前线程：&quot; + Thread.currentThread().getId()); int i = 10 / 2; System.out.println(&quot;运行结果：&quot; + i); return i; }, executor).thenApplyAsync(res -&gt; { System.out.println(&quot;任务2启动了...&quot; + res); return &quot;Hello &quot; + res; }, executor); String s = future.get(); System.out.println(&quot;main....stop.....&quot; + s); 8.2.5 两任务组合 - 都要完成 两个任务必须都完成，触发该任务 thenCombine: 组合两个 future，获取两个 future的返回结果，并返回当前任务的返回值 thenAccpetBoth: 组合两个 future，获取两个 future 任务的返回结果，然后处理任务，没有返回值 runAfterBoth:组合 两个 future，不需要获取 future 的结果，只需要两个 future处理完成任务后，处理该任务， 1234567891011121314151617181920212223242526272829303132 /** * 两个都完成 */ CompletableFuture&lt;Integer&gt; future01 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;任务1当前线程：&quot; + Thread.currentThread().getId()); int i = 10 / 4; System.out.println(&quot;任务1结束：&quot; + i); return i; }, executor); CompletableFuture&lt;String&gt; future02 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;任务2当前线程：&quot; + Thread.currentThread().getId()); System.out.println(&quot;任务2结束：&quot;); return &quot;Hello&quot;; }, executor); // f1 和 f2 执行完成后在执行这个// future01.runAfterBothAsync(future02,() -&gt; {// System.out.println(&quot;任务3开始&quot;);// },executor); // 返回f1 和 f2 的运行结果// future01.thenAcceptBothAsync(future02,(f1,f2) -&gt; {// System.out.println(&quot;任务3开始....之前的结果:&quot; + f1 + &quot;==&gt;&quot; + f2);// },executor); // f1 和 f2 单独定义返回结果 CompletableFuture&lt;String&gt; future = future01.thenCombineAsync(future02, (f1, f2) -&gt; { return f1 + &quot;:&quot; + f2 + &quot;-&gt; Haha&quot;; }, executor); System.out.println(&quot;main....end.....&quot; + future.get()); 8.2.6 两任务组合 - 一个完成 当两个任务中，任意一个future 任务完成时，执行任务 applyToEither;两个任务有一个执行完成，获取它的返回值，处理任务并有新的返回值 acceptEither: 两个任务有一个执行完成，获取它的返回值，处理任务，没有新的返回值 runAfterEither:两个任务有一个执行完成，不需要获取 future 的结果，处理任务，也没有返回值 123456789101112131415161718/** * 两个任务，只要有一个完成，我们就执行任务 * runAfterEnitherAsync：不感知结果，自己没有返回值 * acceptEitherAsync：感知结果，自己没有返回值 * applyToEitherAsync：感知结果，自己有返回值 */// future01.runAfterEitherAsync(future02,() -&gt;{// System.out.println(&quot;任务3开始...之前的结果:&quot;);// },executor);// future01.acceptEitherAsync(future02,(res) -&gt; {// System.out.println(&quot;任务3开始...之前的结果:&quot; + res);// },executor); CompletableFuture&lt;String&gt; future = future01.applyToEitherAsync(future02, res -&gt; { System.out.println(&quot;任务3开始...之前的结果：&quot; + res); return res.toString() + &quot;-&gt;哈哈&quot;; }, executor); 8.2.7 多任务组合 allOf：等待所有任务完成 anyOf:只要有一个任务完成 123456789101112131415161718192021222324 CompletableFuture&lt;String&gt; futureImg = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;查询商品的图片信息&quot;); return &quot;hello.jpg&quot;; }); CompletableFuture&lt;String&gt; futureAttr = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;查询商品的属性&quot;); return &quot;黑色+256G&quot;; }); CompletableFuture&lt;String&gt; futureDesc = CompletableFuture.supplyAsync(() -&gt; { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(&quot;查询商品介绍&quot;); return &quot;华为&quot;; }); // 等待全部执行完// CompletableFuture&lt;Void&gt; allOf = CompletableFuture.allOf(futureImg, futureAttr, futureDesc);// allOf.get(); // 只需要有一个执行完 CompletableFuture&lt;Object&gt; anyOf = CompletableFuture.anyOf(futureImg, futureAttr, futureDesc); anyOf.get(); System.out.println(&quot;main....end.....&quot; + anyOf.get()); 都是操作 CompletableFuture 类 更多方法还请参考该类 9、商品详情 9.1 详情数据 **需求分析：**通过 skuId 查询出商品的相关信息，图片、标题、价格，属性对应版本等等 9.1.1 返回数据模型抽取 12345678910111213141516171819/** * @author gcq * @Create 2020-11-06 */@Datapublic class SkuItemVo { // 1、sku基本获取 pms_sku_info SkuInfoEntity skuInfo; // 是否有库存 boolean hasStock = true; // 2、sku的图片信息 pms_sku_images List&lt;SkuImagesEntity&gt; images; // 3、获取spu的销售属性组 List&lt;SkuItemSalAttrVo&gt; saleAttr; // 4、获取spu的介绍 SpuInfoDescEntity desc; // 5、获取spu的规格参数信息 List&lt;SpuItemAttrGroupVo&gt; groupAttrs;} 12345678910/** * @author gcq * @Create 2020-11-08 */@Datapublic class SkuItemSalAttrVo { private Long attrId; private String attrName; private List&lt;AttrValueWithSkuIdVo&gt; attrValues;} 1234567891011/** * @author gcq * @Create 2020-11-08 */@Datapublic class AttrValueWithSkuIdVo { private String attrValue; private String skuIds; } 9.2 查询详情 1234567891011121314151617181920212223242526272829303132333435363738394041@Overridepublic SkuItemVo item(Long skuId) throws ExecutionException, InterruptedException { SkuItemVo skuItemVo = new SkuItemVo(); // 1、sku基本获取 pms_sku_info CompletableFuture&lt;SkuInfoEntity&gt; infoFuture = CompletableFuture.supplyAsync(() -&gt; { // 根据 skuid 查询出 spuInfo对象 SkuInfoEntity info = getById(skuId); skuItemVo.setSkuInfo(info); return info; }, executor); CompletableFuture&lt;Void&gt; saleAttrFuture = infoFuture.thenAcceptAsync((res) -&gt; { // 3、获取spu的销售属性组合 List&lt;SkuItemSalAttrVo&gt; saleAttrVos = skuSaleAttrValueService.getSaleAttrBySpuId(res.getSpuId()); skuItemVo.setSaleAttr(saleAttrVos); }, executor); CompletableFuture&lt;Void&gt; descFuture = infoFuture.thenAcceptAsync(res -&gt; { // 4、获取spu的介绍 // 通过spuid查询出spu描述信息 SpuInfoDescEntity spuInfoDescEntity = spuInfoDescService.getById(res.getSpuId()); skuItemVo.setDesc(spuInfoDescEntity); }, executor); CompletableFuture&lt;Void&gt; baseFuture = infoFuture.thenAcceptAsync(res -&gt; { // 5、获取spu的规格参数信息 List&lt;SpuItemAttrGroupVo&gt; attrGroupVos = attrGroupService.getAttrGroupWithAttrBySpuId(res.getSpuId(), res.getCatalogId()); skuItemVo.setGroupAttrs(attrGroupVos); }, executor); CompletableFuture&lt;Void&gt; imageFuture = CompletableFuture.runAsync(() -&gt; { // 2、sku的图片信息 pms_sku_images List&lt;SkuImagesEntity&gt; images = imagesService.getImagesBySkuId(skuId); skuItemVo.setImages(images); }, executor); // 等待所有任务完成 CompletableFuture.allOf(saleAttrFuture, descFuture, baseFuture, imageFuture).get(); return skuItemVo;} 获取spu的销售属性组合 12345678910111213&lt;select id=&quot;getSaleAttrBySpuId&quot; resultMap=&quot;SkuItemSaleAttrVo&quot;&gt; SELECT ssav.`attr_id` attr_id, ssav.`attr_name` attr_name, ssav.`attr_value` , GROUP_CONCAT(DISTINCT info.`sku_id`) sku_ids FROM `pms_sku_info` info LEFT JOIN `pms_sku_sale_attr_value` ssav ON ssav.`sku_id` = info.`sku_id` WHERE info.`spu_id` = #{spuId} GROUP BY ssav.`attr_id`,ssav.`attr_name`,ssav.`attr_value` &lt;/select&gt; 获取spu的规格参数信息 1234567891011121314151617181920&lt;select id=&quot;getAttrGroupWithAttrsBySpuId&quot; resultMap=&quot;spuItemAttrGroupVo&quot;&gt; SELECT pav.`spu_id`, ag.`attr_group_name` , ag.`attr_group_id`, aar.`attr_id`, attr.`attr_name`, pav.`attr_value` FROM `pms_attr_group` ag LEFT JOIN `pms_attr_attrgroup_relation` aar ON aar.`attr_group_id` = ag.`attr_group_id` LEFT JOIN `pms_attr` attr ON attr.`attr_id` = aar.`attr_id` LEFT JOIN `pms_product_attr_value` pav ON pav.`attr_id` = attr.`attr_id` WHERE ag.`catelog_id` = #{catalogId} AND pav.`spu_id` = #{spuId}&lt;/select&gt; 其他都是通过对应关联的 id 进行查询，就上面两个查询销售属性、规格参数属性 比较难，SQL这块比较难以理解 9.3 sku 组合切换 123456789101112131415161718192021222324252627282930313233// sku 组合切换 $(&quot;.sku_attr_value&quot;).click(function () { var skus = new Array(); // 1、点击的元素添加上自定义属性,为了识别我们刚在被点击的 $(this).addClass(&quot;clicked&quot;) // 1.1、当前被选中的属性的skus信息 var curr = $(this).attr(&quot;skus&quot;).split(&quot;,&quot;); // 2、添加到数组中 skus.push(curr); // 3、去掉同一行的所有的 checked a-&gt;dd-&gt;dl $(this).parent().parent().find(&quot;.sku_attr_value&quot;).removeClass(&quot;checked&quot;) // 4、其他项也需要添加到数组里面 $(&quot;a[class='sku_attr_value checked']&quot;).each(function () { skus.push($(this).attr(&quot;skus&quot;).split(&quot;,&quot;)) }) // 5、取出他们的交集 jquery 中使用 filter方法来取出他们的交集 // 5.1 属性项可能有多个，内存、版本等等，所以需要遍历 var filterEle = skus[0] for (var i = 1; i &lt; skus.length; i++) { filterEle = $(filterEle).filter(skus[i]) } // 6、跳转到交集页面 console.log(filterEle[0]) location.href = &quot;http://item.gulimall.com/&quot; + filterEle[0] + &quot;.html&quot; }) $(function () { // 默认父元素是灰色 $(&quot;.sku_attr_value&quot;).parent().css({&quot;border&quot;: &quot;solid 1px #CCC&quot;}); // 拥有checked样式表示选中当前元素 $(&quot;a[class='sku_attr_value checked']&quot;).parent().css({&quot;border&quot;: &quot;solid 1px red&quot;}); }) 10、商品业务 &amp; 认证服务 10.1 环境搭建 为登录和注册创建一个服务 讲提供的前端放到 templates 目录下 10.2 前端验证码倒计时 定义id 使用 Jquery 触发点击事件 Jquery 12345678910111213141516171819202122232425262728293031323334353637$(function () { /** * 验证码发送 */ $(&quot;#sendCode&quot;).click(function () { //判断是否有该样式 if ($(this).hasClass(&quot;disabled&quot;)) { // 正在倒计时 } else { // 发送验证码 $.get(&quot;/sms/sendCode?phone=&quot; + $(&quot;#phoneNum&quot;).val(), function (data) { if (data.code != 0) { alert(data.msg) } }) timeoutChangeStyle(); } })})// 60秒var num = 60;function timeoutChangeStyle() { // 先添加样式，防止重复点击 $(&quot;#sendCode&quot;).attr(&quot;class&quot;, &quot;disabled&quot;) // 到达0秒后 重置时间，去除样式 if (num == 0) { $(&quot;#sendCode&quot;).text(&quot;发送验证码&quot;) num = 60; // 时间到达后清除样式 $(&quot;#sendCode&quot;).attr(&quot;class&quot;, &quot;&quot;); } else { var str = num + &quot;s 后再次发送&quot; $(&quot;#sendCode&quot;).text(str); setTimeout(&quot;timeoutChangeStyle()&quot;, 1000); } num--;} 对应效果 10.3 整合短信验证码 1、短信验证我们选择的是阿里云的短信服务 2、选择对应短信服务进行开通 在云市场就能看到购买的服务 3、验证短信功能是否能发送 在购买短信的页面，能进行调试短信 输入对应手机号，appCode 具体功能不做演示 4、使用 Java 测试短信是否能进行发送 往下拉找到对应 Java 代码 注意： ​ 服务商提供的接口地址，请求参数都不同，请参考服务商提供的测试代码 12345678910111213141516171819202122232425262728293031323334@Testpublic void contextLoads() { String host = &quot;http://dingxin.market.alicloudapi.com&quot;; String path = &quot;/dx/sendSms&quot;; String method = &quot;POST&quot;; String appcode = &quot;你自己的AppCode&quot;; Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;(); //最后在header中的格式(中间是英文空格)为Authorization:APPCODE 83359fd73fe94948385f570e3c139105 headers.put(&quot;Authorization&quot;, &quot;APPCODE &quot; + appcode); Map&lt;String, String&gt; querys = new HashMap&lt;String, String&gt;(); querys.put(&quot;mobile&quot;, &quot;159xxxx9999&quot;); querys.put(&quot;param&quot;, &quot;code:1234&quot;); querys.put(&quot;tpl_id&quot;, &quot;TP1711063&quot;); Map&lt;String, String&gt; bodys = new HashMap&lt;String, String&gt;(); try { /** * 重要提示如下: * HttpUtils请从 * https://github.com/aliyun/api-gateway-demo-sign-java/blob/master/src/main/java/com/aliyun/api/gateway/demo/util/HttpUtils.java * 下载 * * 相应的依赖请参照 * https://github.com/aliyun/api-gateway-demo-sign-java/blob/master/pom.xml */ HttpResponse response = HttpUtils.doPost(host, path, method, headers, querys, bodys); System.out.println(response.toString()); //获取response的body //System.out.println(EntityUtils.toString(response.getEntity())); } catch (Exception e) { e.printStackTrace(); }} 需要导入对应工具类，参照注释就行 10.4 验证码防刷校验 用户要是一直提交验证码 前台：限制一分钟后提交 后台：存入redis 如果有就返回 1234567891011121314151617181920212223242526272829303132/** * 发送短信验证码 * @param phone 手机号 * @return */@GetMapping(&quot;/sms/sendCode&quot;)@ResponseBodypublic R sendCode(@RequestParam(&quot;phone&quot;) String phone) { // TODO 1、接口防刷 // 先从redis中拿取 String redisCode = redisTemplate.opsForValue().get(AuthServerConstant.SMS_CODE_CACHE_PREFIX + phone); if(!StringUtils.isEmpty(redisCode)) { // 拆分 long l = Long.parseLong(redisCode.split(&quot;_&quot;)[1]); // 当前系统事件减去之前验证码存入的事件 小于60000毫秒=60秒 if (System.currentTimeMillis() -l &lt; 60000) { // 60秒内不能再发 R.error(BizCodeEnume.SMS_CODE_EXCEPTION.getCode(),BizCodeEnume.SMS_CODE_EXCEPTION.getMsg()); } } // 2、验证码的再次效验 // 数据存入 =》redis key-phone value - code sms:code:131xxxxx - &gt;45678 String code = UUID.randomUUID().toString().substring(0,5).toUpperCase(); // 拼接验证码 String substring = code+&quot;_&quot; + System.currentTimeMillis(); // redis缓存验证码 防止同一个phone在60秒内发出多次验证吗 redisTemplate.opsForValue().set(AuthServerConstant.SMS_CODE_CACHE_PREFIX+phone,substring,10, TimeUnit.MINUTES); // 调用第三方服务发送验证码 thirdPartFeignService.sendCode(phone,code); return R.ok();} 10.5 一步一坑注册页环境 1、编写 vo 接收页面提交 使用到了 JSR303校验 12345678910111213141516171819202122/** * 注册数据封装Vo * @author gcq * @Create 2020-11-09 */@Datapublic class UserRegistVo { @NotEmpty(message = &quot;用户名必须提交&quot;) @Length(min = 6,max = 18,message = &quot;用户名必须是6-18位字符&quot;) private String userName; @NotEmpty(message = &quot;密码必须填写&quot;) @Length(min = 6,max = 18,message = &quot;密码必须是6-18位字符&quot;) private String password; @NotEmpty(message = &quot;手机号码必须提交&quot;) @Pattern(regexp = &quot;^[1]([3-9])[0-9]{9}$&quot;,message = &quot;手机格式不正确&quot;) private String phone; @NotEmpty(message = &quot;验证码必须填写&quot;) private String code;} 2、页面提交数据与Vo一致 设置 name 属性与 Vo 一致，方便将传递过来的数据转换成 JSON 3、数据校验 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * //TODO 重定向携带数据，利用session原理，将数据放在session中， * 只要跳转到下一个页面取出这个数据，session中的数据就会删掉 * //TODO分布式下 session 的问题 * RedirectAttributes redirectAttributes 重定向携带数据 * redirectAttributes.addFlashAttribute(&quot;errors&quot;, errors); 只能取一次 * @param vo 数据传输对象 * @param result 用于验证参数 * @param redirectAttributes 数据重定向 * @return */@PostMapping(&quot;/regist&quot;)public String regist(@Valid UserRegistVo vo, BindingResult result, RedirectAttributes redirectAttributes) { // 校验是否通过 if (result.hasErrors()) { // 拿到错误信息转换成Map Map&lt;String, String&gt; errors = result.getFieldErrors().stream().collect(Collectors.toMap(FieldError::getField, FieldError::getDefaultMessage)); //用一次的属性 redirectAttributes.addFlashAttribute(&quot;errors&quot;,errors); // 校验出错，转发到注册页 return &quot;redirect:http://auth.gulimall.com/reg.html&quot;; } // 将传递过来的验证码 与 存redis中的验证码进行比较 String code = vo.getCode(); String s = redisTemplate.opsForValue().get(AuthServerConstant.SMS_CODE_CACHE_PREFIX + vo.getPhone()); if (!StringUtils.isEmpty(s)) { // 验证码和redis中的一致 if(code.equals(s.split(&quot;_&quot;)[0])) { // 删除验证码：令牌机制 redisTemplate.delete(AuthServerConstant.SMS_CODE_CACHE_PREFIX + vo.getPhone()); // 调用远程服务，真正注册 R r = memberFeignService.regist(vo); if (r.getCode() == 0) { // 远程调用注册服务成功 return &quot;redirect:http://auth.gulimall.com/login.html&quot;; } else { Map&lt;String, String&gt; errors = new HashMap&lt;&gt;(); errors.put(&quot;msg&quot;,r.getData(new TypeReference&lt;String&gt;(){})); redirectAttributes.addFlashAttribute(&quot;errors&quot;, errors); return &quot;redirect:http://auth.gulimall.com/reg.html&quot;; } } else { Map&lt;String, String&gt; errors = new HashMap&lt;&gt;(); errors.put(&quot;code&quot;, &quot;验证码错误&quot;); redirectAttributes.addFlashAttribute(&quot;code&quot;, &quot;验证码错误&quot;); // 校验出错，转发到注册页 return &quot;redirect:http://auth.gulimall.com/reg.html&quot;; } } else { Map&lt;String, String&gt; errors = new HashMap&lt;&gt;(); errors.put(&quot;code&quot;, &quot;验证码错误&quot;); redirectAttributes.addFlashAttribute(&quot;code&quot;, &quot;验证码错误&quot;); // 校验出错，转发到注册页 return &quot;redirect:http://auth.gulimall.com/reg.html&quot;; }} 4、前端页面接收错误信息 5、异常机制 &amp; 用户注册 用户注册单独抽出了一个服务 Controller 1234567891011121314151617/** * 注册 * @param registVo * @return */@PostMapping(&quot;/regist&quot;)public R regist(@RequestBody MemberRegistVo registVo) { try { memberService.regist(registVo); } catch (PhoneExsitException e) { // 返回对应的异常信息 return R.error(BizCodeEnume.PHONE_EXIST_EXCEPTION.getCode(),BizCodeEnume.PHONE_EXIST_EXCEPTION.getMsg()); } catch (UserNameExistException e) { return R.error(BizCodeEnume.USER_EXIST_EXCEPTION.getCode(),BizCodeEnume.USER_EXIST_EXCEPTION.getMsg()); } return R.ok();} 123456789101112131415161718192021222324252627282930313233343536373839404142@Overridepublic void regist(MemberRegistVo registVo) { MemberDao memberDao = this.baseMapper; MemberEntity entity = new MemberEntity(); // 设置默认等级 MemberLevelEntity memberLevelEntity = memberLevelDao.getDefaultLevel(); entity.setLevelId(memberLevelEntity.getId()); // 检查手机号和用户名是否唯一 checkPhoneUnique(registVo.getPhone()); checkUserNameUnique(registVo.getUserName()); entity.setMobile(registVo.getPhone()); entity.setUsername(registVo.getUserName()); //密码要加密存储 BCryptPasswordEncoder passwordEncoder = new BCryptPasswordEncoder(); String encode = passwordEncoder.encode(registVo.getPassword()); entity.setPassword(encode); memberDao.insert(entity);}@Overridepublic void checkPhoneUnique(String phone) throws PhoneExsitException { MemberDao memberDao = this.baseMapper; Integer mobile = memberDao.selectCount(new QueryWrapper&lt;MemberEntity&gt;().eq(&quot;mobile&quot;, phone)); if (mobile &gt; 0) { throw new PhoneExsitException(); }}@Overridepublic void checkUserNameUnique(String username) throws UserNameExistException { MemberDao memberDao = this.baseMapper; Integer count = memberDao.selectCount(new QueryWrapper&lt;MemberEntity&gt;().eq(&quot;username&quot;, username)); if (count &gt; 0) { throw new PhoneExsitException(); }} 此处引入一个问题 密码是直接存入数据库吗？ 这样子会导致数据的不安全， 引出了使用 MD5进行加密，但是MD5加密后，别人任然可以暴力破解 可以使用加盐的方式，将密码加密后，得到一串随机字符， 随机字符和密码和进行验证相同结果返回true否则false 至此注册相关结束~ 10.6 账号密码登录 1、定义 Vo 接收数据提交 123456789/** * @author gcq * @Create 2020-11-10 */@Datapublic class UserLoginVo { private String loginacct; private String password;} 同时需要保证前端页面提交字段与 Vo 类中一致 2、在 Member 服务中编写接口 123456789101112131415161718192021222324252627@Overridepublic MemberEntity login(MemberLoginVo vo) { String loginacct = vo.getLoginacct(); String password = vo.getPassword(); // 1、去数据库查询 select * from ums_member where username=? or mobile =? MemberDao memberDao = this.baseMapper; MemberEntity memberEntity = memberDao.selectOne(new QueryWrapper&lt;MemberEntity&gt;() .eq(&quot;username&quot;, loginacct).or(). eq(&quot;mobile&quot;, loginacct)); if (memberDao == null) { // 登录失败 return null; } else { // 获取数据库的密码 String passwordDB = memberEntity.getPassword(); BCryptPasswordEncoder passwordEncoder = new BCryptPasswordEncoder(); // 和用户密码进行校验 boolean matches = passwordEncoder.matches(password, passwordDB); if(matches) { // 密码验证成功 返回对象 return memberEntity; } else { return null; } }} 11.7 分布式 Session不共享不同步问题 我们在auth.gulimall.com中保存session，但是网址跳转到 gulimall.com中，取不出auth.gulimall.com中保存的session，这就造成了微服务下的session不同步问题 1、Session同步解决方案-分布式下session共享问题 同一个服务复制多个，但是session还是只能在一个服务上保存，浏览器也是只能读取到一个服务的session 2、Session共享问题解决-session复制 3、Session共享问题解决-客户端存储 4、Session共享问题解决-hash一致性 5、Session共享问题解决-统一存储 11.8 SpringSession整合 1、官网文档 阅读 进入到 Spring Framework 2、选择Spring Session文档 3、开始使用Spring Session 2、整合SpringBoot 1、添加Pom.xml依赖 2、application.yml 配置 3、reids配置 4、启动类加上 @EnableRedisHttpSession 1@EnableRedisHttpSession // 整合spring session 3、自定义 SpringSession 完成 Session 子域共享 CookieSerializer api文档参考：https://docs.spring.io/spring-session/docs/2.4.1/reference/html5/index.html#api-cookieserializer 指定redis序列化 文档地址： https://docs.spring.io/spring-session/docs/2.4.1/reference/html5/index.html#api-redisindexedsessionrepository-config redis中json序列化 官网文档地址：https://docs.spring.io/spring-session/docs/2.4.1/reference/html5/index.html#samples 提供的实例： https://github.com/spring-projects/spring-session/blob/2.4.1/spring-session-samples/spring-session-sample-boot-redis-json/src/main/java/sample/config/SessionConfig.java 1234567891011121314151617181920212223242526272829303132333435/** * SpringSession整合子域 * 以及redis数据存储为json * @author gcq * @Create 2020-11-11 */@Configurationpublic class GulimallSessionConfig { /** * 设置cookie信息 * @return */ @Bean public CookieSerializer CookieSerializer(){ DefaultCookieSerializer cookieSerializer = new DefaultCookieSerializer(); // 设置一个域名的名字 cookieSerializer.setDomainName(&quot;gulimall.com&quot;); // cookie的路径 cookieSerializer.setCookieName(&quot;GULIMALLSESSION&quot;); return cookieSerializer; } /** * 设置json转换 * @return */ @Bean public RedisSerializer&lt;Object&gt; springSessionDefaultRedisSerializer() { // 使用jackson提供的转换器 return new GenericJackson2JsonRedisSerializer(); }} 4、SpringSession 原理 12345678910111213141516/** * 核心原理 * 1、@EnableRedisHttpSession导入RedisHttpSessionConfiguration配置 * 1、给容器中添加了一个组件 * sessionRepository = 》》》【RedisOperationsSessionRepository】 redis 操作 session session的增删改查封装类 * 2、SessionRepositoryFilter==&gt;:session存储过滤器，每个请求过来必须经过Filter * 1、创建的时候，就自动从容器中获取到了SessionRepostiory * 2、原始的request,response都被包装了 SessionRepositoryRequestWrapper、SessionRepositoryResponseWrapper * 3、以后获取session.request.getSession() * SessionRepositoryResponseWrapper * 4、wrappedRequest.getSession() ==&gt;SessionRepository * * 装饰者模式 * spring-redis的相关功能: * 执行session相关操作后，redis里面存储的时间也会刷新 */ 核心源码是： SessionRepositoryFilter 类下面的 doFilterInternal 方法 及那个 request、response 包装成 SessionRepositoryRequestWrapper 11、单点登录和社交登录 11.1 社交登录 QQ、微博，github等网站的用户量非常大，别的网站为了简化网站的登陆和注册逻辑，引入社交登录功能 步骤 1、用户点击 QQ按钮 2、引导跳转进 QQ 授权页 3、用户主动点击授权，跳回之前网页 11.1.1 OAuth2.0 **OAuth：**OAuth（开放授权）是一个开放标准，允许用户授权第三方网站访问他们存储在另外的服务提供者上的信息，而不需要将用户名和密码提供给第三方网站或分享他们的数据的内容 **OAuth2.0：**对于用户相关的 OpenAPI（例如获取用户信息，动态同步，照片，日志，分享等），为了保存用户数据的安全和隐私，第三方网站访问用户数据前都需要显示向用户授权 官方版流程 文档地址： 相关流程分析 11.2 微博登录准备工作 1、进入微博开放平台 2、登录微博，进入微连接，选择网站接入 3、选择立即接入 4、创建自己的应用 5、我们可以在开发阶段 6、进入高级信息 7、添加测试账号 8、进入文档 11.3 微博登录代码实现 登陆注册流程图 看不清，放大一点 微博登录流程 注册流程 账号密码登录流程： 手机验证码发送流程： 11.3.1 查看微博开放平台文档 https://open.weibo.com/wiki/授权机制说明 11.3.2 点击微博登录后，跳转到微博授权页面 11.3.3 用户授权后调用回调接口，并带上参数code换取AccessToken 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * 回调接口 * @param code * @return * @throws Exception */ @GetMapping(&quot;/oauth2.0/weibo/success&quot;) public String weibo(@RequestParam(&quot;code&quot;) String code) throws Exception { // 1、根据code换取accessToken Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(&quot;client_id&quot;, &quot;1133714539&quot;); map.put(&quot;client_secret&quot;, &quot;f22eb330342e7f8797a7dbe173bd9424&quot;); map.put(&quot;grant_type&quot;, &quot;authorization_code&quot;); map.put(&quot;redirect_uri&quot;, &quot;http://auth.gulimall.com/oauth2.0/weibo/success&quot;); map.put(&quot;code&quot;, code); HttpResponse response = HttpUtils.doPost(&quot;https://api.weibo.com&quot;, &quot;/oauth2/access_token&quot;, &quot;post&quot;, new HashMap&lt;&gt;(), map, new HashMap&lt;&gt;()); // 状态码为200请求成功 if (response.getStatusLine().getStatusCode() == 200 ){ // 获取到了accessToken String json = EntityUtils.toString(response.getEntity()); SocialUser socialUser = JSON.parseObject(json, SocialUser.class); R r = memberFeignService.OAuthlogin(socialUser); if (r.getCode() == 0) { MemberRespVo data = r.getData(&quot;data&quot;, new TypeReference&lt;MemberRespVo&gt;() { }); log.info(&quot;登录成功:用户:{}&quot;,data.toString()); // 2、登录成功跳转到首页 return &quot;redirect:http://gulimall.com&quot;; } else { // 注册失败 return &quot;redirect:http://auth.gulimall.com/login.html&quot;; } } else { // 请求失败 // 注册失败 return &quot;redirect:http://auth.gulimall.com/login.html&quot;; } // 2、登录成功跳转到首页 return &quot;redirect:http://gulimall.com&quot;; } 11.3.4 拿到AccessToken 请求对应接口拿到信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Overridepublic MemberEntity login(SocialUser vo) { // 登录和注册合并逻辑 String uid = vo.getUid(); MemberDao memberDao = this.baseMapper; // 根据社交用户的uuid查询 MemberEntity memberEntity = memberDao.selectOne(new QueryWrapper&lt;MemberEntity&gt;() .eq(&quot;social_uid&quot;, uid)); // 能查询到该用户 if (memberEntity != null ){ // 更新对应值 MemberEntity update = new MemberEntity(); update.setId(memberEntity.getId()); update.setAccessToken(vo.getAccess_token()); update.setExpiresIn(vo.getExpires_in()); memberDao.updateById(update); memberEntity.setAccessToken(vo.getAccess_token()); memberEntity.setExpiresIn(vo.getExpires_in()); return memberEntity; } else { // 2、没有查询到当前社交用户对应的记录就需要注册一个 MemberEntity regist = new MemberEntity(); try { Map&lt;String,String&gt; query = new HashMap&lt;&gt;(); // 设置请求参数 query.put(&quot;access_token&quot;,vo.getAccess_token()); query.put(&quot;uid&quot;,vo.getUid()); // 发送get请求获取社交用户信息 HttpResponse response = HttpUtils.doGet(&quot;https://api.weibo.com/&quot;, &quot;2/users/show.json&quot;, &quot;get&quot;, new HashMap&lt;&gt;(), query); // 状态码为200 说明请求成功 if (response.getStatusLine().getStatusCode() == 200){ // 将返回结果转换成json String json = EntityUtils.toString(response.getEntity()); // 利用fastjson将请求返回的json转换为对象 JSONObject jsonObject = JSON.parseObject(json); // 拿到需要的值 String name = jsonObject.getString(&quot;name&quot;); String gender = jsonObject.getString(&quot;gender&quot;); //.. 拿到多个信息 regist.setNickname(name); regist.setGender(&quot;m&quot;.equals(gender) ? 1 : 0); } } catch (Exception e) { e.printStackTrace(); } // 设置社交用户相关信息 regist.setSocialUid(vo.getUid()); regist.setAccessToken(vo.getAccess_token()); regist.setExpiresIn(vo.getExpires_in()); memberDao.insert(regist); return regist; }} 11.2 SSO(单点登录) 1、什么是SSO 单点登录(SingleSignOn，SSO)，就是通过用户的一次性鉴别登录。当用户在身份认证服务器上登录一次以后，即可获得访问单点登录系统中其他关联系统和应用软件的权限，同时这种实现是不需要管理员对用户的登录状态或其他信息进行修改的，这意味着在多个应用系统中，用户只需一次登录就可以访问所有相互信任的应用系统。这种方式减少了由登录产生的时间消耗，辅助了用户管理，是目前比较流行的 [1] 2、前置知识 https://gitee.com/xuxueli0323/xxl-sso 3、同域下的单点登录 4、不同域下的单点登录 5、单点登录框架 &amp; 原理演示 XXL-SSO 是一个分布式单点登录框架。只需要登录一次就可以访问所有相互信任的应用系统。 拥有&quot;轻量级、分布式、跨域、Cookie+Token均支持、Web+APP均支持&quot;等特性。现已开放源代码，开箱即用。 首先对整个项目进行：mvn clean package -Dmaven.skip.test=true xxl-sso-server: 8080/xxl-sso-server 编排： ssoserver.com 登陆验证服务器 client1.com 客户端1 client2.com 客户端2 先启动xxl-sso-server 然后启动client1 只要 client1 登录成功 client2 就不用进行登录直接登录成功 代码测试: sso-client 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * @author gcq * @Create 2020-11-12 */@Controllerpublic class HelloController { @Value(&quot;${sso.server.url}&quot;) private String ssoServerUrl; /** * 无需登录就可以访问 * @return */ @ResponseBody @RequestMapping(&quot;/hello&quot;) public String hello() { return &quot;hello&quot;; } /** * 需要验证的连接 * @param model * @param token 只要是ssoserver登陆成功回来就会带上 * @return */ @GetMapping(&quot;/employees&quot;) public String employees(Model model, HttpSession session, @RequestParam(value=&quot;token&quot;,required = false) String token) { if (!StringUtils.isEmpty(token)) { // 去ssoserver登录成功调回来就会带上 //TODO 1、去ssoserver获取当前token真正对应的用户信息 RestTemplate restTemplate = new RestTemplate(); // 使用restTemplate进行远程请求 ResponseEntity&lt;String&gt; forEntity = restTemplate.getForEntity(&quot;http://ssoserver.com:8080/userInfo?token=&quot; + token, String.class); // 拿到数据 String body = forEntity.getBody(); // 设置到session中 session.setAttribute(&quot;loginUser&quot;,body); } Object loginUser = session.getAttribute(&quot;loginUser&quot;); if (loginUser == null ){ // 没有登录重定向到登陆页面，并带上当前地址 return &quot;redirect:&quot; + ssoServerUrl + &quot;?redirect_url=http://client1.com:8081/employees&quot;; } else { List&lt;String&gt; emps = new ArrayList&lt;&gt;(); emps.add(&quot;张三&quot;); emps.add(&quot;李四&quot;); model.addAttribute(&quot;emps&quot;,emps); return &quot;list&quot;; } }} sso-server 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * @author gcq * @Create 2020-11-12 */@Controllerpublic class LoginController { @Autowired StringRedisTemplate redisTemplate; /** * 根据token从redis中查询用户信息 * @param token * @return */ @ResponseBody @GetMapping(&quot;/userInfo&quot;) public String userInfo(@RequestParam(&quot;token&quot;) String token) { String s = redisTemplate.opsForValue().get(token); return s; } @GetMapping(&quot;login.html&quot;) public String login(@RequestParam(&quot;redirect_url&quot;) String url, Model model, @CookieValue(value = &quot;sso_token&quot;,required = false)String sso_token) { if (!StringUtils.isEmpty(sso_token)) { //说明有人之前登录过，给浏览器留下了痕迹 return &quot;redirect:&quot; + url + &quot;?token=&quot; + sso_token; } // 添加url到model地址中，在前端页面进行取出 model.addAttribute(&quot;url&quot;,url); return &quot;login&quot;; } /** * 登录 * @param username * @param password * @param url client端带过来的地址 * @return */ @PostMapping(&quot;/doLogin&quot;) public String doLogin(@RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, @RequestParam(&quot;url&quot;) String url, HttpServletResponse response){ // 账号密码不为空 if (!StringUtils.isEmpty(username) &amp;&amp; !StringUtils.isEmpty(password)) { // 登陆成功 // 把登录成功的用户存起来 String uuid = UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;); redisTemplate.opsForValue().set(uuid,username); // 将uuid存入cookie Cookie token = new Cookie(&quot;sso_token&quot;,uuid); response.addCookie(token); // 保存到cookie return &quot;redirect:&quot; + url + &quot;?token=&quot; + uuid; } // 登录失败，展示登录页 return &quot;login&quot;; }} 6、使用 jwt 11.3 JWT 12、购物车 12.1 购物车需求 1、需求描述： 用户可以在登录状态下将商品添加到购物车**【登录购物车/在线购物车】** 放入数据库 mongodb 放入 redis（采用） 用户可以在未登录状态下将商品添加到购物车**【游客购物车/离线购物车】** 放入 localstorage cookie WebSQL 放入 redis (采用) 用户可以使用购物车一起结算下单 用户可以查询自己的购物车 用户可以在购物中修改购买商品的数量 用户可以在购物车中删除商品 选中不选中商品 在购物车中展示商品优惠信息 提示购物车商品价格变化 购物车数据结构 每一个购物项信息，都是一个对象，基本字段包括 12345678skuId:123123, // 商品idcheck:true, // 是否选中title:&quot;apple phone&quot;, // 商品标题defaultImage:'', // 商品默认显示的图片price:4999, // 商品价格count:1, // 商品数量totalPrice:4999 // 购物车中选中商品总价格skuSaleVo:{} // 。。。。 另外，购物车中不止一条数据，因此最终会是对象的数组 1{...},{....},{....} Redis有5种不同数据结构，这里选择哪一种 比较合适呢? Map&lt;String, List&gt; 首先不同用户应该有独立的购物车。 因此购物车应该以用户的作为key来存储，Value是用户的所有购物车信息。这样看来基本的 k-v 结构就可以了。 但是，我们对购物车中的商品进行增、删、改操作，基本都需要根据商品id进行判断，为了方便后期处理，我们的购物车也应该是 k-v 结构，key 是商品 id, value 才是这个商品的购物车信息。 综上所述，我们的购物车结构是一一个双层Map: Map&lt;String,Map&lt;String,String&gt;&gt;，在redis中类型为 hash 外头key就是用户的id,里面的key就是skuid redis中： 新增商品：hset gulimall:cart:7 (key)334488[商品skuid] (value)商品的信息 增加商品数量：hincrby shopcar:uuid1024 334477 1 商品总数：hlen shopcar:uuid 全部选择：hgetall shopcar:uuid1024 12.2 Vo编写 &amp; ThreadLocal身份验证 12.2.1 Vo 编写 购物车 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * 整个购物车 * 需要计算的属性，必须重写他的get方法，保证每次获取属性都会进行计算 * @author gcq * @Create 2020-11-13 */public class Cart { /** * 商品项 */ List&lt;CartItem&gt; items; /** * 商品数量 */ private Integer countNum; /** * 商品类型数量 */ private Integer countType; /** * 商品总价 */ private BigDecimal totalAmount; /** * 减免价格 */ private BigDecimal reduce = new BigDecimal(&quot;0&quot;);; public Integer getCountType() { int count = 0; if (items !=null &amp;&amp; items.size() &gt; 0) { for (CartItem item : items) { count+=1; } } return count; } public BigDecimal getTotalAmount() { BigDecimal amount = new BigDecimal(&quot;0&quot;); // 1、计算购物项总价 if (items != null &amp;&amp; items.size() &gt; 0) { for (CartItem item : items) { // 拿到购物车中单个商品的总金额 BigDecimal totalPrice = item.getTotalPrice(); // 添加到购物总价中 amount = amount.add(totalPrice); } } // 2、减去优惠总价 BigDecimal subtract = amount.subtract(getReduce()); return subtract; } } 购物车中的实体 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 购物项 * @author gcq * @Create 2020-11-13 */public class CartItem { /** * 商品id */ private Long skuId; /** * 购物车中是否选中 */ private Boolean check = true; /** * 商品的标题 */ private String title; /** * 商品的图片 */ private String image; /** * 商品的属性 */ private List&lt;String&gt; skuAttr; /** * 商品的价格 */ private BigDecimal price; /** * 商品的数量 */ private Integer count; /** * 购物车价格 使用自定义get、set */ private BigDecimal totalPrice; /** * 计算购物项价格 * @return */ public BigDecimal getTotalPrice() { //价格乘以数量 return this.price.multiply(new BigDecimal(&quot;&quot; + this.count)); } 12.2.3 ThreadLocal 身份验证 需求分析： 12345678910/** * 浏览器有一个cookie：user-key,标识用户身份，一个月后过期 * 如果第一次使用jd的购物车功能，都会给一个临时的用户身份 * 浏览器以后保存，每次访问都会带上这个cookie * * 登录 session 有 * 没登录，按照cookie中的user-key来做 * 第一次：如果没有临时用户，帮忙创建一个临时用户 --&gt; 使用拦截器 * @return */ 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071** * 在执行目标方法之前,判断用户的登录状态，并封装给Controller目标请求 * @author gcq * @Create 2020-11-13 */public class CartInterceptor implements HandlerInterceptor { public static ThreadLocal&lt;UserInfo&gt; threadLocal = new ThreadLocal&lt;&gt;(); /** * 目标方法执行之前 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { UserInfo userInfo = new UserInfo(); // 拿到session HttpSession session = request.getSession(); // 查看session中是否保存了用户的值 MemberRespVo member = (MemberRespVo) session.getAttribute(AuthServerConstant.LOGIN_USER); if (member != null) { // 用户登录 userInfo.setUserId(member.getId()); } Cookie[] cookies = request.getCookies(); if (cookies != null &amp;&amp; cookies.length &gt; 0) { for (Cookie cookie : cookies) { String name = cookie.getName(); // 拿到cookie名字进行判断如果包含 user-key 就复制到userInfo中 if (name.equals(CartConstant.TEMP_USER_COOKIE_NAME)) { userInfo.setUserKey(cookie.getValue()); userInfo.setTempUser(true); } } } // 如果没有临时用户一定要分配一个临时用户 if (StringUtils.isEmpty(userInfo.getUserKey())) { String uuid = UUID.randomUUID().toString(); userInfo.setUserKey(uuid); } // 全部放行 threadLocal.set(userInfo); return true; } /** * 业务执行之后,分配临时用户，让浏览器保存 * @param request * @param response * @param handler * @param modelAndView * @throws Exception */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { // 拿到用户信息 UserInfo userInfo = threadLocal.get(); // 如果没有临时用户一定要保存一个临时用户 if (!userInfo.isTempUser()) { // 持续的延长临时用户的过期时间 Cookie cookie = new Cookie(CartConstant.TEMP_USER_COOKIE_NAME, userInfo.getUserKey()); cookie.setDomain(&quot;gulimall.com&quot;); cookie.setMaxAge(CartConstant.TEMP_USER_COOKIE_TIMEOUT); response.addCookie(cookie); } }} 12.3 添加购物车 在页面点击加入购物车后将商品添加进购物车 需求分析： 首先需要在页面拿到要提交的参数，skuid，购买数量，并提交后台完成购物车数据添加 后端如何处理这个数据？ 通过 skuid 远程查询这个商品的信息 远程查询sku组合的信息 如果购物车中已经有该数据如何进行提交？ 如果有该数据，那么就行更改，根据 skuid 从 reids 中取出数据 将数据转换成对象然后加上对应的数量，再次转换为json存入redis 前端页面频繁添加购物车如何解决？ 请求发送过来后我们重定向到其他的页面用来显示数据，这时候用户刷新的话也是在其他页面进行刷新 1234567891011/** * 加入到购物车 */ $(&quot;#addToCartA&quot;).click(function() { // 购买的数量 var num = $(&quot;#numInput&quot;).val(); // skuid var skuid = $(this).attr(&quot;skuId&quot;); location.href=&quot;http://cart.gulimall.com/addToCart?skuId=&quot; + skuid + &quot;&amp;num=&quot; + num; return false }) 请求发送到Controller后 1234567891011@GetMapping(&quot;/addToCart&quot;)public String addToCart(@RequestParam(&quot;skuId&quot;) Long skuId, @RequestParam(&quot;num&quot;) Integer num, RedirectAttributes res) throws ExecutionException, InterruptedException { cartService.addToCart(skuId,num); //将数据拼接到url后面 res.addAttribute(&quot;skuId&quot;,skuId); // 重定向到对应的地址 return &quot;redirect:http://cart.gulimall.com/addToCartSuccess.html&quot;;} Service 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 给购物车里面添加商品 * @param skuId 商品id * @param num 数量 * @throws ExecutionException * @throws InterruptedException * 如果远程查询比较慢，比如方法当中有好几个远程查询，都要好几秒以上，等整个方法返回就需要很久，这块你是怎么处理的? * 1、为了提交远程查询的效率，可以使用线程池的方式，异步进行请求 * 2、要做的操作就是将所有的线程全部放到自己手写的线程池里面 * 3、每一个服务都需要配置一个自己的线程池 * 4、完全使用线程池来控制所有的请求 */@Overridepublic void addToCart(Long skuId, Integer num) throws ExecutionException, InterruptedException { // 获取到要操作的购物车 BoundHashOperations&lt;String, Object, Object&gt; cartOps = getCartOps(); // 1、如果购物车中已有该商品那么需要做的就是修改商品的数量，如果没有该商品需要进行添加该商品到购物车中 String result = (String) cartOps.get(skuId.toString()); // 商品id作为键，先去Redis里面查看有没有这个相同的商品 if (StringUtils.isEmpty(result)) { // 如果返回为空，那么说明购物车中没有这个商品，那就要执行添加 // 2、添加商品到购物车 // 第一个异步任务,查询当前商品信息 CartItem cartItem = new CartItem(); // 购物车中每一个都是一个购物项，封装购物车的内容 CompletableFuture&lt;Void&gt; completableFutureSkuInfo = CompletableFuture.runAsync(() -&gt; { // 2.1、远程查询出当前要添加的商品信息 // 添加哪个商品到购物车，先查询到这个商品的信息 R r = productFeignService.getSkuInfo(skuId); // 获取远程返回的数据，远程数据都封装在skuInfo中 SkuInfoVo skuInfo = r.getData(&quot;skuInfo&quot;, new TypeReference&lt;SkuInfoVo&gt;() { }); cartItem.setCheck(true); // 添加商品到购物车，那么这个商品一定是选中的 cartItem.setSkuId(skuInfo.getSkuId()); // 查询的是哪个商品的id，那么这个商品id就是哪个 cartItem.setImage(skuInfo.getSkuDefaultImg()); // 当前商品的图片信息 cartItem.setPrice(skuInfo.getPrice()); // 当前商品的价格 cartItem.setTitle(skuInfo.getSkuTitle()); // 当前商品的标题 cartItem.setCount(num); // 当前商品添加的数量 }, executor); // 3、第二个异步任务，远程查询sku组合信息 CompletableFuture&lt;Void&gt; completableFutureSkuAttr = CompletableFuture.runAsync(() -&gt; { List&lt;String&gt; skuSaleAttrValues = productFeignService.getSkuSaleAttrValues(skuId); // 根据skuid来查 cartItem.setSkuAttr(skuSaleAttrValues); // 远程查询出来到sku组合信息，需要在购物车中进行显示 }, executor); // 4、两个异步任务都完成，才能把数据放到redis中 CompletableFuture.allOf(completableFutureSkuInfo,completableFutureSkuAttr).get(); // 把购物项的数据保存redis中 String cartItemJson = JSON.toJSONString(cartItem); cartOps.put(skuId.toString(),cartItemJson); // 添加商品到购物车中 } else { // 如果购物车中有这个商品，那么需要做的就是将商品的数量进行更改,也即是新添加的商品数量加上当前购物车中商品数量 CartItem cartItem = JSON.parseObject(result, CartItem.class); cartItem.setCount(cartItem.getCount() + num); String cartItemJson = JSON.toJSONString(cartItem); // 更新redis cartOps.put(skuId.toString(),cartItemJson); }} 解决刷新问题 1234567891011121314/** * 跳转到成功页面 * @param skuId * @param model * @return */@GetMapping(&quot;/addToCartSuccess.html&quot;)public String addToCartSuccessPage(@RequestParam(&quot;skuId&quot;) Long skuId,Model model) { // 重定向到成功页面，再次查询购物车数据 CartItem item = cartService.getCartItem(skuId); model.addAttribute(&quot;item&quot;,item); return &quot;success&quot;;} 12.4 获取合并购物车 12.4.1 需求分析 如果用户登录，临时购物车的数据如何显示？ 将临时购物车的数据合并到用户购物车中 如何显示购物车的数据？ 从 redis 取出数据放到对象中并渲染出来 12.4.2 代码编写 Controller 123456@GetMapping(&quot;/cart.html&quot;)public String cartListPage(Model model) throws ExecutionException, InterruptedException { Cart cart = cartService.getCart(); model.addAttribute(&quot;cart&quot;,cart); return &quot;cartList&quot;;} Service 拼装 Key 从 redis 中查询数据 临时购物车如果有数据，当前状态是登录，那就将临时购物车的数据合并到当前用户的购物车 123456789101112131415161718192021222324252627282930313233343536373839@Override public Cart getCart() throws ExecutionException, InterruptedException { /** * 需求分析1 * 1、如果用户登录后，那么临时购物车的数据如何处理？ * 将临时购物车的数据，放到用户购物车中进行合并 * 2、如何显示购物车中的数据？ * 从redis中查询到数据放到对象中，返回到页面进行渲染 */ Cart cart = new Cart(); // 1、购物车的获取操作，分为登录后购物车 和 没登录购物车 UserInfo userInfo = CartIntercept.threadLocal.get(); // 拿到共享数据 if (userInfo.getUserId() != null) { // 登录后的购物车 String cartKey = CART_PREFIX + userInfo.getUserKey(); String tempCartKey = CART_PREFIX + userInfo.getUserKey(); // 如果临时购物车中还有数据，那就需要将临时购物车合并到已登录购物车里面 // 判断临时购物车中是否有数据 List&lt;CartItem&gt; cartItems = getCartItems(tempCartKey); if (cartItems != null ) { // 临时购物车中有数据，需要将临时购物车的数据合并到登录后的购物车 for (CartItem item : cartItems) { // 拿到临时购物车的所有数据，将他添加到已登录购物车里面来 // 调用addToCart()这个方法，他会根据登录状态进行添加，当前是登录状态 // 所以这个方法会将临时购物车的数据添加到已登录购物车中 addToCart(item.getSkuId(),item.getCount()); // 合并临时和登录后的购物车 } // 合并完成后还需要将临时购物车中的数据删除 clearCart(tempCartKey); } // 再次获取用户登录后的购物车【包含临时购物车数据和已登录购物车数据】 List&lt;CartItem&gt; cartItemList = getCartItems(cartKey); cart.setItems(cartItemList);// 将多个购物项设置到购物车中 } else { // 没有登录的购物车，拿到没有登录购物车的数据 String cartKey = CART_PREFIX + userInfo.getUserKey(); // 获取购物车中的所有购物项 List&lt;CartItem&gt; cartItems = getCartItems(cartKey); cart.setItems(cartItems); } return cart; } getCaerItems 123456789101112131415161718/** * 获取指定用户 (登录用户/临时用户) 购物车里面的数据 * @param cartKey * @return */ private List&lt;CartItem&gt; getCartItems(String cartKey) { BoundHashOperations&lt;String, Object, Object&gt; operations = redisTemplate.boundHashOps(cartKey); List&lt;Object&gt; values = operations.values(); // 拿到多个购物项 if (values != null) { List&lt;CartItem&gt; cartItemList = values.stream().map((obj) -&gt; { String s = (String) obj; CartItem cartItem = JSON.parseObject(s, CartItem.class); return cartItem; }).collect(Collectors.toList()); return cartItemList; } return null; } 12.5 选中购物项 &amp; 改变购物项的数量 &amp; 删除购物项 12.5.1 需求分析： 1、选中购物项 在页面中选中购物项后，数据应该要和redis中的数据进行同步 页面选中，reids中数据就要更改 2、改变购物项数量 购物车中，增加了商品的数量，对应的价格，总价也需要进行改变 3、删除购物项 在购物项中删除该条数据，从redis中根据skuid删除这条记录 4、将数据修改或删除后，重新跳转到cart.html 重新加载数据 12.5.2 代码实现 前端页面方法 123456789101112131415161718192021// 选中购物项 $(&quot;.itemCheck&quot;).click(function () { var skuId = $(this).attr(&quot;skuId&quot;); var check = $(this).prop(&quot;checked&quot;); location.href = &quot;http://cart.gulimall.com/checkItem?skuId=&quot; + skuId + &quot;&amp;check=&quot; + (check?1:0); }) // 改变购物项数量 $(&quot;.countOpsBtn&quot;).click(function() { var skuId = $(this).parent().attr(&quot;skuId&quot;); var num = $(this).parent().find(&quot;.countOpsNum&quot;).text(); location.href = &quot;http://cart.gulimall.com/countItem?skuId=&quot; + skuId + &quot;&amp;num=&quot; + num; }) var deleteId = 0; // 删除购物项 function deleteItem() { location.href = &quot;http://cart.gulimall.com/deleteItem?skuId=&quot; + deleteId; } $(&quot;.deleteItemBtn&quot;).click(function() { deleteId = $(this).attr(&quot;skuId&quot;); }) Controller 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * @需求描述: 系统管理员 购物车组 模块 用户选中购物项更新redis中对象 * @创建人: * @创建时间: 2021/01/04 15:37 * @修改需求: * @修改人: * @修改时间: * @需求思路: */ @GetMapping(&quot;/checkItem&quot;) public String checkItem(@RequestParam(&quot;skuId&quot;) Long skuId, @RequestParam(&quot;checked&quot;) Integer checked) { cartService.checkItem(skuId,checked); // 重定向到购物车页面，重新获取购物车数据 return &quot;redirect:http://cart.gulimall.com/cart.html&quot;; } /** * @需求描述: 系统管理员 购物车组 模块 用户更改购物车中购物项的数量 * @创建人: * @创建时间: 2021/01/04 16:05 * @修改需求: * @修改人: * @修改时间: * @需求思路: */ @GetMapping(&quot;/updateItem&quot;) public String updateItem(@RequestParam(&quot;skuId&quot;) Long skuId, @RequestParam(&quot;count&quot;) Integer count) { cartService.updateItem(skuId,count); return &quot;redirect:http://cart.gulimall.com/cart.html&quot;; } /** * @需求描述: 系统管理员 购物车服务 模块 用户删除购物车中的购物项 * @创建人: * @创建时间: 2021/01/04 16:40 * @修改需求: * @修改人: * @修改时间: * @需求思路: */ @GetMapping(&quot;/deleteItem&quot;) public String deleteItem(@RequestParam(&quot;skuId&quot;) Long skuId) { cartService.deleteItem(skuId); return &quot;redirect:http://cart.gulimall.com/cart.html&quot;; } Service 1234567891011121314151617181920212223242526@Override public void checkItem(Long skuId, Integer checked) { // 1、从redis中查出购物项设置购物项是否选中。 BoundHashOperations&lt;String, Object, Object&gt; cartOps = getCartOps(); String cart = (String) cartOps.get(skuId.toString()); CartItem cartItem = JSON.parseObject(cart, CartItem.class); cartItem.setCheck(checked == 1 ? true : false); String jsonStirng = JSON.toJSONString(cartItem); cartOps.put(skuId.toString(),jsonStirng); // 更新redis } @Override public void updateItem(Long skuId, Integer count) { // 1、用户在页面对某个购物项增加或减少购物项的数量，那么redis中应该也要进行更新 CartItem cartItem = getCartItem(skuId); cartItem.setCount(count); BoundHashOperations&lt;String, Object, Object&gt; cartOps = getCartOps(); cartOps.put(skuId.toString(),JSON.toJSONString(cartItem)); // 更新redis中数据 } @Override public void deleteItem(Long skuId) { // 获取到当前购物车，然后根据id删除购物车里面的购物项 BoundHashOperations&lt;String, Object, Object&gt; cartOps = getCartOps(); cartOps.delete(skuId.toString()); } 13、消息队列 - MQ 13.1 RabbitMQ 异步处理 消息发送的时间取决于业务执行的最长的时间 应用解耦 原本是需要订单系统直接调用库存系统 只需要将请求发送给消息队列，其他的就不需要去处理了，节省了处理业务逻辑的时间 流量消峰 某一时刻如果请求特别的大，那就先把它放入消息队列，从而达到流量消峰的作用 流程图地址：https://www.processon.com/view/link/5fbda8c35653bb1d54f7077b 13.2 概述 大多应用中，可通过消息服务中间件来提升系统异步通信，扩展解耦能力 消息服务中两个重要概念： 消息代理（message broker） 和 目的地（destination） 当消息发送者发送消息后，将由消息代理接管，消息代理保证消息传递到指定目的地 消息队列主要有两种形式的目的地 队列（Queue）:点对点消息通信（point - to - point） 主题（topic）：发布（publish）/订阅（subscribe）消息通信 点对点式： 消息发送者发送消息，消息代理将其放入一个队列中，消息接收者从队列中获取消息内容，消息读取后被移出队列 消息只有唯一的发送者和接受者，单并不是说只能有一个接收者 发布订阅式: 发送者（发布者）发到消息到主题，多个接收者（订阅者）监听（订阅）这个主题，那么就会在消息到达时同时收到消息 JMS（Java Message Service） Java消息服务： 基于JVM消息代理的规范，ActiveMQ、HornetMQ是JMS的实现 AMQP（Advanced Message Queuing Protocol） 高级消息队列协议，也是一个消息代理的规范，兼容JMS RabbitMQ是AMQP的实现 Spring 支持 spring - jms提供了对JMS的支持 spring - rabbit提供了对AMQP的支持 需要ConnectionFactory的实现来连接消息代理 提供 JmsTemplate、RabbitTemplate 来发送消息 @JmsListener（JMS）、@RabbitListener（AMQP）注解在方法上监听消息代理发布的消息 @EnableJms、@EnableRabbit开启支持 Spring Boot 自动配置 JmsAutoConfiguration RabbitAutoConfiguration 市面上的MQ产品 ActiveMQ、RabbitMQ、RocketMQ，kafka 13.3 RabbitMQ概念 RabbitMQ简介： RabbitMQ是一由erlang开发的AMQP（Advanved Message Queue Protocol）的开源实现 核心概念 Message 消息，消息是不具名的，它是由消息头和消息体组成，消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括 routing - key （路由键），priority（相对于其他消息的优先权），**delivery - mode（指出该消息可能需要持久性存储）**等 Publisher 消息的生产者，也是一个像交换器发布消息的客户端应用程序 Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列 Exchange有4种类型：direct(默认)、fanout、topic，和heades，不同类型的 Exchange 转发消息的策略有所区别 Queue 消息队列，用来保存消息直到发送给消费者，他是消息的容器，也是消息的重点，一个消息可以投入一个或多个队列，消息一直在队列里面，等待消费者连接到这个队列将其取走 Binding 绑定，用于消息队列和交换器之间的关联，一个绑定就是基于路由键将交换器和消息队列连接起来的规则，所有可以将交换器理解成一个由绑定构成的路由表 Connection 网路连接，比如一个TCP连接 Channel 信道，多路复用连接中的一个独立的双向数据流通道，信道是建立在真实的TCP连接的内的虚拟连接，AMQP 命令都是通过信息到发送出去的，不管是发布消息，订阅队列还是接收消息，这些动作都是通过队列完成，因为对应操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条TCP连接 Consumer 消息的消费者，表示一个消息队列中取得消息的客户端应用程序 Virtual Host 虚拟主机，表示交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个Virtual host本质上就是一个 mini 版的RabbitMQ 服务器,拥有自己的队列、交换器、绑定和权限机制。Virtual host 是 AMQP 概念的基础，必须在连接时指定,RabbitMQ默认的vhost是/。 Broker 表示消息队列服务器实体 13.4 Docker 安装RabbitMQ 12345678910docker run -d --name rabbitmq -p 5671:5671 -p 5672:5672 -p 4369:4369 -p 25672:25672 -p 15671:15671 -p 15672:15672 rabbitmq:management4369, 25672 (Erlang发现&amp;集群端口)5672, 5671 (AMQP端口)15672 (web管理后台端口)61613, 61614 (STOMP协议端口)1883, 8883 (MQTT协议端口) # 自动启动docker update rabbitmq --restart=always 13.5 RabbitMQ 运行机制 AMQP 中的消息路由 AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP中增加了 Exchange 和 Binding 的角色 生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送给那个队列 Exchange 类型 Exchange 分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、tanout、topic、headers header匹配AMQP消息的 header 而不是路由键，headers 交换器和 direct 交换器完全一致，但性能差能多，目前几乎用不到了，所以直接看另外三种类型 13.6 RabbitMQ 整合 1、引入 Spring-boot-starter-amqp 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; 2、application.yml配置 12345spring: rabbitmq: host: 192.168.56.10 port: 5672 virtual-host: / 3、测试RabbitMQ 1、AmqpAdmin:管理组件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 /** * 创建Exchange * 1、如何利用Exchange,Queue,Binding * 1、使用AmqpAdmin进行创建 * 2、如何收发信息 */ @Test public void contextLoads() { // public DirectExchange( // String name, 交换机的名字 // boolean durable, 是否持久 // boolean autoDelete, 是否自动删除 // Map&lt;String, Object&gt; arguments) // { DirectExchange directExchange = new DirectExchange(&quot;hello-java.exchange&quot;,true,false); amqpAdmin.declareExchange(directExchange); log.info(&quot;Exchange[{}]创建成功：&quot;,&quot;hello-java.exchange&quot;); } /** * 创建队列 */ @Test public void createQueue() { // public Queue(String name, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) { Queue queue = new Queue(&quot;hello-java-queue&quot;,true,false,false); amqpAdmin.declareQueue(queue); log.info(&quot;Queue[{}]:&quot;,&quot;创建成功&quot;); }/** * 绑定队列 */ @Test public void createBinding() { // public Binding(String destination, 目的地 // DestinationType destinationType, 目的地类型 // String exchange,交换机 // String routingKey,//路由键 Binding binding = new Binding(&quot;hello-java-queue&quot;, Binding.DestinationType.QUEUE, &quot;hello-java.exchange&quot;, &quot;hello.java&quot;,null); amqpAdmin.declareBinding(binding); log.info(&quot;Binding[{}]创建成功&quot;,&quot;hello-java-binding&quot;); } 2、RabbitTemplate：消息发送处理组件 12345678910111213141516171819202122@Autowired @Test public void sendMessageTest() { for(int i = 1; i &lt;=5; i++) { if(i%2==0) { OrderReturnReasonEntity reasonEntity = new OrderReturnReasonEntity(); reasonEntity.setId(1l); reasonEntity.setCreateTime(new java.util.Date()); reasonEntity.setName(&quot;哈哈&quot;); // String msg = &quot;Hello World&quot;; // 发送的对象类型的消息，可以是一个json rabbitTemplate.convertAndSend(&quot;hello-java.exchange&quot;,&quot;hello.java&quot;,reasonEntity); } else { OrderEntity orderEntity = new OrderEntity(); orderEntity.setOrderSn(UUID.randomUUID().toString()); rabbitTemplate.convertAndSend(&quot;hello-java.exchange&quot;,&quot;hello.java&quot;,orderEntity); } log.info(&quot;消息发送完成{}&quot;); } } 13.7 RabbitMQ消息确认机制 - 可靠到达 保证消息不丢失，可靠抵达，可以使用事务消息，性能下降250倍，为此引入确认机制 publisher confirmCallback 确认模式 publisher returnCallback 未投递到 queue 退回 consumer ack 机制 可靠抵达 - ConfirmCallback spring.rabbitmq.publisher-confirms=true 在创建 connectionFactory 的时候设置 PublisherConfirms(true) 选项，开启 confirmcallback。 CorrelationData 用来表示当前消息唯一性 消息只要被 broker 接收到就会执行 confirmCallback,如果 cluster 模式，需要所有 broker 接收到才会调用 confirmCallback 被 broker 接收到只能表示 message 已经到达服务器，并不能保证消息一定会被投递到目标 queue 里，所以需要用到接下来的 returnCallback 可靠抵达 - ReturnCallback spring.rabbitmq.publisher-retuns=true spring.rabbitmq.template.mandatory=true confirm 模式只能保证消息到达 broker，不能保证消息准确投递到目标 queue 里。在有些模式业务场景下，我们需要保证消息一定要投递到目标 queue 里，此时就需要用到 return 退回模式 这样如果未能投递到目标 queue 里将调用 returnCallback，可以记录下详细到投递数据，定期的巡检或者自动纠错都需要这些数据 可靠抵达 - Ack 消息确认机制 消费者获取到消息，成功处理，可以回复Ack给Broker basic.ack 用于肯定确认：broker 将移除此消息 basic.nack 用于否定确认：可以指定 beoker 是否丢弃此消息，可以批量 basic.reject用于否定确认，同上，但不能批量 默认，消息被消费者收到，就会从broker的queue中移除 消费者收到消息，默认自动ack，但是如果无法确定此消息是否被处理完成，或者成功处理，我们可以开启手动ack模式 消息处理成功，ack()，接受下一条消息，此消息broker就会移除 消息处理失败，nack()/reject() 重新发送给其他人进行处理，或者容错处理后ack 消息一直没有调用ack/nack方法，brocker认为此消息正在被处理，不会投递给别人，此时客户端断开，消息不会被broker移除，会投递给别人 13.8 RabbitMQ 延时队列(实现定时任务) 场景: 比如未付款的订单，超过一定时间后，系统自动取消订单并释放占有物品 常用解决方案： Spring的schedule 定时任务轮询数据库 缺点： 消耗系统内存，增加了数据库的压力，存在较大的时间误差 **解决：**rabbitmq的消息TTL和死信Exchange结合 使用场景 时效问题 上一轮扫描刚好扫描，而这个时候刚好下了订单，就没有扫描到，下一轮扫描的时候，订单还没有过期，等到订单过期后30分钟才被扫描到 消息的TTL（Time To Live） 消息的TTL 就是消息的存活时间， RabbitMQ可以对队列还有消息分别设置TTL 对队列设置就是没有消费者连着的保持时间，也可以对每一个消息单独的设置，超过了这个时间我们可以认为这个消息他死了，称之为死信 如果队列设置了，消息也设置了，那么会取小，所以一个消息如果被路由到不同的队列中，这个消息死亡时间有可能不一样的（不同队列设置），这里讲的是单个TTL 因为他是实现延时任务的关键，可以通过设置消息的 expiration 字段或者 x-message-ttl 来设置时间两者是一样的效果 Dead Letter Exchange（DLX） 一个消息在满足如下条件下，会进死信路由，记住这里是路由不是队列，一个路由可以对应很多队列，（什么是死信） 一个消息被Consumer拒收了，并且reject方法的参数里requeue是false。也就是说不被再次放在队列里，被其他消费者使用。(basic.reject/ basic.nack) requeue= false上面的消息的TTL到了，消息过期了。 队列的长度限制满了。排在前面的消息会被丢弃或者扔到死信路由上 Dead Letter Exchange其实就是一种普通的exchange, 和创建其他exchange没有两样。只是在某一个设置 Dead Letter Exchange的队列中有消息过期了自动触发消息的转发，发送到Dead Letter Exchange中去。 我们既可以控制消息在一段时间后变成死信， 又可以控制变成死信的消息被路由到某一个指定的交换机， 结合C者，其实就可以实现一个延时队列 延时队列实现 - 1 延时队列实现 - 2 代码实现： 下单场景 模式升级 代码实现： SpringBoot可以使用@Bean 来初始化Queue、exchange、Biding等 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 监听队列信息 * @param orderEntity */@RabbitListener(queues = &quot;order.release.order.queue&quot;)public void listener(OrderEntity orderEntity, Channel channel, Message message) throws IOException { System.out.println(&quot;收到过期的订单信息，准备关闭订单&quot; + orderEntity.getOrderSn()); // 确认接收到消息，不批量接收 channel.basicAck(message.getMessageProperties().getDeliveryTag(), false);}/** * 容器中的 Binding、Queue、exchange 都会自动创建，(RabbitMQ没有的情况下) * @return */@Beanpublic Queue orderDelayQueue(){ // 特殊参数 Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); // 设置交换器 map.put(&quot;x-dead-letter-exchange&quot;, &quot;order-event-exchange&quot;); // 路由键 map.put(&quot;x-dead-letter-routing-key&quot;,&quot;order.release.order&quot;); // 消息过期时间 map.put(&quot;x-message-ttl&quot;,60000); Queue queue = new Queue(&quot;order.delay.queue&quot;, true, false, false,map); return queue;}/** * 创建队列 * @return */@Beanpublic Queue orderReleaseOrderQueue() { Queue queue = new Queue(&quot;order.release.order.queue&quot;, true, false, false); return queue;}/** * 创建交换机 * @return */@Beanpublic Exchange orderEventExchange() { return new TopicExchange(&quot;order-event-exchange&quot;,true,false);}/** * 绑定关系 将delay.queue和event-exchange进行绑定 * @return */@Beanpublic Binding orderCreateOrderBingding(){ return new Binding(&quot;order.delay.queue&quot;, Binding.DestinationType.QUEUE, &quot;order-event-exchange&quot;, &quot;order.create.order&quot;, null);}/** * 将 release.queue 和 event-exchange进行绑定 * @return */@Beanpublic Binding orderReleaseOrderBinding(){ return new Binding(&quot;order.release.order.queue&quot;, Binding.DestinationType.QUEUE, &quot;order-event-exchange&quot;, &quot;order.release.order&quot;, null);} 13.9 如何保证消息可靠性 - 消息丢失 &amp; 消息重复 1、消息丢失 消息发送出去，由于网络问题没有抵达服务器 做好容错方法(try-Catch) ，发送消息可能会网络失败，失败后要有重试机制，可记录到数据库，采用定期扫描重发的方式 做好日志记录，每个消息状态是否都被服务器收到都应该记录 做好定期重发，如果消息没有发送成功，定期去数据库扫描未成功的消息进行重发 消息抵达Broker, Broker要将消息写入磁盘(持久化)才算成功。此时Broker尚未持久化完成，宕机。 publisher也必须加入确认回调机制，确认成功的消息，修改数据库消息状态。 自动ACK的状态下。消费者收到消息，但没来得及消息然后宕机 一定开启手动ACK，消费成功才移除，失败或者没来得及处理就noAck并重新入队 2、消息重复 消息消费成功，事务已经提交，ack时，机器宕机。导致没有ack成功，Broker的消息重新由unack变为ready,并发送给其他消费者 消息消费失败，由于重试机制，自动又将消息发送出去 成功消费，ack时宕机，消息由unack变为ready, Broker又重新发送 消费者的业务消费接口应该设计为幂等性的。比如扣库存有工作单的状态标志 使用防重表(redis/mysq|) ，发送消息每一 个都有业务的唯一 标识，处理过就不用处理 rabbitMQ的每一个消息都有redelivered字段， 可以获取是否是被重新投递过来的，而不是第一次投递过来的 3、消息积压 消费者积压 消费者消费能力不足积压 发送者发送流量太大 上线更多消费者，进行正常消费 上线专门的队列消费服务，将消息先批量取出来，记录数据库，离线慢慢处理 14、订单 14.1 订单中心 1、订单中心 电商系列涉及到 3 流，分别为信息流、资金流、物流，而订单系统作为中枢将三者有机的集合起来 订单模块是电商系统的枢纽，在订单这个模块上获取多个模块的数据和信息，同时对这些信息进行加工处理后流向下个环节，这一系列就构成了订单的信息疏通 1、订单构成 1、用户信息 用户信息包括是用户账号、用户等级、用户的收货地址、收货人、收货人电话、用户账号需要绑定手机号码，但是用户绑定的手机号码不一定是收货信息上的电话。用户可以添加多个收货信息，用户等级信息可以用来和促销系统进行匹配，获取商品折扣，同时用户等级还可以获取积分的奖励等 2、订单基础信息 订单基础信息是订单流转的核心，其包括订单类型，父/子订单、订单编号、订单状态、订单流转时间等 订单类型包括实体订单和虚拟订单商品等，这个根据商城商品和服务类型进行区分 同时订单都需要做父子订单处理，之前在初创公司一直只有一个订单，没有做父子订单处理后期需 要进行拆单的时候就比较麻烦，尤其是多商户商场，和不同仓库商品的时候，父子订单就是为后期做拆单准备的。 订单编号不多说了，需要强调的一点是父子订单都需要有订单编号，需要完善的时候可以对订单编号的每个字段进行统一定义和诠释。 订单状态记录订单每次流转过程，后面会对订单状态进行单独的说明。 订单流转时间需要记录下单时间，支付时间，发货时间，结束时间/关闭时间等等 3、商品信息 商品信息从商品库中获取商品的SKU信息、图片、名称、属性规格、商品单价、商户信息等，从用户 下单行为记录的用户下单数量，商品合计价格等 4、优惠信息 优惠信息记录用户参与过的优惠活动，包括优惠促销活动，比如满减、满赠、秒杀等，用户使用的优 惠卷信息，优惠卷满足条件的优惠卷需要展示出来，另外虚拟币抵扣信息等进行记录 为什么把优惠信息单独拿出来而不放在支付信息里面呢? 因为优惠信息只是记录用户使用的条目，而支付信息需要加入数据进行计算，所以做为区分。 5、支付信息 支付流水单号，这个流水单号是在唤起网关支付后支付通道返回给电商业务平台的支付流水号，财务 通过订单号和流水单号与支付通道进行对账使用。 支付方式用户使用的支付方式，比如微信支付、支付宝支付、钱包支付、快捷支付等。支付方式有时候可能有两个一-余额支付+第三方支付。 商品总金额，每个商品加总后的金额:运费，物流产生的费用;优惠总金额，包括促销活动的优惠金额， 优惠券优惠金额，虚拟积分或者虛拟币抵扣的金額，会员折扣的金额等之和;实付金额，用户实际需要 付款的金额。 用户实付金额=商品总金额+运费 - 优惠总金额 6、物流信息 物流信息包括配送方式，物流公司，物流单号，物流状态，物流状态可以通过第三方接口来 获取和向用户展示物流每个状态节点。 2、订单状态 1、待付款 用户提交订单后，订单进行预下单，目前主流电商网站都会唤起支付，便于用户快速完成支 付，需要注意的是待付款状态下可以对库存进行锁定，锁定库存需要配置支付超时时间，超 时后将自动取消订单，订单变更关闭状态。 2、已付款/代发货 用户完成订单支付，订单系统需要记录支付时间，支付流水单号便于对账，订单下放到WMS系统，仓库进行调动、配货、分拣，出库等操作 3、待收货/已发货 仓库将商品出库后，订单进入物流环节，订单系统需要同步物流信息，便于用户实时熟悉商品的物流状态 4、已完成 用户确认收货后吗，订单交易完成，后续支付则进行计算，如果订单存在问题进入售后状态 5、已取消 付款之前取消订单，包括超时未付款或用户商户取消订单都会产生这种订单状态 6、售后中 用户在付款后申请退款，或商家发货后用户申请退货 售后也同样存在各种状态，当发起售后申请后生成售后订单，售后订单状态为待审核，等待 商家审核，商家审核通过后订单状态变更为待退货，等待用户将商品寄回，商家收货后订单 状态更新为待退款状态，退款到用户原账户后订单状态更新为售后成功。 14.2 订单流程 订单流程是指从订单产生到完成整个流转的过程，从而行程了-套标准流程规则。而不同的产品类型或业务类型在系统中的流程会千差万别，比如上面提到的线上实物订单和虚拟订单的流程，线上实物订单与020订单等，所以需要根据不同的类型进行构建订单流程。不管类型如何订单都包括正向流程和逆向流程，对应的场景就是购买商品和退换货流程，正向流程就是一一个正常的网购步骤:订单生成&gt;支付订单-&gt;卖家发货一确认收货&gt;交易成功。而每个步骤的背后，订单是如何在多系统之间交互流转的， 可概括如下图 1、订单创建与支付 订单创建前需要预览订单，选择收货信息等 订单创建需要锁定库存，库存有才可创建，否则不能创建 订单创建后超时未支付需要解锁库存 支付成功后，需要进行拆单，根据商品打包方式，所在仓库，物流等进行拆单 支付的每笔流水都需要记录，以待查账 订单创建，支付成功等状态都需要给MQ发送消息，方便其他系统感知订阅 2、逆向流程 修改订单，用户没有提交订单，可以对订单一些信息进行修改，比如配送信息，优惠信息，及其他一些订单可修改范围的内容，此时只需对数据进行变更即可。 订单取消**，用户主动取消订单和用户超时未支付**，两种情况下订单都会取消订单，而超时情况是系统自动关闭订单，所以在订单支付的响应机制上面要做支付的 14.3 幂等性处理 14.4 订单业务 1、搭建环境 在订单服务下准备好页面 可以发现订单结算页，包含以下信息: 1.收货人信息:有更多地址，即有多个收货地址，其中有一个默认收货地址 2.支付方式:货到付款下在线支付，不需要后台提供 3.送货清单:配送方式(不做)及商品列表(根据购物车选中的skuld到数据库中查询) 4.发票:不做 5.优惠:查询用户领取的优惠券(不做)及可用积分(京豆) 1.1、整合SpringSession 1、引入pom 12345&lt;!--整合spring session 解决session问题--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 2、配置文件添加 123Spring: session: store-type: redis 3、启动类加注解 1@EnableRedisHttpSession 4、修改页面中登录 1234567&lt;li style=&quot;border: 0;&quot;&gt; &lt;a th:if=&quot;${session.loginUser != null }&quot; class=&quot;aa&quot;&gt;[[${session.loginUser.nickname}]]&lt;/a&gt; &lt;a th:if=&quot;${session.loginUser == null }&quot; href=&quot;http://auth.gulimall.com/login.html&quot;&gt;你好请登录&lt;/a&gt;&lt;/li&gt;&lt;li&gt; &lt;a th:if=&quot;${session.loginUser == null }&quot; style=&quot;color: red;&quot; href=&quot;http://auth.gulimall.com/reg.html&quot; class=&quot;li_2&quot;&gt;免费注册&lt;/a&gt;&lt;/li&gt; 1.2、订单登录拦截 任何请求都需要先经过拦截器的验证，才能去执行目标方法，这里是用户是否登录，用户登录了则放行，否则跳转到登陆页面 1234567891011121314151617181920212223242526272829/** * 目标方法执行之前 * @param request * @param response * @param handler * @return * @throws Exception */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { String requestURI = request.getRequestURI(); // 指定的请求不拦截 boolean match1 = new AntPathMatcher().match(&quot;/order/order/status/**&quot;, requestURI); boolean match2 = new AntPathMatcher().match(&quot;/payed/notify&quot;, requestURI); if (match1 || match2) { return true; } MemberRespVo memberRespVo= (MemberRespVo) request.getSession().getAttribute(AuthServerConstant.LOGIN_USER); if (memberRespVo != null) { // 用户登陆了 loginUser.set(memberRespVo); // 放到共享数据中 return true; } else { // 用户没登录 // 给前端显示提示信息 request.getSession().setAttribute(&quot;msg&quot;,&quot;请先进行登录&quot;); // 重定向到登录页面 response.sendRedirect(&quot;http://auth.gulimall.com/login.html&quot;); return false; } } 2、订单确认页 根据图片中商品信息抽取成Vo 2.1、抽取Vo 订单确认OrderConfirmVo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * 订单确认页需要的数据 * @author gcq * @Create 2020-11-17 */public class OrderConfirmVo { // 收货地址，ums_member_receive_address @Getter @Setter List&lt;MemberAddressVo&gt; address; // 所有选中的购物项 @Getter @Setter List&lt;OrderItemVo&gt; item; // 发票记录... /** * 优惠卷信息 */ @Getter @Setter Integer integration; /** * 订单总额 */ BigDecimal total; public BigDecimal getTotal() { BigDecimal sum = new BigDecimal(&quot;0&quot;); if(item != null) { for (OrderItemVo orderItemVo : item) { BigDecimal multiply = orderItemVo.getPrice().multiply(new BigDecimal(orderItemVo.getCount().toString())); sum = sum.add(multiply); } } return sum; } @Getter @Setter Map&lt;Long,Boolean&gt; stocks; /** * 应付价格 */ BigDecimal payPrice; public BigDecimal getPayPrice() { BigDecimal sum = new BigDecimal(&quot;0&quot;); if(item != null) { for (OrderItemVo orderItemVo : item) { BigDecimal multiply = orderItemVo.getPrice().multiply(new BigDecimal(orderItemVo.getCount().toString())); sum = sum.add(multiply); } } return sum; } @Setter private Integer count; /** * 遍历item 拿到商品的数量 * @return */ public Integer getCount() { Integer i = 0; if (item != null) { for (OrderItemVo orderItemVo : item) { i+=orderItemVo.getCount(); } } return i; } @Getter @Setter private String orderToken;} 商品项orderItemVo 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * 商品项 * @author gcq * @Create 2020-11-17 */@Datapublic class OrderItemVo { /** * 商品id */ private Long skuId; /** * 购物车中是否选中 */ private Boolean check = true; /** * 商品的标题 */ private String title; /** * 商品的图片 */ private String image; /** * 商品的属性 */ private List&lt;String&gt; skuAttr; /** * 商品的价格 */ private BigDecimal price; /** * 商品的数量 */ private Integer count; /** * 购物车价格 使用自定义get、set */ private BigDecimal totalPrice; private BigDecimal weight;} 用户地址MemberAddressVo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * 用户地址信息 * @author gcq * @Create 2020-11-17 */@Datapublic class MemberAddressVo { private Long id; /** * member_id */ private Long memberId; /** * 收货人姓名 */ private String name; /** * 电话 */ private String phone; /** * 邮政编码 */ private String postCode; /** * 省份/直辖市 */ private String province; /** * 城市 */ private String city; /** * 区 */ private String region; /** * 详细地址(街道) */ private String detailAddress; /** * 省市区代码 */ private String areacode; /** * 是否默认 */ private Integer defaultStatus;} 2.2、订单确认页数据查询 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Override public OrderConfirmVo confirmOrder() throws ExecutionException, InterruptedException { OrderConfirmVo confirmVo = new OrderConfirmVo(); MemberRespVo memberRespVo = OrderInterceptor.loginUser.get();// 获取当前登录后的用户 // 异步任务执行之前，先共享数据 RequestAttributes requestAttributes = RequestContextHolder.getRequestAttributes(); // 1、第一个异步任务 远程查询用户地址信息 CompletableFuture&lt;Void&gt; memberFuture = CompletableFuture.runAsync(() -&gt; { // 在主线程中拿到原来的数据，在父线程里面共享RequestContextHolder // 只有共享，拦截其中才会有数据 RequestContextHolder.setRequestAttributes(requestAttributes); // 根据会员id查出之前会员保存过的收货地址信息 // 远程查询会员服务的收获地址信息 List&lt;MemberAddressVo&gt; address = memberFeignService.getAddress(memberRespVo.getId()); log.error(&quot;address:{}&quot;,address); confirmVo.setAddress(address); }, executor); // 2、第二个异步任务远程查询购物车中选中给的购物项 CompletableFuture&lt;Void&gt; addressFuture = CompletableFuture.runAsync(() -&gt; { // 每一个线程都来共享之前的请求数据 RequestContextHolder.setRequestAttributes(requestAttributes); // 远程查询购物车中的购物项信息，用来结账 List&lt;OrderItemVo&gt; currentUserCartItem = cartFeignServicea.getCurrentUserCartItem();// 获取当前用户的购物项数据 log.error(&quot;currentUserCartItem:{}&quot;,currentUserCartItem); confirmVo.setItem(currentUserCartItem); // 查询到购物项信息后，再看查询购物的库存信息 }, executor).thenRunAsync(() -&gt; { // 只要上面任务执行完成，就开始执行thenRunAsync的任务 // 3、商品是否有库存 List&lt;OrderItemVo&gt; items = confirmVo.getItem(); // 批量查询每一个商品的信息 // 收集好商品id List&lt;Long&gt; collect = items.stream().map(item -&gt; item.getSkuId()).collect(Collectors.toList()); // 远程查询购物项对应的库存信息 R data = wareFeignService.hasStock(collect); // 得到每一个商品的库存状态信息 List&lt;SkuHasStockVo&gt; hasStockVo = data.getData(new TypeReference&lt;List&lt;SkuHasStockVo&gt;&gt;() { }); if (hasStockVo != null) { Map&lt;Long, Boolean&gt; stockMap = hasStockVo.stream().collect(Collectors.toMap(SkuHasStockVo::getSkuId, SkuHasStockVo::getHasStock)); confirmVo.setStocks(stockMap); log.error(&quot;stockMap:{}&quot;,stockMap); } },executor); // 4、查询积分信息 Integer integration = confirmVo.getIntegration(); confirmVo.setIntegration(integration); // 等两个异步任务都完成 CompletableFuture.allOf(memberFuture, addressFuture).get(); // 4、防重令牌 /** * 接口幂等性就是用户对同一操作发起的一次请求和多次请求结果是一致的 * 不会因为多次点击而产生了副作用，比如支付场景，用户购买了商品，支付扣款成功， * 但是返回结果的时候出现了网络异常，此时钱已经扣了，用户再次点击按钮， * 此时就会进行第二次扣款，返回结果成功，用户查询余额发现多扣钱了， * 流水记录也变成了两条。。。这就没有保证接口幂等性 */ // 先是再页面中生成一个随机码把他叫做token先存到redis中，然后放到对象中在页面进行渲染。 // 用户提交表单的时候，带着这个token和redis里面去匹配如果一直那么可以执行下面流程。 // 匹配成功后再redis中删除这个token，下次请求再过来的时候就匹配不上直接返回 // 生成防重令牌 String token = UUID.randomUUID().toString().replace(&quot;-&quot;,&quot;&quot;); // 存到redis中 设置30分钟超时 redisTemplate.opsForValue().set(OrderConstant.USER_ORDER_TOKEN_PREFIX + memberRespVo.getId(),token,30, TimeUnit.SECONDS); // 放到页面进行显示token，然后订单中带着token来请求 confirmVo.setOrderToken(token); return confirmVo; 3、创建订单 1、OrderWebController 12345678910111213141516171819202122232425262728293031/** * @需求描述: 系统管理员 订单组 模块 用户下单功能 * @创建人: 郭承乾 * @创建时间: 2021/01/06 12:01 * @修改需求: * @修改人: * @修改时间: * @需求思路: */ @PostMapping(&quot;/submitOrder&quot;) public String submitOrder(OrderSubmitVo vo, Model model, RedirectAttributes redirectAttributes) { SubmitOrderResponseVo responseVo = orderService.submitOrder(vo); log.error(&quot;======================订单创建成功{}:&quot;,responseVo); // 根据vo中定义的状态码来验证 if (responseVo.getCode() == 0 ) { // 订单创建成功 // 下单成功返回到支付页 model.addAttribute(&quot;submitOrderResp&quot;,responseVo); return &quot;pay&quot;; } else { // 下单失败 // 根据状态码验证对应的状态 String msg = &quot;下单失败&quot;; switch (responseVo.getCode()) { case 1: msg += &quot;订单信息过期，请刷新后再次提交&quot;; break; case 2: msg += &quot;订单商品价格发生变化，请确认后再次提交&quot;; break; case 3: msg += &quot;库存锁定失败，商品库存不足&quot;; break; } redirectAttributes.addFlashAttribute(&quot;msg&quot;,msg); // 重新回到订单确认页面 return &quot;redirect:http://order.gulimall.com/toTrade&quot;; } } 2、Service 具体业务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305 @Transactional @Override public SubmitOrderResponseVo submitOrder(OrderSubmitVo vo) { // 先将参数放到共享变量中，方便之后方法使用该参数 confirmVoThreadLocal.set(vo); // 接收返回数据 SubmitOrderResponseVo response = new SubmitOrderResponseVo(); response.setCode(0); // 通过拦截器拿到用户的数据 MemberRespVo memberRespVo = LoginInterceptor.loginUser.get(); /** * 不使用原子性验证令牌 * 1、用户带着两个订单，提交速度非常快，两个订单的令牌都是123，去redis里面查查到的也是123。 * 两个对比都通过，然后来删除令牌，那么就会出现用户重复提交的问题， * 2、第一次差的快，第二次查的慢，只要没删就会出现这些问题 * 3、因此令牌的【验证和删除必须保证原子性】 * String orderToken = vo.getOrderToken(); * String redisToken = redisTemplate.opsForValue().get(OrderConstant.USER_ORDER_TOKEN_PREFIX + memberRespVo.getId()); * if (orderToken != null &amp;&amp; orderToken.equals(redisToken)) { * // 令牌验证通过 进行删除 * redisTemplate.delete(OrderConstant.USER_ORDER_TOKEN_PREFIX + memberRespVo.getId()); * } else { * // 不通过 * } */ // 验证令牌【令牌的对比和删除必须保证原子性】 // 因此使用redis中脚本来进行验证并删除令牌 // 0【删除失败/验证失败】 1【删除成功】 String script = &quot;if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end&quot;; /** * redis lur脚本命令解析 * if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end * 1、redis调用get方法来获取一个key的值，如果这个get出来的值等于我们传过来的值 * 2、然后就执行删除，根据这个key进行删除，删除成功返回1，验证失败返回0 * 3、删除否则就是0 * 总结：相同的进行删除，不相同的返回0 * 脚本大致意思 */ // 拿到令牌 String orderToken = vo.getOrderToken(); /** * public &lt;T&gt; T execute(RedisScript&lt;T&gt; script // redis的脚本 * , List&lt;K&gt; keys // 对应的key 参数中使用了Array.asList 将参数转成list集合 * , Object... args) { // 要删除的值 */ // 原子验证和删除 Long result = redisTemplate.execute(new DefaultRedisScript&lt;Long&gt;(script, Long.class) , Arrays.asList(OrderConstant.USER_ORDER_TOKEN_PREFIX + memberRespVo.getId()) , orderToken); if (result == 0L) { // 验证令牌验证失败 // 验证失败直接返回结果 response.setCode(1); return response; } else { // 原子验证令牌成功 // 下单 创建订单、验证令牌、验证价格、验证库存 // 1、创建订单、订单项信息 OrderCreateTo order = createOrder(); // 2、应付总额 BigDecimal payAmount = order.getOrder().getPayAmount(); // 应付价格 BigDecimal payPrice = vo.getPayPrice(); /** * 电商项目对付款的金额精确到小数点后面两位 * 订单创建好的应付总额 和购物车中计算好的应付价格求出绝对值。 */ if(Math.abs(payAmount.subtract(payPrice).doubleValue()) &lt; 0.01) { // 金额对比成功 保存订单 saveOrder(order); // 创建锁定库存Vo WareSkuLockedVo wareSkuLockedVo = new WareSkuLockedVo(); // 准备好商品项 List&lt;OrderItemVo&gt; lock = order.getOrderItem().stream().map(orderItemEntity -&gt; { OrderItemVo orderItemVo = new OrderItemVo(); // 商品购买数量 orderItemVo.setCount(orderItemEntity.getSkuQuantity()); // skuid 用来查询商品信息 orderItemVo.setSkuId(orderItemEntity.getSkuId()); // 商品标题 orderItemVo.setTitle(orderItemEntity.getSkuName()); return orderItemVo; }).collect(Collectors.toList()); // 订单号 wareSkuLockedVo.setOrderSn(order.getOrder().getOrderSn()); // 商品项 wareSkuLockedVo.setLocks(lock); // 远程调用库存服务锁定库存 R r = wareFeignService.orderLockStock(wareSkuLockedVo); if (r.getCode() == 0) { // 库存锁定成功 // 将订单对象放到返回Vo中 response.setOrder(order.getOrder()); // 设置状态码 response.setCode(0); // 订单创建成功发送消息给MQ rabbitTemplate.convertAndSend(&quot;order-event-exchange&quot; ,&quot;order.create.order&quot; ,order.getOrder()); return response; } else { // 远程锁定库存失败 response.setCode(3); return response; } } else { // 商品价格比较失败 response.setCode(2); return response; } } } /** * 创建订单和订单项 * @return */ private OrderCreateTo createOrder() { OrderCreateTo orderCreateTo = new OrderCreateTo(); // 1、生成订单号 String orderSn = IdWorker.getTimeId(); // 2、构建订单 OrderEntity orderEntity = buildOrder(orderSn); // 3、构建订单项 List&lt;OrderItemEntity&gt; itemEntities = builderOrderItems(orderSn); // 4、设置价格、积分相关信息 computPrice(orderEntity,itemEntities); // 5、设置订单项 orderCreateTo.setOrderItem(itemEntities); // 6、设置订单 orderCreateTo.setOrder(orderEntity); return orderCreateTo; } /** * 构建订单 * @param orderSn * @return */ private OrderEntity buildOrder(String orderSn) { // 拿到共享数据 OrderSubmitVo orderSubmitVo = confirmVoThreadLocal.get(); // 用户登录登录数据 MemberRespVo memberRespVo = LoginInterceptor.loginUser.get(); OrderEntity orderEntity = new OrderEntity(); // 设置订单号 orderEntity.setOrderSn(orderSn); // 用户id orderEntity.setMemberId(memberRespVo.getId()); // 根据用户收货地址id查询出用户的收获地址信息 R fare = wareFeignService.getFare(orderSubmitVo.getAddrId()); FareVo data = fare.getData(new TypeReference&lt;FareVo&gt;() { }); //将查询到的会员收货地址信息设置到订单对象中 // 运费金额 orderEntity.setFreightAmount(data.getFare()); // 城市 orderEntity.setReceiverCity(data.getMemberAddressVo().getCity()); // 详细地区 orderEntity.setReceiverDetailAddress(data.getMemberAddressVo().getDetailAddress()); // 收货人姓名 orderEntity.setReceiverName(data.getMemberAddressVo().getName()); // 收货人手机号 orderEntity.setReceiverPhone(data.getMemberAddressVo().getPhone()); // 区 orderEntity.setReceiverRegion(data.getMemberAddressVo().getRegion()); // 省份直辖市 orderEntity.setReceiverProvince(data.getMemberAddressVo().getProvince()); // 订单刚创建状态设置为 待付款，用户支付成功后将该该状态改成已付款 orderEntity.setStatus(OrderStatusEnum.CREATE_NEW.getCode()); // 自动确认时间 orderEntity.setAutoConfirmDay(7); return orderEntity; }/** * 构建订单项 * @param orderSn * @return */ private List&lt;OrderItemEntity&gt; builderOrderItems(String orderSn) { // 获取购物车中选中的商品 List&lt;OrderItemVo&gt; currentUserCartItem = cartFeignServicea.getCurrentUserCartItem(); if (currentUserCartItem != null &amp;&amp; currentUserCartItem.size() &gt; 0) { List&lt;OrderItemEntity&gt; collect = currentUserCartItem.stream().map(orderItemVo -&gt; { // 构建订单项 OrderItemEntity itemEntity = builderOrderItem(orderItemVo); itemEntity.setOrderSn(orderSn); return itemEntity; }).collect(Collectors.toList()); return collect; } return null; } /** * 构建订单项信息 * @param cartItem * @return */ private OrderItemEntity builderOrderItem(OrderItemVo cartItem) { OrderItemEntity itemEntity = new OrderItemEntity(); // 1、根据skuid查询关联的spuinfo信息 Long skuId = cartItem.getSkuId(); R spuinfo = productFeignService.getSpuInfoBySkuId(skuId); SpuInfoVo spuInfoVo = spuinfo.getData(new TypeReference&lt;SpuInfoVo&gt;() { }); // 2、设置商品项spu信息 // 品牌信息 itemEntity.setSpuBrand(spuInfoVo.getBrandId().toString()); // 商品分类信息 itemEntity.setCategoryId(spuInfoVo.getCatalogId()); // spuid itemEntity.setSpuId(spuInfoVo.getId()); // spu_name 商品名字 itemEntity.setSpuName(spuInfoVo.getSpuName()); // 3、设置商品sku信息 // skuid itemEntity.setSkuId(skuId); // 商品标题 itemEntity.setSkuName(cartItem.getTitle()); // 商品图片 itemEntity.setSkuPic(cartItem.getImage()); // 商品sku价格 itemEntity.setSkuPrice(cartItem.getPrice()); // 商品属性以 ; 拆分 String skuAttr = StringUtils.collectionToDelimitedString(cartItem.getSkuAttr(), &quot;;&quot;); itemEntity.setSkuAttrsVals(skuAttr); // 商品购买数量 itemEntity.setSkuQuantity(cartItem.getCount()); // 4、设置商品优惠信息【不做】 // 5、设置商品积分信息 // 赠送积分 移弃小数值 itemEntity.setGiftIntegration(cartItem.getPrice().multiply(new BigDecimal(cartItem.getCount().toString())).intValue()); // 赠送成长值 itemEntity.setGiftGrowth(cartItem.getPrice().multiply(new BigDecimal(cartItem.getCount().toString())).intValue()); // 6、订单项的价格信息 // 这里需要计算商品的分解信息 // 商品促销分解金额 itemEntity.setPromotionAmount(new BigDecimal(&quot;0&quot;)); // 优惠券优惠分解金额 itemEntity.setCouponAmount(new BigDecimal(&quot;0&quot;)); // 积分优惠分解金额 itemEntity.setIntegrationAmount(new BigDecimal(&quot;0&quot;)); // 商品价格乘以商品购买数量=总金额(未包含优惠信息) BigDecimal origin = itemEntity.getSkuPrice().multiply(new BigDecimal(itemEntity.getSkuQuantity().toString())); // 总价格减去优惠卷-积分优惠-商品促销金额 = 总金额 origin.subtract(itemEntity.getPromotionAmount()) .subtract(itemEntity.getCouponAmount()) .subtract(itemEntity.getIntegrationAmount()); // 该商品经过优惠后的分解金额 itemEntity.setRealAmount(origin); return itemEntity; } /** * 计算订单涉及到的积分、优惠卷抵扣、促销优惠信息等信息 * @param orderEntity * @param itemEntities * @return */ private OrderEntity computPrice(OrderEntity orderEntity, List&lt;OrderItemEntity&gt; itemEntities) { // 1、定义好相关金额，然后遍历购物项进行计算 // 总价格 BigDecimal total = new BigDecimal(&quot;0&quot;); //相关优惠信息 // 优惠卷抵扣金额 BigDecimal coupon = new BigDecimal(&quot;0&quot;); // 积分优惠金额 BigDecimal integration = new BigDecimal(&quot;0&quot;); // 促销优惠金额 BigDecimal promotion = new BigDecimal(&quot;0&quot;); // 积分 BigDecimal gift = new BigDecimal(&quot;0&quot;); // 成长值 BigDecimal growth = new BigDecimal(&quot;0&quot;); // 遍历订单项将所有的优惠信息进行相加 for (OrderItemEntity itemEntity : itemEntities) { coupon = coupon.add(itemEntity.getCouponAmount()); // 优惠卷抵扣 integration = integration.add(itemEntity.getIntegrationAmount()); // 积分优惠分解金额 promotion = promotion.add(itemEntity.getPromotionAmount()); // 商品促销分解金额 gift = gift.add(new BigDecimal(itemEntity.getGiftIntegration().toString())); // 赠送积分 growth = growth.add(new BigDecimal(itemEntity.getGiftGrowth())); // 赠送成长值 total = total.add(itemEntity.getRealAmount()); //优惠后的总金额 } // 2、设置订单金额 // 订单总金额 orderEntity.setTotalAmount(total); // 应付总额 = 订单总额 + 运费信息 orderEntity.setPayAmount(total.add(orderEntity.getFreightAmount())); // 促销优化金额（促销价、满减、阶梯价） orderEntity.setPromotionAmount(promotion); // 优惠券抵扣金额 orderEntity.setCouponAmount(coupon); // 3、设置积分信息 // 订单购买后可以获得的成长值 orderEntity.setGrowth(growth.intValue()); // 积分抵扣金额 orderEntity.setIntegrationAmount(integration); // 可以获得的积分 orderEntity.setIntegration(gift.intValue()); // 删除状态【0-&gt;未删除；1-&gt;已删除】 orderEntity.setDeleteStatus(0); return orderEntity; } 3、库存自动解锁---&gt;MQ 库存解锁、StockReleaseListener 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.atguigu.gulimall.ware.listener;import com.atguigu.common.to.mq.OrderTo;import com.atguigu.common.to.mq.StockLockedTo;import com.atguigu.gulimall.ware.service.WareSkuService;import com.rabbitmq.client.Channel;import org.springframework.amqp.core.Message;import org.springframework.amqp.rabbit.annotation.RabbitHandler;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.io.IOException;/** * 监听库存延时队列 * @author gcq * @Create 2021-01-07 */@Service@RabbitListener(queues = &quot;stock.release.stock.queue&quot;)public class StockReleaseListener { @Autowired WareSkuService wareSkuService; /** * 监听库存队列 * @param lockedTo * @param message */ @RabbitHandler public void handleStockLockedRelease(StockLockedTo lockedTo, Message message, Channel channel) throws IOException { System.out.println(&quot;收到解锁库存的信息&quot;); try { wareSkuService.unLockStock(lockedTo); //库存解锁成功没有抛出异常，自动ack机制确认 channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); } catch (Exception e) { e.printStackTrace(); // 重发 channel.basicReject(message.getMessageProperties().getDeliveryTag(),true); } } /** * 订单释放 * 订单30分钟未支付，订单关闭后发送的消息 */ @RabbitHandler public void handlerOrderCloseRelease(OrderTo orderTo, Message message, Channel channel) throws IOException { System.out.println(&quot;订单关闭准备解锁库存......&quot;); try { wareSkuService.unLockStock(orderTo); channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); } catch (Exception e) { e.printStackTrace(); channel.basicReject(message.getMessageProperties().getDeliveryTag(),true); } }} Service 1234567891011121314151617181920212223242526272829303132333435363738/** * 解锁库存 * @param lockedTo */ @Override public void unLockStock(StockLockedTo lockedTo) { // 工作单详情 StockDetailTo detail = lockedTo.getDetail(); // 工作单详情id Long detailId = detail.getId(); // 查询到库存工作单详情 WareOrderTaskDetailEntity taskDetailEntity = wareOrderTaskDetailService.getById(detailId); if (taskDetailEntity != null) { // 解锁库存 // 库存工作单id Long id = lockedTo.getId(); // 查询到库存工作单 WareOrderTaskEntity TaskEntity = wareOrderTaskService.getById(id); // 拿到订单号 String orderSn = TaskEntity.getOrderSn(); // 根据订单号查询订单的状态 R orderStatus = orderFeignService.getOrderStatus(orderSn); if (orderStatus.getCode() == 0) { OrderVo data = orderStatus.getData(new TypeReference&lt;OrderVo&gt;() { }); // 订单状态为已关闭,那么就需要解锁库存 if (data == null || data.getStatus() == 4) { // 库存工作单锁定状态为锁定才进行解锁 if (taskDetailEntity.getLockStatus() == 1) { unLockStock(detail.getSkuId(), detail.getWareId(), detail.getSkuNum(), detailId); } } } else { // 消息被拒绝后重新放到队列里面，让别人继续消费解锁 throw new RuntimeException(&quot;远败&quot;); } } } 4、支付 选择的是支付宝支付，根据老师所提供的素材 alipayTemplate、PayVo、PaySyncVo,引入到项目中进行开发 1、Controller 跳转到支付宝支付页面，支付完成后跳转到支付成功的回调页面 12345678910111213141516171819202122/** * 1、跳转到支付页面 * 2、用户支付成功后，我们要跳转到用户的订单列表页 * produces 明确方法会返回什么类型，这里返回的是html页面 * @param orderSn * @return * @throws AlipayApiException */ @ResponseBody @GetMapping(value = &quot;/payOrder&quot;,produces = &quot;text/html&quot;) public String payOrder(@RequestParam(&quot;orderSn&quot;) String orderSn) throws AlipayApiException {// PayVo payVo = new PayVo();// payVo.setBody(); // 商品描述// payVo.setSubject(); //订单名称// payVo.setOut_trade_no(); // 订单号// payVo.setTotal_amount(); //总金额 PayVo payvo = orderService.payOrder(orderSn); // 将返回支付宝的支付页面，需要将这个页面进行显示 String pay = alipayTemplate.pay(payvo); System.out.println(pay); return pay; } 2、Service 123456789101112131415161718192021222324/** * 计算商品支付需要的信息 * @param orderSn * @return */ @Override public PayVo payOrder(String orderSn) { PayVo payVo = new PayVo(); OrderEntity orderEntity = this.getOrderByOrderSn(orderSn);// 根据订单号查询到商品 // 数据库中付款金额小数有4位，但是支付宝只接受2位，所以向上取整两位数 BigDecimal decimal = orderEntity.getPayAmount().setScale(2, BigDecimal.ROUND_UP); payVo.setTotal_amount(decimal.toString()); // 商户订单号 payVo.setOut_trade_no(orderSn); // 查询出订单项，用来设置商品的描述和商品名称 List&lt;OrderItemEntity&gt; itemEntities = orderItemService.list(new QueryWrapper&lt;OrderItemEntity&gt;() .eq(&quot;order_sn&quot;, orderSn)); OrderItemEntity itemEntity = itemEntities.get(0); // 订单名称使用商品项的名字 payVo.setSubject(itemEntity.getSkuName()); // 商品的描述使用商品项的属性 payVo.setBody(itemEntity.getSkuAttrsVals()); return payVo; } 3、支付成功后跳转页面 支付成功后跳转到订单页面 12345678910111213141516171819202122/** * @需求描述: 系统管理员 会员服务组 模块 用户支付成功后跳转到该页面 * @创建人: * @创建时间: 2021/01/08 11:13 * @修改需求: * @修改人: * @修改时间: * @需求思路: */ @GetMapping(&quot;/memberOrder.html&quot;) public String memberList(@RequestParam(value = &quot;pageNum&quot;,defaultValue = &quot;1&quot;) Integer pageNum, Model model) { // 准备分页参数 Map&lt;String,Object&gt; params = new HashMap&lt;&gt;(); params.put(&quot;page&quot;,pageNum); // 远程查询当前用户的所有订单 R r = orderFeignService.listwithItem(params); System.out.println(JSON.toJSONString(r)); if (r.getCode() == 0) { model.addAttribute(&quot;orders&quot;,r); } return &quot;list&quot;; } Service 1234567891011121314151617181920212223242526272829/** * 查询当前用户所有订单 * @param params * @return */ @Override public PageUtils queryPageWithItem(Map&lt;String, Object&gt; params) { // 当前用户登录数据 MemberRespVo memberRespVo = LoginInterceptor.loginUser.get(); // 查询当前用户所有的订单记录 IPage&lt;OrderEntity&gt; page = this.page( new Query&lt;OrderEntity&gt;().getPage(params), new QueryWrapper&lt;OrderEntity&gt;() .eq(&quot;member_id&quot;,memberRespVo.getId()) .orderByDesc(&quot;id&quot;) ); List&lt;OrderEntity&gt; records = page.getRecords(); // 拿到分页查询结果 List&lt;OrderEntity&gt; orderEntityList = records.stream().map(item -&gt; { // 根据订单号查询当订单号对应的订单项 List&lt;OrderItemEntity&gt; itemEntities = orderItemService.list(new QueryWrapper&lt;OrderItemEntity&gt;() .eq(&quot;order_sn&quot;, item.getOrderSn())); item.setOrderEntityItem(itemEntities); return item; }).collect(Collectors.toList()); // 重新设置分页数据 page.setRecords(orderEntityList); return new PageUtils(page); } 然后页面渲染数据 支付宝文档地址: https://opendocs.alipay.com/open/270/105900 5、收单 订单：OrderListener 123456789101112131415161718192021222324252627282930@RabbitListener(queues = &quot;order.release.order.queue&quot;) // 监听订单的释放队列，能到这个里面的消息都是30分钟后过来的@Servicepublic class OrderListener { @Autowired OrderService orderService; /** * 订单定时关单 * 商品下单后，会向MQ中发送一条消息告诉MQ订单创建成功。 * 那么订单创建30分钟后用户还没有下单，MQ就会关闭该订单 * @param orderEntity 订单对象 * @param channel 信道 * @param message 交换机 * @throws IOException */ @RabbitHandler public void listener(OrderEntity orderEntity, Channel channel, Message message) throws IOException { System.out.println(&quot;收到过期的订单信息，准备关闭订单：&quot; + orderEntity.getOrderSn()); try { orderService.closeOrder(orderEntity); // 关闭订单成功后，ack信息确认 channel.basicAck(message.getMessageProperties().getDeliveryTag(),false); } catch (Exception e) { e.printStackTrace(); channel.basicReject(message.getMessageProperties().getDeliveryTag(),true); } }} Service 123456789101112131415161718192021222324@Override public void closeOrder(OrderEntity orderEntity) { // 订单30分钟的时间可能有属性变动，所以需要根据属性再次查询一次 OrderEntity entity = this.getById(orderEntity.getId()); // 当前状态为待付款，说明用户30分钟内还没有付款 if(entity.getStatus() == OrderStatusEnum.CREATE_NEW.getCode()) { OrderEntity updateOrder = new OrderEntity(); // 根据订单id更新 updateOrder.setId(entity.getId()); // 订单状态改成已取消 updateOrder.setStatus(OrderStatusEnum.CANCLED.getCode()); // 根据订单对象更新 this.updateById(updateOrder); // 准备共享对象用于发送到MQ中 OrderTo orderTo = new OrderTo(); // 拷贝属性 BeanUtils.copyProperties(entity,orderTo); try { rabbitTemplate.convertAndSend(&quot;order-event-exchange&quot;,&quot;order.release.other&quot;,orderTo); } catch (Exception e) { e.printStackTrace(); } } } 订单在支付页，不支付，一直刷新，订单过期了才支付，订单状态改为已支付了，但是库存解锁了。 使用支付宝自动收单功能解决。只要一段时间不支付，就不能支付了。 由于时延等问题。订单解锁完成，正在解锁库存的时候，异步通知才到 订单解锁，手动调用收单 网络阻塞问题，订单支付成功的异步通知一直不到达 查询订单列表时，ajax获取当前未支付的订单状态，查询订单状态时，再获取一下支付宝此订单的状态 其他各种问题 每天晚上闲时下载支付宝对账单，一 一 进行对账 15、幂等性 15.1 什么是幂等性 接口幂等性就是用户对同一操作发起的一次请求和多次请求结果是一致的，不会因为多次点击而产生了副作用，比如支付场景，用户购买了商品，支付扣款成功，但是返回结果的时候出现了网络异常，此时钱已经扣了，用户再次点击按钮，此时就会进行第二次扣款，返回结果成功，用户查询余额发现多扣钱了，流水记录也变成了两条。。。这就没有保证接口幂等性 15.2 那些情况需要防止 用户多次点击按钮 用户页面回退再次提交 微服务互相调用，由于网络问题，导致请求失败，feign触发重试机制 其他业务情况 15.3 什么情况下需要幂等 以 SQL 为例，有些操作时天然幂等的 SELECT * FROM table WHERE id =? 无论执行多少次都不会改变状态是天然的幂等 UPDATE tab1 SET col1=1 WHERE col2=2 无论执行成功多少状态都是一致的，也是幂等操作 delete from user where userid=1 多次操作，结果一样，具备幂等性 insert into user(userid,name) values(1,' a' ) 如userid为唯一主键，即重复上面的业务，只会插入一条用户记录，具备幂等性 UPDATE tab1 SET col1=col1+1 WHERE col2=2,每次执行的结果都会发生变化，不是幂等的。insert into user(userid,name) values(,a&quot;)如userid不是主键，可以重复，那上面业务多次操作，数据都会新增多条，不具备幂等性。 15.4 幂等解决方案 1、token 机制 1、服务端提供了发送 token 的接口，我们在分析业务的时候，哪些业务是存在幂等性问题的，就必须在执行业务前，先获取 token，服务器会把 token 保存到 redis 中 2、然后调用业务接口请求时， 把 token 携带过去，一般放在请求头部 3、服务器判断 token 是否存在 redis，存在表示第一次请求，然后删除 token，继续执行业务 4、如果判断 token 不存在 redis 中，就表示重复操作，直接返回重复标记给 client，这样就保证了业务代码，不被重复执行 危险性： 1、先删除 token 还是后删除 token： 先删除可能导致，业务确实没有执行，重试还得带上之前的 token, 由于防重设计导致，请求还是不能执行 后删除可能导致，业务处理成功，但是服务闪断，出现超时，没有删除掉token，别人继续重试，导致业务被执行两次 我们最后设计为先删除 token，如果业务调用失败，就重新获取 token 再次请求 2、Token 获取，比较 和删除 必须是原子性 redis.get（token），token.equals、redis.del（token）,如果说这两个操作都不是原子，可能导致，在高并发下，都 get 同样的数据，判断都成功，继续业务并发执行 可以在 redis 使用 lua 脚本完成这个操作 1&quot;if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end&quot; 2、各种锁机制 1、数据库悲观锁 select * from xxx where id = 1 for update; for update 查询的时候锁定这条记录 别人需要等待 悲观锁使用的时候一般伴随事务一起使用，数据锁定时间可能会很长，需要根据实际情况选用，另外需要注意的是，id字段一定是主键或唯一索引，不然可能造成锁表的结果，处理起来会非常麻烦 2、数据库的乐观锁 这种方法适合在更新的场景中 update t_goods set count = count - 1,version = version + 1 where good_id = 2 and version = 1 根据 version 版本，也就是在操作数据库存前先获取当前商品的 version 版本号，然后操作的时候带上 version 版本号，我们梳理下，我们第一次操作库存时，得 到 version 为 1，调用库存服务 version = 2，但返回给订单服务出现了问题，订单服务又一次调用了库存服务，当订单服务传的 version 还是 1，再执行上面的 sql 语句 就不会执行，因为 version 已经变成 2 了，where 条件不成立，这样就保证了不管调用几次，只会真正处理一次，乐观锁主要使用于处理读多写少的问题 3、业务层分布锁 如果多个机器可能在同一时间处理相同的数据，比如多台机器定时任务拿到了相同的数据，我们就可以加分布式锁，锁定此数据，处理完成后后释放锁，获取锁必须先判断这个数据是否被处理过 3、各种唯一约束 1、数据库唯一约束 插入数据，应该按照唯一索引进行插入，比如订单号，相同订单就不可能有两条订单插入，我们在数据库层面防止重复 这个机制利用了数据库的主键唯一约束的特性，解决了 insert场 景时幂等问题，但主键的要求不是自增的主键，这样就需要业务生成全局唯一的主键 如果是分库分表场景下，路由规则要保证相同请求下，落地在同一个数据库和同一表中，要不然数据库主键约束就不起效果了，因为是不同的数据库和表主键不相关 2、redis set 防重 很多数据需要处理，只能被处理一次，比如我们可以计算数据的 MD5 将其放入 redis 的 set,每次处理数据，先看这个 MD5 是否已经存在，存在就不处理 4、防重表 使用订单表 orderNo 做为去重表的唯一索引，把唯一索引插入去重表，再进行业务操作，且他们在同一个事务中，这样就保证了重复请求时，因为去重表有唯一 约束，导致请求失败，避免了幂等性等问题，去重表和业务表应该在同一个库中，这样就保证了在同一个事务，即使业务操作失败，也会把去重表的数据回滚，这 个很好的保证了数据的一致性， redis防重也算 5、全局请求唯一id 调用接口时，生成一个唯一的id，redis 将数据保存到集合中（去重），存在即处理过，可以使用 nginx 设置每一个请求一个唯一id proxy_set_header X-Request-Id $Request_id 16、本地事务 16.1 事务的基本性质 数据库事物的几个特性：原子性（Atomiccity）、一致性（Consistetcy）、隔离性或独立性（Isolation）和持久性（Durabilily），简称ACID **原子性：**一系列操作整体不可拆分，要么同时成功要么同时失败 一致性：数据在业务的前后，业务整体一致 转账 A:1000 B:1000 转200 事务成功 A：800 B：1200 **隔离性：**事物之间需要相互隔离 **持久性：**一旦事务成功，数据一定会落盘在数据库 在以往的单体应用中，我们多个业务操作使用同一条连接操作不同的表，一旦有异常我们很容易整体回滚 Business：我们具体的业务代码 Storage：库存业务代码;扣库存 Order：订单业务代码;保存订单 Account：账号业务代码:减账户余额 比如买东西业务，扣库存，下订单，账户扣款，是一个整体；必须同时成功或者失败 一个事务开始，代表以下的所有操作都在同一个连接里面 16.2 事物的隔离级别 READ UNCOMMITEED(读未提交) 该隔离级别的事务会读到其他未提交事务的数据，此现象也称为脏读 READ COMMITTED（读提交） 一个事物可以读取另一个已提交的事务，多次读取会造成不一样的结果，此现象称为不可重复读，复读问题，Oracle 和SQL Server 的默认隔离级别 REPEATABLE READ（可重复读） 该隔离级别是 MySQL 默认的隔离级别，在同一个事物中，SELECT 的结果是事务开始时时间点的状态，因此，同样的 SELECT 操作读到的结果会是一致 但是会有幻读一致,MySQL的 InnoDB 引擎可以通过 next-key locks 机制 来避免幻读 SERIALIZABLE（序列化） 在该隔离级别下事务都是串行顺序执行的，MySQL 数据库的 InnoDB 引擎会给读操作隐士加一把读共享锁，从而避免了脏读、不可重复读、幻读等问题 越大的隔离级别，并发能力越低 16.3 事务传播行为 **PROPAGATION_REQUIRED：**如果当前没事事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置时最常使用的配置 **PROPAGATION_SUPPORTS：**支持当前事务。如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行 **PROPAGATION_MANDATORY：**支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在该事务，就抛出异常 **PROPAGATION_REQUIRES_NEW：**创建新事务，无论当前存不存在事务，都创建新事务 **PROPAGATION_NOT_SUPPORTED：**以非事务的方式执行操作，如果当前存在事务，就把当前事务挂起 **PROPAGATION_NEVER：**以非事务方式运行，如果当前存在事务，则抛出异常 **PROPAGATION_NESTED：**如果当前存在事务，就嵌套在该事务内执行，如果当前没有事务，则执行PROPAGATION_REQUIRED相关操作 16.4 SpringBoot 事务关键点 在同一个事物内编写两个方法，内部调用的时候，会导致事务失效，原因是没有用到代理对象的缘故 解决 0)、导入spring-boot-starter-aop 1)、@EnableTransactionManagement(proxyTargetClass= true) 2)、@EnableAspectJAutoProxy(lexposeProxy-true) 3)、AopContext.currentProxy() 调用方法 1、事物的自动配置 2、事物的坑 17、分布式事务 17.1 为什么要有分布式事务 分布式系统经常出现的异常 机器宕机、网络异常、消息丢失、消息乱序、不可靠的TCP、存储数据丢失.... 分布式事务是企业中集成的一个难点，也是每一个分布式系统架构中都会涉及到的一个东西，特别是在微服务架构中，几乎是无法避免的 17.2 CAP 定理与 BASE 理论 1、CAP 定理 CAP 原则又称 CAP 定理指的是在一个分布式系统中 一致性（Consistency） 在分布式系统中所有数据备份，在同一时刻是否是同样的值，（等同于所有节点访问同一份最新数据的副本） 可用性（Avaliability） 在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求，（对数据更新具备高可用性） 分区容错性（Partition tolerance） 大多数分布式系统都分布在多个子网络，每个自网络叫做一个区（partition）。分区容错性的意思是，区间通信可能失败，比如，一台服务器放在中国另一台服务器放在美国，这就是两个区，它们之间可能无法通信 CAP 的原则是，这三个要素最多只能满足两个点，不可能三者兼顾 一般来说，分区容错无法避免，因此我们可以认为 CAP 的 P 总是成立，CAP 定理告诉我们 剩下的 C 和 A 无法同时做到 分布式实现一致性的 raft 算法 http://thesecretlivesofdata.com/raft/ 2、面临的问 对于大多数互联网应用的场景、主机众多、部署分散，而且集群规模越来越大，所以节点故障，网络故障是常态，而且要保证服务可用性达到99.999%，即保证P 和 A,舍弃C 3、BASE 理论 是对CAP的延申，思想即是无法做到强一致性（CAP的一致性就是强一致性），但可以采用适当的弱一致性，即最终一致性 BASE 是指 基本可用（Basically Avaliable） 基本可用是指分布式系统中在出现故障的时候，允许损失部分可用性（列入响应时间，功能上的可用性）允许损失部分可用性。需要注意的是基本可用不等价于系统不可用 响应时间上的损失，正常情况下搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障（比如系统部分机房断电断网故障），查询的结果响应时间增加到了1~2秒 功能上的损失，购物网站双十一购物高峰，为了保证系统的稳定性，部分消费者会被引入到一个降级页面 软状态（Soft State） 软状态是指允许 系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据会有 多个副本，允许不同副本同步的延时就是软状态的体现。mysglreplication的异步复制也是一种体现。 最终一致性( Eventual Consistency) 最终致性是指系统中的所有数据副本经过一定时间后，最终能够达到一 致的状态。弱一致性和强一致性相反，最终-致性是弱一致性的一种特殊情况。 4、强一致性、弱一致性、最终一致性 从客户端角度，多进程并发访问时，更新过的数据在不同进程如何获取的不同策略，决定了不同的一致性。对于关系型数据库，要求更新过的数据能被后续的访问都能看到，这是强一致性。如果能容忍后续的部分或者全部访问不到，则是弱一致性。如果经过一段时间后要求能访问到更新后的数据，则是最终一致性 17.3 分布式事务的几种方案 1、2PC 模式 数据库支持的 2PC[2 phase commit 二阶提交]，又叫 XA Transactions MySQL 从5.5版本开始支持，Sql Server 2005 开始支持，oracle7 开始支持 其中，XA 是一个两阶段提交协议，该协议分为两个阶段： 第一阶段：事务协调器要求每个涉及到事务的数据库预提交（precommit）此操作，并反应是否可以提交 第二阶段：事务协调器要求每个数据库提交数据 其中，如果有任何一个数据库否认这次提交，那么所有数据库都会要求回滚他们在此事务中的那部分信息 XA协议比较简单，而且一旦商业数据库实现了XA协议，使用分布式事务的成本也比较低。 XA性能不理想，特别是在交易下单链路，往往并发量很高，XA无法满足高并发场景 XA目前在商业数据库支持的比较理想，在mysq!数据库中支持的不太理想，mysgl的 XA实现，没有记录prepare阶段日志，主备切换回导致主库与备库数据不一致。 许多nosgI也没有支持XA，这让XA的应用场景变得非常狭隘。 也有3PC,引入了超时机制(无论协调者还是参与者，在向对方发送请求后，若长时间未收到回应则做出相应处理) 2、柔性事务 - TCC 事务 刚性事务:遵循ACID原则，强一致性。 柔性事务:遵循BASE理论，最终一致性; 与刚性事务不同，柔性事务允许一定时间内，不同节点的数据不一致，但要求最终一致。 一阶段 prepare 行为:调用自定义的 prepare 逻辑。 二阶段 commit 行为:调用自定义的 commit 逻辑。 二阶段 rollback行为:调用自定义的 rollback 逻辑。 所谓TCC模式，是指支持把自定义的分支事务纳入到全局事务的管理中。 3、柔性事务 - 最大努力通知型方案 按规律进行通知，不保证数据定能通知成功， 但会提供可查询操作接口进行核对。这种方案主要用在与第三方系统通讯时，比如:调用微信或支付宝支付后的支付结果通知。这种方案也是结合MQ进行实现，例如:通过MQ发送http请求，设置最大通知次数。达到通知次数后即不再通知。 案例:银行通知、商户通知等(各大交易业务平台间的商户通知:多次通知、查询校对、对账文件)，支付宝的支付成功异步回调 4、柔性事务 - 可靠信息 + 最终一致性方案（异步通知型） 实现:业务处理服务在业务事务提交之前，向实时消息服务请求发送消息，实时消息服务只记录消息数据，而不是真正的发送。业务处理服务在业务事务提交之后，向实时消息服务确认发送。只有在得到确认发送指令后，实时消息服务才会真正发送。 18、支付宝支付 1、进入蚂蚁金服开放平台 https://open.alipay.com/platform/home.htm 2、下载支付宝官方Demo，进行配置和测试 文档地址： https://openhome.alipay.com/docCenter/docCenter.htm 创建应用对应文档 https://opendocs.alipay.com/open/200/105304 网页移动应用文档 https://opendocs.alipay.com/open/54/cyz7do 相关Demo 密码 创建应用 3、使用沙箱进行测试 https://openhome.alipay.com/platform/appDaily.htm?tab=info 4、什么是公钥、私钥、加密、签名和验签? 1、公钥私钥 公钥和私钥是一一个相对概念 它们的公私性是相对于生成者来说的。 一对密钥生成后，保存在生成者手里的就是私钥，生成者发布出去大家用的就是公钥 5、支付宝支付流程 https://opendocs.alipay.com/open/270/105898 1、引导用户进入到支付宝页面 1、pom.xml 123456&lt;!--支付宝模块--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alipay.sdk&lt;/groupId&gt; &lt;artifactId&gt;alipay-sdk-java&lt;/artifactId&gt; &lt;version&gt;4.9.28.ALL&lt;/version&gt; &lt;/dependency&gt; 2、将素材提供的文件引入到项目中 AlipayTemplate、PayVo、PaySyncVo AlipayTemplate 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package com.atguigu.gulimall.order.config;import com.alipay.api.AlipayApiException;import com.alipay.api.AlipayClient;import com.alipay.api.DefaultAlipayClient;import com.alipay.api.request.AlipayTradePagePayRequest;import com.atguigu.gulimall.order.vo.PayVo;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@ConfigurationProperties(prefix = &quot;alipay&quot;)@Component@Datapublic class AlipayTemplate { //在支付宝创建的应用的id private String app_id = &quot;2016092200568607&quot;; // 商户私钥，您的PKCS8格式RSA2私钥 private String merchant_private_key = &quot;XXX&quot;; // 支付宝公钥,查看地址：https://openhome.alipay.com/platform/keyManage.htm 对应APPID下的支付宝公钥。 private String alipay_public_key = &quot;XXX&quot;; // 服务器[异步通知]页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数，必须外网可以正常访问 // 支付宝会悄悄的给我们发送一个请求，告诉我们支付成功的信息 private String notify_url = &quot;http://member.gulimall.com/memberOrder.html&quot;; // 页面跳转同步通知页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数，必须外网可以正常访问 //同步通知，支付成功，一般跳转到成功页 private String return_url = &quot;http://member.gulimall.com/memberOrder.html&quot;; // 签名方式 private String sign_type = &quot;RSA2&quot;; // 字符编码格式 private String charset = &quot;utf-8&quot;; // 订单超时时间，到达超时时间后自动关闭订单不能再继续支付 private String timeout = &quot;30m&quot;; // 支付宝网关； https://openapi.alipaydev.com/gateway.do private String gatewayUrl = &quot;https://openapi.alipaydev.com/gateway.do&quot;; public String pay(PayVo vo) throws AlipayApiException { //AlipayClient alipayClient = new DefaultAlipayClient(AlipayTemplate.gatewayUrl, AlipayTemplate.app_id, AlipayTemplate.merchant_private_key, &quot;json&quot;, AlipayTemplate.charset, AlipayTemplate.alipay_public_key, AlipayTemplate.sign_type); //1、根据支付宝的配置生成一个支付客户端 AlipayClient alipayClient = new DefaultAlipayClient(gatewayUrl, app_id, merchant_private_key, &quot;json&quot;, charset, alipay_public_key, sign_type); //2、创建一个支付请求 //设置请求参数 AlipayTradePagePayRequest alipayRequest = new AlipayTradePagePayRequest(); alipayRequest.setReturnUrl(return_url); alipayRequest.setNotifyUrl(notify_url); //商户订单号，商户网站订单系统中唯一订单号，必填 String out_trade_no = vo.getOut_trade_no(); //付款金额，必填 String total_amount = vo.getTotal_amount(); //订单名称，必填 String subject = vo.getSubject(); //商品描述，可空 String body = vo.getBody(); // timeout_express 订单支付超时时间 alipayRequest.setBizContent(&quot;{\\&quot;out_trade_no\\&quot;:\\&quot;&quot;+ out_trade_no +&quot;\\&quot;,&quot; + &quot;\\&quot;total_amount\\&quot;:\\&quot;&quot;+ total_amount +&quot;\\&quot;,&quot; + &quot;\\&quot;subject\\&quot;:\\&quot;&quot;+ subject +&quot;\\&quot;,&quot; + &quot;\\&quot;body\\&quot;:\\&quot;&quot;+ body +&quot;\\&quot;,&quot; + &quot;\\&quot;timeout_express\\&quot;:\\&quot;&quot; + timeout + &quot;\\&quot;,&quot; + &quot;\\&quot;product_code\\&quot;:\\&quot;FAST_INSTANT_TRADE_PAY\\&quot;}&quot;); String result = alipayClient.pageExecute(alipayRequest).getBody(); //会收到支付宝的响应，响应的是一个页面，只要浏览器显示这个页面，就会自动来到支付宝的收银台页面 System.out.println(&quot;支付宝的响应：&quot;+result); return result; }} PayVo 123456789101112131415package com.atguigu.gulimall.order.vo;import lombok.Data;/** * 支付使用Vo */@Datapublic class PayVo { private String out_trade_no; // 商户订单号 必填 private String subject; // 订单名称 必填 private String total_amount; // 付款金额 必填 private String body; // 商品描述 可空} payAsyncVo 1234567891011121314151617181920212223242526272829303132333435363738package com.atguigu.gulimall.order.vo;import lombok.Data;import lombok.ToString;import java.util.Date;/** * 支付宝回调参数Vo */@ToString@Datapublic class PayAsyncVo { private String gmt_create; private String charset; private String gmt_payment; private Date notify_time; private String subject; private String sign; private String buyer_id;//支付者的id private String body;//订单的信息 private String invoice_amount;//支付金额 private String version; private String notify_id;//通知id private String fund_bill_list; private String notify_type;//通知类型； trade_status_sync private String out_trade_no;//订单号 private String total_amount;//支付的总额 private String trade_status;//交易状态 TRADE_SUCCESS private String trade_no;//流水号 private String auth_app_id;// private String receipt_amount;//商家收到的款 private String point_amount;// private String app_id;//应用id private String buyer_pay_amount;//最终支付的金额 private String sign_type;//签名类型 private String seller_id;//商家的id} 3、编写业务跳转到支付页面 123456789101112131415161718192021222324252627282930313233343536/** * @author gcq * @Create 2021-01-08 */@Controllerpublic class PayWebController { @Autowired AlipayTemplate alipayTemplate; @Autowired OrderService orderService; /** * 1、跳转到支付页面 * 2、用户支付成功后，我们要跳转到用户的订单列表页 * produces 明确方法会返回什么类型，这里返回的是html页面 * @param orderSn * @return * @throws AlipayApiException */ @ResponseBody @GetMapping(value = &quot;/payOrder&quot;,produces = &quot;text/html&quot;) public String payOrder(@RequestParam(&quot;orderSn&quot;) String orderSn) throws AlipayApiException {// PayVo payVo = new PayVo();// payVo.setBody(); // 商品描述// payVo.setSubject(); //订单名称// payVo.setOut_trade_no(); // 订单号// payVo.setTotal_amount(); //总金额 PayVo payvo = orderService.payOrder(orderSn); // 将返回支付宝的支付页面，需要将这个页面进行显示 String pay = alipayTemplate.pay(payvo); System.out.println(pay); return pay; }} Service 123456789101112131415161718192021222324/** * 计算商品支付需要的信息 * @param orderSn * @return */ @Override public PayVo payOrder(String orderSn) { PayVo payVo = new PayVo(); OrderEntity orderEntity = this.getOrderByOrderSn(orderSn); // 根据订单号查询到商品 // 数据库中付款金额小数有4位，但是支付宝只接受2位，所以向上取整两位数 BigDecimal decimal = orderEntity.getPayAmount().setScale(2, BigDecimal.ROUND_UP); payVo.setTotal_amount(decimal.toString()); // 商户订单号 payVo.setOut_trade_no(orderSn); // 查询出订单项，用来设置商品的描述和商品名称 List&lt;OrderItemEntity&gt; itemEntities = orderItemService.list(new QueryWrapper&lt;OrderItemEntity&gt;() .eq(&quot;order_sn&quot;, orderSn)); OrderItemEntity itemEntity = itemEntities.get(0); // 订单名称使用商品项的名字 payVo.setSubject(itemEntity.getSkuName()); // 商品的描述使用商品项的属性 payVo.setBody(itemEntity.getSkuAttrsVals()); return payVo; } 最后生成页面 2、用户点击付款 3、付款成功后跳转到成功页面 跳转的页面是根据AlipayTemplate定义的回调地址来进行跳转 notify_url：支付成功异步回调，返回支付成功相关的信息 return_url：同步通知，支付成功后页面跳转到那里 1234567 // 服务器[异步通知]页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数，必须外网可以正常访问 // 支付宝会悄悄的给我们发送一个请求，告诉我们支付成功的信息 private String notify_url = &quot;http://member.gulimall.com/memberOrder.html&quot;;// 页面跳转同步通知页面路径 需http://格式的完整路径，不能加?id=123这类自定义参数，必须外网可以正常访问 //同步通知，支付成功，一般跳转到成功页 private String return_url = &quot;http://member.gulimall.com/memberOrder.html&quot;; 这里跳转到的会员服务的订单页面，需要自己处理请求,已在14.4.4.3节完成该功能 1、支付成功后异步回调接口处理 需要有服务器或配置了内网穿透才能接收到该方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 支付宝成功异步回调 * @author gcq * @Create 2021-01-08 */@RestControllerpublic class OrderPayedListener { @Autowired AlipayTemplate alipayTemplate; @Autowired OrderService orderService; /** * 支付宝异步通知回调接口,需要拥有内网穿透或服务器 * @param request * @return */ @PostMapping(&quot;/payed/notify&quot;) public String handleAlipayed(PayAsyncVo vo, HttpServletRequest request) throws UnsupportedEncodingException, AlipayApiException { /** * 重要一步验签名 * 防止别人通过postman给我们发送一个请求，告诉我们请求成功，为了防止这种效果通过验签 */ Map&lt;String,String&gt; params = new HashMap&lt;String,String&gt;(); Map&lt;String,String[]&gt; requestParams = request.getParameterMap(); for (Iterator&lt;String&gt; iter = requestParams.keySet().iterator(); iter.hasNext();) { String name = (String) iter.next(); String[] values = (String[]) requestParams.get(name); String valueStr = &quot;&quot;; for (int i = 0; i &lt; values.length; i++) { valueStr = (i == values.length - 1) ? valueStr + values[i] : valueStr + values[i] + &quot;,&quot;; } //乱码解决，这段代码在出现乱码时使用 valueStr = new String(valueStr.getBytes(&quot;ISO-8859-1&quot;), &quot;utf-8&quot;); params.put(name, valueStr); } // 支付宝验签 防止恶意提交 boolean signVerified = AlipaySignature.rsaCheckV1(params , alipayTemplate.getAlipay_public_key() , alipayTemplate.getCharset() , alipayTemplate.getSign_type()); if (signVerified) { String result = orderService.handleAlipayed(vo); return result; } else { return &quot;error&quot;; } }} Service 12345678910111213141516171819202122232425@Override public String handleAlipayed(PayAsyncVo vo) { // 保存交易流水信息,每个月和支付宝进行对账 PaymentInfoEntity infoEntity = new PaymentInfoEntity(); // 设置核心字段 infoEntity.setOrderSn(vo.getOut_trade_no()); infoEntity.setAlipayTradeNo(vo.getTrade_no()); infoEntity.setPaymentStatus(vo.getTrade_status()); infoEntity.setCallbackTime(vo.getNotify_time()); // 保存订单流水 paymentInfoService.save(infoEntity); /** * 支付宝交易状态说明 * https://opendocs.alipay.com/open/270/105902 */ // TRADE_FINISHED 交易结束、不可退款 // TRADE_SUCCESS 交易支付成功 if (vo.getTrade_status().equals(&quot;TRADE_SUCCESS&quot;) || vo.getTrade_status().equals(&quot;TRADE_FINISHED&quot;)) { String outTradeNo = vo.getOut_trade_no(); // 支付宝回调成功后，更改订单的支付状态位已支付 this.baseMapper.updateOrderStatus(outTradeNo,OrderStatusEnum.PAYED.getCode()); } return &quot;success&quot;; } 6、内网穿透 1、简介 内网穿透功能可以允许我们使用外网的网址来访问主机 正常的外网需要访问我们项目的流程是: 1、买服务器并且有公网固定IP 2、买域名映射到服务器的IP 3、域名需要进行备案和审核 2、使用场景 1、开发测试(微信、支付宝) 2、智慧互联 3、远程控制 4、私有云 3、内网穿透常用的软件 1、natapp:https://natapp.cn/ 2、续断: www.zhexi.tech 3、花生壳: https://www.oray.com/ 19、秒杀 19.1 秒杀业务 秒杀具有瞬间高并发的特点，针对这一特点，必须要做到限流 + 异步 + 缓存（页面静态化） + 独立部署 限流方式： 1、前端限流，一些高并发的网站直接在前端页面开始限流，列如：小米的验证码 2、nginx限流，直接负载部分请求到错误的静态页面，令牌算法、漏斗算法 3、网关限流、限流的过滤器 4、代码中使用分布式信号量 5、rabbitmq限流（能者多劳 channel.basicQos(1)）保证发挥服务器的所用性能 19.2 秒杀流程 参考京东秒杀流程 见秒杀流程图 完整流程图地址：https://www.processon.com/view/link/5fbda8c35653bb1d54f7077b 秒杀方式一 秒杀方式2 19.3 限流 19.4 秒杀核心业务 20、定时任务 20.1 cron表达式 语法：秒 分 时 日 月 周 年（Spring不支持） http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html 特殊字符： *（ “所有值”）-用于选择字段中的所有值。例如，分钟字段中的“ ***** ”表示*“每分钟”*。 ？（ “无特定值”）-当您需要在允许使用字符的两个字段之一中指定某个内容而在另一个不允许的字段中指定某些内容时很有用。例如，如果我希望在某个月的某个特定日期（例如，第10天）触发触发器，但不在乎一周中的哪一天发生，则将“ 10”设置为月字段，以及“？” 在“星期几”字段中。请参阅下面的示例以进行澄清。 --用于指定范围。例如，小时字段中的“ 10-12”表示*“小时10、11和12”*。 ， -用于指定其他值。例如，“星期几”字段中的“ MON，WED，FRI”表示*“星期一，星期三和星期五的日子”*。 / -用于指定增量。例如，秒字段中的“ 0/15”表示*“秒0、15、30和45”。秒字段中的“ 5/15”表示“秒5、20、35和50”。您还可以在“ ”字符后指定“ /” -在这种情况下，“ ”等效于在“ /”之前使用“ 0”。每月日期字段中的“ 1/3”表示“从该月的第一天开始每三天触发一次”*。 L（ “最后一个”）-在允许使用的两个字段中都有不同的含义。例如，“月”字段中的值“ L”表示*“月的最后一天”，即非January年的1月31日，2月28日。如果单独用于星期几字段，则仅表示“ 7”或“ SAT”。但是，如果在星期几字段中使用另一个值，则表示“该月的最后xxx天”，例如“ 6L”表示“该月的最后一个星期五”*。您还可以指定与该月最后一天的偏移量，例如“ L-3”，这表示日历月的倒数第三天。 使用“ L”选项时，不要指定列表或值的范围很重要，因为这样会导致混淆/意外结果。 W（ “工作日”）-用于指定最接近给定日期的工作日（星期一至星期五）。例如，如果您要指定“ 15W”作为“月日”字段的值，则含义是： “离月15日最近的工作日”。因此，如果15号是星期六，则触发器将在14号星期五触发。如果15日是星期日，则触发器将在16日星期一触发。如果15号是星期二，那么它将在15号星期二触发。但是，如果您将“ 1W”指定为月份的值，并且第一个是星期六，则触发器将在第3个星期一触发，因为它不会“跳过”一个月日的边界。仅当月份中的某天是一天，而不是范围或天数列表时，才可以指定“ W”字符。 “ L”和“ W”字符也可以在“月日”字段中组合以产生“ LW”，这表示*“每月的最后一个工作日” *。 ＃ -用于指定每月的第“ XXX”天。例如，“星期几”字段中的值“ 6＃3”表示*“该月的第三个星期五”（第6天=星期五，“＃3” =该月的第三个星期五*）。其他示例：“ 2＃1” =该月的第一个星期一，“ 4＃5” =该月的第五个星期三。请注意，如果您指定“＃5”，并且该月的指定星期几中没有5个，则该月将不会触发。 法定字符以及月份和星期几的名称不区分大小写。MON 与mon相同。 20.2 cron 示例 阅读技巧：秒 分 时 日 月 周 使用谷歌翻译后中文意思是这样的 20.3 SpringBoot整合定时任务 定时任务相关注解: 1234@EnableAsync // 启用Spring异步任务支持@EnableScheduling // 启用Spring的计划任务执行功能@Async 异步@Scheduled(cron = &quot;* * * * * ?&quot;) 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @author gcq * @Create 2020-11-23 * * 定时任务 * 1、@EnableScheduling 开启定时任务 * 2、Scheduled 开启一个定时任务 * 3、自动配置类 TaskSchedulingAutoConfiguration 属性绑定在TaskExecutionProperties * * 异步任务 * 1、@EnableAsync 开启异步任务功能 * 2、@Async 给希望异步执行的方法上标注 * 3、自动配置类 TaskExecutionAutoConfiguration * */@Slf4j@Component@EnableAsync // 启用Spring异步任务支持@EnableScheduling // 启用Spring的计划任务执行功能public class HelloSchedule { /** * 1、Spring中6位组成，不允许第7位的年 * 2、在周几的位置，1-7代表周一到周日：MON-SUN * 3、定时任务应该阻塞，默认是阻塞的 * 1、可以让业务以异步的方式运行，自己提交到线程池 * CompletableFuture.runAsync(() -&gt; { * xxxService.hello(); * }) * 2、支持定时任务线程池，设置 TaskSchedulingProperties * spring.task.scheduling.pool.size=5 * 3、让定时任务异步执行 * 异步执行 * 解决：使用异步 + 定时任务来完成定时任务不阻塞的功能 */// @Async 异步// @Scheduled(cron = &quot;* * * * * ?&quot;)// public void hello() {// log.info(&quot;hello...&quot;);// }} 20.4 分布式定时任务 20.5定时任务的问题 20.6 扩展 - 分布式调整 21、秒杀（高并发）系统关注的问题 21.1 服务单一职责 秒杀服务即使自己扛不住压力，挂掉，不要影响别人 21.2 秒杀链接加密 防止恶意攻击模拟秒杀请求，1000次/s攻击 防止连接暴露，自己工作人员，提前秒杀商品 21.3 库存预热 + 快速扣减 提前把数据加载缓存中， 21.4 动静分离 nginx做好动静分离，保证秒杀和商品详情页的动态请求才打到后端的服务集群 使用 CDN 网络，分担服务器压力 21.5 恶意请求拦截 识别非法攻击的请求并进行拦截 ，网管层 21.6 流量错峰 使用各种手段，将流量分担到更大宽度的时间点，比如验证码，加入购物车 21.7 限流 &amp; 熔断 &amp; 降级 前端限流 + 后端限流 限制次数，限制总量，快速失败降级运行，熔断隔离防止雪崩 21.8 队列消峰 1万个请求，每个1000件被秒杀，双11所有秒杀成功的请求，进入队列，慢慢创建订单，扣减库存即可","link":"/2021/01/20/Draft/2021/%E8%B0%B7%E7%B2%92%E5%95%86%E5%9F%8E/"},{"title":"博客模板","text":"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 嗨，请准确无误地输入密码查看哟！","link":"/2021/01/19/Draft/2021/Hexo%E4%BD%BF%E7%94%A8%E8%8C%83%E4%BE%8B/"},{"title":"魑魅先生 | CRUD","text":"各种 CRUD 呀，CRUD只学一次。 🛩️ SSM SpringBoot SpringCloud Redis","link":"/2021/05/13/Draft/2021/CRUD/"},{"title":"文件目录","text":"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 嗨，请准确无误地输入密码查看哟！","link":"/2021/03/12/Draft/2021/%E5%A4%A7%E7%BA%B2%E7%9B%AE%E5%BD%95/"},{"title":"年月总结","text":"4d02fb5b32d4ff17604ac73d9008aa6207c92f8bed33bd3156da7ba1e22b4e270d2eaa168bb51445b05ae47a3eac1a4aa0bfdb57f71dd9f0527d69208619b11b15dc3220e59d8a5b73bd2b116a43f17b2034bc7843a0ff6233c0d49758436887255b8f3f0fa46b4ce31673f9bf7ae3e20a15690b8b5e9d5239e1e669e1c3a9b1adebe88090ddd3d9d7f8ff3e0db7157d57eaa4ca81b1e2d581c61fbdb6e92fe8f1617de0506e19df50af99e7eb12dcb6baf7126901f8827d8a6c5b2db24c8f52afbecaf223d50ba3950a5c5c55cdc20bee2ea92807a92a2bf5c37c625d00de34ad7ba312314348ea97206f616516eb61e2a8f78f5f5cdb8f2a7f96ebbf039d15f32bef5f3093e7f3dab0e338c36950c9ca89258353a2e59070c5e1020429e5cc11f6a39b5ef26a52a499c2dca07e59e76a84a61e70fe15f7910235f8f5207eee93435fa62a027c300bf563ebd2babfa36c8aa9bc054a35d9ae5a5fe150aa65d9d73dd67f8db3b737d53175a2a04f7729d9f27df6912ddd574fce90e829dd997f7c10487abd145bf9cdbbb7b8eb81e990a7f9fe5ed0767d8958707cb3aa0f3eb20545f0064e336c78a362cebb222a8f3a5e2c2ca6c2ee4ad4eb98efc3158294fffcfb3fc4f0deeabb6d8bcc4e825d3b4120a671bc6d5b65893b98e896c025415176809b5d32cb2d5b8355d789102802efdd100cf49c9a6aa35a8f0c7c4945af3a79494bc0d15200361e840474b34d42cbd3b09532b9136687d54c58001e533e24539ca6054071218aba8dba729dba254488b0a9451f99495f671df2c430f2f43051d3e62042ae5b18c8a1ff32c7ec3a3cbc8bc1f04a4ae6fd92612eb616ec7dc4f0aefa54d7757df7991888bce1a2695c4ca7bb16045e67e63c931a8b1c32de36911b3dd6fb9738344cd4ca535af14518946117fa1b6944568766759ec193ec0f3ebfd6925bad86c5aae320f3494bee609f4fd6a8f7d26461502b69b72e7893249d7a5f64eaae1a6b41e1068edd8a942911ab15208f5d5be12e4c11f67e32681dc8f97780f50cb095c3106b547aba77cb48730138d098a318d5e0f2f911b25c2fff3de934ded3ef46537e84133b301890b3374355456cc8154b56608d0633d5ad95a08c0978ce9ccab9762bf93a4b46eb849bacf384e181a8ab38eca2b2ac504192fdafc33aa5cf8fa49f35d7aaa45fc614ed928a45f663d9e43e7dee48fcf7de290bba29b8b503f7b2752c5aad193f4dc4ddbe297d3e95a850f90c9f0b9fd8df 嗨，请准确无误地输入密码查看哟！","link":"/2021/03/12/Draft/2021/%E5%B9%B4%E6%9C%88%E6%80%BB%E7%BB%93/"},{"title":"MyBatis","text":"MyBatis 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】：Bili、官方文档、源码 **LEVEL【不是每个都学精】：**速看速记 进度：【17】两天 快查 常用语句 一、简介 二、动态SQL 三、缓存 什么是缓存？是存在内存中的缓存数据，此数据一般不经常改变，以此提升查询效率。 一级缓存 默认开启，在开启连接到关闭连接之间有效，即一次sqlSession中有效。 缓存失效情况 查不同数据 增删改，可能会改变数据的操作 查询不同的mapper.xml 二级缓存 12345&lt;!--mybatis-config.xml配置文件中 --&gt;&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot; /&gt; &lt;!-- 全局映射器启用缓存 --&gt;&lt;!--mapper中开启缓存，可自定义一些参数，没有eviction时需要序列化实体类（implement Serializable） --&gt;&lt;cache/&gt; 一级缓存失效时二级缓存失效或提交会话后将数据提交到二级缓存。先读二级缓存再一级缓存。 自定义缓存 ehcache 分布式缓存 可增加保存到磁盘等增强实现。 现在一般用redis。 1&lt;cache type=&quot;自定义缓存的包.自定义缓存&quot;/&gt; ${}和#{}区别","link":"/2021/01/20/Draft/2021/MyBatis/"},{"title":"Life","text":"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 嗨，请准确无误地输入密码查看哟！","link":"/2021/01/12/Draft/2021/Life/"}],"tags":[{"name":"BUG","slug":"BUG","link":"/tags/BUG/"},{"name":"CRUD","slug":"CRUD","link":"/tags/CRUD/"},{"name":"工具教程","slug":"工具教程","link":"/tags/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/"},{"name":"程序人生","slug":"程序人生","link":"/tags/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"},{"name":"Life","slug":"Life","link":"/tags/Life/"},{"name":"Frame","slug":"Frame","link":"/tags/Frame/"},{"name":"NO GAME NO LIFE","slug":"NO-GAME-NO-LIFE","link":"/tags/NO-GAME-NO-LIFE/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"Software Engineer","slug":"Software-Engineer","link":"/tags/Software-Engineer/"},{"name":"待办","slug":"待办","link":"/tags/%E5%BE%85%E5%8A%9E/"},{"name":"模板","slug":"模板","link":"/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"书影音","slug":"书影音","link":"/tags/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"name":"健身外型","slug":"健身外型","link":"/tags/%E5%81%A5%E8%BA%AB%E5%A4%96%E5%9E%8B/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"文件目录","slug":"文件目录","link":"/tags/%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/"},{"name":"学习方法","slug":"学习方法","link":"/tags/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"电脑知识","slug":"电脑知识","link":"/tags/%E7%94%B5%E8%84%91%E7%9F%A5%E8%AF%86/"},{"name":"摄影摄像","slug":"摄影摄像","link":"/tags/%E6%91%84%E5%BD%B1%E6%91%84%E5%83%8F/"},{"name":"架构师","slug":"架构师","link":"/tags/%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"每日算法","slug":"每日算法","link":"/tags/%E6%AF%8F%E6%97%A5%E7%AE%97%E6%B3%95/"},{"name":"English","slug":"English","link":"/tags/English/"},{"name":"每日面题","slug":"每日面题","link":"/tags/%E6%AF%8F%E6%97%A5%E9%9D%A2%E9%A2%98/"},{"name":"自媒体","slug":"自媒体","link":"/tags/%E8%87%AA%E5%AA%92%E4%BD%93/"},{"name":"临时记录","slug":"临时记录","link":"/tags/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/"},{"name":"音乐","slug":"音乐","link":"/tags/%E9%9F%B3%E4%B9%90/"},{"name":"信息系统项目管理师论文模板","slug":"信息系统项目管理师论文模板","link":"/tags/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/"},{"name":"Dubbo","slug":"Dubbo","link":"/tags/Dubbo/"},{"name":"SpringCloud","slug":"SpringCloud","link":"/tags/SpringCloud/"},{"name":"信息系统项目管理师","slug":"信息系统项目管理师","link":"/tags/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88/"},{"name":"微信小程序","slug":"微信小程序","link":"/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"快查","slug":"快查","link":"/tags/%E5%BF%AB%E6%9F%A5/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"RabbitMQ","slug":"RabbitMQ","link":"/tags/RabbitMQ/"},{"name":"Python学习","slug":"Python学习","link":"/tags/Python%E5%AD%A6%E4%B9%A0/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"底层学习","slug":"底层学习","link":"/tags/%E5%BA%95%E5%B1%82%E5%AD%A6%E4%B9%A0/"},{"name":"编程语言","slug":"编程语言","link":"/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"谷粒商城","slug":"谷粒商城","link":"/tags/%E8%B0%B7%E7%B2%92%E5%95%86%E5%9F%8E/"}],"categories":[{"name":"BUG记录","slug":"BUG记录","link":"/categories/BUG%E8%AE%B0%E5%BD%95/"},{"name":"程序基础","slug":"程序基础","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/"},{"name":"工具教程","slug":"工具教程","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/"},{"name":"BUG","slug":"BUG记录/BUG","link":"/categories/BUG%E8%AE%B0%E5%BD%95/BUG/"},{"name":"程序人生","slug":"程序人生","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"},{"name":"CRUD","slug":"程序基础/CRUD","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/CRUD/"},{"name":"Frame","slug":"Frame","link":"/categories/Frame/"},{"name":"主题工具","slug":"工具教程/主题工具","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E4%B8%BB%E9%A2%98%E5%B7%A5%E5%85%B7/"},{"name":"游戏人生","slug":"游戏人生","link":"/categories/%E6%B8%B8%E6%88%8F%E4%BA%BA%E7%94%9F/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Office","slug":"工具教程/Office","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/Office/"},{"name":"Software Engineer","slug":"程序人生/Software-Engineer","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/Software-Engineer/"},{"name":"Interesting Programs","slug":"程序人生/Interesting-Programs","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/Interesting-Programs/"},{"name":"待办","slug":"待办","link":"/categories/%E5%BE%85%E5%8A%9E/"},{"name":"Life","slug":"程序人生/Life","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/Life/"},{"name":"模板","slug":"模板","link":"/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"书影音","slug":"书影音","link":"/categories/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"name":"博客搭建","slug":"工具教程/博客搭建","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"健康","slug":"健康","link":"/categories/%E5%81%A5%E5%BA%B7/"},{"name":"Markdown","slug":"工具教程/Markdown","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/Markdown/"},{"name":"前端","slug":"程序基础/前端","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/%E5%89%8D%E7%AB%AF/"},{"name":"文件目录","slug":"文件目录","link":"/categories/%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/"},{"name":"MyBatis-Plus","slug":"Frame/MyBatis-Plus","link":"/categories/Frame/MyBatis-Plus/"},{"name":"学习方法","slug":"学习方法","link":"/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"年月总结","slug":"程序人生/年月总结","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E5%B9%B4%E6%9C%88%E6%80%BB%E7%BB%93/"},{"name":"电脑知识","slug":"电脑知识","link":"/categories/%E7%94%B5%E8%84%91%E7%9F%A5%E8%AF%86/"},{"name":"生活爱好","slug":"生活爱好","link":"/categories/%E7%94%9F%E6%B4%BB%E7%88%B1%E5%A5%BD/"},{"name":"MyBatis","slug":"Frame/MyBatis","link":"/categories/Frame/MyBatis/"},{"name":"系统设计","slug":"系统设计","link":"/categories/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"name":"每日任务","slug":"每日任务","link":"/categories/%E6%AF%8F%E6%97%A5%E4%BB%BB%E5%8A%A1/"},{"name":"NO GAME NO LIFE","slug":"游戏人生/NO-GAME-NO-LIFE","link":"/categories/%E6%B8%B8%E6%88%8F%E4%BA%BA%E7%94%9F/NO-GAME-NO-LIFE/"},{"name":"English","slug":"English","link":"/categories/English/"},{"name":"Nginx","slug":"中间件/Nginx","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Nginx/"},{"name":"自媒体","slug":"自媒体","link":"/categories/%E8%87%AA%E5%AA%92%E4%BD%93/"},{"name":"资源篇","slug":"工具教程/资源篇","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E8%B5%84%E6%BA%90%E7%AF%87/"},{"name":"临时记录","slug":"临时记录","link":"/categories/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/"},{"name":"SpringMVC","slug":"Frame/SpringMVC","link":"/categories/Frame/SpringMVC/"},{"name":"Everyday-TODO","slug":"待办/Everyday-TODO","link":"/categories/%E5%BE%85%E5%8A%9E/Everyday-TODO/"},{"name":"模板","slug":"模板/模板","link":"/categories/%E6%A8%A1%E6%9D%BF/%E6%A8%A1%E6%9D%BF/"},{"name":"书影音","slug":"书影音/书影音","link":"/categories/%E4%B9%A6%E5%BD%B1%E9%9F%B3/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"name":"健身穿搭","slug":"健康/健身穿搭","link":"/categories/%E5%81%A5%E5%BA%B7/%E5%81%A5%E8%BA%AB%E7%A9%BF%E6%90%AD/"},{"name":"文件目录","slug":"文件目录/文件目录","link":"/categories/%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95/"},{"name":"如何学习一个新知识","slug":"学习方法/如何学习一个新知识","link":"/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9F%A5%E8%AF%86/"},{"name":"开发修电脑","slug":"电脑知识/开发修电脑","link":"/categories/%E7%94%B5%E8%84%91%E7%9F%A5%E8%AF%86/%E5%BC%80%E5%8F%91%E4%BF%AE%E7%94%B5%E8%84%91/"},{"name":"摄影摄像","slug":"生活爱好/摄影摄像","link":"/categories/%E7%94%9F%E6%B4%BB%E7%88%B1%E5%A5%BD/%E6%91%84%E5%BD%B1%E6%91%84%E5%83%8F/"},{"name":"架构师","slug":"系统设计/架构师","link":"/categories/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/%E6%9E%B6%E6%9E%84%E5%B8%88/"},{"name":"算法基础","slug":"每日任务/算法基础","link":"/categories/%E6%AF%8F%E6%97%A5%E4%BB%BB%E5%8A%A1/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"程序员英语","slug":"English/程序员英语","link":"/categories/English/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%8B%B1%E8%AF%AD/"},{"name":"每日面题","slug":"每日任务/每日面题","link":"/categories/%E6%AF%8F%E6%97%A5%E4%BB%BB%E5%8A%A1/%E6%AF%8F%E6%97%A5%E9%9D%A2%E9%A2%98/"},{"name":"自媒体","slug":"自媒体/自媒体","link":"/categories/%E8%87%AA%E5%AA%92%E4%BD%93/%E8%87%AA%E5%AA%92%E4%BD%93/"},{"name":"思维闪光","slug":"临时记录/思维闪光","link":"/categories/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/%E6%80%9D%E7%BB%B4%E9%97%AA%E5%85%89/"},{"name":"兴趣爱好","slug":"兴趣爱好","link":"/categories/%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD/"},{"name":"考证","slug":"考证","link":"/categories/%E8%80%83%E8%AF%81/"},{"name":"Dubbo","slug":"中间件/Dubbo","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Dubbo/"},{"name":"音乐","slug":"兴趣爱好/音乐","link":"/categories/%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD/%E9%9F%B3%E4%B9%90/"},{"name":"SpringCloud","slug":"Frame/SpringCloud","link":"/categories/Frame/SpringCloud/"},{"name":"信息系统项目管理师","slug":"考证/信息系统项目管理师","link":"/categories/%E8%80%83%E8%AF%81/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88/"},{"name":"信息系统项目管理师论文模板","slug":"考证/信息系统项目管理师论文模板","link":"/categories/%E8%80%83%E8%AF%81/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/"},{"name":"编程","slug":"编程","link":"/categories/%E7%BC%96%E7%A8%8B/"},{"name":"编程工具","slug":"编程工具","link":"/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"},{"name":"编程基础","slug":"编程基础","link":"/categories/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"},{"name":"微信小程序","slug":"编程/微信小程序","link":"/categories/%E7%BC%96%E7%A8%8B/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"快查","slug":"编程工具/快查","link":"/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/%E5%BF%AB%E6%9F%A5/"},{"name":"计算机网络","slug":"编程基础/计算机网络","link":"/categories/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"RabbitMQ","slug":"中间件/RabbitMQ","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/RabbitMQ/"},{"name":"业务技术","slug":"程序人生/业务技术","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/%E4%B8%9A%E5%8A%A1%E6%8A%80%E6%9C%AF/"},{"name":"编程语言","slug":"编程语言","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"Spring","slug":"Frame/Spring","link":"/categories/Frame/Spring/"},{"name":"Python学习","slug":"编程语言/Python学习","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python%E5%AD%A6%E4%B9%A0/"},{"name":"实施维护","slug":"实施维护","link":"/categories/%E5%AE%9E%E6%96%BD%E7%BB%B4%E6%8A%A4/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"服务器","slug":"实施维护/服务器","link":"/categories/%E5%AE%9E%E6%96%BD%E7%BB%B4%E6%8A%A4/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"MongoDB","slug":"数据库/MongoDB","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/"},{"name":"MYSQL优化","slug":"数据库/MYSQL优化","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL%E4%BC%98%E5%8C%96/"},{"name":"设计模式","slug":"程序基础/设计模式","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Redis","slug":"程序基础/Redis","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/Redis/"},{"name":"Docker","slug":"Linux/Docker","link":"/categories/Linux/Docker/"},{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"},{"name":"SpringBoot","slug":"Frame/SpringBoot","link":"/categories/Frame/SpringBoot/"},{"name":"GIS","slug":"前端/GIS","link":"/categories/%E5%89%8D%E7%AB%AF/GIS/"},{"name":"底层学习","slug":"底层学习","link":"/categories/%E5%BA%95%E5%B1%82%E5%AD%A6%E4%B9%A0/"},{"name":"JVM 与上层技术","slug":"底层学习/JVM-与上层技术","link":"/categories/%E5%BA%95%E5%B1%82%E5%AD%A6%E4%B9%A0/JVM-%E4%B8%8E%E4%B8%8A%E5%B1%82%E6%8A%80%E6%9C%AF/"},{"name":"Java","slug":"编程语言/Java","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/"},{"name":"项目","slug":"项目","link":"/categories/%E9%A1%B9%E7%9B%AE/"},{"name":"谷粒商城","slug":"项目/谷粒商城","link":"/categories/%E9%A1%B9%E7%9B%AE/%E8%B0%B7%E7%B2%92%E5%95%86%E5%9F%8E/"}]}