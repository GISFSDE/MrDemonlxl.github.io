{"pages":[{"title":"","text":"【个人简介】 分享不知出处的一段话： &quot;你所有的烦恼都在于你想得太多，而做得太少。&quot; 无论多么精美浩瀚的绝世计划，都应该点点滴滴赋予行动。 在这里你可以得到什么? 😎 效率的种种利器！ 😎 Java全栈技术学习！ 😎 代码之外的丰富生活！ 【总内容导图】 【关于魑魅先生】 【动漫人物介绍】 魑魅魍魉（chī mèi wǎng liǎng），形形色色妖魔鬼怪，现通指坏人。而魑魅先生乃妖中之首，掌控妖魔万物，专克魑魅魍魉，其面容清秀，千变万化，善学好思矣。其间见霸戈【BUG】，祸害人世间，便习编程术，从此不归路。 绘画作者: @YYu 【联系方式】 如有任何交流需求，请添加个人微信或个人公众号 添加前请备注来由。(๑‾ ꇴ ‾๑) 微信扫码或点击链接关注我哦︿(￣︶￣)︿ 凡事预则立，不预则废。 计划 2021-PLANS DAY 读书 每日算法 每日英语 理财，记账 WEEK 绘画 吉他 运动两次 练字 MONTH 总结 YEAR 熟练一个以上技能 目标 2021-GOALS 读书 《社会心理学》 《大问题：简明哲学导论》 《局外人》 《三体》 《我是猫》 专业 设计模式代码Demo 网络 JVM VUE项目 Linux 音乐 吉他谱扒谱学习 运动 画画 时间轴记录 本站推荐索引 技术知识点 Java并发知识点 法律法规 法律法规数据库 中华人民共和国国旗法 中华人民共和国宪法 中华人民共和国消费者权益保护法 中华人民共和国刑事诉讼法 中华人民共和国婚姻法 中华人名共和国网络安全法 中华人民共和国劳动法 其他 网易云音乐歌单分享","link":"/about/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://mrdemonlxl.github.io/images/avatar.jpg 网站名称：魑魅先生 网站地址：https://mrdemonlxl.github.io/ 网站简介：Java全栈开发，技术分享，程序员的生活分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"","text":"🎈🎈摄影-平面设计-绘画欣赏🎈🎈 每天看一点，审美高一点 😊薇薇安·迈尔😊 ​ 薇薇安·迈尔（Vivian Maier），1926年2月1日出生于美国纽约，美国业余街头摄影师、家庭保姆。薇薇安是法国人后裔，出生在纽约，但在法国长大，后回到美国先后生活在纽约和芝加哥，她一生拍摄了超过10万张照片。2007年，芝加哥当地历史学家约翰·马鲁夫发现了她的大量底片并开始整理，此后她的作品登上美国以至意大利、阿根廷和英国等地的报纸。2009年4月21日病逝于芝加哥。2010年，薇薇安的作品开始在芝加哥进行展出，成为摄影圈中热议的人物，并被认可为美国当代最重要的街头摄影师之一。 图片部分搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &nbsp;&nbsp;看看视频 ->点击以下条目开始播放视频,向下滑动查看更多","link":"/media/index.html"},{"title":"音乐歌单收藏","text":"个人学习食用，如有推荐请留言歌单或歌曲链接。点击左下角箭头可全局播放哦 - 🤲😎 温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"碎碎念树洞 tips：github登录后按时间正序查看、可点赞加❤️「+99次查看」 碎碎念加载中，请稍等... $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ clientID: '20ef8321698ab8c0c37b', clientSecret: '237515b7faa3e1097f2980762f772c6f02dbcbbb', id: '666666', repo: 'MrDemonlxl.github.io', owner: 'MrDemonlxl', admin: \"MrDemonlxl\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container1'); });","link":"/self-talking/index.html"},{"title":"","text":"好问则裕，自用则小。——《尚书·仲虺之诰》","link":"/message/index.html"}],"posts":[{"title":"BUG","text":"一次相见，绝不再见 BUG分类解决记录，常见排查方法，常见避免方法总结。 MYSQL Client does not support authentication protocol requested by server; 123mysql -hlocalhost -uroot -pALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root';FLUSH PRIVILEGES; Unknown initial character set index '255' received from server. Initial client character set can be forced via the 'characterEncoding' property. 12String url后添加?useUnicode=true&amp;characterEncoding=utf8 mybatis url后添加?useUnicode=true&amp;characterEncoding=utf8 cannot open git-upload-pack IDEA 设置 请求有参数有特殊字符 post参数写body里面 tomcat connecter加relaxedPathChars=&quot;|{}[]^&quot; relaxedQueryChars=&quot;|{}[]^&quot; 123456789101112131415161718192021222324252627package com.jeethink.framework.config;import org.apache.catalina.connector.Connector;import org.springframework.boot.web.embedded.tomcat.TomcatConnectorCustomizer;import org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory;import org.springframework.boot.web.servlet.server.ConfigurableServletWebServerFactory;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;/** * 解决高版本Tomcat不支持在URL中传入特殊字符（比如|）的问题 */@Configurationpublic class TomcatServerConfig { @Bean public ConfigurableServletWebServerFactory webServerFactory() { TomcatServletWebServerFactory factory = new TomcatServletWebServerFactory(); factory.addConnectorCustomizers(new TomcatConnectorCustomizer() { @Override public void customize(Connector connector) { //允许特殊字符 connector.setProperty(&quot;relaxedQueryChars&quot;, &quot;|{}[]&quot;); } }); return factory; }}","link":"/2021/02/24/Draft/2021/BUG/"},{"title":"魑魅先生 | CRUD","text":"各种 CRUD 呀，CRUD只学一次。 🛩️ SSM SpringBoot SpringCloud Redis","link":"/2021/12/13/Draft/2021/CRUD/"},{"title":"博客模板","text":"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 嗨，请准确无误地输入密码查看哟！","link":"/2021/01/19/Draft/2021/Hexo%E4%BD%BF%E7%94%A8%E8%8C%83%E4%BE%8B/"},{"title":"Interesting Programs","text":"一些有趣的编程项目，回到乐趣本身。 编程绘画，图形，深度学习等CRUD意外有趣的东西。 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度：上篇 【】 大数据 深度学习 机器学习 快查 引用：","link":"/2022/01/15/Draft/2021/Interesting%20Programs/"},{"title":"Markdown 学习","text":"改变颜色，内嵌 HTML 标签 Markdown 学习 Markdown作为自媒体人非常方便的一个工具，可以再各大自媒体网站通用，实现一篇多发的效果。其语法简单，几乎可以抛弃鼠标，生成一切样式。 Markdown 学习 符号(英文输入状态下，开头输入“:”+英文字母有提示) Markdown箭头的输入方法汇总 普通箭头 箭头形状 MarkDown $\\uparrow$ $\\uparrow$ $\\Uparrow$ $\\Uparrow$ $\\downarrow$ $\\downarrow$ $\\Downarrow$ $\\Downarrow$ $\\leftarrow$ $\\leftarrow$ $\\Leftarrow$ $\\Leftarrow$ $\\rightarrow$ $\\rightarrow$ $\\Rightarrow$ $\\Rightarrow$ $\\updownarrow$ $\\updownarrow$ $\\Updownarrow$ $\\Updownarrow$ $\\leftrightarrow$ $\\leftrightarrow$ $\\Leftrightarrow$ $\\Leftrightarrow$ 长箭头 箭头形状 MarkDown $\\longleftarrow$ $\\longleftarrow$ $\\Longleftarrow$ $\\Longleftarrow$ $\\longrightarrow$ $\\longrightarrow$ $\\Longrightarrow$ $\\Longrightarrow$ $\\longleftrightarrow$ $\\longleftrightarrow$ $\\Longleftrightarrow$ $\\Longleftrightarrow$ 其他箭头 箭头形状 MarkDown $\\twoheadrightarrow$ $\\twoheadrightarrow$ $\\rightarrowtail$ $\\rightarrowtail$ $\\looparrowright$ $\\looparrowright$ $\\curvearrowright$ $\\curvearrowright$ $\\circlearrowright$ $\\circlearrowright$ $\\Rsh$ $\\Rsh$ $\\multimap$ $\\multimap$ $\\leftrightsquigarrow$ $\\leftrightsquigarrow$ $\\rightsquigarrow$ $\\rightsquigarrow$ $\\leadsto$ $\\leadsto$ $\\nearrow$ $\\nearrow$ $\\searrow$ $\\searrow$ $\\swarrow$ $\\swarrow$ $\\nwarrow$ $\\nwarrow$ $\\nleftarrow$ $\\nleftarrow$ $\\nLeftarrow$ $\\nLeftarrow$ $\\nrightarrow$ $\\nrightarrow$ $\\nRightarrow$ $\\nRightarrow$ $\\nleftrightarrow$ $\\nleftrightarrow$ $\\nLeftrightarrow$ $\\nLeftrightarrow$ $\\dashrightarrow$ $\\dashrightarrow$ $\\dashleftarrow$ $\\dashleftarrow$ $\\leftleftarrows$ $\\leftleftarrows$ $\\leftrightarrows$ $\\leftrightarrows$ $\\Lleftarrow$ $\\Lleftarrow$ $\\twoheadleftarrow$ $\\twoheadleftarrow$ $\\leftarrowtail$ $\\leftarrowtail$ $\\looparrowleft$ $\\looparrowleft$ $\\curvearrowleft$ $\\curvearrowleft$ 箭头形状 MarkDown $\\circlearrowleft$ $\\circlearrowleft$ $\\Lsh$ $\\Lsh$ $\\mapsto$ $\\mapsto$ $\\hookleftarrow$ $\\hookleftarrow$ $\\hookrightarrow$ $\\hookrightarrow$ $\\upharpoonright$ $\\upharpoonright$ $\\upharpoonleft$ $\\upharpoonleft$ $\\downharpoonright$ $\\downharpoonright$ $\\downharpoonleft$ $\\downharpoonleft$ $\\leftharpoonup$ $\\leftharpoonup$ $\\rightharpoonup$ $\\rightharpoonup$ $\\rightharpoondown$ $\\rightharpoondown$ $\\leftharpoondown$ $\\leftharpoondown$ $\\upuparrows$ $\\upuparrows$ $\\downdownarrows$ $\\downdownarrows$ $\\rightrightarrows$ $\\rightrightarrows$ $\\rightleftarrows$ $\\rightleftarrows$ $\\leftleftarrows$ $\\leftleftarrows$ $\\leftrightarrows$ $\\leftrightarrows$ $\\rightleftharpoons$ $\\rightleftharpoons$ $\\leftrightharpoons$ $\\leftrightharpoons$ 分割线 LaTeX符号 语法：$\\clubsuit$ $\\triangleright$ 格式 123&amp;emsp;首行缩进~~删除线~~~缩小~ 导图 图 状态转移图 stateDiagram %% 单程生命周期起点是实心圆，终点是同心圆，内圆为实心。 %%这个例子包含是3个状态Still, Moving 和 Crash. 从Still状态可以转移到Moving，从Moving可以转移到Still 或者 Crash。不能从Still转移到Crash [*] --> Still Still --> [*] Still --> Moving: A transition note right of Moving Moving可以转移到Still或者Crash end note Moving --> Still Moving --> Crash Crash --> [*] 类图 classDiagram class Duck{ -String beakColor - double weight +swim() +quack() #count() +getPrice(count) double +someAbstractMethod() * -someStaticMethod() $ } class Shape{ %% This whole line is a comment classDiagram class Shape noOfVertices draw() } class Color{ RED BLUE GREEN WHITE BLACK } class Relation{ } classK ..> classL : 依赖关系 classA --|> classB : 继承关系（泛化） classM ..|> classN : 实现关系 classG --> classH : 关联关系 classE --o classF : 聚合关系 classC --* classD : 组合关系 Customer \"1\" --> \"*\" Ticket Student \"1\" --> \"1..*\" Course Galaxy --> \"many\" Star : Contains Sky \"1\"--> \"1\" Sun Parent \"1\" -- \"0..2\" Children Person \"1\" -- \"2\" Eyes 饼图 pie title Pie Chart \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 150 \"Cows\" : 150 渲染效果 graph LR id1(Start)-->id2(Stop) style id1 fill:#f9f,stroke:#333,stroke-width:4px; style id2 fill:#f00,stroke:#000,stroke-width:2px,stroke-dasharray:5,5; 基础fontawesome支持 graph TD B[\"fa:fa-twitter for peace\"] B-->C[fa:fa-ban forbidden] B-->D(fa:fa-spinner); B-->E(A fa:fa-camerra-retro perhaps?); 连线 graph TD A1==TEXT===B1 A2-->|text|B2 A3..-B3 节点形状 graph TD B[bname] C(cname) D((dname)) E>ename] F{fname} 甘特图 关键词说明： title—标题 dateFormat—日期格式 section—模块 Completed—已经完成 Active—当前正在进行 Future—后续待处理 crit—关键阶段 gantt dateFormat YYYY-MM-DD title Adding GANTT diagram functionality to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d future task : des3, after des2, 5d future task2 : des4, after des3, 5d section Critical tasks Completed task in the critical line :crit, done, 2014-01-06,24h Implement parser and json :crit, done, after des1, 2d Create tests for parser :crit, active, 3d Future task in critical line :crit, 5d Create tests for renderer :2d Add to ,mermaid :1d 流程图 graph TD A[方形] -->B(圆角) B --> C{条件a} C -->|a=1| D[结果1] C -->|a=2| E[结果2] F[竖向流程图] 时序图示例 123456789Title:时序图示例客户端-&gt;服务端: 我想找你拿下数据 SYN服务端--&gt;客户端: 我收到你的请求啦 ACK+SYN客户端-&gt;&gt;服务端: 我收到你的确认啦，我们开始通信吧 ACKNote right of 服务端: 我是一个服务端Note left of 客户端: 我是一个客户端Note over 服务端,客户端: TCP 三次握手participant 观察者 公式 $$ \\mathbf{V}_1 \\times \\mathbf{V}_2 = \\begin{vmatrix} \\mathbf{i} &amp; \\mathbf{j} &amp; \\mathbf{k} \\ \\frac{\\partial X}{\\partial u} &amp; \\frac{\\partial Y}{\\partial u} &amp; 0 \\ \\frac{\\partial X}{\\partial v} &amp; \\frac{\\partial Y}{\\partial v} &amp; 0 \\ \\end{vmatrix} ${$tep1}{\\style{visibility:hidden}{(x+1)(x+1)}} $$ 表格 ctl+回车新增行 左对齐 右对齐 居中对齐 单元格 单元格 单元格 单元格 单元格 单元格 代码 123456hello!--- 分割线+空格[空格]空格 勾选框[内容文字](#标题) 页内跳转[内容文字](跳转目标文件的相对路径) 脚注 文字内容 [1] todo列表 +空格[空格]空格 勾选框+ [ ] [ ] [ ] 读书 《》 专业 （设计模式代码Demo， 网络， JVM， VUE项目， Linux） 音乐 吉他谱扒谱学习 运动 画画 其他 emoji【外可访】，Typora 快捷键WIN+。 🎈🎈 🤲 😎 inserted =&gt; inserted 29th =&gt; 29th H20 =&gt; H2O basic footnote[1:1] here is an inline footnote[^2](inline footnote) and another one[2] and another one[3] footnote content The HTML specification is maintained by the W3C. flowchat st=>start: Start|past:>http://www.google.com[blank] e=>end: End:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something…|request st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options); basic footnote content ↩︎ ↩︎ paragraph ↩︎ footnote content with some markdown ↩︎","link":"/2022/03/05/Draft/2021/Markdown%20%E5%AD%A6%E4%B9%A0/"},{"title":"MyBatis-Plus","text":"MyBatis-Plus 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度：上篇 【】 快查 引用：","link":"/2022/01/15/Draft/2021/MyBatis-Plus/"},{"title":"MyBatis","text":"MyBatis 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】：Bili、官方文档、源码 **LEVEL【不是每个都学精】：**速看速记 进度：【17】两天 快查 常用语句 一、简介 二、动态SQL 三、缓存 什么是缓存？是存在内存中的缓存数据，此数据一般不经常改变，以此提升查询效率。 一级缓存 默认开启，在开启连接到关闭连接之间有效，即一次sqlSession中有效。 缓存失效情况 查不同数据 增删改，可能会改变数据的操作 查询不同的mapper.xml 二级缓存 12345&lt;!--mybatis-config.xml配置文件中 --&gt;&lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot; /&gt; &lt;!-- 全局映射器启用缓存 --&gt;&lt;!--mapper中开启缓存，可自定义一些参数，没有eviction时需要序列化实体类（implement Serializable） --&gt;&lt;cache/&gt; 一级缓存失效时二级缓存失效或提交会话后将数据提交到二级缓存。先读二级缓存再一级缓存。 自定义缓存 ehcache 分布式缓存 可增加保存到磁盘等增强实现。 现在一般用redis。 1&lt;cache type=&quot;自定义缓存的包.自定义缓存&quot;/&gt;","link":"/2022/01/15/Draft/2021/MyBatis/"},{"title":"NO GAME NO LIFE","text":"游戏人生 🎈🎈🎈😎 吉他扒谱--数绘--英语--摄影 😎🎈🎈🎈 目前学习路线内容 优先等级 吉他乐理，扒谱 😎😎😎😎😎 数绘 😎😎😎😎😎 英语 😎😎😎😎 摄影 😎😎😎😎 运动 😎😎😎😎 自媒体 😎😎😎😎 平面设计 😎😎😎😎 目前学习路线内容 优先等级 音乐 吉他，钢琴，尤克里里，拇指琴，箫，口琴，演唱，词曲 绘画 数绘 设计 平面 运动 篮球，网球 语言 英语，日语 摄影 人像，景色 自媒体 剪辑，特效 修图 办公效率工具高端操作 OFFICE","link":"/2021/02/25/Draft/2021/NO%20GAME%20NO%20LIFE/"},{"title":"Nginx","text":"Nginx(&quot;engine x&quot;)是一款是由俄罗斯的程序设计师Igor Sysoev所开发高性能的 Web和 反向代理 服务器，也是一个 IMAP/POP3/SMTP 代理服务器。 在高连接并发的情况下，Nginx是Apache服务器不错的替代品。 下载 安装部署 ​ 解压后不要直接点击exe，会导致修改配置后重启、停止nginx无效，需要手动关闭任务管理器内的所有nginx进程，再启动才可以。 12345start nginx//启动服务nginx -s reload//重新加载配置并启动nginx -s stop// 快速停止nginx -s quit//完整有序的关闭nginx -t // 检查配置是否正确 配置自定义nginx.conf 指令必须以分号结束 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#user nobody;#==工作进程数，一般设置为cpu核心数worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events { #==最大连接数，一般设置为cpu*2048 worker_connections 1024;}http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' # '$status $body_bytes_sent &quot;$http_referer&quot; ' # '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; #==客户端链接超时时间 keepalive_timeout 65; #gzip on; #当配置多个server节点时，默认server names的缓存区大小就不够了，需要手动设置大一点 server_names_hash_bucket_size 512; #server表示虚拟主机可以理解为一个站点，可以配置多个server节点搭建多个站点 #每一个请求进来确定使用哪个server由server_name确定 server { #站点监听端口 listen 8800; #站点访问域名 server_name localhost; #编码格式，避免url参数乱码 charset utf-8; #access_log logs/host.access.log main; #location用来匹配同一域名下多个URI的访问规则 #比如动态资源如何跳转，静态资源如何跳转等 #location后面跟着的/代表匹配规则 location / { #站点根目录，可以是相对路径，也可以使绝对路径 root html; #默认主页 index index.html index.htm; #转发后端站点地址，一般用于做软负载，轮询后端服务器 #proxy_pass http://10.11.12.237:8080; #拒绝请求，返回403，一般用于某些目录禁止访问 #deny all; #允许请求 #allow all; add_header 'Access-Control-Allow-Origin' '*'; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type'; #重新定义或者添加发往后端服务器的请求头 #给请求头中添加客户请求主机名 proxy_set_header Host $host; #给请求头中添加客户端IP proxy_set_header X-Real-IP $remote_addr; #将$remote_addr变量值添加在客户端“X-Forwarded-For”请求头的后面，并以逗号分隔。 如果客户端请求未携带“X-Forwarded-For”请求头，$proxy_add_x_forwarded_for变量值将与$remote_addr变量相同 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #给请求头中添加客户端的Cookie proxy_set_header Cookie $http_cookie; #将使用代理服务器的主域名和端口号来替换。如果端口是80，可以不加。 proxy_redirect off; #浏览器对 Cookie 有很多限制，如果 Cookie 的 Domain 部分与当前页面的 Domain 不匹配就无法写入。 #所以如果请求 A 域名，服务器 proxy_pass 到 B 域名，然后 B 服务器输出 Domian=B 的 Cookie， #前端的页面依然停留在 A 域名上，于是浏览器就无法将 Cookie 写入。 #不仅是域名，浏览器对 Path 也有限制。我们经常会 proxy_pass 到目标服务器的某个 Path 下， #不把这个 Path 暴露给浏览器。这时候如果目标服务器的 Cookie 写死了 Path 也会出现 Cookie 无法写入的问题。 #设置“Set-Cookie”响应头中的domain属性的替换文本，其值可以为一个字符串、正则表达式的模式或一个引用的变量 #转发后端服务器如果需要Cookie则需要将cookie domain也进行转换，否则前端域名与后端域名不一致cookie就会无法存取 #配置规则：proxy_cookie_domain serverDomain(后端服务器域) nginxDomain(nginx服务器域) proxy_cookie_domain localhost .testcaigou800.com; #取消当前配置级别的所有proxy_cookie_domain指令 #proxy_cookie_domain off; #与后端服务器建立连接的超时时间。一般不可能大于75秒； proxy_connect_timeout 30; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } #当需要对同一端口监听多个域名时，使用如下配置，端口相同域名不同，server_name也可以使用正则进行配置 #但要注意server过多需要手动扩大server_names_hash_bucket_size缓存区大小 server { listen 80; server_name www.abc.com; charset utf-8; location / { proxy_pass http://localhost:10001; } } server { listen 80; server_name aaa.abc.com; charset utf-8; location / { proxy_pass http://localhost:20002; } } 功能 1）反向代理 正向代理：特定情况下，代理用户访问服务器，需要用户手动的设置代理服务器的ip和端口号。 反向代理：是用来代理服务器，代理用户要访问的目标服务器。代理服务器接受请求，然后将请求转发给内部网络的服务器(服务集群模式)，并将从服务器上得到的结果返回给客户端，此时代理服务器对外就表现为一个服务器。 Nginx在反向代理上，提供灵活的功能，可以根据不同的正则采用不同的转发策略，如图设置好后不同的请求就可以走不同的服务器。 2）负载均衡 负载均衡：多在高并发情况下需要使用。其原理就是将数据流量分摊到多个服务器执行，减轻每台服务器的压力，多台服务器(集群)共同完成工作任务，从而提高了数据的吞吐量。 Nginx可使用的负载均衡策略有：轮询（默认）、权重、ip_hash、url_hash(第三方)、fair(第三方)。 3）动静分离 常用于前后端分离，Nginx提供的动静分离是指把动态请求和静态请求分离开，合适的服务器处理相应的请求，使整个服务器系统的性能、效率更高。 Nginx可以根据配置对不同的请求做不同转发，这是动态分离的基础。静态请求对应的静态资源可以直接放在Nginx上做缓冲，更好的做法是放在相应的缓冲服务器上。动态请求由相应的后端服务器处理。","link":"/2021/02/24/Draft/2021/Nginx/"},{"title":"Office","text":"Office高级教程记录，边用边记录 WORD PPT EXCEL 函数篇 替换 =LEFT(C2835,LEN(C2835)-1)&amp;&quot;路-散盘&quot; 提取 =MID（） 结合 =CONCATENATE(A2,B2) 随机 =CHOOSE(RANDBETWEEN(1,2),&quot;出让&quot;,&quot;划拨&quot;) 随机数 =RANDBETWEEN(5,100) 奇偶 =MOD(ROW(),2) 替换，插入 =REPLACE(A1,2,,&quot;09&quot;) 查找 =FIND(&quot;K&quot;,A2,1) 模糊匹配删除 计算重复数量 Countif（统计范围，统计对象） =INDEX(Sheet0!A:AA,ROW(),MATCH(INDIRECT((CHAR(COLUMN()+ 64)&amp;1)),Sheet0!A1:AA1,0))复制对应列名单元格","link":"/2022/01/15/Draft/2021/Office/"},{"title":"Software Engineer","text":"高级全栈软件工程师养成记 GIS研发工程师，全栈Java开发工程师 🎈🎈🎈😎 TODO 😎🎈🎈🎈 设计模式 Redis 算法 目题(Java基础深入，其他面试题) Springcloud 目前学习路线内容 优先等级 设计模式 😎😎😎😎😎 Redis 😎😎😎😎😎 RabbitMQ 😎😎😎😎 数据库优化 😎😎😎😎 网络 😎😎😎😎 算法 😎😎😎😎 SpringCloud 😎😎😎😎 GIS 😎😎😎😎 Linux 😎😎😎 JVM 😎😎 Dubbo 😎😎 ElasticSearch 😎😎 Node.js 😎😎 ES6 😎😎 Docker 😎😎 分布式 😎😎 高并发 😎😎 Python 😎😎 学习内容 项目名称 详细内容 进度记录 软件工程 （微服务，分布式，高并发，多线程，性能调优，缓存，消息，搜索）其他应用服务器了解。 项目 个人博客 个人博客完成 前端 VUE，Bootstrap，JS，Html，JQuery，Ajax，Thymeleaf,Axios 后端 基础深入，代码多写 框架 框架扎实深入，SpringCloud尝试了解 数据库 Oracle基础使用，MySQL,索引，触发器，存储过程，优化（文件化，从设计到e-r图到创建），Redis,MangoDB 算法 算法机试题 设计模式 Mybatis（ 1、Builder模式5、组合模式9、迭代器模式2、工厂模式3、单例模式4、代理6、模板方法模式7、适配器模式8、装饰者模式）Spring（1.简单工厂2.工厂方法3.单例模式4.适配器模式5.装饰器模式6.代理模式7.观察者模式8.策略模式9.模版方法模式） 网络 协议 系统 Linux常用命令，JVM 工具 Git，SVN高级操作，Pageoffice 实操 机试题 中间件 RabbitMQ ElasticSearch Docker Dubbo 测试 单元测试 部署 服务器Jboss 维护 其他 Python【熟悉基础】安卓【有时间了解】 公司 springboot+mybatis+gis+vue+redis+框架+基础前后端数据交互+springcloud【有时间了解】","link":"/2021/02/25/Draft/2021/Software%20Engineer/"},{"title":"SpringMVC","text":"SpringMVC 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度：【一天 】 快查 一、简介 二、流程 DispatcherServlet调用链 重要成员 1234567891011121314151617181920212223242526272829303132333435// 文件上传组件/** MultipartResolver used by this servlet */private MultipartResolver multipartResolver;// 资源定位组件/** LocaleResolver used by this servlet */private LocaleResolver localeResolver;// 主题解析组件/** ThemeResolver used by this servlet */private ThemeResolver themeResolver;// 处理器映射器组件集合/** List of HandlerMappings used by this servlet */private List&lt;HandlerMapping&gt; handlerMappings;// 处理器适配器组件集合/** List of HandlerAdapters used by this servlet */private List&lt;HandlerAdapter&gt; handlerAdapters;// 异常处理解析器集合/** List of HandlerExceptionResolvers used by this servlet */private List&lt;HandlerExceptionResolver&gt; handlerExceptionResolvers;// 视图名解析器/** RequestToViewNameTranslator used by this servlet */private RequestToViewNameTranslator viewNameTranslator;// 重定向及FlashMap存储组件/** FlashMapManager used by this servlet */private FlashMapManager flashMapManager;// 视图解析组件集合/** List of ViewResolvers used by this servlet */private List&lt;ViewResolver&gt; viewResolvers; 流程 第一步：发起请求到前端控制器(DispatcherServlet) 第二步：前端控制器请求HandlerMapping查找 Handler （可以根据xml配置、注解进行查找） 第三步：处理器映射器HandlerMapping向前端控制器返回Handler，HandlerMapping会把请求映射为HandlerExecutionChain对象（包含一个Handler处理器（页面控制器）对象，多个HandlerInterceptor拦截器对象），通过这种策略模式，很容易添加新的映射策略 第四步：前端控制器调用处理器适配器去执行Handler 第五步：处理器适配器HandlerAdapter将会根据适配的结果去执行Handler 第六步：Handler执行完成给适配器返回ModelAndView 第七步：处理器适配器向前端控制器返回ModelAndView （ModelAndView是springmvc框架的一个底层对象，包括 Model和view） 第八步：前端控制器请求视图解析器去进行视图解析 （根据逻辑视图名解析成真正的视图(jsp)），通过这种策略很容易更换其他视图技术，只需要更改视图解析器即可 第九步：视图解析器向前端控制器返回View 第十步：前端控制器进行视图渲染 （视图渲染将模型数据(在ModelAndView对象中)填充到request域） 第十一步：前端控制器向用户响应结果 三、拦截器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package com.jeethink.framework.interceptor;import java.lang.reflect.Method;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.springframework.stereotype.Component;import org.springframework.web.method.HandlerMethod;import org.springframework.web.servlet.handler.HandlerInterceptorAdapter;import com.alibaba.fastjson.JSONObject;import com.jeethink.common.utils.ServletUtils;import com.jeethink.framework.interceptor.annotation.RepeatSubmit;import com.jeethink.framework.web.domain.AjaxResult;/** * 防止重复提交拦截器 * @author 官方网址 */@Componentpublic abstract class RepeatSubmitInterceptor extends HandlerInterceptorAdapter{ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { if (handler instanceof HandlerMethod) { HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); RepeatSubmit annotation = method.getAnnotation(RepeatSubmit.class); if (annotation != null) { if (this.isRepeatSubmit(request)) { AjaxResult ajaxResult = AjaxResult.error(&quot;不允许重复提交，请稍后再试&quot;); ServletUtils.renderString(response, JSONObject.toJSONString(ajaxResult)); return false; } } return true; } else { return super.preHandle(request, response, handler); } } /** * 验证是否重复提交由子类实现具体的防重复提交的规则 * * @param httpServletRequest * @return * @throws Exception */ public abstract boolean isRepeatSubmit(HttpServletRequest request);} 引用：","link":"/2022/01/15/Draft/2021/SpringMVC/"},{"title":"TODO","text":"工具：时光序 流程：拟计划–》时光序—》简化记录 charging： 所有题不断刷，不断复习，主次分明。 确定地点 各个技术框架化复习 背【ANKI】 时间：2.8-3.16（36天） 1-2点 6-12点 六小时左右 路线： 介绍 项目 Redis（实践高级部分）、MySQL 优化（牛客） 、MongoDB 实践 JUC、IO、NET、集合、JVM 框架化、lambda Java应用（工作流） VUE等前端知识、地图开发、小程序 Linux（服务器运用） SSM 底层、SpringBoot 底层、SpringCloud 实践 MQ、Dubbo 维护实施等 设计网络算法 工具： ANKI 牛客 三大方面：艺术、技术、自媒体 优先顺序：技术、自媒体 艺术、生活 自媒体： 引流、内容、 艺术： 乐理、唱技巧、钢琴基础、吉他深入、其他乐理深入后深入学习、摄影【风格化】、电子绘画【兴趣】 生活： 两周末出去一天（该关心的人）、一周至少运动一次不久坐、注意健康 语言： 英语口语（音乐电影）、日语未来再期。 自媒体： 创建精美文章 艺术： 钢琴 生活 首要计划 SQL优化 Redis 强制使用Redis 网络 设计模式 算法 JVM 乐理bili Java多线程并发、IO、NET 框架原理、分布式、中间件 MQ Dubbo MongoDB SpringCloud ElasticSearch 拟计划 拟计划，分于每个月的目标 考完后补充软考笔记，每周细化记忆，项目管理对应实践 设计模式 Java工作流 Python复习 DevOps VUE视频一遍 Jenkins Nginx 系统设计（数据库，软件流程） Linux环境开发部署，熟悉shell 小计划 小计划，有空就做 爬虫了解，Python复习 JAVA知识每日一题 Android 国外技术网站 严格执行奖惩制度 深度学习复习 一周一思考出篇公众号文章 文件上传下载清晰化 平台开发 编程思想重点查看 Pageoffice 健身开始肚肚 周期总结 微信小程序聊天提醒机器人 Postgresql学习准备 固定计划 固定计划，每天都做 算法 英语 面题 [ ] MONTH-----六 运动三次 数码绘画一次 摄影 剪辑 公众号运营 WEEK🤲 吉他练习 DAY-2021.5.28 固定计划 [ ] MONTH-----五 项管师 MONTH-----四 运动三次 数码绘画一次 摄影 剪辑 公众号运营【粉丝引导，自媒体矩阵搭建】 RabbitMQ基础第一轮 springcloud第一轮 VUE复习一遍 WEEK🤲🤲🤲 吉他练习 运动 DAY-2021.4.15 SpringCloud【】一节 项管师基础 四节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.4.14 SpringCloud【】一节 项管师基础 四节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.4.13 SpringCloud【】一节 项管师基础 四节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.4.12 SpringCloud【】一节 项管师基础 两节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 WEEK🤲🤲 吉他练习 ### DAY-2021.4.11 SpringCloud【】一节 项管师基础 两节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 ### DAY-2021.4.8 SpringCloud【】一节 项管师基础 两节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 ### DAY-2021.4.7 SpringCloud【】一节 项管师基础 两节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.4.6 SpringCloud【】一节 项管师基础 两节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 WEEK🤲 吉他练习 运动 DAY-2021.4.3 SpringCloud【】一节 项管师基础 两节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.4.2 SpringCloud【】一节 项管师基础 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.4.1 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.3.31 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.3.30 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 WEEK🤲🤲🤲🤲 吉他练习 运动 MONTH------三 运动三次 数码绘画一次 摄影 剪辑 公众号运营【粉丝引导，自媒体矩阵搭建】 RabbitMQ基础第一轮 springcloud第一轮 WEEK🤲 吉他练习 运动 WEEK🤲🤲 吉他练习 WEEK🤲🤲🤲 吉他练习 运动 WEEK🤲🤲🤲🤲 吉他练习 运动 DAY-2021.3.29 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.3.25 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.3.24 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 面题【】一题 Redis【】回忆 DAY-2021.3.23 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 虚拟机环境安装 面题【】一题 Redis【】回忆 DAY-2021.3.22 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 RabbitMQ 7节 面题【】一题 Redis【】回忆 DAY-2021.3.19 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.18 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.17 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.16 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.15 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.12 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.11 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.10 SpringCloud【】一节 设计模式【】一个 英语【中午】强制外语网站-新闻-书，100单词刷 算法【中午】一题，一知识 面题【】一题 Redis【】回忆 DAY-2021.3.9 SpringCloud 设计模式 英语【中午】强制外语网站，100单词刷 算法【中午】 面题 Redis DAY-2021.3.8 SpringCloud 设计模式 英语【中午】强制外语网站，100单词刷 算法【中午】 面题 Redis 模板 MONTH-----四 运动三次 数码绘画一次 摄影 剪辑 公众号运营 WEEK🤲🤲🤲🤲 吉他练习 运动 DAY-2021.3.9 SpringCloud【】一节 固定计划 DAY-2021.3.9 SpringCloud【】一节 固定计划 WEEK🤲🤲🤲 吉他练习 DAY-2021.3.9 SpringCloud【】一节 固定计划 WEEK🤲🤲 吉他练习 运动 DAY-2021.3.9 SpringCloud【】一节 固定计划 WEEK🤲 吉他练习 运动 DAY-2021.3.9 SpringCloud【】一节 固定计划 FIXED RECORDS 健身记录 BUG排查总结 项管师总结，周末五节课 图书馆借阅信管师书籍，周六 软考高级备考 11.10 交费成功的报考人员于5月24日10:00至28日16:00通过浙江软件考试网下载并打印考试准考证，逾期未打印准考证视为放弃考试。报考人员按准考证规定的时间、地点和要求参加考试。","link":"/2021/02/24/Draft/2021/TODO/"},{"title":"魑魅先生 | 音乐","text":"乐理、吉他、尤克里里、钢琴、拇指琴","link":"/2021/02/25/Draft/2021/%E4%B9%90%E7%90%86%E7%9F%A5%E8%AF%86/"},{"title":"模板","text":"介绍 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度： 【】 快查 一、简介 二、 + 横竖图 + 横图4 + 竖图5 引用：","link":"/2022/03/05/Draft/2021/template/"},{"title":"书影音","text":"记录与回忆才能让那些美好的不美好的存留脑中，阵阵回荡与发酵。 单月更新 BOOKS--OTHER 《白夜行》（东野奎吾） 《解忧杂货铺》（东野奎吾） 《许三观卖血记》（余华） 《我们仨》（杨绛） 《我的家》（巴金） 《局外人》 《小王子》 《恶意》（东野奎吾） 《摆渡人》克莱尔麦克福尔 《偷影子的人》（马克李维） 《岛上书店》（东野奎吾） 《嫌疑人X的献身》 《活着》（余华） 《穆斯林的葬礼》（霍达） 《乖，摸摸头》（大冰） 《水彩从入门到精通》（飞乐鸟） 《人，诗意的栖居》（海德格尔） 《明朝那些事》当年明月，石锐 《看见》柴静 《混血豹王》沈石溪 《狼王梦》沈石溪 《第七天》余华 《地心游记》 《目送》龙应台 《人间失格》 《别上了摄影的当》 《拍出明星范》 《人像摄影教程》 唐东平 《安徒生童话》 BOOKS--MAJOR 《Effective Java》 《图解HTTP》 《深入理解Java虚拟机3》 《计算机网络自顶下下方法》 《程序员的自我修养》 1.实践出真知，通用最宝贵，最新乐于求，沟通不可无，环境影响大，切保身体好，薪酬等量级，单项求发展，声誉建品牌，不断学进步 2.英语很重要，阅读优秀项目， 《阿里巴巴开发手册泰山版》 1.所有数据库都要配置gmy_create(创建时间)、gmt_modified(更新时间)且需要自动化 MUSIC MOVIES 《银河护卫队2》 《新木乃伊》 《加勒比海盗1234》 《这个杀手不太冷》 《肖申克的救赎》 《泰坦尼克号》 《怦判心动》 《星际穿越》 《源代码》 《黑客帝国》 《曾经》 《爱丽丝的梦游仙境》 《冈仁波次》 《天空之城》 《变形金刚5》 《机器管家》 《独立日》 《幽灵行动阿尔法队》 《太空旅客》 《太空一号》 《地心引力》 《暮光之城》 《异形》 《火星救援》 《美丽人生》 《深夜食堂》 《战狼1/2》 《摆渡人》 《安德得游戏》 《十二只猴子》 《小森林》 《逆世界》 《金刚骷髅岛》 《夏洛特烦恼》 《左耳》 《钢铁骑士》 《捉妖记》 《重返20岁》 《机械师》 《攻壳机动队》 《一万公里的约定》 《生化危机全》 《谋杀似水年华》 《第九区》 《与君相恋100次》 《悟空传》 《刺客信条》 《绣春刀，修罗场》 《权利的游戏1234567》 嫌疑人X的献身 寻梦环游记 缝纫机乐队 敦刻尔克 洛丽塔 魁拔123 解忧杂货店 前任3 神奇女侠 芳华 被偷走的那五年 从你的全世界路过 分手合约 夏洛特烦恼 匆匆那年 失恋33天 海上钢琴师 公牛历险记 179小时 月球 记忆大师 黑天鹅 银翼杀手 血战钢锯岭 自杀小队 看不见的客人 二代妖精之今生有幸 比得兔 天空之眼 小萝莉的猴神大叔 弱点 阿甘正传 当幸福来敲门 硅谷123 国王的演讲 我是江小白 影 绿皮书 盗梦空间 调音师 少年派的奇幻漂流 怦然心动 霸王别姬 复仇者联盟四 蜘蛛侠平行宇宙 头号玩家 明日边缘 小时光 疯狂动物城 驯龙高手12","link":"/2021/02/24/Draft/2021/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"title":"五分钟搭建在线博客","text":"博客已成为程序员学习与输出重要一环，当你时间宝贵或者没有必要从零构建个人博客的话，一个可以进行快速搭建的静态博客就显得比较重要了。不但丰富的主题可以任你选择，各个组件还可以任你搭配。有的甚至无需服务器和域名注册，分分钟实现自己分享的小基地。 目前比较流行的开源框架有 Hexo、WordPress、VuePress、Hugo、Solo、Halo 、Jekyll 白嫖党【无需服务器，免费域名】喜欢的无个人域名无法被百度搜索 Jekyll、Hugo、Hexo 本博客使用 Hexo 接下来开始Hexo博客快速搭建 环境准备 Github账号，新库名:用户名.github.io node.js、npm、JS、Git 安装与基础学习 图床 Gitee+PicGo 初步搭建 创建博客文件夹，shift+右键进入目录cmd 安装hexo npm install -g hexo-cli 初始化文件夹 hexo init hexo_blog 进入博客文件夹 cd E:\\my\\hexo_blog 安装博客需要的依赖文件 npm install Hexo命令 hexo cl #清理 hexo g #生成 hexo s #本地服务 hexo d #发布 测试http://locakhost:4000 或127.0.0.1:4000 初步搭建完成 Hexo相关配置 项目目录说明 _config.yml配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117title: 博客名subtitle: 副标题description: 博客描述keywords: SEO搜索关键词author: 文章作者language: zh-CN #语言timezone: ''#时区# URL## If your site is put in a subdirectory, set url as 'http://example.com/child' and root as '/child/'url: http://MrDemonlxl.github.io #网址root: /permalink: :year/:month/:day/:title/permalink_defaults:pretty_urls: trailing_index: true # Set to false to remove trailing 'index.html' from permalinks trailing_html: true # Set to false to remove trailing '.html' from permalinks# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: enable: true # Open external links in new tab field: site # Apply to the whole site exclude: ''filename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: '' wrap: true hljs: falseprismjs: enable: false preprocess: true line_number: true tab_replace: ''# Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Metadata elements## https://developer.mozilla.org/en-US/docs/Web/HTML/Element/metameta_generator: true# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss## updated_option supports 'mtime', 'date', 'empty'updated_option: 'mtime'# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Include / Exclude file(s)## include:/exclude: options only apply to the 'source/' folderinclude:exclude:ignore:# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: amazing#landscape# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy: type: git repo: https://github.com/MrDemonlxl/MrDemonlxl.github.io.git branch: maincomment: #评论 type: gitalk language: zh-CN #zh-CN #Localization language key, en, zh-CN and zh-TW are currently available. owner: MrDemonlxl # (required) GitHub user name repo: MrDemonlxl.github.io # (required) GitHub repository name client_id: # (required) OAuth application client id client_secret: # (required) OAuth application client secret admin: [''] create_issue_manually: true distraction_free_mode: false has_hot_recommend: true # 是否有热门推荐 has_latest_comment: true #是否有最新评论 # 主题选择 https://hexo.io/themes/ 选择喜欢主题，下载相应主题包放入主题文件夹，进行对应主题说明配置。 # 继续学习完善 https://hexo.io/zh-cn/docs/index.html # 效果预览 https://mrdemonlxl.github.io/","link":"/2021/02/18/Draft/2021/%E4%BA%94%E5%88%86%E9%92%9F%E6%90%AD%E5%BB%BA%E5%9C%A8%E7%BA%BF%E5%8D%9A%E5%AE%A2/"},{"title":"BUG","text":"身体是革命的本钱 软件：小红书，记录心得，读书电影笔记，摄影，旅游 第一个小目标：减掉小肚肚 健身计划： 至少每天 80 个仰卧起坐 代替方案：篮球，跑步 记录： 2021.3.25 100仰卧起坐","link":"/2021/02/24/Draft/2021/%E5%81%A5%E8%BA%AB%E5%A4%96%E5%9E%8B/"},{"title":"","text":"大纲目录以概览 博客计划 JAVA应用 乐理知识 TODO MYSQL优化 Linux Redis [Markdown 学习](Markdown 学习.md) 设计模式 快查 每日算法 Python3学习 计算机网络 每日面题 书影音 工具篇 Nginx GIS Hexo使用范例 BUG 如何学习一个新知识 健身外型 信息系统项目管理师 RabbitMQ SpringCloud 每日英语 [Software Engineer](Software Engineer.md) [NO GAME NO LIFE](NO GAME NO LIFE.md) jvm与上层技术 五分钟搭建在线博客 资源篇 ===","link":"/2021/06/30/Draft/2021/%E5%A4%A7%E7%BA%B2%E7%9B%AE%E5%BD%95/"},{"title":"如何学习一个新知识","text":"个人学习流程工具记录，如何将一个学习目标最高效率化 学习===》熟练》大师》创造 新学四问 WHY【与前代优化了什么，弥补了什么空白】 WHAT【框架，思维导图，主题框架】 HOW【如何记忆，学习资源】 LEVEL【不是每个都学精】 回顾四问【补充】 WHY【与前代优化了什么，弥补了什么空白】 WHAT【框架，思维导图，主题框架】 HOW【如何记忆，学习资源】 LEVEL【不是每个都学精】 工具 Xmaind MarginNote Typora Anki 印象笔记 专项工具 坚果（笔记在线同步） 百度在线脑图 流程 确定官方文档 Google 为什么学【与前代优化了什么】，学什么【框架，思维导图】，如何学【重复运用，直接背诵】，学到哪【会用即可】 资源准备 Xmaind，MarginNote，Goodnotes【思维导图，笔记】，百度在线脑图 Typora，印象笔记 导入思维导图 Markdown 笔记细化，方便整理分享 Anki 艾宾浩斯记忆稳固 结构化====》细化====》固化====》更新 代码学习：思考&gt;敲代码&gt;看 方法 分类学习 自问自答 重复记忆 实操运用 最高标准 笔记方法 合理计划 督促验收 实践发现未知 80%时间在基础，18%在框架，2%在英文，视频更细，博客更快，官方文档为王 技术的进步是一点一滴实操而来的 网络方法 费曼学习法","link":"/2021/02/09/Draft/2021/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9F%A5%E8%AF%86/"},{"title":"工具篇","text":"工具篇 工具篇 工欲善其事，必先利其器 编程 IDEA GIT 错误：git@github.com: Permission denied (publickey) 解决：原因：密匙不匹配 git config --global --list 显示 git 配置信息 git config --global user.name “yourname”，git config --global user.email “email@email.com ”设置全局用户名和邮箱 ssh-keygen -t rsa -C “邮箱” clip &lt; ~/.ssh/id_rsa.pub 复制密匙，在github设置中新建SSH key粘贴即可 电脑应用 Synergy键鼠共享工具 坚果云，文件同步工具 Quicker PicGO Snipaste ScreenToGif 学习 流程图 在线流程图 ProcessOn 离线流程图 XMaind 网站推荐 艺术 记录 时光序【备忘，提醒，TODO】 插件 Edge Vimium Navigating the page j, Scroll down (scrollDown) k, Scroll up (scrollUp) gg Scroll to the top of the page (scrollToTop) G Scroll to the bottom of the page (scrollToBottom) d Scroll a half page down (scrollPageDown) u Scroll a half page up (scrollPageUp) Scroll a full page down (scrollFullPageDown) Scroll a full page up (scrollFullPageUp) h Scroll left (scrollLeft) l Scroll right (scrollRight) r Reload the page (reload) yy Copy the current URL to the clipboard (copyCurrentUrl) p Open the clipboard's URL in the current tab (openCopiedUrlInCurrentTab) P Open the clipboard's URL in a new tab (openCopiedUrlInNewTab) i Enter insert mode (enterInsertMode) v Enter visual mode (enterVisualMode) gi Focus the first text input on the page (focusInput) f Open a link in the current tab (LinkHints.activateMode) F Open a link in a new tab (LinkHints.activateModeToOpenInNewTab) Open a link in a new tab &amp; switch to it (LinkHints.activateModeToOpenInNewForegroundTab) gf Select the next frame on the page (nextFrame) gF Select the page's main/top frame (mainFrame) Using the vomnibar o Open URL, bookmark or history entry (Vomnibar.activate) O Open URL, bookmark or history entry in a new tab (Vomnibar.activateInNewTab) b Open a bookmark (Vomnibar.activateBookmarks) B Open a bookmark in a new tab (Vomnibar.activateBookmarksInNewTab) T Search through your open tabs (Vomnibar.activateTabSelection) Using find / Enter find mode (enterFindMode) n Cycle forward to the next find match (performFind) N Cycle backward to the previous find match (performBackwardsFind) Navigating history H Go back in history (goBack) L Go forward in history (goForward) Manipulating tabs t Create new tab (createTab) J, gT Go one tab left (previousTab) K, gt Go one tab right (nextTab) ^ Go to previously-visited tab (visitPreviousTab) g0 Go to the first tab (firstTab) g$ Go to the last tab (lastTab) yt Duplicate current tab (duplicateTab) Pin or unpin current tab (togglePinTab) Mute or unmute current tab (toggleMuteTab) x Close current tab (removeTab) X Restore closed tab (restoreTab) Miscellaneous ? Show help (showHelp) 办公： 替换 =LEFT(C2835,LEN(C2835)-1)&amp;&quot;路-散盘&quot; 提取 =MID（） 结合 =CONCATENATE(A2,B2) 随机 =CHOOSE(RANDBETWEEN(1,2),&quot;出让&quot;,&quot;划拨&quot;) 随机数 =RANDBETWEEN(5,100) 奇偶 =MOD(ROW(),2) 替换，插入 =REPLACE(A1,2,,&quot;09&quot;) 查找 =FIND(&quot;K&quot;,A2,1) 模糊匹配删除 计算重复数量 Countif（统计范围，统计对象） =INDEX(Sheet0!A:AA,ROW(),MATCH(INDIRECT((CHAR(COLUMN()+ 64)&amp;1)),Sheet0!A1:AA1,0))复制对应列名单元格","link":"/2022/03/05/Draft/2021/%E5%B7%A5%E5%85%B7%E7%AF%87/"},{"title":"开发修电脑","text":"介绍 工欲善其事，必先利其器。---《论语·卫灵公》 硬软件各种知识 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】： LEVEL【不是每个都学精】： 进度：上篇 【】 快查 一、硬件 1.主板 参数： 2.显卡 3.内存 4.硬盘 5. 二、系统 三、工具 CPU - Z 查看CPU等相关信息 AID64 查看全部电脑硬件信息 XTU 测试稳定性 四、服务器 五、 引用：","link":"/2022/03/05/Draft/2021/%E5%BC%80%E5%8F%91%E4%BF%AE%E7%94%B5%E8%84%91/"},{"title":"魑魅先生 | 每日算法","text":"为什么学？编程的灵魂，不管什么语言都得需要。 如何学？多想多写多用 学到什么程度？实际运用 题源：Leetcode 一.基本数据结构 第一章 算法定义 在特定计算模型下，在信息处理过程中为了解决某一类问题而设计的一个指令序列。 要素： 输入：待处理的信息，即对具体问题的描述。 输出：经过处理之后得到的信息，即问题的答案。 确定性：任一算法都可以描述为由若干种基本操作组成的序列。 可行性：在相应的计算模型中，每一基本操作都可以实现，且能够在常数时间内完成。 有穷性：对于任何输入，按照算法，经过有穷次基本操作都可以得到正确的输出。 1.2性能分析预评价 三个层次：合法程序，确定尺度度量算法效率，通过对算法设计编写效率高，能处理大规模数据的程序， 时间复杂度 T(n) 度量算法执行速度并评价其效率，算法需要多少时间才能得到结果 ==》针对不同规模的输入，算法的执行时间各是多少 ==》统一规模算法处理时间也不相同 空间复杂度 算法所需存储空间 1.3 算法复杂度及其分析 1.3.1 O(1)⎯⎯取非极端元素 1.3.2 O(logn)⎯⎯进制转换 1.3.3 O(n)⎯⎯数组求和 1.3.4 O(n2)⎯⎯起泡排序 1.3.5 O(2r)⎯⎯幂函数 1.4 计算模型 1.4.1 可解性 1.4.2 有效可解 1.4.3 下界 1.5 递归 1.5.1 线性递归 1.5.2 递归算法的复杂度分析 1.5.3 二分递归 1.5.4 多分支递归 第二章 栈与队列 栈与队列最简单基本，但也是最重要的。JVM，CPU，Java提供对应内建类， 2.1 栈 后进先出（Last-in-first-out，LIFO），比如浏览器访问记录与回退，编辑回退。 元素： 栈容量，栈顶指针，初始化 进栈push()、出栈pop()，查栈顶peek() Java java.util.Stack ：push()、pop()、peek()（功能等价于top()）、getSize()以及empty()（功能等价于isEmpty()） 应用：数组倒置，括号匹配算法， 三种实现方式 数组 链表 LinkedList 2.2 队列 先进先出（First-In-First-Out, FIFO），羽毛球筒 2.2.1 队列ADT Queue 接口 元素 队头、队尾、队尾加元素add()，队头删除元素poll(),查队头元素peek() 2.2.2 基于数组的实现 顺序数组，整体移动 循环数组， 性能分析，O(1)。 2.2.3 队列应用实例 循环分配器，Josephus 环 2.3 链表 数组长度必须固定，在空间效率及适应性方面还存在不足。 2.3.1 单链表 元素 首节点，末节点 ​ 链表的第一个和最后一个节点，分别称作链表的首节点（Head）和末节点（Tail）。末节点的特征是，其next 引用为空。如此定义的链表，称作单链表（Singly linkedlist）。 ​ 与数组类似，单链表中的元素也具有一个线性次序⎯⎯若P 的next 引用指向S，则P 就是S的直接前驱，而S 是P 的直接后继。与数组不同的是，单链表的长度不再固定，而是可以根据实际需要不断变化。如此一来，包含n 个元素的单链表只需占用O(n)空间⎯⎯这要比定长数组更为灵活。 2.4 位置 2.5 双端队列 第三章 向量、列表与序列 ​ 序列（Sequence），就是依次排列的多个对象，就是一组对象之间的后继与前驱关系，是数据结构设计的基础。两种典型的序列：向量（Vector）和列表（List）。 3.1 向量与数组 3.1.1 向量ADT 第四章 树 ​ 前面所有的数据结构根据其实现方式，可以划分为基于数组实现和基于链表实现，其各有长短，数组善查找读取，修改耗时，链表反之。两者有点能结合?树或许可以回答这个问题。 术语及性质 节点的深度、树的深度与高度 ​ 树中的元素也称作节点（Node），每个节点的深度都是一个非负整数；深度为0 的节点有且仅有一个，称作树根（Root）；对于深度为k (k≥1)的每个节点u，都有且仅有一个深度为k-1 的节点v 与之对应，称作u 的父亲（Parent）或父节点。定义四.2 若节点v 是节点u 的父亲，则u 称作v 的孩子（Child），并在二者之间建立一条树边（Edge）。同一节点的孩子互称“兄弟”（Sibling）。树中所有节点的最大深度，称作树的深度或高度。树中节点的数目，总是等于边数加一。 度、内部节点与外部节点 ​ 任一节点的孩子数目，称作它的“度”（Degree）。至少拥有一个孩子的节点称作“内部节点”（Internal node）；没有任何孩子的节点则称作 “外部节点”（External node）或“叶子”（Leaf）。 路径 ​ 由树中k+1 节点通过树边首尾衔接而构成的序列{ (v0, v1), (v1, v2), …, (vk-1, vk) | k ≥ 0}，称作树中长度为k 的一条路径（Path）。由单个节点、零条边构成的路径也是合法的，其长度为0。树中任何两个节点之间都存在唯一的一条路径。若v 是u 的父亲，则depth(v) + 1 = depth(u)。从树根通往任一节点的路径长度，恰好等于该节点的深度。 祖先、后代、子树和节点的高度 每个节点都是自己的“祖先”（Ancestor），也是自己的“后代”（Descendent）； 若v 是u 的父节点的祖先，则v 也是u 的祖先； 若u 的父节点是v 的后代，则u 也是v 的后代。 除节点本身以外的祖先（后代），称作真祖先（后代）。任一节点v 的深度，等于其真祖先的数目。任一节点v 的祖先，在每一深度上最多只有一个。树T 中每一节点v 的所有后代也构成一棵树，称作T 的“以v 为根的子树（Subtree）”若子树v 的深度（高度）为h，则称v 的高度为h，记作height(v) = h。对于叶子节点u 的任何祖先v，必有depth(v) + height(v) ≥ depth(u)。 共同祖先及最低共同祖先 在树T 中，若节点u 和v 都是节点a 的后代，则称节点a 为节点u 和v 的共同祖先（Commonancestor）。每一对节点至少存在一个共同祖先。在一对节点u 和v 的所有共同祖先中，深度最大者称为它们的最低共同祖先（Lowerestcommon ancestor），记作lca(u, v)。每一对节点的最低共同祖先必存在且唯一。 有序树、m 叉树 在树T 中，若在每个节点的所有孩子之间都可以定义某一线性次序，则称T 为一棵“有序树（Ordered tree）”每个内部节点均为m 度的有序树，称作m 叉树。 二叉树 每个节点均不超过2 度的有序树，称作二叉树（Binary tree）。不含1 度节点的二叉树，称作真二叉树（Proper binary tree），否则称作非真二叉树 （Improper binary tree）。在二叉树中，深度为k 的节点不超过2k 个。高度为h 的二叉树最多包含2h+1-1 个节点。由n 个节点构成的二叉树，高度至少为⎣log2n⎦。在二叉树中，叶子总是比2 度节点多一个。 满二叉树与完全二叉树 若二叉树T 中所有叶子的深度完全相同，则称之为满二叉树（Full binary tree）高度为h 的二叉树是满的，当且仅当它拥有2h 匹叶子、2h+1-1 个节点。若在一棵满二叉树中，从最右侧起将相邻的若干匹叶子节点摘除掉，则得到的二叉树称作完全二叉树（Complete binary tree）。由n 个节点构成的完全二叉树，高度h = ⎣log2n⎦。在由固定数目的节点所组成的所有二叉树中，完全二叉树的高度最低。 第五章 优先队列 第六章 映射与词典 第七章 查找树 第八章 串 第九章 图 二、常用算法 排序 1、冒泡排序 2、选择排序 3、插入排序 4、希尔排序 5、归并排序 6、快速排序 然后基准两边分别快速排序 7、堆排序 8、计数排序 9、桶排序 10、基数排序 查找 1. 顺序查找 2. 二分查找 3. 插值查找 4. 斐波那契查找 5. 树表查找 6. 分块查找 7. 哈希查找 三、算法题 2.18题目：序号-题目 思路1： 123思路1优点：缺点： 代码1： 1代码1 思路2： 123思路2优点：缺点： 代码2： 1代码2 参考文献：数据结构预算法(Java描述)邓俊辉","link":"/2021/02/19/Draft/2021/%E6%AF%8F%E6%97%A5%E7%AE%97%E6%B3%95/"},{"title":"魑魅先生 | 程序员英语","text":"新学四问 WHY【与前代优化了什么，弥补了什么空白】学习交流娱乐 WHAT【框架，思维导图，主题框架】程序员专业词汇，日常英语 HOW【如何记忆，学习资源】:Google日常化、英文文档日常化、词汇记忆 LEVEL【不是每个都学精】熟练使用 图解日常英语单词 通俗易懂的英语语法 针对计算机英语 口语练习记录 英文文档记录 JAVA Spring全家桶","link":"/2021/02/25/Draft/2021/%E6%AF%8F%E6%97%A5%E8%8B%B1%E8%AF%AD/"},{"title":"魑魅先生 | 每日面题","text":"面试题不但是门槛，也是检测。 Java刷题 主要分类： JVM，网络，Java基础，算法数据结构，数据库高级知识，框架原理，设计模式，分布式 Java基础 查看JAVA基础项目并补充，包含java基础， JVM 网络 算法数据结构 数据库高级知识 框架原理 设计模式 分布式","link":"/2021/03/01/Draft/2021/%E6%AF%8F%E6%97%A5%E9%9D%A2%E9%A2%98/"},{"title":"自媒体计划","text":"自媒体规划，构建ing 2021-GOALS 前端栏目 Java栏目 Python栏目 每日算法栏目 设计模式栏目 网络知识栏目 音乐乐理栏目 职场经验栏目 艺术欣赏栏目 设计栏目 工具栏目 语言栏目 自媒体矩阵 抖音【艺术，生活，剪辑，Vlog】短视频 VUE【Vlog】长视频 bilibili【合集】长视频 头条【知识，新闻】 知乎【生活、知识】 CSDN【专业知识】 微博【宣传】 简书【文字】 小红书【时尚、生活】 豆瓣【记录艺术】 公众号【输出】","link":"/2021/02/09/Draft/2021/%E8%87%AA%E5%AA%92%E4%BD%93%E8%AE%A1%E5%88%92/"},{"title":"魑魅先生 | 资源篇","text":"JVM 与上层技术 魑魅先生 | 资源篇 魑魅先生 | 资源篇 一.程序员书屋（编程相关，Kindle 相关 一共112个 G 资源） Java资源101个 G 囊括大部分 Java 开发小白到全栈大神所需阅读全部书籍，设计模式、网络、算法、框架、职场经验 ​ 关注公众号发送 小书屋 获取下载链接 不仅如此，还有 Kindle 各类电子书籍11个G 索引在此点击 搜索之后有需要的 关注公众号发送 Kindle获取下载链接！ 二.学习网站（持续更新中） 1.编程网站： Refactoring.Guru ​ 设计模式在线学习，程序员之间的沟通语言，学习必不可少的知识，有各种代码示范，UML图，动漫演示。 Visualgo ​ 算法在线可视化过程，理解更加容易，过程速度可调，步骤可控，代码可观。 Github ​ 将自己的学习代码进行版本化管理，将学习资源整理归纳，学习别人的优秀项目，获取全球最新资源，好处就不一一列举了，每个程序员都了解。 CSDN ​ 一个人的成就往往不在于得到多少，而在于输出多少，一个有贡献的程序员往往是会分享的。这个博客网站汇聚很多大牛，也有很多 demo，让你可以不重复造轮子。 2.设计网站： doyoudo ​ 学习 Ae、Ps、Pr、C4D、乐理等等。印象中小白老师的生动课堂是让我对设计感兴趣第一原因，也是第一个让我有想上付费课冲动的学习网站。 Colordrop ​ 配色网站，直观且复制方便，妈妈再也不用担心我的配色了。 创客贴 ​ 不会 PS 没关系，创客贴直接拉动修改元素，一切都直接设计好了，小白式拖动便可快速完成设计更改，还可以直接修改 GIF 动图！ 3.通用网站： Bilibili 不仅仅有鬼畜，你想有的教程都有，摄影、设计、编程、绘画等，是现代学习者不可缺少聚集地。 Coursera 国外学习网站，免费资源很多，可以接触更加及时的知识。 三.软件推荐（持续更新中） 电脑篇 英语语法书写纠正，毕竟语法纠正除了老师就只有他了。 自媒体必备，Markdown最佳利器。 字体中央管理，免费商用就够你用一辈子了，版权问题不用考虑了 Markdown图床管理优秀软件之一 有他还要啥百度云，配合chrome使用更香 你有个ipad放那盖泡面不如拿来用作你的第二屏幕 艾宾浩斯遗忘曲线最佳实践，记忆任何知识，自主卡片形式 目前使用过的最佳思维导图软件 三端互传文件，无缝同步，在鸿蒙系统出来之前它可能一直是我的主力 安卓可用，电脑操控手机，可无线控制，上班划水利器，免费开源 抓包，测试，网络模拟，新世界，一不小心从抓包到入狱 浏览器插件 ​ chrome有了它，啥会员都不用开。 ​ 有了它，任何复杂页面都可以变得小清新。 ipad篇 （待更新[如何抓包获取各种围哎皮，如何高效率利用学习软件]） 手机篇 （待更新[各种黑科技小工具]） ​ 魑魅先生，一只拼搏成为魑魅大能的小鬼(●—●) 四.设计资源（持续更新中） ​ 包括各大海报，字体，笔刷，素材，LOGO，全家桶等等，公众号回复设计资源获取下载链接 ​ ! 五.视频资源（持续更新中） Java，Python相关基础到架构课程，项目实战 ​ ​ 音乐相关 ​ 吉他，尤克里里，乐理，编曲，作词，相关软件，声源 ​ ​ 绘画 语言 摄影 ​ 包含美国摄影学院摄影教材，学习摄影不可缺少的书籍 最后想说，资源虽多，但如果只当一个收藏党那永远不会是你的。只有在这飞速发展的时代，卸去浮躁，一点点消化，才会让成为你茁壮成长的养分。 ​ 资源分享，部分资源请勿作商业用途，如有侵权，联系删除。如果有能力，请支持正版！","link":"/2021/07/22/Draft/2021/%E8%B5%84%E6%BA%90%E7%AF%87/"},{"title":"思维闪光","text":"突然想到的些东西。。 众文 闪光：公众文档，共同编写一个笔记，每人可上交笔记完善，通过投票决定是否修改，以此不再每个人都做相同的笔记，多人完善同一个笔记。 技术： 首页 现在的首页都丑，书签收藏，动态排名，书签分类，TODO组件，极美UI","link":"/2022/01/15/Draft/2021/%E9%97%AA%E5%85%89/"},{"title":"BUG","text":"项目管理，项目经验，开发之外【实施，测试，维护，部署】 项目总结，开发之外技能提升","link":"/2021/02/24/Draft/2021/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"},{"title":"信息系统项目管理师论文模板","text":"架构 一、项目背景(项目基本情况介绍、项目特点、要论述的论点) 二、提出论点(描述对××管理的重要性的认识) ​ 2020年6月，我参加了╳╳市资源局发起的“城市大脑”一地一码协同平台系统的建设工作，担任承建方项目经理，该信息综合管理平台系统的主要模块包括平台门户、“多规合一”业务协同系统优化、行政审批业务整合、土地码建设、数字驾驶舱规划和自然资源系统、驾驶舱指标库建设等9个业务管理模块，建设费用780万人民币，该项目于2021年6月通过了业主方的验收上线运行后,赢得了用户的一致好评，使项目获得了圆满成功。本文结合我的实际经验,以该项目为例,讨论了信息系统项目建设过程中的XX管理,主要从以下几个方面进行了阐述:XXXXX等过程内容，有效提高了XX管理水平，满足了项目干系人需求和期望。 ​ 2018年12月，“多规合一”业务协同系统中“多规共享”、“项目生成”等模块为各部门提供了非常便捷和快速的服务，取得了良好的效益。随着系统应用的不断深入，用户对“项目生成”等应用场景提出了更高的使用要求，如：项目类型、业务类型细化不足，导致在实际工作过程中存在盲区，影响了部分项目的推进等。另一方面，部门平台各自为政、系统间的数据存在壁垒，导致无法有效的进行数据共享，无法实现项目审批减材料、减时间、减环节，急需对原系统的各应用场景进行优化。杭州市XX资源局成立后，为尽快落实机构改革要求，释放改革红利，以自然资源“两统一”和业务需求为根本出发点，聚焦中心工作，立足已有基础，统筹整合现有资源，积极开展立足“一块土地一件事”全服务工作，由此一地一码协同平台应运而生。该项目采用了公开招标的方式，我公司参与了投标并顺利中标，于2020年6月签订了合同，随后成立了专门项目小组，任命我为项目经理。我公司的组织方式为项目型，项目成员直接归属项目经理领导，我组核心成员共15人。考虑到项目的复杂性、各区县地理位置不集中等原因，系统采用JAVA语言开发，项目组决定此项目采用SSM的三层b/s架构模式，地图基于leaflet的WebGIS可视化，数据库使用Postgres，这样的设计有利于后期的维护及扩展。项目共分为数字驾驶舱规划和自然资源系统、“一地一码”协同服务平台等子系统。经过全组成员的共同努力，项目于2021年6月顺利通过验收，目前运行情况良好。 ​ 由于该系统具有建设规模大（涵盖物价部门几乎所有业务），建设时间紧（建设期限为一年），涉及的干系人多（内部干系人包括项目组各类成员、公司领导和相关部门人员，外部干系人不仅包括市局各科室，也包括12个区县物价局的相关人员、运营商等），参与的项目成员多（高峰时达到60人，我将小组成员分为硬件集成部署组、数据中心组、软件开发组、平台测试组（四个小组），功能复杂等特点，为了保证项目圆满完成，我组建了强矩阵的项目组织结构，通过有效的项目管理，特别是出色的XX管理，带领项目团队全体成员经过奋战获得了良好的绩效，取得了项目的成功。本文将围绕该项目的XX管理进行重点讨论。 三、按照所涉及管理领域的过程分段进行论述(先给出每个过程的定义、输入、输出、工具与方法，再结合做过的项目描述项目中是如何做的) 一、 二、 三、 四、 五、 四、小结(对上面的论述进行一下小结) 五、分析项目存在的问题及改进的建议(写够三条就可以了) 六、总结(主要是写项目体会，比如通过项目实施，自己自觉应用项目管理知识、工具、技术，对成功实施项目所起的积极作用等)。 ​ 经过我们团队不懈的努力，历时1 年，本项目终于于2021年6月，通过了业主方组织的验收，实现从规划编制和实施、资源保护和利用、确权登记、地理资源等多方面进行业务和数据的融合，涵盖了规资局统一行使全民所有自然资源资产所有者职责，统一行使所有国土空间用途管制和生态保护修复职责。通过门户串联“多规合一”业务协同系统、行政审批系统、不动产登记系统等三大系统，以模块化形式，搭建多规共享、编制统筹、项目生成、实施监测、立项用地规划许可、工程许可、施工许可、竣工验收、确权登记等9个应用场景。本项目的成功在某些方面得益于我成功的xx 管理，在此我总结一下几条管理经验：（1）重视项目的调研，充分了解项目需求与范围；（2）树立正确的思想，采用适当的方法、遵循一定的流程，严格按照进度管理的要求做好活动定义、活动排序、活动的资源估算、活动的历时估算、编制进度计划、进度控制工作；（3）建立问题跟踪机制，对每个阶段的问题进行记录和跟踪，将每个问题落实到具体负责人。 ​ 当然，在本项目中，还有一些不足之处，比如：在项目的实施过程中，由于项目组2 名成员因为自身原因突然离职，导致项目的团队建设出现一些小问题，还有，曾经由于疫情原因导致一位技术人员居家隔离在家办公影响了些项目进度，不过，经过我后期的纠偏，并没有对项目产生什么影响。在后续的学习和工作中，我将不断的充电学习，和同行进行交流，提升自己的业务和管理水平，力争为我国信息化建设做出自己的努力。","link":"/2021/03/29/Draft/2021/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/"},{"title":"Dubbo","text":"分布式RPC框架Apache Dubbo 1. 软件架构的演进过程 软件架构的发展经历了由单体架构、垂直架构、SOA架构到微服务架构的演进过程，下面我们分别了解一下这几个架构。 1.1 单体架构 架构说明： ​ 全部功能集中在一个项目内（All in one）。 架构优点： ​ 架构简单，前期开发成本低、开发周期短，适合小型项目。 架构缺点： ​ 全部功能集成在一个工程中，对于大型项目不易开发、扩展和维护。 ​ 技术栈受限，只能使用一种语言开发。 ​ 系统性能扩展只能通过扩展集群节点，成本高。 1.2 垂直架构 架构说明： ​ 按照业务进行切割，形成小的单体项目。 架构优点： ​ 技术栈可扩展（不同的系统可以用不同的编程语言编写）。 架构缺点： ​ 功能集中在一个项目中，不利于开发、扩展、维护。 ​ 系统扩张只能通过集群的方式。 ​ 项目之间功能冗余、数据冗余、耦合性强。 1.3 SOA架构 SOA全称为Service-Oriented Architecture，即面向服务的架构。它可以根据需求通过网络对松散耦合的粗粒度应用组件(服务)进行分布式部署、组合和使用。一个服务通常以独立的形式存在于操作系统进程中。 站在功能的角度，把业务逻辑抽象成可复用的服务，通过服务的编排实现业务的快速再生，目的：把原先固有的业务功能转变为通用的业务服务，实现业务逻辑的快速复用。 架构说明： ​ 将重复功能或模块抽取成组件的形式，对外提供服务，在项目与服务之间使用ESB（企业服务总线）的形式作为通信的桥梁。 架构优点： ​ 重复功能或模块抽取为服务，提高开发效率。 ​ 可重用性高。 ​ 可维护性高。 架构缺点： ​ 各系统之间业务不同，很难确认功能或模块是重复的。 ​ 抽取服务的粒度大。 ​ 系统和服务之间耦合度高。 1.4 微服务架构 架构说明： ​ 将系统服务层完全独立出来，抽取为一个一个的微服务。 ​ 抽取的粒度更细，遵循单一原则。 ​ 采用轻量级框架协议传输。 架构优点： ​ 服务拆分粒度更细，有利于提高开发效率。 ​ 可以针对不同服务制定对应的优化方案。 ​ 适用于互联网时代，产品迭代周期更短。 架构缺点： ​ 粒度太细导致服务太多，维护成本高。 ​ 分布式系统开发的技术成本高，对团队的挑战大。 2. Apache Dubbo概述 2.1 Dubbo简介 Apache Dubbo是一款高性能的Java RPC框架。其前身是阿里巴巴公司开源的一个高性能、轻量级的开源Java RPC框架，可以和Spring框架无缝集成。 什么是RPC？ RPC全称为remote procedure call，即远程过程调用。比如两台服务器A和B，A服务器上部署一个应用，B服务器上部署一个应用，A服务器上的应用想调用B服务器上的应用提供的方法，由于两个应用不在一个内存空间，不能直接调用，所以需要通过网络来表达调用的语义和传达调用的数据。 需要注意的是RPC并不是一个具体的技术，而是指整个网络远程调用过程。 RPC是一个泛化的概念，严格来说一切远程过程调用手段都属于RPC范畴。各种开发语言都有自己的RPC框架。Java中的RPC框架比较多，广泛使用的有RMI、Hessian、Dubbo等。 Dubbo官网地址：http://dubbo.apache.org Dubbo提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。 2.2 Dubbo架构 Dubbo架构图（Dubbo官方提供）如下： 节点角色说明： 节点 角色名称 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 虚线都是异步访问，实线都是同步访问 蓝色虚线:在启动时完成的功能 红色虚线(实线)都是程序运行过程中执行的功能 调用关系说明: 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 3. 服务注册中心Zookeeper 通过前面的Dubbo架构图可以看到，Registry（服务注册中心）在其中起着至关重要的作用。Dubbo官方推荐使用Zookeeper作为服务注册中心。 3.1 Zookeeper介绍 Zookeeper 是 Apache Hadoop 的子项目，是一个树型的目录服务，支持变更推送，适合作为 Dubbo 服务的注册中心，工业强度较高，可用于生产环境，并推荐使用 。 为了便于理解Zookeeper的树型目录服务，我们先来看一下我们电脑的文件系统(也是一个树型目录结构)： 我的电脑可以分为多个盘符（例如C、D、E等），每个盘符下可以创建多个目录，每个目录下面可以创建文件，也可以创建子目录，最终构成了一个树型结构。通过这种树型结构的目录，我们可以将文件分门别类的进行存放，方便我们后期查找。而且磁盘上的每个文件都有一个唯一的访问路径，例如：C:\\Windows\\itcast\\hello.txt。 Zookeeper树型目录服务： 流程说明： 服务提供者(Provider)启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者(Consumer)启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心(Monitor)启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址 3.2 安装Zookeeper 下载地址：http://archive.apache.org/dist/zookeeper/ 本课程使用的Zookeeper版本为3.4.6，下载完成后可以获得名称为zookeeper-3.4.6.tar.gz的压缩文件。 安装步骤： 第一步：安装 jdk（略） 第二步：把 zookeeper 的压缩包（zookeeper-3.4.6.tar.gz）上传到 linux 系统 第三步：解压缩压缩包 ​ tar -zxvf zookeeper-3.4.6.tar.gz 第四步：进入zookeeper-3.4.6目录，创建data目录 ​ mkdir data 第五步：进入conf目录 ，把zoo_sample.cfg 改名为zoo.cfg ​ cd conf ​ mv zoo_sample.cfg zoo.cfg 第六步：打开zoo.cfg文件, 修改data属性：dataDir=/root/zookeeper-3.4.6/data 3.3 启动、停止Zookeeper 进入Zookeeper的bin目录，启动服务命令 ./zkServer.sh start 停止服务命令 ./zkServer.sh stop 查看服务状态： ./zkServer.sh status 4. Dubbo快速入门 Dubbo作为一个RPC框架，其最核心的功能就是要实现跨网络的远程调用。本小节就是要创建两个应用，一个作为服务的提供方，一个作为服务的消费方。通过Dubbo来实现服务消费方远程调用服务提供方的方法。 4.1 服务提供方开发 开发步骤： （1）创建maven工程（打包方式为war）dubbodemo_provider，在pom.xml文件中导入如下坐标 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- dubbo相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.7&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;version&gt;3.12.1.GA&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!-- 指定端口 --&gt; &lt;port&gt;8081&lt;/port&gt; &lt;!-- 请求路径 --&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; （2）配置web.xml文件 1234567891011121314&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext*.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt;&lt;/web-app&gt; （3）创建服务接口 1234package com.itheima.service;public interface HelloService { public String sayHello(String name);} （4）创建服务实现类 12345678910package com.itheima.service.impl;import com.alibaba.dubbo.config.annotation.Service;import com.itheima.service.HelloService;@Servicepublic class HelloServiceImpl implements HelloService { public String sayHello(String name) { return &quot;hello &quot; + name; }} 注意：服务实现类上使用的Service注解是Dubbo提供的，用于对外发布服务 （5）在src/main/resources下创建applicationContext-service.xml 123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo_provider&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 注册 协议和port 端口默认是20880 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20881&quot;&gt;&lt;/dubbo:protocol&gt; &lt;!-- 扫描指定包，加入@Service注解的类会被发布为服务 --&gt; &lt;dubbo:annotation package=&quot;com.itheima.service.impl&quot; /&gt;&lt;/beans&gt; （6）启动服务 tomcat7:run 4.2 服务消费方开发 开发步骤： （1）创建maven工程（打包方式为war）dubbodemo_consumer，pom.xml配置和上面服务提供者相同，只需要将Tomcat插件的端口号改为8082即可 （2）配置web.xml文件 1234567891011121314151617181920&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext-web.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; （3）将服务提供者工程中的HelloService接口复制到当前工程 （4）编写Controller 12345678910111213141516171819202122package com.itheima.controller;import com.alibaba.dubbo.config.annotation.Reference;import com.itheima.service.HelloService;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(&quot;/demo&quot;)public class HelloController { @Reference private HelloService helloService; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String getName(String name){ //远程调用 String result = helloService.sayHello(name); System.out.println(result); return result; }} 注意：Controller中注入HelloService使用的是Dubbo提供的@Reference注解 （5）在src/main/resources下创建applicationContext-web.xml 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo-consumer&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 扫描的方式暴露接口 --&gt; &lt;dubbo:annotation package=&quot;com.itheima.controller&quot; /&gt;&lt;/beans&gt; （6）运行测试 tomcat7:run启动 在浏览器输入http://localhost:8082/demo/hello.do?name=Jack，查看浏览器输出结果 **思考一：**上面的Dubbo入门案例中我们是将HelloService接口从服务提供者工程(dubbodemo_provider)复制到服务消费者工程(dubbodemo_consumer)中，这种做法是否合适？还有没有更好的方式？ **答：**这种做法显然是不好的，同一个接口被复制了两份，不利于后期维护。更好的方式是单独创建一个maven工程，将此接口创建在这个maven工程中。需要依赖此接口的工程只需要在自己工程的pom.xml文件中引入maven坐标即可。 **思考二：**在服务消费者工程(dubbodemo_consumer)中只是引用了HelloService接口，并没有提供实现类，Dubbo是如何做到远程调用的？ **答：**Dubbo底层是基于代理技术为HelloService接口创建代理对象，远程调用是通过此代理对象完成的。可以通过开发工具的debug功能查看此代理对象的内部结构。另外，Dubbo实现网络传输底层是基于Netty框架完成的。 **思考三：**上面的Dubbo入门案例中我们使用Zookeeper作为服务注册中心，服务提供者需要将自己的服务信息注册到Zookeeper，服务消费者需要从Zookeeper订阅自己所需要的服务，此时Zookeeper服务就变得非常重要了，那如何防止Zookeeper单点故障呢？ **答：**Zookeeper其实是支持集群模式的，可以配置Zookeeper集群来达到Zookeeper服务的高可用，防止出现单点故障。 5. Dubbo管理控制台 我们在开发时，需要知道Zookeeper注册中心都注册了哪些服务，有哪些消费者来消费这些服务。我们可以通过部署一个管理中心来实现。其实管理中心就是一个web应用，部署到tomcat即可。 5.1 安装 安装步骤： （1）将资料中的dubbo-admin-2.6.0.war文件复制到tomcat的webapps目录下 （2）启动tomcat，此war文件会自动解压 （3）修改WEB-INF下的dubbo.properties文件，注意dubbo.registry.address对应的值需要对应当前使用的Zookeeper的ip地址和端口号 ​ dubbo.registry.address=zookeeper://192.168.134.129:2181 ​ dubbo.admin.root.password=root ​ dubbo.admin.guest.password=guest （4）重启tomcat 5.2 使用 操作步骤： （1）访问http://localhost:8080/dubbo-admin-2.6.0/，输入用户名(root)和密码(root) （2）启动服务提供者工程和服务消费者工程，可以在查看到对应的信息 6. Dubbo相关配置说明 6.1 包扫描 1&lt;dubbo:annotation package=&quot;com.itheima.service&quot; /&gt; 服务提供者和服务消费者都需要配置，表示包扫描，作用是扫描指定包(包括子包)下的类。 如果不使用包扫描，也可以通过如下配置的方式来发布服务： 12&lt;bean id=&quot;helloService&quot; class=&quot;com.itheima.service.impl.HelloServiceImpl&quot; /&gt;&lt;dubbo:service interface=&quot;com.itheima.api.HelloService&quot; ref=&quot;helloService&quot; /&gt; 作为服务消费者，可以通过如下配置来引用服务： 12&lt;!-- 生成远程服务代理，可以和本地bean一样使用helloService --&gt;&lt;dubbo:reference id=&quot;helloService&quot; interface=&quot;com.itheima.api.HelloService&quot; /&gt; 上面这种方式发布和引用服务，一个配置项(dubbo:service、dubbo:reference)只能发布或者引用一个服务，如果有多个服务，这种方式就比较繁琐了。推荐使用包扫描方式。 6.2 协议 1&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot;/&gt; 一般在服务提供者一方配置，可以指定使用的协议名称和端口号。 其中Dubbo支持的协议有：dubbo、rmi、hessian、http、webservice、rest、redis等。 推荐使用的是dubbo协议。 dubbo 协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 也可以在同一个工程中配置多个协议，不同服务可以使用不同的协议，例如： 1234567&lt;!-- 多协议配置 --&gt;&lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20880&quot; /&gt;&lt;dubbo:protocol name=&quot;rmi&quot; port=&quot;1099&quot; /&gt;&lt;!-- 使用dubbo协议暴露服务 --&gt;&lt;dubbo:service interface=&quot;com.itheima.api.HelloService&quot; ref=&quot;helloService&quot; protocol=&quot;dubbo&quot; /&gt;&lt;!-- 使用rmi协议暴露服务 --&gt;&lt;dubbo:service interface=&quot;com.itheima.api.DemoService&quot; ref=&quot;demoService&quot; protocol=&quot;rmi&quot; /&gt; 6.3 启动时检查 1&lt;dubbo:consumer check=&quot;false&quot;/&gt; 上面这个配置需要配置在服务消费者一方，如果不配置默认check值为true。Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题。可以通过将check值改为false来关闭检查。 建议在开发阶段将check值设置为false，在生产环境下改为true。 6.4 负载均衡 负载均衡（Load Balance）：其实就是将请求分摊到多个操作单元上进行执行，从而共同完成工作任务。 在集群负载均衡时，Dubbo 提供了多种均衡策略（包括随机、轮询、最少活跃调用数、一致性Hash），缺省为random随机调用。 配置负载均衡策略，既可以在服务提供者一方配置，也可以在服务消费者一方配置，如下： 12345678910111213141516@Controller@RequestMapping(&quot;/demo&quot;)public class HelloController { //在服务消费者一方配置负载均衡策略 @Reference(check = false,loadbalance = &quot;random&quot;) private HelloService helloService; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String getName(String name){ //远程调用 String result = helloService.sayHello(name); System.out.println(result); return result; }} 1234567//在服务提供者一方配置负载均衡@Service(loadbalance = &quot;random&quot;)public class HelloServiceImpl implements HelloService { public String sayHello(String name) { return &quot;hello &quot; + name; }} 可以通过启动多个服务提供者来观察Dubbo负载均衡效果。 注意：因为我们是在一台机器上启动多个服务提供者，所以需要修改tomcat的端口号和Dubbo服务的端口号来防止端口冲突。 在实际生产环境中，多个服务提供者是分别部署在不同的机器上，所以不存在端口冲突问题。 7. 解决Dubbo无法发布被事务代理的Service问题 前面我们已经完成了Dubbo的入门案例，通过入门案例我们可以看到通过Dubbo提供的标签配置就可以进行包扫描，扫描到@Service注解的类就可以被发布为服务。 但是我们如果在服务提供者类上加入@Transactional事务控制注解后，服务就发布不成功了。原因是事务控制的底层原理是为服务提供者类创建代理对象，而默认情况下Spring是基于JDK动态代理方式创建代理对象，而此代理对象的完整类名为com.sun.proxy.$Proxy42（最后两位数字不是固定的），导致Dubbo在发布服务前进行包匹配时无法完成匹配，进而没有进行服务的发布。 7.1 问题展示 在入门案例的服务提供者dubbodemo_provider工程基础上进行展示 操作步骤： （1）在pom.xml文件中增加maven坐标 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; （2）在applicationContext-service.xml配置文件中加入数据源、事务管理器、开启事务注解的相关配置 1234567891011121314&lt;!--数据源--&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;username&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot; /&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/test&quot; /&gt;&lt;/bean&gt;&lt;!-- 事务管理器 --&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;&lt;/bean&gt;&lt;!--开启事务控制的注解支持--&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; 上面连接的数据库可以自行创建 （3）在HelloServiceImpl类上加入@Transactional注解 （4）启动服务提供者和服务消费者，并访问 上面的错误为没有可用的服务提供者 查看dubbo管理控制台发现服务并没有发布，如下： 可以通过断点调试的方式查看Dubbo执行过程，Dubbo通过AnnotationBean的postProcessAfterInitialization方法进行处理 7.2 解决方案 通过上面的断点调试可以看到，在HelloServiceImpl类上加入事务注解后，Spring会为此类基于JDK动态代理技术创建代理对象，创建的代理对象完整类名为com.sun.proxy.$Proxy35，导致Dubbo在进行包匹配时没有成功（因为我们在发布服务时扫描的包为com.itheima.service），所以后面真正发布服务的代码没有执行。 解决方式操作步骤： （1）修改applicationContext-service.xml配置文件，开启事务控制注解支持时指定proxy-target-class属性，值为true。其作用是使用cglib代理方式为Service类创建代理对象 12&lt;!--开启事务控制的注解支持--&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot; proxy-target-class=&quot;true&quot;/&gt; （2）修改HelloServiceImpl类，在Service注解中加入interfaceClass属性，值为HelloService.class，作用是指定服务的接口类型 1234567@Service(interfaceClass = HelloService.class)@Transactionalpublic class HelloServiceImpl implements HelloService { public String sayHello(String name) { return &quot;hello &quot; + name; }} 此处也是必须要修改的，否则会导致发布的服务接口为SpringProxy，而不是HelloService接口，如下：","link":"/2021/02/24/Draft/2021/Dubbo/"},{"title":"魑魅先生 | 业务技术","text":"所有的技术都是为了更好的解决业务 菜鸟一站： 图例： 记录 进行中 完成 日志记录（tomcat、mysql） Excel数据操作 文件上传下载解析到数据库 集成自己的Java前后台框架 CURD（所有框架） 网络通信 单元测试 实际业务 加密解密【AES(CBC)】 缓存 hutool（JAVA工具类包） 扫码登录 图片处理 jeekins与部署实施 动态表单设计与实现 验证码 外部接口应用 工作流 JWT spring security 代码生成 数据库设计(见MySQL优化) 文件格式转换（word--》pdf） 数据格式转换 工具类合集 分页 跨域 单点登录 外发接口规范token验证 支付 短信 后台创建数据【创建时间，id，初始数据】 目录可参考kuangstudy 数据格式 权限管理 JDK高版本 开源相关 开源项目研究：JEECG BOOT 低代码开发平台【学习业务：代码生成，工作流，文件管理，单点登录，数据性能监控，消息中心】文档 工具类【文件【Excel、图片、Word等】、网络、安全【Xss】、格式、线程、文本【日期，json】、日志、】 开源项目 JEECG 环境： 123456789101112npm i yarn -gyarn config set registry https://registry.npm.taobao.org --globalyarn config set disturl https://npm.taobao.org/dist --globalyarn install yarn常用命令yarn / yarn install 等同于npm install 批量安装依赖yarn add xxx 等同于 npm install xxx —save 安装指定包到指定位置yarn remove xxx 等同于 npm uninstall xxx —save 卸载指定包yarn add xxx —dev 等同于 npm install xxx —save-devyarn upgrade 等同于 npm update 升级全部包yarn global add xxx 等同于 npm install xxx -g 全局安装指定包 CRUD 所有知识从CRUD开始 SSM SpringBoot SpringCloud 文件上传下载 相关对象： 1、MultipartFile 和 CommonsMultipartFile都是用来接收上传的文件流的 ！ 2、MultipartFile是一个接口，CommonsMultipartFile是MultipartFile接口的实现类 ！ 3、使用MultipartFile作为形参接收上传文件时，直接用即可。CommonsMultipartFile作为形参接收上传文件时，必需添加@RequestParam注解（否则会报错：exception is java.lang.NoSuchMethodException: org.springframework.web.multipart.commons.CommonsMultipartFile） 12MultipartFile 文件路径： 1HttpServletRequest req Servlet获取当前项目的上下文路径（web文件下的路径）： 1req.getContextPath() Servlet获取当前项目的上下文的绝对路径（web文件下的路径）： 1req.getServletContext().getRealPath(); 获取Java程序中的resources文件路径： 1Resources.getResourceAsStream() 前后端交互： 跨域 解决根本原理 什么情况会跨域 同一协议， 如http或https 同一IP地址, 如127.0.0.1 同一端口, 如8080 以上三个条件中有一个条件不同就会产生跨域问题。 解决方案 前端解决方案 使用JSONP方式实现跨域调用； 使用NodeJS服务器做为服务代理，前端发起请求到NodeJS服务器， NodeJS服务器代理转发请求到后端服务器； 后端解决方案 nginx反向代理解决跨域 服务端设置Response Header(响应头部)的Access-Control-Allow-Origin 在需要跨域访问的类和方法中设置允许跨域访问（如Spring中使用@CrossOrigin注解）； 继承使用Spring Web的CorsFilter（适用于Spring MVC、Spring Boot） 实现WebMvcConfigurer接口（适用于Spring Boot） WebService 简介 跨编程语言和跨操作系统平台的远程调用技术 原理 XML,SOAP和WSDL就是构成WebService平台的三大技术 。 WebService采用Http协议来在客户端和服务端之间传输数据。WebService使用XML来封装数据，XML主要的优点在于它是跨平台的。 WebService通过HTTP协议发送请求和接收结果时，发送的请求内容和结果内容都采用XML格式封装，并增加了一些特定的HTTP消息头，以说明HTTP消息的内容格式，这些特定的HTTP消息头和XML内容格式就是SOAP协议规定的。 WebService服务器端首先要通过一个WSDL文件来说明自己有什么服务可以对外调用。简单的说，WSDL就像是一个说明书，用于描述WebService及其方法、参数和返回值。 WSDL文件保存在Web服务器上，通过一个url地址就可以访问到它。客户端要调用一个WebService服务之前，要知道该服务的WSDL文件的地址。WebService服务提供商可以通过两种方式来暴露它的WSDL文件地址：1.注册到UDDI服务器，以便被人查找；2.直接告诉给客户端调用者。 RESTful 简介 数据格式 JSON 数据示例 1234567{ &quot;sites&quot;: [ { &quot;name&quot;:&quot;a&quot; , &quot;url&quot;:&quot;www.runoob.com&quot; }, { &quot;name&quot;:&quot;b&quot; , &quot;url&quot;:&quot;www.google.com&quot; }, { &quot;name&quot;:&quot;c&quot; , &quot;url&quot;:&quot;www.weibo.com&quot; } ]} 不同数据相互转换方式 字符串，Map，对象，JSONObject（无序），JSONArray（有序），Java数组 字符串2Json 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849```### 判断是否包含.has### CRUD### 取值```javastatic Map analysisJsonResultMap = new HashMap(); public static Map analysisJson(Object objJson) { //如果obj为json数组 if (objJson instanceof JSONArray) { JSONArray objArray = (JSONArray) objJson; for (int i = 0; i &lt; objArray.size(); i++) { analysisJson(objArray.get(i)); } } //如果为json对象 else if (objJson instanceof JSONObject) { JSONObject jsonObject = (JSONObject) objJson; Iterator it = jsonObject.keySet().iterator(); while (it.hasNext()) { String key = it.next().toString(); Object object = jsonObject.get(key); //如果得到的是数组 if (object instanceof JSONArray) { JSONArray objArray = (JSONArray) object; analysisJson(objArray); } //如果key中是一个json对象 else if (object instanceof JSONObject) { analysisJson((JSONObject) object); } //如果key中是其他 else { System.out.println(&quot;[&quot; + key + &quot;]:&quot; + object.toString() + &quot; &quot;); analysisJsonResultMap.put(key, object.toString()); } } } return analysisJsonResultMap; } 开源相关 https://www.runoob.com/w3cnote/open-source-license.html 单元测试 基础知识 Springboot引入 spring-boot-starter-test JUnit — The de-facto standard for unit testing Java applications. Spring Test &amp; Spring Boot Test — Utilities and integration test support for Spring Boot applications. AssertJ — A fluent assertion library. Hamcrest — A library of matcher objects (also known as constraints or predicates). Mockito — A Java mocking framework. JSONassert — An assertion library for JSON. JsonPath — XPath for JSON. assertThat 基础语法 assertThat( [value], [matcher statement] ); value 是接下来想要测试的变量值； matcher statement 是使用 Hamcrest 匹配符来表达的对前面变量所期望的值的声明，如果 value 值与 matcher statement 所表达的期望值相符，则测试成功，否则测试失败。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859字符相关匹配符/**equalTo匹配符断言被测的testedValue等于expectedValue，* equalTo可以断言数值之间，字符串之间和对象之间是否相等，相当于Object的equals方法*/assertThat(testedValue, equalTo(expectedValue));/**equalToIgnoringCase匹配符断言被测的字符串testedString*在忽略大小写的情况下等于expectedString*/assertThat(testedString, equalToIgnoringCase(expectedString));/**equalToIgnoringWhiteSpace匹配符断言被测的字符串testedString*在忽略头尾的任意个空格的情况下等于expectedString，*注意：字符串中的空格不能被忽略*/assertThat(testedString, equalToIgnoringWhiteSpace(expectedString);/**containsString匹配符断言被测的字符串testedString包含子字符串subString**/assertThat(testedString, containsString(subString) );/**endsWith匹配符断言被测的字符串testedString以子字符串suffix结尾*/assertThat(testedString, endsWith(suffix));/**startsWith匹配符断言被测的字符串testedString以子字符串prefix开始*/assertThat(testedString, startsWith(prefix));一般匹配符/**nullValue()匹配符断言被测object的值为null*/assertThat(object,nullValue());/**notNullValue()匹配符断言被测object的值不为null*/assertThat(object,notNullValue());/**is匹配符断言被测的object等于后面给出匹配表达式*/assertThat(testedString, is(equalTo(expectedValue)));/**is匹配符简写应用之一，is(equalTo(x))的简写，断言testedValue等于expectedValue*/assertThat(testedValue, is(expectedValue));/**is匹配符简写应用之二，is(instanceOf(SomeClass.class))的简写，*断言testedObject为Cheddar的实例*/assertThat(testedObject, is(Cheddar.class));/**not匹配符和is匹配符正好相反，断言被测的object不等于后面给出的object*/assertThat(testedString, not(expectedString));/**allOf匹配符断言符合所有条件，相当于“与”（&amp;&amp;）*/assertThat(testedNumber, allOf( greaterThan(8), lessThan(16) ) );/**anyOf匹配符断言符合条件之一，相当于“或”（||）*/assertThat(testedNumber, anyOf( greaterThan(16), lessThan(8) ) );数值相关匹配符/**closeTo匹配符断言被测的浮点型数testedDouble在20.0¡À0.5范围之内*/assertThat(testedDouble, closeTo( 20.0, 0.5 ));/**greaterThan匹配符断言被测的数值testedNumber大于16.0*/assertThat(testedNumber, greaterThan(16.0));/** lessThan匹配符断言被测的数值testedNumber小于16.0*/assertThat(testedNumber, lessThan (16.0));/** greaterThanOrEqualTo匹配符断言被测的数值testedNumber大于等于16.0*/assertThat(testedNumber, greaterThanOrEqualTo (16.0));/** lessThanOrEqualTo匹配符断言被测的testedNumber小于等于16.0*/assertThat(testedNumber, lessThanOrEqualTo (16.0));集合相关匹配符/**hasEntry匹配符断言被测的Map对象mapObject含有一个键值为&quot;key&quot;对应元素值为&quot;value&quot;的Entry项*/assertThat(mapObject, hasEntry(&quot;key&quot;, &quot;value&quot; ) );/**hasItem匹配符表明被测的迭代对象iterableObject含有元素element项则测试通过*/assertThat(iterableObject, hasItem (element));/** hasKey匹配符断言被测的Map对象mapObject含有键值“key”*/assertThat(mapObject, hasKey (&quot;key&quot;));/** hasValue匹配符断言被测的Map对象mapObject含有元素值value*/assertThat(mapObject, hasValue(value)); 可表达全部的测试思想。 Springboot 可自动生成测试代码 IDEA Ctrl + Shift +T 生成测试类 Service 123456789101112131415161718192021import org.junit.Assert;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import static org.hamcrest.CoreMatchers.*;@RunWith(SpringRunner.class)@SpringBootTestpublic class LearnServiceTest { @Autowired private LearnService learnService; @Test public void getLearn(){ LearnResource learnResource=learnService.selectByKey(1001L); Assert.assertThat(learnResource.getAuthor(),is(&quot;嘟嘟MD独立博客&quot;)); }} Controller MockMvc：可以不启动工程测试接口，MockMvc 实现了对 Http 请求的模拟，能够直接使用网络的形式，转换到 Controller 的调用，这样可以使得测试速度快、不依赖网络环境，而且提供了一套验证的工具，这样可以使得请求的验证统一而且很方便。 mockMvc.perform执行一个请求 MockMvcRequestBuilders.get(“/user/1”)构造一个请求，Post请求就用.post方法 contentType(MediaType.APPLICATION_JSON_UTF8)代表发送端发送的数据格式是application/json;charset=UTF-8 accept(MediaType.APPLICATION_JSON_UTF8)代表客户端希望接受的数据类型为application/json;charset=UTF-8 session(session)注入一个session，这样拦截器才可以通过 ResultActions.andExpect添加执行完成后的断言 ResultActions.andExpect(MockMvcResultMatchers.status().isOk())方法看请求的状态响应码是否为200如果不是则抛异常，测试不通过 andExpect(MockMvcResultMatchers.jsonPath(“$.author”).value(“嘟嘟MD独立博客”))这里jsonPath用来获取author字段比对是否为嘟嘟MD独立博客,不是就测试不通过 ResultActions.andDo添加一个结果处理器，表示要对结果做点什么事情，比如此处使用MockMvcResultHandlers.print()输出整个响应结果信息 单元测试回滚 测试的垃圾数据清理，添加注解 @Transactional 默认引擎是InnoDB有效，MyISAM（MySQL5.5之前默认引擎）不支持事务、也不支持外键 想关闭回滚，只要加上@Rollback(false)注解即可。@Rollback表示事务执行完回滚，支持传入一个参数value，默认true即回滚，false不回滚。 修改默认引擎的步骤 MyISAM适合：(1)做很多count 的计算；(2)插入不频繁，查询非常频繁；(3)没有事务。 InnoDB适合：(1)可靠性要求比较高，或者要求事务；(2)表更新和查询都相当的频繁，并且表锁定的机会比较大的情况。(4)性能较好的服务器，比如单独的数据库服务器，像阿里云的关系型数据库RDS就推荐使用InnoDB引擎。 show variables like '%storage_engine%'; show create table user; ALTER TABLE user ENGINE=INNODB; Swagger UI 接口文档，接口测试 123456789101112131415161718192021222324252627282930313233343536@Api：用在请求的类上，表示对类的说明 tags=&quot;说明该类的作用，可以在UI界面上看到的注解&quot; value=&quot;该参数没什么意义，在UI界面上也看到，所以不需要配置&quot; @ApiOperation：用在请求的方法上，说明方法的用途、作用 value=&quot;说明方法的用途、作用&quot; notes=&quot;方法的备注说明&quot; @ApiImplicitParams：用在请求的方法上，表示一组参数说明 @ApiImplicitParam：用在@ApiImplicitParams注解中，指定一个请求参数的各个方面 name：参数名 value：参数的汉字说明、解释 required：参数是否必须传 paramType：参数放在哪个地方 · header --&gt; 请求参数的获取：@RequestHeader · query --&gt; 请求参数的获取：@RequestParam · path（用于restful接口）--&gt; 请求参数的获取：@PathVariable · body（不常用） · form（不常用） dataType：参数类型，默认String，其它值dataType=&quot;Integer&quot; defaultValue：参数的默认值 @ApiResponses：用在请求的方法上，表示一组响应 @ApiResponse：用在@ApiResponses中，一般用于表达一个错误的响应信息 code：数字，例如400 message：信息，例如&quot;请求参数没填好&quot; response：抛出异常的类 @ApiModel：用于响应类上，表示一个返回响应数据的信息 （这种一般用在post创建的时候，使用@RequestBody这样的场景， 请求参数无法使用@ApiImplicitParam注解进行描述的时候） @ApiModelProperty：用在属性上，描述响应类的属性 http转https 两者区别： https：优： 1.数据加密 2.SSL安全机制【客户端访问服务器-&gt;服务器把数字证书+公用密匙 发给客户端-&gt;客户端验证服务器，确保访问的是正确的服务器（不是钓鱼网站）-&gt;客户端生产会话密匙并用公用密匙进行加密再次发给服务器-&gt;服务器用私人密匙进行解密（也就相当于验证客户端），验证成功建立起一条安全的数据传递通道-&gt;服务器把客户端请求的数据打包加密发送给客户端-&gt;客户端浏览器接收数据并解析 3.安全标识，提高用户信任感 步骤： 申请SSL证书 安装证书。 整改网站链接 全站做301转向，减少网站权重的流失 相关 服务器 的 80、8080、443（https默认）端口开放需要备案。 nginx 【win】 购买ssl证书获得key和pem文件 123456789101112131415161718192021222324252627server { listen 8080 ssl; server_name aaa.bbb.com; ssl on; ssl_certificate E:/tys/nginx-1.12.2/key_pem//hzsgis.pem; ssl_certificate_key E:/tys/nginx-1.12.2/key_pem/hzsgis.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 20m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #charset koi8-r; #access_log logs/host.access.log main; location /main/map3d/rest { #add_header 'Access-Control-Allow-Headers' '*'; proxy_pass http://111.111.11.11:1111/main/map3d/rest; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; }} nginx 【Linux】 购买ssl证书获得key和pem文件并添加到nginx目录下 改443端口配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events { worker_connections 1024;}http { include mime.types; default_type application/octet-stream; #log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' # '$status $body_bytes_sent &quot;$http_referer&quot; ' # '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; upstream jq_one{server 10.32.250.176:8080 weight=5;server 10.32.250.177:8080 weight=5;}upstream jq_two{server 10.32.250.178:8080 weight=5;server 10.32.250.179:8080 weight=5;}server {listen 8080;server_name localhost;#charset koi8-r;#access_log logs/host.access.log main;location / {root html;index index.html index.htm;proxy_pass http://jq_two;proxy_set_header Host $http_host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header REMOTE-HOST $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;}} server { listen 80; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; proxy_pass http://jq_one; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } server { listen 443 ssl; server_name fyditu.fuyang.gov.cn; ssl_certificate /usr/local/nginx/key_pem//fyditu.fuyang.gov.cn_bundle.pem; ssl_certificate_key /usr/local/nginx/key_pem//fyditu.fuyang.gov.cn.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 20m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; #charset koi8-r; #access_log logs/host.access.log main; location / { root html; index index.html index.htm; proxy_pass http://jq_one; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header REMOTE-HOST $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # proxy_pass http://127.0.0.1; #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} # deny access to .htaccess files, if Apache's document root # concurs with nginx's one # #location ~ /\\.ht { # deny all; #} } # another virtual host using mix of IP-, name-, and port-based configuration # #server { # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / { # root html; # index index.html index.htm; # } #} # HTTPS server # #server { # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / { # root html; # index index.html index.htm; # } #}} 检验配置文件 12/usr/local/nginx/sbin 安装目录下执行./nginx -t nginx安装SSL模块 123456#进入nginx源码文件夹执行，/usr/local/nginx为安装文件夹./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module#替换安装 非make install覆盖安装make#备份cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak 重启 12345678910111213#查看nginx端口号ps -ef|grep nginx#关闭进程kill -QUIT 端口号#从源码文件夹复制到安装文件夹cp ./objs/nginx /usr/local/nginx/sbin/#先切换到sbin目录cd /usr/local/nginx/sbin/#检测nginx的配置文件是否有错误./niginx -t#启动/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf 开通443端口 123456789101112131415161718#1.先查看服务器防火墙开放的端口firewall-cmd --zone=public --list-ports //查看防火墙的开放端口#2.允许防火墙放行443端口# 命令含义：# --zone #作用域# --add-port=443/tcp #添加端口，格式为：端口/通讯协议# --permanent #代表永久生效，没有此参数重启后失效firewall-cmd --zone=public --add-port=443/tcp --permanent#3.重启防火墙firewall-cmd --reload1.systemctl start firewalld.service（开启防火墙）2.systemctl stop firewalld.service（开启防火墙）3.service firewalld restart（重启防火墙）4.firewall-cmd --zone=public --add-port=4400-4600/udp --permanen(指定端口范围为4400-4600通过防火墙)Warning: ALREADY_ENABLED: 3306:tcp（说明3306端口通过成功）5.firewall-cmd --zone=public --remove-port=80/tcp --permanent（关闭指定端口）6.firewall-cmd --zone=public --list-ports（查看通过的端口）7.查看防火墙状态 ：firewall-cmd --state 定时任务 SpringBoot @Scheduled注解，接口SchedulingConfigurer,Quartz 工作流 Activit 分库分表 在访问量连接数或者数据量大的组合情况下选择不同分库分表方案。根据数据访问量，单库承受量选择分多少。根据表信息选择水平【比如不同等级】或垂直拆分（比如买家买家信息），","link":"/2021/03/01/Draft/2021/JAVA%E5%BA%94%E7%94%A8/"},{"title":"SpringCloud","text":"新学四问 WHY【与前代优化了什么，弥补了什么空白】微服务，主流 WHAT【框架，思维导图，主题框架】eureka注册中心，Gateway网关，Ribbon负载均衡，Feign服务调用，Hystrix熔断器等，springcloudalibaba HOW【如何记忆，学习资源】:bilibili，官网 LEVEL【不是每个都学精】当前阶段熟练运用 Spring Cloud 【Hoxton】 简介 1. 系统架构演变 随着互联网的发展，网站应用的规模不断扩大。需求的激增，带来的是技术上的压力。系统架构也因此也不断的演 进、升级、迭代。从单一应用，到垂直拆分，到分布式服务，到SOA，以及现在火热的微服务架构，还有在Google 带领下来势汹涌的Service Mesh。 1.1. 集中式架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。 优点： 系统开发速度快 维护成本低 适用于并发要求较低的系统 缺点： 代码耦合度高，后期维护困难 无法针对不同模块进行针对性优化 无法水平扩展 单点容错率低，并发能力差 1.2. 垂直拆分 ​ 当访问量逐渐增大，单一应用无法满足需求，此时为了应对更高的并发和业务需求，我们根据业务功能对系统进行拆 分： 优点： 系统拆分实现了流量分担，解决了并发问题 可以针对不同模块进行优化 方便水平扩展，负载均衡，容错率提高 缺点： 系统间相互独立，会有很多重复开发工作，影响开发效率 1.3. 分布式服务 ​ 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。 优点： 将基础服务进行了抽取，系统间相互调用，提高了代码复用和开发效率 缺点： 系统间耦合度变高，调用关系错综复杂，难以维护 1.4. 面向服务架构（SOA） ​ SOA（Service Oriented Architecture）面向服务的架构：它是一种设计方法，其中包含多个服务， 服务之间通过相互依赖最终提供一系列的功能。一个服务 通常以独立的形式存在与操作系统进程中。各个服务之间 通过网络调用。SOA结构图： ESB（企业服务总线），简单 来说 ESB 就是一根管道，用来连接各个服务节点。为了集 成不同系统，不同协议的服务，ESB 做了消息的转化解释和路由工作，让不同的服务互联互通。 SOA缺点：每个供应商提供的ESB产品有偏差，自身实现较为复杂；应用服务粒度较大，ESB集成整合所有服务和协 议、数据转换使得运维、测试部署困难。所有服务都通过一个通路通信，直接降低了通信速度。 1.5. 微服务架构 ​ 微服务架构是使用一套小服务来开发单个应用的方式或途径，每个服务基于单一业务能力构建，运行在自己的进程中，并使用轻量级机制通信，通常是HTTP API，并能够通过自动化部署机制来独立部署。这些服务可以使用不同的编程语言实现，以及不同数据存储技术，并保持最低限度的集中式管理。微服务结构图： API Gateway网关是一个服务器，是系统的唯一入口。为每个客户端提供一个定制的API。API网关核心是，所有的客户端和消费端都通过统一的网关接入微服务，在网关层处理所有的非业务功能。如它还可以具有其它职责，如身份验证、监控、负载均衡、缓存、请求分片与管理、静态响应处理。通常，网关提供RESTful/HTTP的方式访问服务。而服务端通过服务注册中心进行服务注册和管理。 微服务的特点： 单一职责：微服务中每一个服务都对应唯一的业务能力，做到单一职责 微：微服务的服务拆分粒度很小，例如一个用户管理就可以作为一个服务。每个服务虽小，但“五脏俱全”。 面向服务：面向服务是说每个服务都要对外暴露Rest风格服务接口API。并不关心服务的技术实现，做到与平台和语言无关，也不限定用什么技术实现，只要提供Rest的接口即可。 自治：自治是说服务间互相独立，互不干扰 团队独立：每个服务都是一个独立的开发团队，人数不能过多。 技术独立：因为是面向服务，提供Rest接口，使用什么技术没有别人干涉 前后端分离：采用前后端分离开发，提供统一Rest接口，后端不用再为PC、移动端开 发不同接口 数据库分离：每个服务都使用自己的数据源 部署独立：服务间虽然有调用，但要做到服务重启不影响其它服务。有利于持续集成和 持续交付。每个服务都是独立的组件，可复用，可替换，降低耦合，易维护 微服务架构与SOA都是对系统进行拆分；微服务架构基于SOA思想，可以把微服务当做去除了ESB的SOA。ESB是SOA架构中的中心总线，设计图形应该是星形的，而微服务是去中心化的分布式软件架构。两者比较类似，但其实也有一些差别： 2. 服务调用方式 2.1. RPC和HTTP 无论是微服务还是SOA，都面临着服务间的远程调用。那么服务间的远程调用方式有哪些呢？ 常见的远程调用方式有以下2种： RPC：Remote Produce Call远程过程调用，RPC基于Socket，工作在会话层。自定义数据格式，速度快，效率高。早期的webservice，现在热门的dubbo，都是RPC的典型代表 Http：http其实是一种网络传输协议，基于TCP，工作在应用层，规定了数据传输的格式。现在客户端浏览器与服务端通信基本都是采用Http协议，也可以用来进行远程服务调用。缺点是消息封装臃肿，优势是对服务的提供和调用方没有任何技术限定，自由灵活，更符合微服务理念。 现在热门的Rest风格，就可以通过http协议来实现。 区别：RPC的机制是根据语言的API（language API）来定义的，而不是根据基于网络的应用来定义的。 如果你们公司全部采用Java技术栈，那么使用Dubbo作为微服务架构是一个不错的选择。 相反，如果公司的技术栈多样化，而且你更青睐Spring家族，那么Spring Cloud搭建微服务是不二之选。在我们的项目中，会选择Spring Cloud套件，因此会使用Http方式来实现服务间调用。 2.2. Http客户端工具 既然微服务选择了Http，那么我们就需要考虑自己来实现对请求和响应的处理。不过开源世界已经有很多的http客户端工具，能够帮助我们做这些事情，例如： HttpClient OKHttp URLConnection 不过这些不同的客户端，API各不相同。而Spring也有对http的客户端进行封装，提供了工具类叫RestTemplate。 2.3. Spring的RestTemplate Spring提供了一个RestTemplate模板工具类，对基于Http的客户端进行了封装，并且实现了对象与json的序列化和 反序列化，非常方便。RestTemplate并没有限定Http的客户端类型，而是进行了抽象，目前常用的3种都有支持： HttpClient OkHttp JDK原生的URLConnection（默认的） 3.Springcloud综述 版本命名：伦敦地铁站字母顺序 实验环境 停更、升级、替换 父工程创建 创建工程前准备 约定》配置》编码 编码UTF-8全套 作用 注解生效激活 作用 JAVA编译版本 文件过滤 作用，关闭对应文件idea可见 Eureka【大楼老板】 Spring Cloud [Finchley] ​ 一系列框架有序集合，封装后屏蔽了复杂的配置和实现原理。Spring Cloud为开发人员提供了工具，以快速构建分布式系统中的一些常见模式（例如，配置管理，服务发现，断路器，智能路由，微代理，控制总线，一次性令牌，全局锁，领导选举，分布式会话，群集状态）。分布式系统的协调导致样板式样，并且使用Spring Cloud开发人员可以快速站起来实现这些样板的服务和应用程序。它们将在任何分布式环境中都能很好地工作，包括开发人员自己的笔记本电脑，裸机数据中心以及诸如Cloud Foundry之类的托管平台。 1.系统架构演变 graph LR; 1[集中式架构] --> 2[垂直拆分] 2 --> 3[分布式服务] 3 --> 4[SOA面向服务架构] 4 --> 5[微服务架构] 微服务架构 一套使用小服务或者单一业务来开发单个应用的方式或途径。 微服务架构特点： 单一职责 服务粒度小 面向服务（对外暴露REST api） 服务之间相互独立 与使用ESB的SOA架构的区别： 微服务架构没有使用ESB，有服务治理注册中心；业务粒度小。 2.服务调用方式 RPC：基于socket，速度快，效率高；webservice、dubbo HTTP：基于TCP，封装比较臃肿；对服务和调用方没有任何技术、语言的限定，自由灵活；RESTful，Spring Cloud 一般情况下有如下三种http客户端工具类包都可以方便的进行http服务调用： httpClient okHttp JDK原生URLConnection spring 提供了RestTemplate的工具类对上述的3种http客户端工具类进行了封装，可在spring项目中使用RestTemplate进行服务调用。 小结： 12345678910111213141516@RunWith(SpringRunner.class)@SpringBootTestpublic class RestTemplateTest { @Autowired private RestTemplate restTemplate; @Test public void test(){ String url = &quot;http://localhost/user/8&quot;; //restTemplate可以对json格式字符串进行反序列化 User user = restTemplate.getForObject(url, User.class); System.out.println(user); }} 3.SpringCloud概述 整合的组件可以有很多组件；常见的组件有：eureka注册中心，Gateway网关，Ribbon负载均衡，Feign服务调用，Hystrix熔断器。在有需要的时候项目添加对于的启动器依赖即可。 版本特征：以英文单词命名（伦敦地铁站名） 4.创建微服务工程 父工程springcloud：添加spring boot父坐标和管理其它组件的依赖 用户服务工程user-service：整合mybatis查询数据库中用户数据；提供查询用户服务 服务消费工程consumer-demo：利用查询用户服务获取用户数据并输出到浏览器 5.搭建配置Service工程 添加启动器依赖（web、通用Mapper）； 创建启动引导类和配置文件； 修改配置文件中的参数； 编写测试代码（UserMapper，UserService，UserController）； 测试 6.搭建配置Client工程 添加启动器依赖； 创建启动引导类（注册RestTemplate）和配置文件； 编写测试代码（ConsumerController中使用restTemplate访问服务获取数据） 测试 7.问题 服务管理 如何自动注册和发现 如何实现状态监管 如何实现动态路由 服务如何实现负载均衡 服务如何解决容灾问题 服务如何实现统一配置 上述问题通过springcloud各种组件解决 8.Eureka Eureka的主要功能是进行服务管理，定期检查服务状态，返回服务地址列表。 8.1Eureka-server Eureka是服务注册中心，只做服务注册；自身并不提供服务也不消费服务。可以搭建web工程使用Eureka，可以使用Spring Boot方式搭建。 搭建步骤： 创建工程； 添加启动器依赖； 编写启动引导类（添加Eureka的服务注解）和配置文件； 修改配置文件（端口，应用名称...）； 启动测试 8.2服务注册与发现 服务注册：在服务提供工程user-service上添加Eureka客户端依赖；自动将服务注册到EurekaServer服务地址列表。 添加依赖； 改造启动引导类；添加开启Eureka客户端发现的注解； 修改配置文件；设置Eureka 服务地址 服务发现：在服务消费工程consumer-demo上添加Eureka客户端依赖；可以使用工具类根据服务名称获取对应的服务地址列表。 添加依赖； 改造启动引导类；添加开启Eureka客户端发现的注解； 修改配置文件；设置Eureka 服务地址； 改造处理器类Controller，可以使用工具类DiscoveryClient根据服务名称获取对应服务地址列表。 8.3高可用配置 ​ 将Eureka Server作为一个服务注册到其它Eureka Server，这样多个Eureka Server之间就能够互相发现对方，同步服务，实现Eureka Server集群。 9.负载均衡Ribbon Ribbon提供了轮询、随机两种负载均衡算法（默认是轮询）可以实现从地址列表中使用负载均衡算法获取地址进行服务调用。 9.1Ribbon应用 在实例化RestTemplate的时候使用@LoadBalanced，服务地址直接可以使用服务名。 10.熔断器Hystrix（豪猪） Hystrix是一个延迟和容错库，用于隔离访问远程服务，防止出现级联失败。 10.1线程隔离&amp;服务降级 Hystrix解决雪崩效应： 线程隔离：用户请求不直接访问服务，而是使用线程池中空闲的线程访问服务，加速失败判断时间。 服务降级：及时返回服务调用失败的结果，让线程不因为等待服务而阻塞。 小结： 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 开启熔断 降级逻辑 12345678910111213141516171819202122232425262728293031323334353637@RestController@RequestMapping(&quot;/consumer&quot;)@Slf4j@DefaultProperties(defaultFallback = &quot;defaultFallback&quot;)public class ConsumerController { @Autowired private RestTemplate restTemplate; @Autowired private DiscoveryClient discoveryClient; @GetMapping(&quot;/{id}&quot;) //@HystrixCommand(fallbackMethod = &quot;queryByIdFallback&quot;) @HystrixCommand public String queryById(@PathVariable Long id){ /*String url = &quot;http://localhost:9091/user/&quot;+id; //获取eureka中注册的user-service的实例 List&lt;ServiceInstance&gt; serviceInstances = discoveryClient.getInstances(&quot;user-service&quot;); ServiceInstance serviceInstance = serviceInstances.get(0); url = &quot;http://&quot; + serviceInstance.getHost() + &quot;:&quot; + serviceInstance.getPort() + &quot;/user/&quot; + id;*/ String url = &quot;http://user-service/user/&quot; + id; return restTemplate.getForObject(url, String.class); } public String queryByIdFallback(Long id){ log.error(&quot;查询用户信息失败。id：{}&quot;, id); return &quot;对不起，网络太拥挤了！&quot;; } public String defaultFallback(){ return &quot;默认提示：对不起，网络太拥挤了！&quot;; }} 修改超时配置 1234567hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 2000 10.2服务熔断 ​ 在服务熔断中，使用的熔断器，也叫断路器，其英文单词为：Circuit Breaker ​ 熔断机制与家里使用的电路熔断原理类似；当如果电路发生短路的时候能立刻熔断电路，避免发生灾难。在分布式系 统中应用服务熔断后；服务调用方可以自己进行判断哪些服务反应慢或存在大量超时，可以针对这些服务进行主动熔 断，防止整个系统被拖垮。 ​ Hystrix的服务熔断机制，可以实现弹性容错；当服务请求情况好转之后，可以自动重连。通过断路的方式，将后续 请求直接拒绝，一段时间（默认5秒）之后允许部分请求通过，如果调用成功则回到断路器关闭状态，否则继续打 开，拒绝请求的服务。 Hystrix的熔断状态机模型： 状态机有3个状态： Closed：关闭状态（断路器关闭），所有请求都正常访问。 Open：打开状态（断路器打开），所有请求都会被降级。Hystrix会对请求情况计数，当一定时间内失败请求百 分比达到阈值，则触发熔断，断路器会完全打开。默认失败比例的阈值是50%，请求次数最少不低于20次。 Half Open：半开状态，不是永久的，断路器打开后会进入休眠时间（默认是5S）。随后断路器会自动进入半开 状态。此时会释放部分请求通过，若这些请求都是健康的，则会关闭断路器，否则继续保持打开，再次进行休 眠计时。 11.服务消费者（Feign） 11.1简介 ​ Feign是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用Feign 注解和JAX-RS注解。Feign支持可插拔的编码器和解码器。Feign默认集成了Ribbon，并和Eureka结合，默认实现了负载均衡的效果。 简而言之： Feign 采用的是基于接口的注解 Feign 整合了ribbon，具有负载均衡的能力 整合了Hystrix，具有熔断的能力 12.断路器（Hystrix） ​ 在微服务架构中，根据业务来拆分成一个个的服务，服务与服务之间可以相互调用（RPC），在Spring Cloud可以用RestTemplate+Ribbon和Feign来调用。为了保证其高可用，单个服务通常会集群部署。由于网络原因或者自身的原因，服务并不能保证100%可用，如果单个服务出现问题，调用这个服务就会出现线程阻塞，此时若有大量的请求涌入，Servlet容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的“雪崩”效应。 为了解决这个问题，业界提出了断路器模型。 12.1断路器简介 ​ Netflix has created a library called Hystrix that implements the circuit breaker pattern. In a microservice architecture it is common to have multiple layers of service calls.Netflix. 开源了Hystrix组件，实现了断路器模式，SpringCloud对这一组件进行了整合。 在微服务架构中，一个请求需要调用多个服务是非常常见的。较底层的服务如果出现故障，会导致连锁故障。当对特定的服务的调用的不可用达到一个阀值（Hystric 是5秒20次） 断路器将会被打开。断路打开后，可用避免连锁故障，fallback方法可以直接返回一个固定值。 13.路由网关(zuul) ​ Zuul的主要功能是路由转发和过滤器。路由功能是微服务的一部分，比如／api/user转发到到user服务，/api/shop转发到到shop服务。zuul默认和Ribbon结合实现了负载均衡的功能。 zuul有以下功能： Authentication Insights Stress Testing Canary Testing Dynamic Routing Service Migration Load Shedding Security Static Response handling Active/Active traffic management 服务过滤 filterType：返回一个字符串代表过滤器的类型，在zuul中定义了四种不同生命周期的过滤器类型，具体如下： pre：路由之前 routing：路由之时 post： 路由之后 error：发送错误调用 filterOrder：过滤的顺序 shouldFilter：这里可以写逻辑判断，是否要过滤，本文true,永远过滤。 run：过滤器的具体逻辑。可用很复杂，包括查sql，nosql去判断该请求到底有没有权限访问。 14.分布式配置中心(Spring Cloud Config) ​ 在分布式系统中，由于服务数量巨多，为了方便服务配置文件统一管理，实时更新，所以需要分布式配置中心组件。在Spring Cloud中，有分布式配置中心组件spring cloud config ，它支持配置服务放在配置服务的内存中（即本地），也支持放在远程Git仓库中。在spring cloud config 组件中，分两个角色，一是config server，二是config client。 15. 高可用的分布式配置中心(Spring Cloud Config) ​ 当服务实例很多时，都从配置中心读取文件，这时可以考虑将配置中心做成一个微服务，将其集群化，从而达到高可用 16.消息总线(Spring Cloud Bus) ​ Spring Cloud Bus 将分布式的节点用轻量的消息代理连接起来。它可以用于广播配置文件的更改或者服务之间的通讯，也可以用于监控。本文要讲述的是用Spring Cloud Bus实现通知微服务架构的配置文件的更改。【修改配置文件，无需重启】 准备：安装rabbitMq 17.服务链路追踪(Spring Cloud Sleuth) ​ 微服务架构上通过业务来划分服务的，通过REST调用，对外暴露的一个接口，可能需要很多个服务协同才能完成这个接口功能，如果链路上任何一个服务出现问题或者网络超时，都会形成导致接口调用失败。随着业务的不断扩张，服务之间互相调用会越来越复杂。 术语 Span：基本工作单元，例如，在一个新建的span中发送一个RPC等同于发送一个回应请求给RPC，span通过一个64位ID唯一标识，trace以另一个64位ID表示，span还有其他数据信息，比如摘要、时间戳事件、关键值注释(tags)、span的ID、以及进度ID(通常是IP地址) span在不断的启动和停止，同时记录了时间信息，当你创建了一个span，你必须在未来的某个时刻停止它。 Trace：一系列spans组成的一个树状结构，例如，如果你正在跑一个分布式大数据工程，你可能需要创建一个trace。 Annotation：用来及时记录一个事件的存在，一些核心annotations用来定义一个请求的开始和结束 cs - Client Sent -客户端发起一个请求，这个annotion描述了这个span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳便可得到网络延迟 ss - Server Sent -注解表明请求处理的完成(当请求返回客户端)，如果ss减去sr时间戳便可得到服务端需要的处理请求时间 cr - Client Received -表明span的结束，客户端成功接收到服务端的回复，如果cr减去cs时间戳便可得到客户端从服务端获取回复的所有所需时间 将Span和Trace在一个系统中使用Zipkin注解的过程图形化： 18.高可用的服务注册中心 ​ 服务注册中心Eureka Server，是一个实例，当成千上万个服务向它注册的时候，它的负载是非常高的，这在生产环境上是不太合适的，这篇文章主要介绍怎么将Eureka Server集群化。Eureka通过运行多个实例，使其更具有高可用性。事实上，这是它默认的熟性，你需要做的就是给对等的实例一个合法的关联serviceurl。 参考文献：黑马课程笔记，方志朋的博客","link":"/2021/02/25/Draft/2021/SpringCloud/"},{"title":"SpringBoot","text":"SpringBoot 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】：尚硅谷学习笔记、springboot官方文档、视频、项目里面做笔记，最后进行综合优化 LEVEL【不是每个都学精】： 路上随便看看 进度：【32】路上看 快查 一、简介 二、注解 @ImportResource(&quot;classpath:beans.xml&quot;) @Configuration @EnableAspectJAutoProxy(exposeProxy = true) @Import({User.class}) @EnableConfigurationProperties(StudentProperties.class) @Bean @ConditionalOnBean(name = &quot;user02&quot;) @EnableSwagger2Doc @SpringBootApplication @Component @ConfigurationProperties(prefix = &quot;student&quot;) 配置类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.lxl.springboot.config;import cn.lxl.springboot.entity.StudentProperties;import cn.lxl.springboot.entity.User;import org.springframework.boot.autoconfigure.condition.ConditionalOnBean;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.boot.context.properties.EnableConfigurationProperties;import org.springframework.context.annotation.*;/*测试均在主程序类*///将配置文件中的bean注入容器@ImportResource(&quot;classpath:beans.xml&quot;)//告诉springboot这是个配置类// proxybeanMethods：代理bean的方法// Full模式：@Configuration(proxybeanMethods=true) 全模式 记录上次用的bean，检查组件是否在容器中有，单实例// Lite模式：@Configuration(proxybeanMethods=false) 轻量级模式 每次加载新的bean，跳过检查@Configuration// 表示通过aop框架暴露该代理对象,AopContext能够访问@EnableAspectJAutoProxy(exposeProxy = true)//给容器中自动创建出对应类型的组件、默认组件名为其全类名、使用其无参构造函数//@Import({User.class})//当不能在原类中添加@component注解时（比如用他人包时），则在使用的地方添加次注解,作用是开启student配置绑定功能且将其注入到容器中//@EnableConfigurationProperties(StudentProperties.class)public class MyConfig {//给容器添加组件，一方法名作为组件的id。返回类型就是组件类型。返回的值就是组建在容器中的实例 @Bean public User user01() { return new User(111111111L, &quot;1&quot;, 1); } //条件装配，有name为user01 的bean才注入注解下的student的bean，反之有 @ConditionalOnMissingBean @ConditionalOnBean(name = &quot;user02&quot;) @Bean public StudentProperties student() { return new StudentProperties(); } @ConditionalOnBean(name = &quot;user01&quot;) @Bean public StudentProperties student1() { return new StudentProperties(); }} 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package cn.lxl.springboot;import cn.lxl.springboot.config.MyConfig;import cn.lxl.springboot.entity.User;import com.spring4all.swagger.EnableSwagger2Doc;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;import org.springframework.context.annotation.ImportResource;/*Spring Boot 项目通常有一个名为 *Application 的入口类，入口类里有一个 main 方法， 这个 main 方法其实就是一个标准的 Javay 应用的入口方法。*///@SpringBootApplication===@Configuration、@EnableAutoConfiguration、@ComponentScan//@EnableAutoConfiguration 让 Spring Boot 根据类路径中的 jar 包依赖为当前项目进行自动配置，// 例如，添加了 spring-boot-starter-web 依赖，会自动添加 Tomcat 和 Spring MVC 的依赖，那么 Spring Boot 会对 Tomcat 和 Spring MVC 进行自动配置。//spring Boot 还会自动扫描 @SpringBootApplication 所在类的同级包以及下级包里的 Bean ，所以入口类建议就配置在 grounpID + arctifactID 组合的包名下（这里为 cn.wmyskxz.springboot 包）//@SpringBootApplication(scanBasePackages = &quot;cn.wmyskxz&quot;) 如需扫描主程序同级外的包则可添加scanBasePackages参数@EnableSwagger2Doc@SpringBootApplicationpublic class SpringbootApplication { public static void main(String[] args) {// SpringApplication.run(SpringbootApplication.class, args);// 返回IOC容器 ConfigurableApplicationContext run = SpringApplication.run(SpringbootApplication.class, args);// 查看所有组件// String[] beanDefinitionNames = run.getBeanDefinitionNames();// for (String name : beanDefinitionNames// ) {// System.out.println(name);// } //配置类本身也是组件// 以类型获取组件 MyConfig bean = run.getBean(MyConfig.class); System.out.println(bean); User user = run.getBean(User.class); System.out.println(user);// 以名称判断是否有组件 boolean user01 = run.containsBean(&quot;user01&quot;); System.out.println(user01);//条件注解测试 boolean student = run.containsBean(&quot;student&quot;); System.out.println(student);//@ImportResource(&quot;classpath:beans.xml&quot;)将配置文件中的bean注入容器 boolean bean1 = run.containsBean(&quot;bean1&quot;); System.out.println(bean1);//配置绑定,需要@Component//@ConfigurationProperties(prefix = &quot;student&quot;)//表示获取所有配置文件中前缀为 sutdent 的配置信息，实体类中使用 //当不能在原类中添加@component注解时（比如用他人包时），则在使用的地方添加此注解,作用是开启student配置绑定功能且将其注入到容器中//@EnableConfigurationProperties(StudentProperties.class) Object student1 = run.getBean(&quot;student1&quot;); System.out.println(student1); }}//运⾏ SpringBoot 有哪⼏种⽅式？//打包⽤命令或者放到容器中运⾏。 ⽤ Maven/Gradle 插件运⾏。//直接执⾏ main ⽅法运⾏。//SpringBoot ⽀持 Java Util Logging，Log4j2，Logback 作为⽇志框架，如果你使⽤ Starters 启动器，SpringBoot 将使⽤ Logback 作为默认框架。//⽤ Maven/Gradle 插件运⾏。可运行thtmeleaf与jsp同时存在的jsp， 1234567891011@Component//表明当前类是一个 Java Bean//配置绑定,需要@Component,或者使用的地方使用//当不能在原类中添加@component注解时（比如用他人包时），则在使用的地方添加此注解,作用是开启student配置绑定功能且将其注入到容器中//@EnableConfigurationProperties(StudentProperties.class)@ConfigurationProperties(prefix = &quot;student&quot;)//表示获取所有配置文件中前缀为 sutdent 的配置信息//student:// name: 真的帅// age: 21public class StudentProperties { private String name; private Integer age; 三、springboot自动装配原理 123@EnableSwagger2Doc@SpringBootApplicationpublic class SpringbootApplication {} @SpringBootApplication注解结构 ⏬⏬⏬⏬⏬⏬⏬⏬⏬⏬⏬ 12345678910111213141516@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration //表明当前为配置类@EnableAutoConfiguration@ComponentScan(//扫描哪些包 excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class}), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class})})public @interface SpringBootApplication {} @SpringBootConfiguration 12345@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Configuration //表明当前为配置类public @interface SpringBootConfiguration {} @EnableAutoConfiguration 1234567@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage //1@Import({AutoConfigurationImportSelector.class}) //2public @interface EnableAutoConfiguration {} //1 @AutoConfigurationPackage 123456@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@Import({Registrar.class})//利用Registrar导入一系列组件到容器中,指定默认包规则public @interface AutoConfigurationPackage {} Registrar.class 12345678910111213static class Registrar implements ImportBeanDefinitionRegistrar, DeterminableImports { Registrar() { } public void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { AutoConfigurationPackages.register(registry, (String[])( new AutoConfigurationPackages.PackageImports(metadata)).getPackageNames().toArray(new String[0]));//获取元注解的包名（com.xxx.xxx）封装到数组中 } public Set&lt;Object&gt; determineImports(AnnotationMetadata metadata) { return Collections.singleton(new AutoConfigurationPackages.PackageImports(metadata)); }} //2 @Import({AutoConfigurationImportSelector.class}) 1、利用getAutoConfigurationEntry(annotationMetadata);给容器中批量导入一些组件 2、调用List configurations = getCandidateConfigurations(annotationMetadata, attributes)获取到所有需要导入到容器中的配置类 3、利用工厂加载 Map&lt;String, List&gt; loadSpringFactories(@Nullable ClassLoader classLoader)；得到所有的组件 4、从META-INF/spring.factories位置来加载一个文件。 默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件 spring-boot-autoconfigure-2.3.4.RELEASE.jar包里面也有META-INF/spring.factories 5、文件里面写死了spring-boot一启动就要给容器中加载的所有配置类 6、虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration 按照条件装配规则（@Conditional），最终会按需配置 12345678910111213141516171819202122public class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered { private static final AutoConfigurationImportSelector.AutoConfigurationEntry EMPTY_ENTRY = new AutoConfigurationImportSelector.AutoConfigurationEntry(); private static final String[] NO_IMPORTS = new String[0]; private static final Log logger = LogFactory.getLog(AutoConfigurationImportSelector.class); private static final String PROPERTY_NAME_AUTOCONFIGURE_EXCLUDE = &quot;spring.autoconfigure.exclude&quot;; private ConfigurableListableBeanFactory beanFactory; private Environment environment; private ClassLoader beanClassLoader; private ResourceLoader resourceLoader; private AutoConfigurationImportSelector.ConfigurationClassFilter configurationClassFilter; public AutoConfigurationImportSelector() { } public String[] selectImports(AnnotationMetadata annotationMetadata) { if (!this.isEnabled(annotationMetadata)) { return NO_IMPORTS; } else { AutoConfigurationImportSelector.AutoConfigurationEntry autoConfigurationEntry = this.getAutoConfigurationEntry(annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());// } } 修改默认配置 @Bean @ConditionalOnBean(MultipartResolver.class) //容器中有这个类型组件 @ConditionalOnMissingBean(name = DispatcherServlet.MULTIPART_RESOLVER_BEAN_NAME) //容器中没有这个名字 multipartResolver 的组件 public MultipartResolver multipartResolver(MultipartResolver resolver) { //给@Bean标注的方法传入了对象参数，这个参数的值就会从容器中找。 //SpringMVC multipartResolver。防止有些用户配置的文件上传解析器不符合规范 // Detect if the user has created a MultipartResolver but named it incorrectly return resolver; } 给容器中加入了文件上传解析器； SpringBoot默认会在底层配好所有的组件。但是如果用户自己配置了以用户的优先 123@Bean @ConditionalOnMissingBean public CharacterEncodingFilter characterEncodingFilter() {} 总结： SpringBoot先加载所有的自动配置类 xxxxxAutoConfiguration 每个自动配置类按照条件进行生效，默认都会绑定配置文件指定的值。xxxxProperties里面拿。xxxProperties和配置文件进行了绑定 生效的配置类就会给容器中装配很多组件 只要容器中有这些组件，相当于这些功能就有了 定制化配置 用户直接自己@Bean替换底层的组件 用户去看这个组件是获取的配置文件什么值就去修改。 xxxxxAutoConfiguration ---&gt; 组件 ---&gt; xxxxProperties里面拿值 ----&gt; application.properties 最佳实践 引入场景依赖 https://docs.spring.io/spring-boot/docs/current/reference/html/using-spring-boot.html#using-boot-starter 查看自动配置了哪些（选做） 自己分析，引入场景对应的自动配置一般都生效了 配置文件中debug=true开启自动配置报告。console中显示Negative（不生效）\\Positive（生效） 是否需要修改 参照文档修改配置项 https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html#common-application-properties 自己分析。xxxxProperties绑定了配置文件的哪些。 自定义加入或者替换组件 @Bean、@Component。。。 自定义器 XXXXXCustomizer； 四、开发技巧 1.lombok @Data ：代替get set 方法 @ToString ：代替tostring方法 @AllArgsConstructor ：代替全参构造器 @NoArgsConstructor ：代替无参构造器 @EqualsAndHashCode：代替HashCode、equals方法 @Slf4j ：添加 Log.info(“xxx”);打印日志 2.dev-tools ctrl+f9 自动重启 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; 3.Spring Initailizr（项目初始化向导） 自动搭建初始项目环境 五、核心功能 1.配置文件 1.1、properties 同以前的properties用法 1.2、yaml 1.2.1、简介 YAML 是 &quot;YAML Ain't Markup Language&quot;（YAML 不是一种标记语言）的递归缩写。在开发的这种语言时，YAML 的意思其实是：&quot;Yet Another Markup Language&quot;（仍是一种标记语言）。 非常适合用来做以数据为中心的配置文件。如果properties和ymal都有，则都生效且properties优先。 1.2.2、基本语法 key: value；kv之间有空格 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格 缩进的空格数不重要，只要相同层级的元素左对齐即可 '#'表示注释 字符串无需加引号，如果要加，' ' 与 &quot; &quot; 表示字符串内容 会被 转义/不转义，比如 \\n 单引号回转义成 \\n 字符串，而双引号会换行。 1.2.3、数据类型 1234567891011121314151617#字面量：单个的、不可再分的值。date、boolean、string、number、nullk: v#对象：键值对的集合。map、hash、set、object 行内写法： k: {k1:v1,k2:v2,k3:v3}#或k: k1: v1 k2: v2 k3: v3#数组：一组按次序排列的值。array、list、queue行内写法： k: [v1,v2,v3]#或者k: - v1 - v2 - v3 1.2.4、示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Datapublic class Person { private String userName; private Boolean boss; private Date birth; private Integer age; private Pet pet; private String[] interests; private List&lt;String&gt; animal; private Map&lt;String, Object&gt; score; private Set&lt;Double&gt; salarys; private Map&lt;String, List&lt;Pet&gt;&gt; allPets;}@Datapublic class Pet { private String name; private Double weight;}# yaml表示以上对象person: userName: zhangsan boss: false birth: 2019/12/12 20:12:33 age: 18 pet: name: tomcat weight: 23.4 interests: [篮球,游泳] animal: - jerry - mario score: english: first: 30 second: 40 third: 50 math: [131,140,148] chinese: {first: 128,second: 136} salarys: [3999,4999.98,5999.99] allPets: sick: - {name: tom} - {name: jerry,weight: 47} health: [{name: mario,weight: 47}] 1.3配置文件自定义类提示 12345678910111213141516171819202122232425&lt;!--让自定义类在配置类中有提示--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;!--打包时去除业务无关依赖processor--&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;excludes&gt; &lt;exclude&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2.Web开发 静态资源与定制化 2.1、静态资源访问 1、静态资源目录 只要静态资源放在类路径下： called /static (or /public or /resources or /META-INF/resources 访问 ： 当前项目根路径/ + 静态资源名 原理： 静态映射/**。 请求进来，先去找Controller看能不能处理。不能处理的所有请求又都交给静态资源处理器。静态资源也找不到则响应404页面 改变默认的静态资源路径 1234567891011spring: mvc: static-path-pattern: /res/** resources: static-locations: [classpath:/haha/]spring: mvc: static-path-pattern: /res/** resources: static-locations: [classpath:/haha/] 2、静态资源访问前缀 默认无前缀 当前项目 + static-path-pattern + 静态资源名 = 静态资源文件夹下找 123spring: mvc: static-path-pattern: /res/** 3、webjar 自动映射 /webjars/** https://www.webjars.org/ 访问地址：http://localhost:8080/webjars/jquery/3.5.1/jquery.js 后面地址要按照依赖里面的包路径 12345&lt;dependency&gt; &lt;groupId&gt;org.webjars&lt;/groupId&gt; &lt;artifactId&gt;jquery&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt; 2.2、欢迎页支持 静态资源路径下 index.html 可以配置静态资源路径 但是不可以配置静态资源的访问前缀。否则导致 index.html不能被默认访问 controller能处理/index 12345spring:# mvc:# static-path-pattern: /res/** 这个会导致welcome page功能失效 resources: static-locations: [classpath:/haha/] 2.3、自定义 Favicon favicon.ico 放在静态资源目录下即可。 123spring:# mvc:# static-path-pattern: /res/** 这个会导致 Favicon 功能失效 不生效则首页添加 1234&lt;!-- &lt;link rel=&quot;shortcut icon&quot; th:href=&quot;@{/favicon.ico}&quot;/&gt;--&gt;&lt;!-- &lt;link rel=&quot;bookmark&quot; th:href=&quot;@{/favicon.ico}&quot;/&gt;--&gt; &lt;link rel=&quot;shortcut icon&quot; href=&quot;/favicon.ico&quot;/&gt; &lt;link rel=&quot;bookmark&quot; href=&quot;/favicon.ico&quot;/&gt; 停用缓存，谷歌ctrl+F5强制刷新 2.4、静态资源配置原理 SpringBoot启动默认加载 xxxAutoConfiguration 类（自动配置类） SpringMVC功能的自动配置类 WebMvcAutoConfiguration，生效 12345678@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class })@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)@AutoConfigureAfter({ DispatcherServletAutoConfiguration.class, TaskExecutionAutoConfiguration.class, ValidationAutoConfiguration.class })public class WebMvcAutoConfiguration {} 给容器中配了什么。 12345@Configuration(proxyBeanMethods = false)@Import(EnableWebMvcConfiguration.class)@EnableConfigurationProperties({ WebMvcProperties.class, ResourceProperties.class })@Order(0)public static class WebMvcAutoConfigurationAdapter implements WebMvcConfigurer {} 配置文件的相关属性和xxx进行了绑定。WebMvcProperties==spring.mvc、ResourceProperties==spring.resources 1、配置类只有一个有参构造器 123456789101112131415161718192021 //有参构造器所有参数的值都会从容器中确定//ResourceProperties resourceProperties；获取和spring.resources绑定的所有的值的对象//WebMvcProperties mvcProperties 获取和spring.mvc绑定的所有的值的对象//ListableBeanFactory beanFactory Spring的beanFactory//HttpMessageConverters 找到所有的HttpMessageConverters//ResourceHandlerRegistrationCustomizer 找到 资源处理器的自定义器。=========//DispatcherServletPath //ServletRegistrationBean 给应用注册Servlet、Filter.... public WebMvcAutoConfigurationAdapter(ResourceProperties resourceProperties, WebMvcProperties mvcProperties, ListableBeanFactory beanFactory, ObjectProvider&lt;HttpMessageConverters&gt; messageConvertersProvider, ObjectProvider&lt;ResourceHandlerRegistrationCustomizer&gt; resourceHandlerRegistrationCustomizerProvider, ObjectProvider&lt;DispatcherServletPath&gt; dispatcherServletPath, ObjectProvider&lt;ServletRegistrationBean&lt;?&gt;&gt; servletRegistrations) { this.resourceProperties = resourceProperties; this.mvcProperties = mvcProperties; this.beanFactory = beanFactory; this.messageConvertersProvider = messageConvertersProvider; this.resourceHandlerRegistrationCustomizer = resourceHandlerRegistrationCustomizerProvider.getIfAvailable(); this.dispatcherServletPath = dispatcherServletPath; this.servletRegistrations = servletRegistrations; } 2、资源处理的默认规则 12345678910111213141516171819202122232425262728293031323334353637383940@Override public void addResourceHandlers(ResourceHandlerRegistry registry) { if (!this.resourceProperties.isAddMappings()) { logger.debug(&quot;Default resource handling disabled&quot;); return; } Duration cachePeriod = this.resourceProperties.getCache().getPeriod(); CacheControl cacheControl = this.resourceProperties.getCache().getCachecontrol().toHttpCacheControl(); //webjars的规则 if (!registry.hasMappingForPattern(&quot;/webjars/**&quot;)) { customizeResourceHandlerRegistration(registry.addResourceHandler(&quot;/webjars/**&quot;) .addResourceLocations(&quot;classpath:/META-INF/resources/webjars/&quot;) .setCachePeriod(getSeconds(cachePeriod)).setCacheControl(cacheControl)); } // String staticPathPattern = this.mvcProperties.getStaticPathPattern(); if (!registry.hasMappingForPattern(staticPathPattern)) { customizeResourceHandlerRegistration(registry.addResourceHandler(staticPathPattern) .addResourceLocations(getResourceLocations(this.resourceProperties.getStaticLocations())) .setCachePeriod(getSeconds(cachePeriod)).setCacheControl(cacheControl)); } }spring:# mvc:# static-path-pattern: /res/** resources: add-mappings: false 禁用所有静态资源规则@ConfigurationProperties(prefix = &quot;spring.resources&quot;, ignoreUnknownFields = false)public class ResourceProperties { private static final String[] CLASSPATH_RESOURCE_LOCATIONS = { &quot;classpath:/META-INF/resources/&quot;, &quot;classpath:/resources/&quot;, &quot;classpath:/static/&quot;, &quot;classpath:/public/&quot; }; /** * Locations of static resources. Defaults to classpath:[/META-INF/resources/, * /resources/, /static/, /public/]. */ private String[] staticLocations = CLASSPATH_RESOURCE_LOCATIONS; 3、欢迎页的处理规则 1234567891011121314151617181920212223242526HandlerMapping：处理器映射。保存了每一个Handler能处理哪些请求。 @Bean public WelcomePageHandlerMapping welcomePageHandlerMapping(ApplicationContext applicationContext, FormattingConversionService mvcConversionService, ResourceUrlProvider mvcResourceUrlProvider) { WelcomePageHandlerMapping welcomePageHandlerMapping = new WelcomePageHandlerMapping( new TemplateAvailabilityProviders(applicationContext), applicationContext, getWelcomePage(), this.mvcProperties.getStaticPathPattern()); welcomePageHandlerMapping.setInterceptors(getInterceptors(mvcConversionService, mvcResourceUrlProvider)); welcomePageHandlerMapping.setCorsConfigurations(getCorsConfigurations()); return welcomePageHandlerMapping; }WelcomePageHandlerMapping(TemplateAvailabilityProviders templateAvailabilityProviders, ApplicationContext applicationContext, Optional&lt;Resource&gt; welcomePage, String staticPathPattern) { if (welcomePage.isPresent() &amp;&amp; &quot;/**&quot;.equals(staticPathPattern)) { //要用欢迎页功能，必须是/** logger.info(&quot;Adding welcome page: &quot; + welcomePage.get()); setRootViewName(&quot;forward:index.html&quot;); } else if (welcomeTemplateExists(templateAvailabilityProviders, applicationContext)) { // 调用Controller /index logger.info(&quot;Adding welcome page template: index&quot;); setRootViewName(&quot;index&quot;); }} 4、favicon 3、请求参数处理 1、请求映射 2、rest使用与原理 @xxxMapping； Rest风格支持（使用HTTP请求方式动词来表示对资源的操作） 以前：/getUser 获取用户 /deleteUser 删除用户 /editUser 修改用户 /saveUser 保存用户 现在： /user GET-获取用户 DELETE-删除用户 PUT-修改用户 POST-保存用户 核心Filter；HiddenHttpMethodFilter 用法： 表单method=post，隐藏域 _method=put SpringBoot中手动开启 扩展：如何把_method 这个名字换成我们自己喜欢的。 123456789101112131415161718192021222324252627282930313233343536373839 @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.GET) public String getUser(){ return &quot;GET-张三&quot;; } @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.POST) public String saveUser(){ return &quot;POST-张三&quot;; } @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.PUT) public String putUser(){ return &quot;PUT-张三&quot;; } @RequestMapping(value = &quot;/user&quot;,method = RequestMethod.DELETE) public String deleteUser(){ return &quot;DELETE-张三&quot;; } @Bean @ConditionalOnMissingBean(HiddenHttpMethodFilter.class) @ConditionalOnProperty(prefix = &quot;spring.mvc.hiddenmethod.filter&quot;, name = &quot;enabled&quot;, matchIfMissing = false) public OrderedHiddenHttpMethodFilter hiddenHttpMethodFilter() { return new OrderedHiddenHttpMethodFilter(); }//自定义filter @Bean public HiddenHttpMethodFilter hiddenHttpMethodFilter(){ HiddenHttpMethodFilter methodFilter = new HiddenHttpMethodFilter(); methodFilter.setMethodParam(&quot;_m&quot;); return methodFilter; } Rest原理（表单提交要使用REST的时候） 表单提交会带上_method=PUT 请求过来被HiddenHttpMethodFilter拦截 请求是否正常，并且是POST 获取到_method的值。 兼容以下请求；PUT.DELETE.PATCH 原生request（post），包装模式requesWrapper重写了getMethod方法，返回的是传入的值。 过滤器链放行的时候用wrapper。以后的方法调用getMethod是调用requesWrapper的。 Rest使用客户端工具， 如PostMan直接发送Put、delete等方式请求，无需Filter。 12345spring: mvc: hiddenmethod: filter: enabled: true #开启页面表单的Rest功能 2、请求映射原理 SpringMVC功能分析都从 org.springframework.web.servlet.DispatcherServlet-》doDispatch（） 1234567891011121314151617181920protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false;​ WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request);​ try { ModelAndView mv = null; Exception dispatchException = null;​ try { processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request);​ // 找到当前请求使用哪个Handler（Controller的方法）处理 mappedHandler = getHandler(processedRequest); //HandlerMapping：处理器映射。/xxx-&gt;&gt;xxxx RequestMappingHandlerMapping：保存了所有@RequestMapping 和handler的映射规则。 所有的请求映射都在HandlerMapping中。 SpringBoot自动配置欢迎页的 WelcomePageHandlerMapping 。访问 /能访问到index.html； SpringBoot自动配置了默认 的 RequestMappingHandlerMapping 请求进来，挨个尝试所有的HandlerMapping看是否有请求信息。 如果有就找到这个请求对应的handler 如果没有就是下一个 HandlerMapping 我们需要一些自定义的映射处理，我们也可以自己给容器中放HandlerMapping。自定义 HandlerMapping 12345678910111213141516171819202122protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { if (this.handlerMappings != null) { for (HandlerMapping mapping : this.handlerMappings) { HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) { return handler; } } } return null;} protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception { if (this.handlerMappings != null) { for (HandlerMapping mapping : this.handlerMappings) { HandlerExecutionChain handler = mapping.getHandler(request); if (handler != null) { return handler; } } } return null; } 3、普通参数与基本注解 1.1、注解： @PathVariable（路径）、@RequestHeader（header）、@ModelAttribute（）、@RequestParam（请求参数）、@MatrixVariable（矩阵变量 用；隔开）、@CookieValue（cookie）、@RequestBody（请求体）、@RequestAttribute（请求域） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147@RestControllerpublic class ParameterTestController { // car/2/owner/zhangsan @GetMapping(&quot;/car/{id}/owner/{username}&quot;) public Map&lt;String,Object&gt; getCar(@PathVariable(&quot;id&quot;) Integer id, @PathVariable(&quot;username&quot;) String name, @PathVariable Map&lt;String,String&gt; pv, @RequestHeader(&quot;User-Agent&quot;) String userAgent, @RequestHeader Map&lt;String,String&gt; header, @RequestParam(&quot;age&quot;) Integer age, @RequestParam(&quot;inters&quot;) List&lt;String&gt; inters, @RequestParam Map&lt;String,String&gt; params, @CookieValue(&quot;_ga&quot;) String _ga, @CookieValue(&quot;_ga&quot;) Cookie cookie){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;();// map.put(&quot;id&quot;,id);// map.put(&quot;name&quot;,name);// map.put(&quot;pv&quot;,pv);// map.put(&quot;userAgent&quot;,userAgent);// map.put(&quot;headers&quot;,header); map.put(&quot;age&quot;,age); map.put(&quot;inters&quot;,inters); map.put(&quot;params&quot;,params); map.put(&quot;_ga&quot;,_ga); System.out.println(cookie.getName()+&quot;===&gt;&quot;+cookie.getValue()); return map; } @PostMapping(&quot;/save&quot;) public Map postMethod(@RequestBody String content){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;content&quot;,content); return map; } //1、语法： 请求路径：/cars/sell;low=34;brand=byd,audi,yd //2、SpringBoot默认是禁用了矩阵变量的功能 // 手动开启：原理。对于路径的处理。UrlPathHelper进行解析。 // removeSemicolonContent（移除分号内容）支持矩阵变量的 //3、矩阵变量必须有url路径变量才能被解析 @GetMapping(&quot;/cars/{path}&quot;) public Map carsSell(@MatrixVariable(&quot;low&quot;) Integer low, @MatrixVariable(&quot;brand&quot;) List&lt;String&gt; brand, @PathVariable(&quot;path&quot;) String path){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;low&quot;,low); map.put(&quot;brand&quot;,brand); map.put(&quot;path&quot;,path); return map; } // /boss/1;age=20/2;age=10 @GetMapping(&quot;/boss/{bossId}/{empId}&quot;) public Map boss(@MatrixVariable(value = &quot;age&quot;,pathVar = &quot;bossId&quot;) Integer bossAge, @MatrixVariable(value = &quot;age&quot;,pathVar = &quot;empId&quot;) Integer empAge){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;bossAge&quot;,bossAge); map.put(&quot;empAge&quot;,empAge); return map; }}@RestControllerpublic class ParameterTestController { // car/2/owner/zhangsan @GetMapping(&quot;/car/{id}/owner/{username}&quot;) public Map&lt;String,Object&gt; getCar(@PathVariable(&quot;id&quot;) Integer id, @PathVariable(&quot;username&quot;) String name, @PathVariable Map&lt;String,String&gt; pv, @RequestHeader(&quot;User-Agent&quot;) String userAgent, @RequestHeader Map&lt;String,String&gt; header, @RequestParam(&quot;age&quot;) Integer age, @RequestParam(&quot;inters&quot;) List&lt;String&gt; inters, @RequestParam Map&lt;String,String&gt; params, @CookieValue(&quot;_ga&quot;) String _ga, @CookieValue(&quot;_ga&quot;) Cookie cookie){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;();// map.put(&quot;id&quot;,id);// map.put(&quot;name&quot;,name);// map.put(&quot;pv&quot;,pv);// map.put(&quot;userAgent&quot;,userAgent);// map.put(&quot;headers&quot;,header); map.put(&quot;age&quot;,age); map.put(&quot;inters&quot;,inters); map.put(&quot;params&quot;,params); map.put(&quot;_ga&quot;,_ga); System.out.println(cookie.getName()+&quot;===&gt;&quot;+cookie.getValue()); return map; } @PostMapping(&quot;/save&quot;) public Map postMethod(@RequestBody String content){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;content&quot;,content); return map; } //1、语法： 请求路径：/cars/sell;low=34;brand=byd,audi,yd //2、SpringBoot默认是禁用了矩阵变量的功能 // 手动开启：原理。对于路径的处理。UrlPathHelper进行解析。 // removeSemicolonContent（移除分号内容）支持矩阵变量的 //3、矩阵变量必须有url路径变量才能被解析 @GetMapping(&quot;/cars/{path}&quot;) public Map carsSell(@MatrixVariable(&quot;low&quot;) Integer low, @MatrixVariable(&quot;brand&quot;) List&lt;String&gt; brand, @PathVariable(&quot;path&quot;) String path){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;low&quot;,low); map.put(&quot;brand&quot;,brand); map.put(&quot;path&quot;,path); return map; } // /boss/1;age=20/2;age=10 @GetMapping(&quot;/boss/{bossId}/{empId}&quot;) public Map boss(@MatrixVariable(value = &quot;age&quot;,pathVar = &quot;bossId&quot;) Integer bossAge, @MatrixVariable(value = &quot;age&quot;,pathVar = &quot;empId&quot;) Integer empAge){ Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;bossAge&quot;,bossAge); map.put(&quot;empAge&quot;,empAge); return map; }}","link":"/2022/03/05/Draft/2021/SpringBoot/"},{"title":"信息系统项目管理师","text":"教材准备:视频一套，历年真题 工具：平板做真题，查阅记录陌生概念，电脑Markdown记下重要笔记， 项目示例：追肖小姐的工程 准备：江山老师（视频），十大管理（打印），历年真题答案（） 技巧：多口诀，&quot;不择手段记忆&quot;，论文字要练，错不涂，十大管理要滚瓜烂熟，速看视频，适当笔记记忆，做题对应细化，论文提前准备 考试计划： 时间段 8.17-11.06（82）周末两天【专业知识，软考】 工作日【中午一个半小时专业知识，下班7-9两个小时以上软考】 阶段 详细步骤 完成情况 用时 8.17-9.15 完成所有视频并笔记，背诵基础知识【选择，案例，计算，论文】专项研究 17【2】18【1】19【】20【1】21【】22【】23【1】24【1】25【2】26【2】27【】28【1】29【】30【3】31【】1【2】2【2】3【2】4【】5【】6【3】7【2基础十大管理完成，案例两节，计算两节】8【3】9【3】案例完成，计算四节10、13【4计算完】 完成 9.15-9.30 16-18年6套真题精做并分类总结 （一） 做真题套题平板或打印，对应基础速览，重点Anki记忆，错题归结，论文课开始。闲时软件刷题。构建知识框架。16年之前20套的直接看解析，陌生记录 三天一套，知识点Anki一遍并添加，狠补基础 16【2005上】26【2005下】 9.30-10.15 19年2套真题精做并分类总结 （二） 同上，完成（一）未完成的 真题外，论文框架准备并背诵 完成 10.15-10.30 20、21年三套真题定时并分类总结 （一） 总结，所有错题排查，查漏补缺 五天周末一次真实模拟，论文写后总结，练练字 完成 10.30-11.05 论文框架梳理，模拟练字速度 考前准备（工具，考场，是否留宿） 加油！ 倒计时：3 考试介绍，学习方法，重难点，10大知识域概述 考试介绍 通过考试相当于拿到高级职称，难度上午大于论文大于案例。 作用 方法 联想记忆，知识术语哪个板块，板块下那些过程，输入输出工具技术，计算相关，陌生词汇 重视基础（上午），考哪里学哪里，重视真题，费曼学习法，艾宾浩斯记忆，利用碎片时间，考上决心，2/8法则，PDCA循环，人机料法环，无事不项目，错题，知识点ANKI记录记忆 狗（沟通）子（质量）整（整体）范（范围）进（进度），成（成本）人（人力资源，干系人）风（风险）采（采购） 佂帆进城志，力够风采干。 掌管质检边界，鬼激钉娃缺恐，鬼火牌子痴恐，鬼故狱恐，鬼食恐，鬼祖见冠，鬼棺孔，鬼时醒两嘤孔，鬼识孔姐，识鬼棺孔 整 章 管 指 监变 结 范 规集定W 确控 进 规活排资持制 控 成 规估预 控 质 规 实 控 力 规 组建管 沟 规 管 控 风 规识性量应 控 采 规实 控 ···结 人 识 规 管 控 所有管理的综合性管理 考点题型，先做题再对应看书细化 重视周末大段时间 三从 从过程想结果，从结果知输入，从输入选工具 四得 一得文件计划；二得成果数据；三得变更请求；四的因素资产 共性总结 1、上一个过程的输出大部分是下一个过程的输入 2、计划和文件是不一样的（每个输入都有计划和文件） 3、被批准的变更请求约等于计划 4、在执行和监控过程产生新的变更请求（变更请求包括变什么和怎么变，这是变更请求和纠正、预防、缺陷修复的关系） 5、执行过程产生工作绩效数据---数据+背景在监控过程组成为了工作绩效信息然后输出工作绩效报告注意工作绩效数据是执行过程的输出→那么就是监控的输入→监控的输出就成了工作绩效信息 6、通过过程的含义记忆每个过程组最主要的成果（输出）！ 7、监控过程组每个过程都有计划+工作绩效报告要记住我说的监控的那些原理 跟踪进展→拿着计划的这把尺子测量实际的工作绩效报告→偏差计算→偏差评估、分析是否变更→预测→趋势分析 8、项目管理计划和其它子计划（范围管理计划、进度管理计划、质量管理计划等）区别和联系下面以项目管理计划和其中一个子计划为例子总结规律如下：了解后对其它过程帮助甚多 （1）当子计划或基准是主要的输入时→专门列出（如定义范围、收集需求等过程） （2）当子计划或基准是首次输出时→专门列出（如范围管理计划），以后的输出都以“项目管理计划更新”的形式出现 （3）子计划或基准是作为输出时候→项目管理计划将作为输入（如规划范围管理） （4）对控制过程组来说，输入和输出都是项目管理计划而不是具体的子计划 重点，十大管理 1.论文写作基本介绍 ​ 二选一，两小时，注意写字整洁。2500左右 摘要300 正文2000。 ​ 学好理论，写字速度整洁，多看范文，选定素材，别写具体项目名称。 2.信息化与信息系统【23】 网络 物数网传会表应 OSI/RM（Open System Interconnection/Reference Model，开放系统互连参考模型） 物理层：比特，串行传输。数据链路层：帧。网络层：分组，处理与寻址和传输有关的管理问题，提供点对点的连接。传输层：报文，建立、维护和撤销传输连接（端对端的连接），并进行流量控制和差错控制。 测试，软件需求，生命周期，面向对象，中间件，新网络技术 UML，测试 重点 新一代信息技术（英语单词），物联网，大数据，云计算，移动互联网，智慧城市，互联网+（互联网+各种传统行业），智能制造2025，AR，VR，AI，区块链，特别联网，5G，华为鸿蒙麒麟，一带一路，两化融合 信息安全技术 3.信息系统项目管理基础 项目特点：（1）临时性：有明确的开始和结束时间。 （2）独特性：世上没有两个完全相同的项目。 （3）渐进明细性：前期只能粗略定义，然后逐渐明朗、完善和精确，这也就意味着变更不可避免，所以要控制变更。 **项目的组织方式：**职能型、项目型、矩阵型 4.立项管理 2 （一个项目从提出申请到批准立项）招投标，合同，采购 项目建议书（立项申请)主要内容：项目的必要性、项目的市场预测、产品方案或服务的市场预测、项目建设必需的条件等 计算: 利率 利率有单利和复利，单利息=本金×利率×期限，F=P×（1+i）n F：复利终值 P：本金 i：利率 N：利率获取时间的整数倍 净现值（Net Present Value，NPV） 净现值大于零则方案可行，且净现值越大，方案越优，投资效益越好。 NPV 的计算步骤如下： （1）根据项目的资本结构设定项目的折现率。 （2）计算每年项目现金流量的净值。 （3）根据设定的折现率计算每年的净现值。 （4）将净现值累加起来。 净现值率 净现值率（NPVR）=项目的净现值（NPV）/原始投资的现值合计 投资回收期 招标 招标（招标公告，招标文件，） 中标通知书发出之日起三十日内应当签订合同，截止时间15日前进行必要的澄清或者修改，招标人应当确定投标人编制投标文件所需要的合理时间。但是，依法必须进行招标的项目，自招标文件开始发出之日起至投标人提交投标文件截止之日止，最短不得少于20日。 投标 投标人少于三个的，招标人应当重新招标。在招标文件要求提交投标文件的截止时间后送达的投标文件，招标人应当拒收。 开标 开标应当在招标文件确定的提交投标文件截止时间的同一时间公开进行。开标地点应当为招标文件中预先确定的地点。开标由招标人主持，邀请所有投标人参加。 评标 评标委员会由招标人的代表和有关技术、经济等方面的专家组成，成员人数为5人以上单数，其中技术、经济等方面的专家不得少于成员总数的三分之二。 中标 中标通知书发出后，招标人改变中标结果的，或者中标人放弃中标项目的，应当依法承担法律责任。 合同 分类： 范围：项目总承包合同、项目单项承包合同、项目分包合同。 付款方式：项目总价合同、项目单价合同、项目成本加酬金合同 合同的主要内容包括：项目名称；标的内容和范围；项目的质量要求：通常情况下采用技术指标限定等各种方式来描述信息系统工程的整体质量标准以及各部分质量标准，它是判断整个工程项目成败的重要依据；项目的计划、进度、地点、地域和方式；项目建设过程中的各种期限；技术情报和资料的保密；风险责任的承担；技术成果的归属；验收的标准和方法；价款、报酬（或使用费）及其支付方式；违约金或者损失赔偿的计算方法；解决争议的方法：该条款中应尽可能地明确在出现争议与纠纷时采取何种方式来协商解决；名词术语解释等 8.整体管理 ITO ITO 名称及定义 输入 工具技术 输出 项目章程 正式批准一个项目的文档，制定项目的头（项目经理） 项目工作说明书商业论证协议事业环境因素组织过程资产 专家判断引导技术 项目章程 制定项目管理计划 包括定义、准备和协调所有构成计划，形成项目管理计划所必要的行动 项目章程其他过程输入【项目初步范围说明书，管理过程，预测，工作绩效信息】事业环境因素组织过程资产 专家判断引导技术 项目管理计划 指导与管理项目工作 提意见，记考核，做东西 项目管理计划，批准的变更请求，事业环境因素，组织过程资产 专家判断项目管理信息系统会议 可交付成果工作绩效数据变更请求项目管理计划更新项目文件更新 监控项目工作 从项目开始到结束，收集，测量发布绩效信息，及评估会影响过程改进的度量项和趋势 项目管理计划事业环境因素组织过程资产进度预测成本预测确认的变更工作绩效信息 专家判断分析技术项目管理信息系统【PMIS】会议 变更请求工作绩效报告项目管理计划更新项目文件更新 实施整体变更控制 审批变更，项目经理付最终责任 变更请求，工作绩效报告，变更请求，项目管理计划，事业环境因素，组织过程资产 专家判断会议变更控制工具 批准的变更请求变更日志项目管理计划更新项目文件更新 项目收尾 打包交货 项目管理计划验收的可交付成果组织过程资产 专家判断分析技术会议 最终产品、服务或成果移交组织过程资产更新 9.范围管理 ITO ITO 名称及定义 输入 工具技术 输出 10.进度管理 ITO ITO 名称及定义 输入 工具技术 输出 11.成本管理 ITO ITO 名称及定义 输入 工具技术 输出 挣值分析计算 12.质量管理 ITO ITO 名称及定义 输入 工具技术 输出 13.人力资源管理 ITO ITO 名称及定义 输入 工具技术 输出 14.干系人管理 ITO ITO 名称及定义 输入 工具技术 输出 15.沟通管理 ITO ITO 名称及定义 输入 工具技术 输出 16.风险管理 ITO ITO 名称及定义 输入 工具技术 输出 17.采购管理 ITO ITO 名称及定义 输入 工具技术 输出 PPT加星看了就可以了 18.合同管理 上午一分 合同的分类 19.信息文档和配置管理 上午2分 文档分类（开发，产品，管理） 上午一般三分 20.20-28章 知识管理 显性知识，隐性知识,隐性知识分享途经，知识产权保护，软件著作权 战略管理 组织战略因素组成，战略实施，类型层次，平衡计分卡 组织级项目管理 流程管理 项目集，项目组合管理 信息安全管理 综合测试管理、量化项目管理，成熟度模型 CMMI（能力成熟度模型）、 法律法规 合同法 1 合同内容，要约，标的，格式（非格式【手写】）条款，违心合同， 招标法 1 必须进行招标的项目、标底（必须保密） 著作权法 政府采购法 2 常用技术标准 2-3 软件工程国家标准GB/T11457-2006（审计-代码审记-配置审计-认证-走查-鉴定-基线-配置控制委员会-配置状态报告-设计评审-桌面检查-评价-故障-功能配置审计） 软件生存周期的过程，软件生命周期各阶段与软件文档编制工作的关系，各类人员与软件文档的使用关系，软件产品质量，6个质量特性21个质量子特性 案例分析 五种题型 问答题，计算题，分析题，理论题，填空，选择，判断题 分析题 回答简练，文字工整清晰，答题有序，多写不扣分，专业化 万金油： 技术出身：开发和管理所需技能不同，需要培训 身兼数职：导致没时间去学习管理知识，工作负荷过载，身心疲惫，全局影响 新技术：风险，需培训，学习，监控技术风险，找到合适的人，实在不行外包 有人对项目不满意：简历有效沟通机制方式发法，缺乏有效的项目绩效管理机制，需加强沟通 变更：书面申请，审批确认，跟踪变更缺一不可 客户验收不通过：说明验收标准没有得到认可确认，没有验收测试规范和方法 愚人有关问题：沟通不到位 过一段时间才发现问题：监控不力 里程碑，时间紧促：没有冗余考虑风险的想法 外部因素导致延工：没有考虑外在因素影响，变更5个理由 争执：沟通问题，计划不够周明 多头汇报：项目章程，多头汇报导致信息沟通不畅通或产生冲突 知识点 如何缩短活动的工期，成本控制，质量新老7工具，提升项目质量， 答题关键词 新人：培训 并行工作：有风险 客户要求都答应： 口头：书面 备忘录：配置管理 部分人：全员参与 开会很久：效率低下 当场修改，能改就改：变更监控分析记录 文档配置管理 人员离职：AB思想，风险 总价合同：乙方风险太大 前期需求调研不充分： 计算 进度类 EV、AC、PV需验算确保后面不错。 管理储备不计入挣值和基准，但是总预算一部分。 注意审题，时间和单位。 总时差 是指在不延误项目完成日期或违反进度因素的前提下，某活动可以推迟的时间。 总时差=LS-ES=LF-EF 自由时差 是指在不影响紧后活动最早开始的情况下，当前活动可以推迟的时间。 自由时差=(后一活动)ES-(前一活动的)EF 所以总时差影响总工期，自由时差影响紧后活动。 （1）总时差（TF）：当一项活动的最早开始时间和最迟开始时间不相同时，它们之间的差值是该工作的总时差。计算公式是：TF=LS-ES。 （2）自由时差（FF）：在不影响紧后活动完成时间的条件下，一项活动可能被延迟的时间是该项活动的自由时差，它由该项活动的最早完成时间EF和它的紧后活动的最早开始时间决定的。计算公式是：FF=min{紧后活动的ES}-EF。 （3）关键路径。项目的关键路径是指能够决定项目最早完成时间的一系列活动。它是网络图中的最长路径，具有最少的时差。在实际求关键路径时，一般的方法是看哪些活动的总时差为0，总时差为0的活动称为关键活动，关键活动组成的路径称为关键路径。 尽管关键路径是最长的路径，但它代表了完成项目所需的最短时间。因此，关键路径上各活动持续时间（历时）的和就是项目的计算工期。 最早开始时间（ES）： 一项活动的最早开始时间取决于它的所有紧前活动的完成时间。通过计算到该活动路径上所有活动的完成时间的和，可得到指定活动的ES。如果有多条路径指向此活动，则计算需要时间最长的那条路径，即ES=max{紧前活动的EF}。 最早结束时间（EF）： 一项活动的最早完成时间取决于该工作的最早开始时间和它的持续时间（D），即EF=ES+D。 最晚结束时间（LF）： 在不影响项目完成时间的条件下，一项活动可能完成的最迟时间。计算公式是：LF=min{紧后活动的LS}。 最晚开始时间（LS）： 在不影响项目完成时间的条件下，一项活动可能开始的最晚时间。计算公式是：LS=LF-D。 前推法来计算最早时间 某一活动的最早开始时间（ES）=指向它的所有紧前活动的最早结束时间的最大值。 某一活动的最早结束时间（EF）=ES+T（作业时间） 逆推法来计算最迟时间 某一活动的最迟结束时间（LF）=指向它的所有紧后活动的最迟开始时间的最小值。 某一活动的最迟开始时间（LS）=LF-T（作业时间） 计算关键路径的步骤 **1. 用有方向的线段标出各结点的紧前活动和紧后活动的关系，使之成为一个有方向的网络图（PDM） \\2. 用正推和逆推法计算出各个活动的ES,LS, EF, LF，并计算出各个活动的自由时差。找出所有总时差为零或为负的活动，就是关键活动 \\3. 关键路径上的活动持续时间决定了项目的工期，总和就是项目工期。 **自由时差 ** 计算公式： 自由时差=所有紧后工作中最早开始时间最小值－ 最早结束时间 **总时差 ** 计算公式： 总时差=最迟开始时间-最早开始时间=最迟结束时间-最早结束时间 单代号网络图 双代号网络图 双代号时标网络图 全为实线的为关键路径， 成本类 挣值分析 PV [Planned Value]计划值：应该完成多少工作？ 要干的活 EV [Earned Value]挣值：完成了多少预算工作？ 干完的活 AC [Actual Cost]实际成本：完成工作的实际成本是多少？ 实际花费 BAC [Budget cost at completion] 基线预算成本：全部工作的预算是多少？不改变成本基准，BAC就不会发生变化 CV [Cost Variance] 成本偏差 CV＝EV－AC，CV&gt;0，成本节约，CV&lt;0，成本超支。 SV [Schedule Variance] 进度偏差 SV＝EV－PV，SV&gt;0，进度超前，SV&lt;0，进度落后。 CPI [Cost Performance Index] 成本执行指数CPI＝EV/AC，CPI＝1，资金使用效率一般；CPI&gt;1成本节约，资金使用效率高；CPI&lt;1，成本超支，资金使用效率低。 SPI [Schedule Performance Index] 进度执行指数 SPI＝EV/PV，SPI＝1，进度与计划相符，SPI&gt;1，进度超前，SPI&lt;1，进度落后。 ETC [Estimate (or Estimated) To Complete] 完工时尚需成本估算：到完成时，剩余工作量还需要多少成本,ETC也就是估计完成项目的剩余工作成本 BAC [Budget cost at completion] 完工预算：全部工作的预算是多少？不改变成本基准，BAC就不会发生变化 EAC [Estimate at completion] 完成预估：全部工作的成本是多少？是根据项目的绩效和风险量化对项目最可能的总成本所做的一种预测。 完工工期估算=预算工期/SPI 预测EAC 与 ETC： 由于存在成本偏差情况，所以在典型偏差与非典型偏差时，计算顺序不一样，如下: 典型偏差：未来项目的CPI、SPI会保持不变，此时预测项目完成时的总成本和预计完成时间，应该用典型偏差公式。 EAC= AC+(BAC-EV) =BAC/CPI ETC=EAC-AC 非典型偏差：非典型偏差的含义是项目未来的工作绩效与当前无关，和原计划保持一样，即项目未来的成本绩效指数和进度绩效指数都是“1”）需要纠偏 ETC=BAC-EV 基线总成本-已挣得部份 EAC=ETC+AC 上午小计算 盈亏平衡点 决策树计算 资源平衡问题 统计抽样问题 自制\\外购分析 沟通渠道数 [n(n-1)]/2 风险曝光度 现值 净现值，投资回收期 运筹学 计算量大 求最短和最长路径 线性规划问题 投资收益最大问题 更换设备问题 车床铣床问题 流量问题 人员分配问题【矩阵法（行列分别减一次最小值，根据行零个数从少到多分配）】 伏格尔方法解传输问题 论文 提前准备，摘要300内，正文不少于2000，字迹清晰，无需写题目 准备一个项目（投标书，方案书） 准备框架 自己写一个","link":"/2021/03/29/Draft/2021/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88/"},{"title":"魑魅先生 | 微信小程序","text":"微信小程序指南 常用函数 跳转 123wx.navigateTo({ url: '../component/L-component' }) 简介 前端，后端任意，上线需配置域名。 WeixinJSBridge对内---》 JS-SDK，前者包装，对内转对外---》 小程序，逻辑层和渲染层是分开的，逻辑层运行在 JSCore 中，并没有一个完整浏览器对象，因而缺少相关的DOM API和BOM API。导致无法运行常用前端库jQuery、 Zepto 等， JSCore 的环境同 NodeJS 环境也是不尽相同，所以一些 NPM 的包在小程序中也是无法运行的。 **初始过程：**申请小程序账号(管理小程序，获取AppID)，安装小程序开发工具、配置项目 结构 app.json 全局配置，包括了小程序的所有页面路径、界面表现、网络超时时间、底部 tab 1234567891011121314151617{ // 描述当前小程序所有页面路径，这是为了让微信客户端知道当前你的小程序页面定义在哪个目录。 &quot;pages&quot;:[ &quot;pages/index/index&quot;, &quot;pages/logs/logs&quot; ], // 定义小程序所有页面的顶部背景颜色，文字颜色定义等。 &quot;window&quot;:{ &quot;backgroundTextStyle&quot;:&quot;light&quot;, &quot;navigationBarBackgroundColor&quot;: &quot;#fff&quot;, &quot;navigationBarTitleText&quot;: &quot;Weixin&quot;, &quot;navigationBarTextStyle&quot;:&quot;black&quot; }, &quot;style&quot;: &quot;v2&quot;, &quot;sitemapLocation&quot;: &quot;sitemap.json&quot;} project.config.json 小程序开发者工具个性化配置，界面颜色、编译配置等等 page.json 表示 pages/logs 目录下的 logs.json 这类和小程序页面相关的配置。独立定义每个页面的一些属性 页面中配置项会覆盖 app.json 的 window 中相同的配置项 index.wxml WXML（WeiXin Markup Language）， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748数据绑定&lt;!--wxml--&gt;&lt;view&gt; {{message}} &lt;/view&gt;// page.jsPage({ data: { message: 'Hello MINA!' }})列表渲染&lt;!--wxml--&gt;&lt;view wx:for=&quot;{{array}}&quot;&gt; {{item}} &lt;/view&gt;// page.jsPage({ data: { array: [1, 2, 3, 4, 5] }})条件渲染&lt;!--wxml--&gt;&lt;view wx:if=&quot;{{view == 'WEBVIEW'}}&quot;&gt; WEBVIEW &lt;/view&gt;&lt;view wx:elif=&quot;{{view == 'APP'}}&quot;&gt; APP &lt;/view&gt;&lt;view wx:else=&quot;{{view == 'MINA'}}&quot;&gt; MINA &lt;/view&gt;// page.jsPage({ data: { view: 'MINA' }})模板&lt;!--wxml--&gt;&lt;template name=&quot;staffName&quot;&gt; &lt;view&gt; FirstName: {{firstName}}, LastName: {{lastName}} &lt;/view&gt;&lt;/template&gt;&lt;template is=&quot;staffName&quot; data=&quot;{{...staffA}}&quot;&gt;&lt;/template&gt;&lt;template is=&quot;staffName&quot; data=&quot;{{...staffB}}&quot;&gt;&lt;/template&gt;&lt;template is=&quot;staffName&quot; data=&quot;{{...staffC}}&quot;&gt;&lt;/template&gt;// page.jsPage({ data: { staffA: {firstName: 'Hulk', lastName: 'Hu'}, staffB: {firstName: 'Shang', lastName: 'You'}, staffC: {firstName: 'Gideon', lastName: 'Lin'} }}) index.wxss CSS,仅支持部分 CSS 选择器,新增了尺寸单位,提供了全局的样式[app.wxss]和局部样式 .js 逻辑层 除此之外，只有后缀名在白名单内的文件可以被上传，不在白名单列表内文件在开发工具能被访问到，但无法被上传。具体白名单列表如下： wxs png jpg jpeg gif svg json cer mp3 aac m4a mp4 wav ogg silk wasm br 基础 场景值： 描述用户进入小程序的路径，目前还无法获取到按 Home 键退出到桌面，然后从桌面再次进小程序的场景值，对于这种情况，会保留上一次的场景值。 获取方式：onLaunch 和 onShow，wx.getLaunchOptionsSync 场景值 场景 appId含义 1020 公众号 profile 页相关小程序列表 来源公众号 1035 公众号自定义菜单 来源公众号 1036 App 分享消息卡片 来源App 1037 小程序打开小程序 来源小程序 1038 从另一个小程序返回 来源小程序 1043 公众号模板消息 来源公众号 逻辑层App Service JavaScript 的基础上，增加一些功能，由于并非运行在浏览器中，window，document等一些web能力无法使用： 增加 App 和 Page 方法，进行程序注册和页面注册。 增加 getApp 和 getCurrentPages 方法，分别用来获取 App 实例和当前页面栈。 提供丰富的 API，如微信用户数据，扫一扫，支付等微信特有能力。 提供模块化能力，每个页面有独立的作用域。 注册小程序 app.js 中调用 App 方法注册小程序实例，绑定生命周期回调函数、错误监听和页面不存在监听函数等。 12345整个小程序只有一个 App 实例，是全部页面共享的。开发者可以通过 getApp 方法获取到全局唯一的 App 实例，获取App上的数据或调用开发者注册在 App 上的函数。// xxx.jsconst appInstance = getApp()console.log(appInstance.globalData) // I am global data 注册页面 Page() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253Page({ data: { text: &quot;This is page data.&quot; }, onLoad: function(options) { // 页面创建时执行 }, onShow: function() { // 页面出现在前台时执行 }, onReady: function() { // 页面首次渲染完毕时执行 }, onHide: function() { // 页面从前台变为后台时执行 }, onUnload: function() { // 页面销毁时执行 }, onPullDownRefresh: function() { // 触发下拉刷新时执行 }, onReachBottom: function() { // 页面触底时执行 }, onShareAppMessage: function () { // 页面被用户分享时执行 }, onPageScroll: function() { // 页面滚动时执行 }, onResize: function() { // 页面尺寸变化时执行 }, onTabItemTap(item) { // tab 点击时执行 console.log(item.index) console.log(item.pagePath) console.log(item.text) }, // 事件响应函数 viewTap: function() { this.setData({ text: 'Set some data for updating view.' }, function() { // this is setData callback }) }, // 自由数据 customData: { hi: 'MINA' }}) behaviors 2.9.2 开始支持,低版本需做兼容处理。 behaviors 可以用来让多个页面有相同的数据字段和方法。 12345678910111213141516171819// my-behavior.jsmodule.exports = Behavior({ data: { sharedText: 'This is a piece of data shared between pages.' }, methods: { sharedMethod: function() { this.data.sharedText === 'This is a piece of data shared between pages.' } }})// page-a.jsvar myBehavior = require('./my-behavior.js')Page({ behaviors: [myBehavior], onLoad: function() { this.data.sharedText === 'This is a piece of data shared between pages.' }}) Component () 基础库 1.6.3 开始支持，低版本需做兼容处理。方法需要放在 methods: { } 里面。类似自定义组件，适合复杂页面 1234567891011121314151617Component({ data: { text: &quot;This is page data.&quot; }, methods: { onLoad: function(options) { // 页面创建时执行 }, onPullDownRefresh: function() { // 下拉刷新时执行 }, // 事件响应函数 viewTap: function() { // ... } }}) 页面生命周期 页面路由 在小程序中所有页面的路由全部由框架进行管理。 页面栈 框架以栈的形式维护了当前的所有页面。 当发生路由切换的时候，页面栈的表现如下： 路由方式 页面栈表现 初始化 新页面入栈 打开新页面 新页面入栈 页面重定向 当前页面出栈，新页面入栈 页面返回 页面不断出栈，直到目标返回页 Tab 切换 页面全部出栈，只留下新的 Tab 页面 重加载 页面全部出栈，只留下新的页面 开发者可以使用 getCurrentPages() 函数获取当前页面栈。 路由方式 对于路由的触发方式以及页面生命周期函数如下： 路由方式 触发时机 路由前页面 路由后页面 初始化 小程序打开的第一个页面 onLoad, onShow 打开新页面 调用 API wx.navigateTo 使用组件 `` onHide onLoad, onShow 页面重定向 调用 API wx.redirectTo 使用组件 `` onUnload onLoad, onShow 页面返回 调用 API wx.navigateBack 使用组件`` 用户按左上角返回按钮 onUnload onShow Tab 切换 调用 API wx.switchTab 使用组件 `` 用户切换 Tab 各种情况请参考下表 重启动 调用 API wx.reLaunch 使用组件 `` onUnload onLoad, onShow Tab 切换对应的生命周期（以 A、B 页面为 Tabbar 页面，C 是从 A 页面打开的页面，D 页面是从 C 页面打开的页面为例）： 当前页面 路由后页面 触发的生命周期（按顺序） A A Nothing happend A B A.onHide(), B.onLoad(), B.onShow() A B（再次打开） A.onHide(), B.onShow() C A C.onUnload(), A.onShow() C B C.onUnload(), B.onLoad(), B.onShow() D B D.onUnload(), C.onUnload(), B.onLoad(), B.onShow() D（从转发进入） A D.onUnload(), A.onLoad(), A.onShow() D（从转发进入） B D.onUnload(), B.onLoad(), B.onShow() 注意事项 navigateTo, redirectTo 只能打开非 tabBar 页面。 switchTab 只能打开 tabBar 页面。 reLaunch 可以打开任意页面。 页面底部的 tabBar 由页面决定，即只要是定义为 tabBar 的页面，底部都有 tabBar。 调用页面路由带的参数可以在目标页面的onLoad中获取。 模块化 公共的代码抽离成为一个单独的 js 文件，作为一个模块。模块只有通过 module.exports 或者 exports【exports 是 module.exports 的一个引用，因此在模块里边随意更改 exports 的指向会造成未知的错误。】 才能对外暴露接口。不支持直接引入 node_modules , 开发者需要使用到 node_modules 时候建议拷贝出相关的代码到小程序的目录中，或者使用小程序支持的 npm 功能 123456789101112131415161718192021// common.jsfunction sayHello(name) { console.log(`Hello ${name} !`)}function sayGoodbye(name) { console.log(`Goodbye ${name} !`)}module.exports.sayHello = sayHelloexports.sayGoodbye = sayGoodbye​在需要使用这些模块的文件中，使用 require 将公共代码引入var common = require('common.js')Page({ helloMINA: function() { common.sayHello('MINA') }, goodbyeMINA: function() { common.sayGoodbye('MINA') }}) 文件作用域 在 JavaScript 文件中声明的变量和函数只在该文件中有效；不同的文件中可以声明相同名字的变量和函数，不会互相影响。 通过全局函数 getApp 可以获取全局的应用实例，如果需要全局的数据可以在 App() 中设置 1234567891011121314151617// app.jsApp({ globalData: 1})// a.js// The localValue can only be used in file a.js.var localValue = 'a'// Get the app instance.var app = getApp()// Get the global data and change it.app.globalData++// b.js// You can redefine localValue in file b.js, without interference with the localValue in a.js.var localValue = 'b'// If a.js it run before b.js, now the globalData shoule be 2.console.log(getApp().globalData) API 事件监听 API on 开头,监听某个事件是否触发,如：wx.onSocketOpen，wx.onCompassChange 等。 同步 API Sync 结尾,如 wx.setStorageSync，wx.getSystemInfoSync 等 异步 API 如 wx.request，wx.login 等 云开发 API 视图层 View WXML WXSS (WeiXin Style Sheets) @import语句可以导入外联样式表 内联样式 框架组件上支持使用 style、class 属性来控制组件的样式。 选择器 目前支持的选择器有： 选择器 样例 样例描述 .class .intro 选择所有拥有 class=&quot;intro&quot; 的组件 #id #firstname 选择拥有 id=&quot;firstname&quot; 的组件 element view 选择所有 view 组件 element, element view, checkbox 选择所有文档的 view 组件和所有的 checkbox 组件 ::after view::after 在 view 组件后边插入内容 ::before view::before 在 view 组件前边插入内容 WXS WXS（WeiXin Script）是小程序的一套脚本语言 WXS 不依赖于运行时的基础库版本，可以在所有版本的小程序中运行。 WXS 与 JavaScript 是不同的语言，有自己的语法，并不和 JavaScript 一致。 WXS 的运行环境和其他 JavaScript 代码是隔离的，WXS 中不能调用其他 JavaScript 文件中定义的函数，也不能调用小程序提供的API。 WXS 函数不能作为组件的事件回调。 由于运行环境的差异，在 iOS 设备上小程序内的 WXS 会比 JavaScript 代码快 2 ~ 20 倍。在 android 设备上二者运行效率无差异。 1234567&lt;wxs module=&quot;m1&quot;&gt;var msg = &quot;hello world&quot;;module.exports.message = msg;&lt;/wxs&gt;&lt;view&gt; {{m1.message}} &lt;/view&gt; 事件系统 简意双向绑定 只能是一个单一字段的绑定 12&lt;input model:value=&quot;值为 {{value}}&quot; /&gt;&lt;input model:value=&quot;{{ a + b }}&quot; /&gt; 都是非法的； 目前，尚不能 data 路径 1&lt;input model:value=&quot;{{ a.b }}&quot; /&gt; 组件中传递双向绑定并触发更新 12345678910111213Component({ properties: { myValue: String }, methods: { update: function() { // 更新 myValue this.setData({ myValue: 'leaf' }) }})&lt;input model:value=&quot;{{myValue}}&quot; /&gt; 基础组件 获取界面上的节点信息 WXML节点信息：获取节点属性、样式、在界面上的位置等信息 WXML节点布局相交状态： 参照节点：监听的参照节点，取它的布局区域作为参照区域。如果有多个参照节点，则会取它们布局区域的 交集 作为参照区域。页面显示区域也可作为参照区域之一。 目标节点：监听的目标，默认只能是一个节点（使用 selectAll 选项时，可以同时监听多个节点）。 相交区域：目标节点的布局区域与参照区域的相交区域。 相交比例：相交区域占参照区域的比例。 阈值：相交比例如果达到阈值，则会触发监听器的回调函数。阈值可以有多个。 1234567891011121314Page({ onLoad: function(){ wx.createIntersectionObserver().relativeToViewport().observe('.target-class', (res) =&gt; { res.id // 目标节点 id res.dataset // 目标节点 dataset res.intersectionRatio // 相交区域占目标节点的布局区域的比例 res.intersectionRect // 相交区域 res.intersectionRect.left // 相交区域的左边界坐标 res.intersectionRect.top // 相交区域的上边界坐标 res.intersectionRect.width // 相交区域的宽度 res.intersectionRect.height // 相交区域的高度 }) }}) 响应显示区域变化 在手机上启用屏幕旋转支持 1app.json` 的 `window` 段中设置 `&quot;pageOrientation&quot;: &quot;auto&quot;` ，或在页面 json 文件中配置 `&quot;pageOrientation&quot;: &quot;auto&quot;, 2.5.0 开始， pageOrientation 还可以被设置为 landscape ，表示固定为横屏显示 在 iPad 上启用屏幕旋转支持 1app.json` 中添加 `&quot;resizable&quot;: true，不能单独配置某个页面是否支持屏幕旋转 Media Query 对于不同尺寸的显示区域，页面的布局会有所差异。此时可以使用 media query 来解决大多数问题。 代码示例： 12345678910.my-class { width: 40px;}@media (min-width: 480px) { /* 仅在 480px 或更宽的屏幕上生效的样式规则 */ .my-class { width: 200px; }} 在 WXML 中，可以使用 match-media 组件来根据 media query 匹配状态展示、隐藏节点。 此外，可以在页面或者自定义组件 JS 中使用 this.createMediaQueryObserver() 方法来创建一个 MediaQueryObserver 对象，用于监听指定的 media query 的匹配状态。 分栏模式 启用 3.3版本以上在pc等大屏幕上支持分栏模式， app.json 中同时添加 &quot;resizable&quot;: true 和 &quot;frameset&quot;: true 分栏占位图片 当某一栏没有展示任何页面时，会展示一张图片在此栏正中央。 如果代码包中的 frameset/placeholder.png 文件存在，这张图片将作为此时展示的图片。 动画 初始渲染缓存 自定义组件 介绍 创建自定义组件 类似于页面，一个自定义组件由 json wxml wxss js 4个文件组成。要编写一个自定义组件，首先需要在组件的 json 文件中进行自定义组件声明： 123{ &quot;component&quot;: true} 同时，还要在 wxml 文件中编写组件模板，在 wxss 文件中加入组件样式，它们的写法与页面的写法类似。 123456789&lt;!-- 这是自定义组件的内部WXML结构 --&gt;&lt;view class=&quot;inner&quot;&gt; {{innerText}}&lt;/view&gt;&lt;slot&gt;&lt;/slot&gt;/* 这里的样式只应用于这个自定义组件 */.inner { color: red;} 组件wxss中不应使用ID选择器、属性选择器和标签名选择器。 在自定义组件的 js 文件中，需要使用 Component() 来注册组件，并提供组件的属性定义、内部数据和自定义方法。 组件的属性值和内部数据将被用于组件 wxml 的渲染，其中，属性值是可由组件外部传入的。 1234567891011121314151617Component({ properties: { // 这里定义了innerText属性，属性值可以在组件使用时指定 innerText: { type: String, value: 'default value', } }, data: { // 这里是一些组件内部数据 someData: {} }, methods: { // 这里是一个自定义方法 customMethod: function(){} }}) 使用自定义组件 使用已注册的自定义组件前，首先要在用组件的页面的 json 文件中进行引用声明。此时需要提供每个自定义组件的标签名和对应的自定义组件文件路径： 12345{ &quot;usingComponents&quot;: { &quot;component-tag-name&quot;: &quot;path/to/the/custom/component&quot;//同目录下直接写文件名 }} 这样，在页面的 wxml 中就可以像使用基础组件一样使用自定义组件。节点名即自定义组件的标签名，节点属性即传递给组件的属性值。 1234&lt;view&gt; &lt;!-- 以下是对一个自定义组件的引用 --&gt; &lt;component-tag-name inner-text=&quot;Some text&quot;&gt;&lt;/component-tag-name&gt;&lt;/view&gt; 组件模板和样式 组件模板 组件模板的写法与页面模板相同。组件模板与组件数据结合后生成的节点树，将被插入到组件的引用位置上。 在组件模板中可以提供一个 &lt;slot&gt; 节点，用于承载组件引用时提供的子节点。 代码示例： 在开发者工具中预览效果 123456789101112&lt;!-- 组件模板 --&gt;&lt;view class=&quot;wrapper&quot;&gt; &lt;view&gt;这里是组件的内部节点&lt;/view&gt; &lt;slot&gt;&lt;/slot&gt;&lt;/view&gt;&lt;!-- 引用组件的页面模板 --&gt;&lt;view&gt; &lt;component-tag-name&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot&gt; 的位置上 --&gt; &lt;view&gt;这里是插入到组件slot中的内容&lt;/view&gt; &lt;/component-tag-name&gt;&lt;/view&gt; 注意，在模板中引用到的自定义组件及其对应的节点名需要在 json 文件中显式定义，否则会被当作一个无意义的节点。除此以外，节点名也可以被声明为抽象节点 模板数据绑定 与普通的 WXML 模板类似，可以使用数据绑定，这样就可以向子组件的属性传递动态数据。 代码示例： 在开发者工具中预览效果 1234567&lt;!-- 引用组件的页面模板 --&gt;&lt;view&gt; &lt;component-tag-name prop-a=&quot;{{dataFieldA}}&quot; prop-b=&quot;{{dataFieldB}}&quot;&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot&gt; 的位置上 --&gt; &lt;view&gt;这里是插入到组件slot中的内容&lt;/view&gt; &lt;/component-tag-name&gt;&lt;/view&gt; 在以上例子中，组件的属性 propA 和 propB 将收到页面传递的数据。页面可以通过 setData 来改变绑定的数据字段。 注意：这样的数据绑定只能传递 JSON 兼容数据。自基础库版本 2.0.9 开始，还可以在数据中包含函数（但这些函数不能在 WXML 中直接调用，只能传递给子组件）。 组件 wxml 的 slot 在组件的 wxml 中可以包含 slot 节点，用于承载组件使用者提供的 wxml 结构。 默认情况下，一个组件的 wxml 中只能有一个 slot 。需要使用多 slot 时，可以在组件 js 中声明启用。 1234567Component({ options: { multipleSlots: true // 在组件定义时的选项中启用多slot支持 }, properties: { /* ... */ }, methods: { /* ... */ }}) 此时，可以在这个组件的 wxml 中使用多个 slot ，以不同的 name 来区分。 123456&lt;!-- 组件模板 --&gt;&lt;view class=&quot;wrapper&quot;&gt; &lt;slot name=&quot;before&quot;&gt;&lt;/slot&gt; &lt;view&gt;这里是组件的内部细节&lt;/view&gt; &lt;slot name=&quot;after&quot;&gt;&lt;/slot&gt;&lt;/view&gt; 使用时，用 slot 属性来将节点插入到不同的 slot 上。 123456789&lt;!-- 引用组件的页面模板 --&gt;&lt;view&gt; &lt;component-tag-name&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot name=&quot;before&quot;&gt; 的位置上 --&gt; &lt;view slot=&quot;before&quot;&gt;这里是插入到组件slot name=&quot;before&quot;中的内容&lt;/view&gt; &lt;!-- 这部分内容将被放置在组件 &lt;slot name=&quot;after&quot;&gt; 的位置上 --&gt; &lt;view slot=&quot;after&quot;&gt;这里是插入到组件slot name=&quot;after&quot;中的内容&lt;/view&gt; &lt;/component-tag-name&gt;&lt;/view&gt; 组件样式 组件对应 wxss 文件的样式，只对组件wxml内的节点生效。编写组件样式时，需要注意以下几点： 组件和引用组件的页面不能使用id选择器（#a）、属性选择器（[a]）和标签名选择器，请改用class选择器。 组件和引用组件的页面中使用后代选择器（.a .b）在一些极端情况下会有非预期的表现，如遇，请避免使用。 子元素选择器（.a&gt;.b）只能用于 view 组件与其子节点之间，用于其他组件可能导致非预期的情况。 继承样式，如 font 、 color ，会从组件外继承到组件内。 除继承样式外， app.wxss 中的样式、组件所在页面的的样式对自定义组件无效（除非更改组件样式隔离选项）。 1234#a { } /* 在组件中不能使用 */[a] { } /* 在组件中不能使用 */button { } /* 在组件中不能使用 */.a &gt; .b { } /* 除非 .a 是 view 组件节点，否则不一定会生效 */ 除此以外，组件可以指定它所在节点的默认样式，使用 :host 选择器（需要包含基础库 1.7.2 或更高版本的开发者工具支持）。 代码示例： 123456/* 组件 custom-component.wxss */:host { color: yellow;}&lt;!-- 页面的 WXML --&gt;&lt;custom-component&gt;这段文本是黄色的&lt;/custom-component&gt; 组件样式隔离 默认情况下，自定义组件的样式只受到自定义组件 wxss 的影响。除非以下两种情况： app.wxss 或页面的 wxss 中使用了标签名选择器（或一些其他特殊选择器）来直接指定样式，这些选择器会影响到页面和全部组件。通常情况下这是不推荐的做法。 指定特殊的样式隔离选项 styleIsolation 。 12345Component({ options: { styleIsolation: 'isolated' }}) 在开发者工具中预览效果 styleIsolation 选项从基础库版本 2.6.5 开始支持。它支持以下取值： isolated 表示启用样式隔离，在自定义组件内外，使用 class 指定的样式将不会相互影响（一般情况下的默认值）； apply-shared 表示页面 wxss 样式将影响到自定义组件，但自定义组件 wxss 中指定的样式不会影响页面； shared 表示页面 wxss 样式将影响到自定义组件，自定义组件 wxss 中指定的样式也会影响页面和其他设置了 apply-shared 或 shared 的自定义组件。（这个选项在插件中不可用。） 使用后两者时，请务必注意组件间样式的相互影响。 如果这个 Component 构造器用于构造页面 ，则默认值为 shared ，且还有以下几个额外的样式隔离选项可用： page-isolated 表示在这个页面禁用 app.wxss ，同时，页面的 wxss 不会影响到其他自定义组件； page-apply-shared 表示在这个页面禁用 app.wxss ，同时，页面 wxss 样式不会影响到其他自定义组件，但设为 shared 的自定义组件会影响到页面； page-shared 表示在这个页面禁用 app.wxss ，同时，页面 wxss 样式会影响到其他设为 apply-shared 或 shared 的自定义组件，也会受到设为 shared 的自定义组件的影响。 从小程序基础库版本 2.10.1 开始，也可以在页面或自定义组件的 json 文件中配置 styleIsolation （这样就不需在 js 文件的 options 中再配置）。例如： 123{ &quot;styleIsolation&quot;: &quot;isolated&quot;} 此外，小程序基础库版本 2.2.3 以上支持 addGlobalClass 选项，即在 Component 的 options 中设置 addGlobalClass: true 。 这个选项等价于设置 styleIsolation: apply-shared ，但设置了 styleIsolation 选项后这个选项会失效。 代码示例： 在开发者工具中预览效果 123456789101112/* 组件 custom-component.js */Component({ options: { addGlobalClass: true, }})&lt;!-- 组件 custom-component.wxml --&gt;&lt;text class=&quot;red-text&quot;&gt;这段文本的颜色由 `app.wxss` 和页面 `wxss` 中的样式定义来决定&lt;/text&gt;/* app.wxss */.red-text { color: red;} 外部样式类 基础库 1.9.90 开始支持，低版本需做兼容处理。 有时，组件希望接受外部传入的样式类。此时可以在 Component 中用 externalClasses 定义段定义若干个外部样式类。 这个特性可以用于实现类似于 view 组件的 hover-class 属性：页面可以提供一个样式类，赋予 view 的 hover-class ，这个样式类本身写在页面中而非 view 组件的实现中。 注意：在同一个节点上使用普通样式类和外部样式类时，两个类的优先级是未定义的，因此最好避免这种情况。 代码示例： 123456/* 组件 custom-component.js */Component({ externalClasses: ['my-class']})&lt;!-- 组件 custom-component.wxml --&gt;&lt;custom-component class=&quot;my-class&quot;&gt;这段文本的颜色由组件外的 class 决定&lt;/custom-component&gt; 这样，组件的使用者可以指定这个样式类对应的 class ，就像使用普通属性一样。在 2.7.1 之后，可以指定多个对应的 class 。 代码示例： 在开发者工具中预览效果 1234567891011&lt;!-- 页面的 WXML --&gt;&lt;custom-component my-class=&quot;red-text&quot; /&gt;&lt;custom-component my-class=&quot;large-text&quot; /&gt;&lt;!-- 以下写法需要基础库版本 2.7.1 以上 --&gt;&lt;custom-component my-class=&quot;red-text large-text&quot; /&gt;.red-text { color: red;}.large-text { font-size: 1.5em;} 引用页面或父组件的样式 基础库 2.9.2 开始支持，低版本需做兼容处理。 即使启用了样式隔离 isolated ，组件仍然可以在局部引用组件所在页面的样式或父组件的样式。 例如，如果在页面 wxss 中定义了： 123.blue-text { color: blue;} 在这个组件中可以使用 ~ 来引用这个类的样式： 1&lt;view class=&quot;~blue-text&quot;&gt; 这段文本是蓝色的 &lt;/view&gt; 如果在一个组件的父组件 wxss 中定义了： 123.red-text { color: red;} 在这个组件中可以使用 ^ 来引用这个类的样式： 1&lt;view class=&quot;^red-text&quot;&gt; 这段文本是红色的 &lt;/view&gt; 也可以连续使用多个 ^ 来引用祖先组件中的样式。 注意：如果组件是比较独立、通用的组件，请优先使用外部样式类的方式，而非直接引用父组件或页面的样式。 虚拟化组件节点 基础库 2.11.2 开始支持，低版本需做兼容处理。 默认情况下，自定义组件本身的那个节点是一个“普通”的节点，使用时可以在这个节点上设置 class style 、动画、 flex 布局等，就如同普通的 view 组件节点一样。 12345&lt;!-- 页面的 WXML --&gt;&lt;view style=&quot;display: flex&quot;&gt; &lt;!-- 默认情况下，这是一个普通的节点 --&gt; &lt;custom-component style=&quot;color: blue; flex: 1&quot;&gt;蓝色、满宽的&lt;/custom-component&gt;&lt;/view&gt; 但有些时候，自定义组件并不希望这个节点本身可以设置样式、响应 flex 布局等，而是希望自定义组件内部的第一层节点能够响应 flex 布局或者样式由自定义组件本身完全决定。 这种情况下，可以将这个自定义组件设置为“虚拟的”： 1234567891011Component({ options: { virtualHost: true }, properties: { style: { // 定义 style 属性可以拿到 style 属性上设置的值 type: String, } }, externalClasses: ['class'], // 可以将 class 设为 externalClasses}) 这样，可以将 flex 放入自定义组件内： 12345678910&lt;!-- 页面的 WXML --&gt;&lt;view style=&quot;display: flex&quot;&gt; &lt;!-- 如果设置了 virtualHost ，节点上的样式将失效 --&gt; &lt;custom-component style=&quot;color: blue&quot;&gt;不是蓝色的&lt;/custom-component&gt;&lt;/view&gt;&lt;!-- custom-component.wxml --&gt;&lt;view style=&quot;flex: 1&quot;&gt; 满宽的 &lt;slot&gt;&lt;/slot&gt;&lt;/view&gt; 需要注意的是，自定义组件节点上的 class style 和动画将不再生效，但仍可以： 将 style 定义成 properties 属性来获取 style 上设置的值； 将 class 定义成 externalClasses 外部样式类使得自定义组件 wxml 可以使用 class 值。 Component 构造器 Component 构造器可用于定义组件，调用 Component 构造器时可以指定组件的属性、数据、方法等。 详细的参数含义和使用请参考 Component 参考文档。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051Component({ behaviors: [], properties: { myProperty: { // 属性名 type: String, value: '' }, myProperty2: String // 简化的定义方式 }, data: {}, // 私有数据，可用于模板渲染 lifetimes: { // 生命周期函数，可以为函数，或一个在methods段中定义的方法名 attached: function () { }, moved: function () { }, detached: function () { }, }, // 生命周期函数，可以为函数，或一个在methods段中定义的方法名 attached: function () { }, // 此处attached的声明会被lifetimes字段中的声明覆盖 ready: function() { }, pageLifetimes: { // 组件所在页面的生命周期函数 show: function () { }, hide: function () { }, resize: function () { }, }, methods: { onMyButtonTap: function(){ this.setData({ // 更新属性和数据的方法与更新页面数据的方法类似 }) }, // 内部方法建议以下划线开头 _myPrivateMethod: function(){ // 这里将 data.A[0].B 设为 'myPrivateData' this.setData({ 'A[0].B': 'myPrivateData' }) }, _propertyChange: function(newVal, oldVal) { } }}) 使用 Component 构造器构造页面 事实上，小程序的页面也可以视为自定义组件。因而，页面也可以使用 Component 构造器构造，拥有与普通组件一样的定义段与实例方法。但此时要求对应 json 文件中包含 usingComponents 定义段。 此时，组件的属性可以用于接收页面的参数，如访问页面 /pages/index/index?paramA=123&amp;paramB=xyz ，如果声明有属性 paramA 或 paramB ，则它们会被赋值为 123 或 xyz 。 页面的生命周期方法（即 on 开头的方法），应写在 methods 定义段中。 代码示例： 123456789101112131415161718{ &quot;usingComponents&quot;: {}}Component({ properties: { paramA: Number, paramB: String, }, methods: { onLoad: function() { this.data.paramA // 页面参数 paramA 的值 this.data.paramB // 页面参数 paramB 的值 } }}) 使用 Component 构造器来构造页面的一个好处是可以使用 behaviors 来提取所有页面中公用的代码段。 例如，在所有页面被创建和销毁时都要执行同一段代码，就可以把这段代码提取到 behaviors 中。 代码示例： 12345678910111213141516171819202122232425// page-common-behavior.jsmodule.exports = Behavior({ attached: function() { // 页面创建时执行 console.info('Page loaded!') }, detached: function() { // 页面销毁时执行 console.info('Page unloaded!') }})// 页面 Avar pageCommonBehavior = require('./page-common-behavior')Component({ behaviors: [pageCommonBehavior], data: { /* ... */ }, methods: { /* ... */ },})// 页面 Bvar pageCommonBehavior = require('./page-common-behavior')Component({ behaviors: [pageCommonBehavior], data: { /* ... */ }, methods: { /* ... */ },}) 组件间通信与事件 组件间通信 组件间的基本通信方式有以下几种。 WXML 数据绑定：用于父组件向子组件的指定属性设置数据，仅能设置 JSON 兼容数据（自基础库版本 2.0.9 开始，还可以在数据中包含函数）。具体在 组件模板和样式 章节中介绍。 事件：用于子组件向父组件传递数据，可以传递任意数据。 如果以上两种方式不足以满足需要，父组件还可以通过 this.selectComponent 方法获取子组件实例对象，这样就可以直接访问组件的任意数据和方法。 监听事件 事件系统是组件间通信的主要方式之一。自定义组件可以触发任意的事件，引用组件的页面可以监听这些事件。关于事件的基本概念和用法，参见 事件 。 监听自定义组件事件的方法与监听基础组件事件的方法完全一致： 代码示例： 123456789&lt;!-- 当自定义组件触发“myevent”事件时，调用“onMyEvent”方法 --&gt;&lt;component-tag-name bindmyevent=&quot;onMyEvent&quot; /&gt;&lt;!-- 或者可以写成 --&gt;&lt;component-tag-name bind:myevent=&quot;onMyEvent&quot; /&gt;Page({ onMyEvent: function(e){ e.detail // 自定义组件触发事件时提供的detail对象 }}) 触发事件 自定义组件触发事件时，需要使用 triggerEvent 方法，指定事件名、detail对象和事件选项： 代码示例： 在开发者工具中预览效果 123456789101112&lt;!-- 在自定义组件中 --&gt;&lt;button bindtap=&quot;onTap&quot;&gt;点击这个按钮将触发“myevent”事件&lt;/button&gt;Component({ properties: {}, methods: { onTap: function(){ var myEventDetail = {} // detail对象，提供给事件监听函数 var myEventOption = {} // 触发事件的选项 this.triggerEvent('myevent', myEventDetail, myEventOption) } }}) 触发事件的选项包括： 选项名 类型 是否必填 默认值 描述 bubbles Boolean 否 false 事件是否冒泡 composed Boolean 否 false 事件是否可以穿越组件边界，为false时，事件将只能在引用组件的节点树上触发，不进入其他任何组件内部 capturePhase Boolean 否 false 事件是否拥有捕获阶段 关于冒泡和捕获阶段的概念，请阅读 事件 章节中的相关说明。 代码示例： 在开发者工具中预览效果 12345678910111213141516171819202122// 页面 page.wxml&lt;another-component bindcustomevent=&quot;pageEventListener1&quot;&gt; &lt;my-component bindcustomevent=&quot;pageEventListener2&quot;&gt;&lt;/my-component&gt;&lt;/another-component&gt;// 组件 another-component.wxml&lt;view bindcustomevent=&quot;anotherEventListener&quot;&gt; &lt;slot /&gt;&lt;/view&gt;// 组件 my-component.wxml&lt;view bindcustomevent=&quot;myEventListener&quot;&gt; &lt;slot /&gt;&lt;/view&gt;// 组件 my-component.jsComponent({ methods: { onTap: function(){ this.triggerEvent('customevent', {}) // 只会触发 pageEventListener2 this.triggerEvent('customevent', {}, { bubbles: true }) // 会依次触发 pageEventListener2 、 pageEventListener1 this.triggerEvent('customevent', {}, { bubbles: true, composed: true }) // 会依次触发 pageEventListener2 、 anotherEventListener 、 pageEventListener1 } }}) 获取组件实例 可在父组件里调用 this.selectComponent ，获取子组件的实例对象。 调用时需要传入一个匹配选择器 selector，如：this.selectComponent(&quot;.my-component&quot;)。 selector 详细语法可查看 selector 语法参考文档。 代码示例： 在开发者工具中预览效果 12345678// 父组件Page({ data: {}, getChildComponent: function () { const child = this.selectComponent('.my-component'); console.log(child) }}) 在上例中，父组件将会获取 class 为 my-component 的子组件实例对象，即子组件的 this 。 注意 ：默认情况下，小程序与插件之间、不同插件之间的组件将无法通过 selectComponent 得到组件实例（将返回 null）。如果想让一个组件在上述条件下依然能被 selectComponent 返回，可以自定义其返回结果（见下）。 自定义的组件实例获取结果 若需要自定义 selectComponent 返回的数据，可使用内置 behavior: wx://component-export 从基础库版本 2.2.3 开始提供支持。 使用该 behavior 时，自定义组件中的 export 定义段将用于指定组件被 selectComponent 调用时的返回值。 代码示例： 在开发者工具中预览效果 1234567891011// 自定义组件 my-component 内部Component({ behaviors: ['wx://component-export'], export() { return { myField: 'myValue' } }})&lt;!-- 使用自定义组件时 --&gt;&lt;my-component id=&quot;the-id&quot; /&gt;// 父组件调用const child = this.selectComponent('#the-id') // 等于 { myField: 'myValue' } 在上例中，父组件获取 id 为 the-id 的子组件实例的时候，得到的是对象 { myField: 'myValue' } 。 开发经验 准备 需要后端接口使用要开通企业账号，且注册费用300元 有后台数据：服务器、域名、端口443、8080、80 开通并且备案。 443端口 业务域名需要，其他端口服务域名需要，开发时可关闭域名校验。 可通过 UniApp ，通过 VUE 方式简介发行小程序。 可通过VSCode打开HBuilder X 运行的项目比较方便 业务域名，服务器域名 注意分包问题，每个包不大于2M. 可以通过代理通过一个域名使用多个代理。 转载整理来源：https://developers.weixin.qq.com/miniprogram/dev/framework/app-service/route.html","link":"/2021/02/25/Draft/2021/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"title":"快查","text":"Linux,Shell,软件等常用命令快捷键 JAVA 1234567891011121314151617181920212223242526272829303132333435java -选项其中选项包括: -d32 使用 32 位数据模型 (如果可用) -d64 使用 64 位数据模型 (如果可用) -server 选择 &quot;server&quot; VM 默认 VM 是 server. -cp &lt;目录和 zip/jar 文件的类搜索路径&gt; -classpath &lt;目录和 zip/jar 文件的类搜索路径&gt; 用 ; 分隔的目录, JAR 档案 和 ZIP 档案列表, 用于搜索类文件。 -D&lt;名称&gt;=&lt;值&gt; 设置系统属性 -verbose:[class|gc|jni] 启用详细输出 -version 输出产品版本并退出 -showversion 输出产品版本并继续 -ea[:&lt;packagename&gt;...|:&lt;classname&gt;] -enableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 按指定的粒度启用断言 -da[:&lt;packagename&gt;...|:&lt;classname&gt;] -disableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] 禁用具有指定粒度的断言 -esa | -enablesystemassertions 启用系统断言 -dsa | -disablesystemassertions 禁用系统断言 -agentlib:&lt;libname&gt;[=&lt;选项&gt;] 加载本机代理库 &lt;libname&gt;, 例如 -agentlib:hprof 另请参阅 -agentlib:jdwp=help 和 -agentlib:hprof=help -agentpath:&lt;pathname&gt;[=&lt;选项&gt;] 按完整路径名加载本机代理库 -javaagent:&lt;jarpath&gt;[=&lt;选项&gt;] 加载 Java 编程语言代理, 请参阅 java.lang.instrument -splash:&lt;imagepath&gt; 使用指定的图像显示启动屏幕有关详细信息, 请参阅 http://www.oracle.com/technetwork/java/javase/documentation/index.html。 CMD cmd窗口点击内容会暂停，右键属性关闭快速编辑即可，但是关闭后无法复制cmd内容。 网络 netstat -aon|findstr &quot;59207&quot;查找对应端口pid tasklist|findstr &quot;1396&quot;查看对应pid程序 taskkill /t /f /pid &quot;8888&quot;关闭对应端口 mstsc 启动远程桌面 gpedit.msc策略组 msg /server:126.11.9.213 * 消息 给同局域网发送消息弹窗 nslookup 域名 查看域名IP telnet ip 端口 查看端口是否互通，需要控制面板打开 telnet功能 wmic memphysical get maxcapacity 查看主板支持最大内存 HEXO Hexo c 清理 Hexo g编译 Hexo s运行 Hexo d部署 hexo new draft（scaffolds中模板名字）“标题”新建草稿 在source/_drafts目录 hexo new “标题”新建文档 hexo publish “标题”草稿移动到source/_post目录 IDEA ctrl +R 替换 ctrl +shift + 数字标记文件内位置 ctrl+数字回到对应位置 双击shift全局搜索 ctrl + shift +y 搜索插件翻译选中文字 ctrl + shift +o codota搜索所选代码示例 不选中代码时显示翻译输入框 alt + F7 快速匹配项目中所有此用法 Ctrl+Alt+Shift+U UML图 Shift+Alt 选中文件汇总所有当前字符 Ctrl+Shift+E 可获取最近查看或更改的代码段的列表 WINDOWS alt + tab 两应用之间切换 LINUX yum install net-tools 安装网络工具 linux可直接复制给别人，复制VMware工作目录然后直接打开 su 回车输入密码转换为超级管理员 ifconfig查询网络相关 clear清空显示 ifconfig eth0 192.16.。。。设置IP地址 命令多数为临时生效，写入配置文件为永久生效 ls [-选项][参数]：ls -la /etc 目录处理命令（list） ls -a/l/lh/ld/i 隐藏文件/文件详细信息/大小单位显示/查看目录信息/查看文件id d l 文件开头 文件、目录、软连接 权限 mkdir【make directories】 创建目录 -p递归创建 创建没有此目录下的目录，可同时创建多个mkdir /tmp/a/b /tmp/a/c cd 【change directory】切换目录 cd ..返回上一级目录 pwd【print working directory】 cd ./.. 当前目录/上级目录 remdir【remove empty dirctories】 删除空目录 cp【copy】复制 源文件 目录 -r复制目录 -p复制并保留文件属性（比如创建时间） 复制并改名 mv【move】 剪切 rm【remove】删除文件，目录 -r删除目录 -f强制删除 -rf直接删除 ctrl+c终止操作 touch 创建文件 可创建多个，创建空格文件名使用双引号但是不建议 cat 浏览文件内容 -n显示行号 tac 倒置浏览文件内容 more 分页显示长文件 空格翻页，回车换行，q退出， less 显示文件内容，可向上翻页pageup，可搜索：/关键词 n显示下一个搜索结果 head 只看文件前几行 -n 7 文件 显示前7行，默认前十行 tail 同上 -f动态显示变化 ctrl+c退出 ln【link】 -s创建软连接 不写-s创建硬链接【软连接相当于快捷方式rwxrwxrwx，权限跟源文件权限无关，硬链接相当于拷贝cp -p并且同步更新，硬链接通过i节点区分，不能跨分区（相当于不能c盘复制到d盘），不能针对目录使用】 yum -y install git sudo -s1获得管理员权限 命令：sudo -s回车 输入密码 编辑保存文件----------------------------------- 第一步：cd到该文件的目录下 第二步：vi 要编辑的文件名，进入普通模式，（可以查看文件内容） sudo gedit /etc/apt/sources.list 可视化编辑 第三步：输入 i 进入编辑模式，开始编辑文本 第四步：编辑之后，按ESC退出到普通模式。 第五步：在普通模式下，输入 : 进入命令模式 第六步：在命令模式下输入wq, 即可保存并退出 【#】代表 root权限 【$】代表普通用户 reboot重启 BUG————————————————————————————– centos7 cannot find a valid baseurl for repo············· https://blog.csdn.net/jiankunking/article/details/82770502 zlib.h: No such file or directory························ yum install zlib-devel CentOS Name or service not known·································· vi /etc/sysconfig/network-scripts/ifcfg-ens33命令来编辑配置文件 Job for network.service failed because the control process exited with error code········································································ 关闭 NetworkManger 服务就好了， service NetworkManager stop 禁止开机启动 chkconfig NetworkManager off su: Authentication failure····················································· sudo passwd root 网络————————————————————————————— 网络不行都用桥接 1systemctl restart network //重启网卡 ====ifup eth0（网卡名称） ip addr查询网络信息 postgresql——————————————————————————- sudo -u postgres psql 进入psql交互环境 alter user postgres with password'密码'; 修改postgres用户的密码 \\q 退出数据库 ? \\password：设置密码 \\h：查看SQL命令的解释，比如\\h select。 ?：查看psql命令列表。 \\l：列出所有数据库。 \\c [database_name]：连接其他数据库。 \\d：列出当前数据库的所有对象，如Table,View等。 \\dt: 列出当前数据库的所有Table。 \\d [table_name]：列出某一张表格的结构。 \\du：列出所有用户。 \\e：打开文本编辑器。 \\conninfo：列出当前数据库和连接的信息。 df 命令 来检查当前磁盘利用率 SecureCRT============================== alt+p进入sftp put -r 路径传目录 shp导入到postgresql——————————————————————— shp2pgsql -s 4544 -c -W &quot;UTF-8&quot; ADDRESS.shp public.rrrrr| psql -h 192.168.22.128 -d postgres -U postgres -W 转换格式——————————————————————— ogr2ogr -f &quot;GeoJSON&quot; ./natural.json PG:&quot;host=localhost dbname=postgres user=postgres password=724111&quot; -sql &quot;select * from natural&quot; 切片—————————————————————————————— tippecanoe -z 14 -Z 5 -ps -Bg -o buildings.mbtiles buildings.json tippecanoe -e lakepbf -pC -Z1 -z17 -f natural.json 各种常用软件环境安装 NPM npm install ---install dependencies npm run dev---serve with hot reload at localhost:8080 npm run build---build for production with minification npm run build --report----build for production and view the bundle analyzer report npm run unit---run unit tests npm run e2e---run e2e tests npm test----run all tests REDIDS ./redis-server redis.windows.conf 启动服务 server.exe --service-install redis.windows.conf --loglevel verbose 开机启动 windows redis-server --service-install redis.windows-service.conf --loglevel verbose VSCODE ctrl+~ ：打开终端 F1 或 Ctrl+Shift+P（俗称万能键） ：打开命令面板。在打开的输入框内，可以输入任何命令 代码格式化: Shift+Alt+F VUE 脚手架 npm config set registry https://registry.npm.taobao.org vue init webpack projectName cd projectName npm install npm run dev npm i element-ui -S npm install axios GIT Git常用命令及方法大全 下面是我整理的常用 Git 命令清单。几个专用名词的译名如下。 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 本地分支关联远程 1git branch --set-upstream-to=origin/分支名 分支名 代码库修改密码后push不上去怎么办？ 1234// 重新输入密码git config --system --unset credential.helper// 密码存储同步git config --global credential.helper store 一、新建代码库 123456# 在当前目录新建一个Git代码库$ git init# 新建一个目录，将其初始化为Git代码库$ git init [project-name]# 下载一个项目和它的整个代码历史$ git clone [url] 二、配置 Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 1234567# 显示当前的Git配置$ git config --list# 编辑Git配置文件$ git config -e [--global]# 设置提交代码时的用户信息$ git config [--global] user.name &quot;[name]&quot;$ git config [--global] user.email &quot;[email address]&quot; 三、增加/删除文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 添加指定文件到暂存区$ git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录$ git add [dir]# 添加当前目录的所有文件到暂存区$ git add . # 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p # 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 四、代码提交 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 提交暂存区到仓库区$ git commit -m [message] # 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a # 提交时显示所有diff信息$ git commit -v # 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 五、分支 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165# 列出所有本地分支$ git branch # 列出所有远程分支$ git branch -r # 列出所有本地分支和远程分支$ git branch -a # 新建一个分支，但依然停留在当前分支$ git branch [branch-name] # 以远程分支为基础新建一个分支，并切换到该分支$ git checkout -b [branch] origin/[remote-branch] # 新建一个分支，指向指定commit$ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区$ git checkout [branch-name] # 切换到上一个分支$ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支$ git merge [branch] # 选择一个commit，合并进当前分支$ git cherry-pick [commit] # 删除分支$ git branch -d [branch-name] # 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 六、标签 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101# 列出所有tag$ git tag # 新建一个tag在当前commit$ git tag [tag] # 新建一个tag在指定commit$ git tag [tag] [commit] # 删除本地tag$ git tag -d [tag] # 删除远程tag$ git push origin :refs/tags/[tagName] # 查看tag信息$ git show [tag] # 提交指定tag$ git push [remote] [tag] # 提交所有tag$ git push [remote] --tags # 新建一个分支，指向某个tag$ git checkout -b [branch] [tag] 七、查看信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237# 显示有变更的文件$ git status # 显示当前分支的版本历史$ git log # 显示commit历史，以及每次commit发生变更的文件$ git log --stat # 搜索提交历史，根据关键词$ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件$ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file] # 显示指定文件相关的每一次diff$ git log -p [file] # 显示过去5次提交$ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序$ git shortlog -sn # 显示指定文件是什么人在什么时间修改过$ git blame [file] # 显示暂存区和工作区的差异$ git diff # 显示暂存区和上一个commit的差异$ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异$ git diff HEAD # 显示两次提交之间的差异$ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码$ git diff --shortstat &quot;@{0 day ago}&quot; # 显示某次提交的元数据和内容变化$ git show [commit] # 显示某次提交发生变化的文件$ git show --name-only [commit] # 显示某次提交时，某个文件的内容$ git show [commit]:[filename] # 显示当前分支的最近几次提交$ git reflog 八、远程同步 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889# 下载远程仓库的所有变动$ git fetch [remote] # 显示所有远程仓库$ git remote -v # 显示某个远程仓库的信息$ git remote show [remote] # 增加一个新的远程仓库，并命名$ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch] # 上传本地指定分支到远程仓库$ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force # 推送所有分支到远程仓库$ git push [remote] --all 九、撤销 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121# 恢复暂存区的指定文件到工作区$ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区$ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit] # 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit] # 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop 十、其他 12345# 生成一个可供发布的压缩包$ git archive Git分支管理策略 一、主分支Master 首先，代码库应该有一个、且仅有一个主分支。所有提供给用户使用的正式版本，都在这个主分支上发布。 Git主分支的名字，默认叫做Master。它是自动建立的，版本库初始化以后，默认就是在主分支在进行开发。 二、开发分支Develop 主分支只用来分布重大版本，日常开发应该在另一条分支上完成。我们把开发用的分支，叫做Develop。 这个分支可以用来生成代码的最新隔夜版本（nightly）。如果想正式对外发布，就在Master分支上，对Develop分支进行&quot;合并&quot;（merge）。 Git创建Develop分支的命令： git checkout -b develop master 将Develop分支发布到Master分支的命令： # 切换到Master分支 git checkout master # 对Develop分支进行合并 git merge --no-ff develop 这里稍微解释一下，上一条命令的--no-ff参数是什么意思。默认情况下，Git执行&quot;快进式合并&quot;（fast-farward merge），会直接将Master分支指向Develop分支。 使用--no-ff参数后，会执行正常合并，在Master分支上生成一个新节点。为了保证版本演进的清晰，我们希望采用这种做法。关于合并的更多解释，请参考Benjamin Sandofsky的《Understanding the Git Workflow》。 三、临时性分支 前面讲到版本库的两条主要分支：Master和Develop。前者用于正式发布，后者用于日常开发。其实，常设分支只需要这两条就够了，不需要其他了。 但是，除了常设分支以外，还有一些临时性分支，用于应对一些特定目的的版本开发。临时性分支主要有三种： * 功能（feature）分支 * 预发布（release）分支 * 修补bug（fixbug）分支 这三种分支都属于临时性需要，使用完以后，应该删除，使得代码库的常设分支始终只有Master和Develop。 四、 功能分支 接下来，一个个来看这三种&quot;临时性分支&quot;。 第一种是功能分支，它是为了开发某种特定功能，从Develop分支上面分出来的。开发完成后，要再并入Develop。 功能分支的名字，可以采用feature-*的形式命名。 创建一个功能分支： git checkout -b feature-x develop 开发完成后，将功能分支合并到develop分支： git checkout develop git merge --no-ff feature-x 删除feature分支： git branch -d feature-x 五、预发布分支 第二种是预发布分支，它是指发布正式版本之前（即合并到Master分支之前），我们可能需要有一个预发布的版本进行测试。 预发布分支是从Develop分支上面分出来的，预发布结束以后，必须合并进Develop和Master分支。它的命名，可以采用release-*的形式。 创建一个预发布分支： git checkout -b release-1.2 develop 确认没有问题后，合并到master分支： git checkout master git merge --no-ff release-1.2 # 对合并生成的新节点，做一个标签 git tag -a 1.2 再合并到develop分支： git checkout develop git merge --no-ff release-1.2 最后，删除预发布分支： git branch -d release-1.2 六、修补bug分支 最后一种是修补bug分支。软件正式发布以后，难免会出现bug。这时就需要创建一个分支，进行bug修补。 修补bug分支是从Master分支上面分出来的。修补结束以后，再合并进Master和Develop分支。它的命名，可以采用fixbug-*的形式。 创建一个修补bug分支： git checkout -b fixbug-0.1 master 修补结束后，合并到master分支： git checkout master git merge --no-ff fixbug-0.1 git tag -a 0.1.1 再合并到develop分支： git checkout develop git merge --no-ff fixbug-0.1 最后，删除&quot;修补bug分支&quot;： git branch -d fixbug-0.1 版本回退-撤销文件修改 工作区修改一个文件后，又想回到修改前(git add前) \\1. 当然可以直接手动再在工作区中将文件修改回去 \\2. 修改后，通过命令git status查看 123456789101112131415161718192021222324252627282930313233$ git status# On branch master# Changes not staged for commit:# (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)# (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)## modified: readme.txt#no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 这时Git会告诉你，git checkout -- file可以丢弃工作区的修改： 1$ git checkout -- readme.txt Note: \\1. git checkout -- file命令中的--很重要，没有--，就变成了“切换到另一个分支”的命令，我们在后面的分支管理中会再次遇到git checkout命令。 \\2. 命令git checkout -- readme.txt意思就是，把readme.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是readme.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；一种是readme.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。总之，就是让这个文件回到最近一次git commit或git add时的状态。 \\3. 工作区、暂存区的概念不清楚的可见于Git版本控制教程 - Git本地仓库 如果在工作区中修改了文件还git add到暂存区（但是在commit之前） 用git status查看一下，修改只是添加到了暂存区，还没有提交： 12345678910111213141516171819202122232425$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)## modified: readme.txt# Git同样告诉我们，用命令git reset HEAD file可以把暂存区的修改撤销掉（unstage），重新放回工作区： 123456789$ git reset HEAD readme.txtUnstaged changes after reset:M readme.txt git reset命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用HEAD时，表示最新的版本。 再用git status查看一下，现在暂存区是干净的，工作区有修改。 然后丢弃工作区的修改 1234567891011121314151617$ git checkout -- readme.txt $ git status# On branch masternothing to commit (working directory clean) 不但修改了文件还从暂存区提交commit到了版本库 - 版本回退 版本回退可以回退到上一个版本。不过，这是有条件的，就是你还没有把自己的本地版本库推送到远程。Git是分布式版本控制系统。 在工作中对某个文件（如readme.txt）进行多次修改交commit。 可以通过版本控制系统命令告诉我们提交的历史记录，在Git中，我们用git log命令查看： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869$ git logcommit 3628164fb26d48395383f8f31179f24e0882e1e0Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Tue Aug 20 15:11:49 2013 +0800 append GPL commit ea34578d5496d7dd233c827ed32a8cd576c5ee85Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file Note: \\1. git log命令显示从最近到最远的提交日志，我们可以看到3次提交，最近的一次是append GPL，上一次是add distributed，最早的一次是wrote a readme file。 \\2. 如果嫌输出信息太多，看得眼花缭乱的，可以试试加上--pretty=oneline参数： 12345678910111213$ git log --pretty=oneline3628164fb26d48395383f8f31179f24e0882e1e0 append GPLea34578d5496d7dd233c827ed32a8cd576c5ee85 add distributedcb926e7ea50ad11b8f9e909c05226233bf755030 wrote a readme file \\3. 你看到的一大串类似3628164...882e1e0的是commit id（版本号），和SVN不一样，Git的commit id不是1，2，3……递增的数字，而是一个SHA1计算出来的一个非常大的数字，用十六进制表示，而且你看到的commit id和我的肯定不一样，以你自己的为准。为什么commit id需要用这么一大串数字表示呢？因为Git是分布式的版本控制系统，后面我们还要研究多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。 \\4. 每提交一个新版本，实际上Git就会把它们自动串成一条时间线。如果使用可视化工具（如GitX、github的客户端、pycharm）查看Git历史，就可以更清楚地看到提交历史的时间线。 现在我们想要把readme.txt回退到上一个版本 如“add distributed”的那个版本，怎么做呢？首先，Git必须知道当前版本是哪个版本，在Git中，用HEAD表示当前版本，也就是最新的提交3628164...882e1e0（注意我的提交ID和你的肯定不一样），上一个版本就是HEAD，上上一个版本就是HEAD，当然往上100个版本写100个比较容易数不过来，所以写成HEAD~100。 现在，我们要把当前版本“append GPL”回退到上一个版本“add distributed”，就可以使用git reset命令： 12345$ git reset --hard HEAD^HEAD is now at ea34578 add distributed 这时readme.txt的内容就成了版本add distributed 我们用git log再看看现在版本库的状态： 123456789101112131415161718192021222324252627282930313233343536373839404142434445$ git logcommit ea34578d5496d7dd233c827ed32a8cd576c5ee85Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Tue Aug 20 14:53:12 2013 +0800 add distributed commit cb926e7ea50ad11b8f9e909c05226233bf755030Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Mon Aug 19 17:51:55 2013 +0800 wrote a readme file 最新的那个版本append GPL已经看不到了！ 恢复文件后，要是我们又想回到修改后的文件呢？（命令行窗口还没有被关掉） 只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，找到那个append GPL的commit id是3628164...，于是就可以指定回到未来的某个版本： 12345$ git reset --hard 3628164HEAD is now at 3628164 append GPL 版本号没必要写全，前几位就可以了，Git会自动去找。 Git的版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向append GPL： 改为指向add distributed： 然后顺便把工作区的文件更新了。所以你让HEAD指向哪个版本号，你就把当前版本定位在哪。 恢复文件后，要是我们又想回到修改后的文件呢？（命令行窗口早就关掉了） 想恢复到新版本怎么办？找不到新版本的commit id怎么办？当你用$ git reset --hard HEAD^回退到add distributed版本时，再想恢复到append GPL，就必须找到append GPL的commit id。 Git提供了一个命令git reflog用来记录你的每一次命令：[Git高级教程:git log与git reflog] 1234567891011121314151617$ git reflogea34578 HEAD@{0}: reset: moving to HEAD^3628164 HEAD@{1}: commit: append GPLea34578 HEAD@{2}: commit: add distributedcb926e7 HEAD@{3}: commit (initial): wrote a readme file 第二行显示append GPL的commit id是3628164，现在，你又可以乘坐时光机回到未来了。","link":"/2021/02/25/Draft/2021/%E5%BF%AB%E6%9F%A5/"},{"title":"魑魅先生 | 计算机网络","text":"网络基础 1. 网络层次划分 OSI/RM模型（Open System Interconnection/Reference Model）开放系统互联参考模型 2. 网络模型 OSI七层网络模型 1）物理层（Physical Layer） · 激活、维持、关闭通信端点之间的机械特性、电气特性、功能特性以及过程特性。该层为上层协议提供了一个传输数据的可靠的物理媒体。简单的说，物理层确保原始的数据可在各种物理媒体上传输。两个重要的设备名称，中继器（Repeater，放大器），集线器。 2）数据链路层（Data Link Layer） · 数据链路层在物理层提供的服务的基础上向网络层提供服务，其最基本的服务是将源自网络层来的数据可靠地传输到相邻节点的目标机网络层。为达到这一目的，数据链路必须具备一系列相应的功能，主要有：如何将数据组合成数据块，在数据链路层中称这种数据块为帧（frame），帧是数据链路层的传送单位；如何控制帧在物理信道上的传输，包括如何处理传输差错，如何调节发送速率以使与接收方相匹配；以及在两个网络实体之间提供数据链路通路的建立、维持和释放的管理。数据链路层在不可靠的物理介质上提供可靠的传输。该层的作用包括：物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。 · 有关数据链路层的重要知识点： 1&gt; 数据链路层为网络层提供可靠的数据传输； 2&gt; 基本数据单位为帧； 3&gt; 主要的协议：以太网协议； 4&gt; 两个重要设备名称：网桥和交换机。 3）网络层（Network Layer） · 实现两个端系统之间的数据透明传送，具体功能包括寻址和路由选择、连接的建立、保持和终止等。它提供的服务使传输层不需要了解网络中的数据传输和交换技术。如果您想用尽量少的词来记住网络层，那就是&quot;路径选择、路由及逻辑寻址&quot;。网络层中涉及众多的协议，其中包括最重要的协议，也是TCP/IP的核心协议——IP协议。IP协议非常简单，仅仅提供不可靠、无连接的传送服务。IP协议的主要功能有：无连接数据报传输、数据报路由选择和差错控制。与IP协议配套使用实现其功能的还有地址解析协议ARP、逆地址解析协议RARP、因特网报文协议ICMP、因特网组管理协议IGMP。 · 具体的协议我们会在接下来的部分进行总结，有关网络层的重点为： 1&gt; 网络层负责对子网间的数据包进行路由选择。此外，网络层还可以实现拥塞控制、网际互连等功能； 2&gt; 基本数据单位为IP数据报； 3&gt; 包含的主要协议： IP协议（Internet Protocol，因特网互联协议）; ICMP协议（Internet Control Message Protocol，因特网控制报文协议）; ARP协议（Address Resolution Protocol，地址解析协议）; RARP协议（Reverse Address Resolution Protocol，逆地址解析协议）。 4&gt; 重要的设备：路由器。 4）传输层（Transport Layer） · 第一个端到端，即主机到主机的层次。传输层负责将上层数据分段并提供端到端的、可靠的或不可靠的传输。此外，传输层还要处理端到端的差错控制和流量控制问题。 传输层的任务是根据通信子网的特性，最佳的利用网络资源，为两个端系统的会话层之间，提供建立、维护和取消传输连接的功能，负责端到端的可靠数据传输。在这一层，信息传送的协议数据单元称为段或报文。 网络层只是根据网络地址将源结点发出的数据包传送到目的结点，而传输层则负责将数据可靠地传送到相应的端口。 · 重点： 1&gt; 传输层负责将上层数据分段并提供端到端的、可靠的或不可靠的传输以及端到端的差错控制和流量控制问题； 2&gt; 包含的主要协议：TCP协议（Transmission Control Protocol，传输控制协议）、UDP协议（User Datagram Protocol，用户数据报协议）； 3&gt; 重要设备：网关。 5）会话层 · 会话层管理主机之间的会话进程，即负责建立、管理、终止进程之间的会话。会话层还利用在数据中插入校验点来实现数据的同步。 6）表示层 · 表示层对上层数据或信息进行变换以保证一个主机应用层信息可以被另一个主机的应用程序理解。表示层的数据转换包括数据的加密、压缩、格式转换等。 7）应用层 · 为操作系统或网络应用程序提供访问网络服务的接口。 · 会话层、表示层和应用层重点： · 1&gt; 数据传输基本单位为报文； · 2&gt; 包含的主要协议：FTP（文件传送协议）、Telnet（远程登录协议）、DNS（域名解析协议）、SMTP（邮件传送协议），POP3协议（邮局协议），HTTP协议（Hyper Text Transfer Protocol）。 TCP/IP四 (五)层模型 3. IP地址 1）网络地址 IP地址由网络号（包括子网号）和主机号组成，网络地址的主机号为全0，网络地址代表着整个网络。 2）广播地址 广播地址通常称为直接广播地址，是为了区分受限广播地址。 广播地址与网络地址的主机号正好相反，广播地址中，主机号为全1。当向某个网络的广播地址发送消息时，该网络内的所有主机都能收到该广播消息。 3）组播地址 D类地址就是组播地址。 先回忆下A，B，C，D类地址吧： A类地址以0开头，第一个字节作为网络号，地址范围为：0.0.0.0~127.255.255.255；(modified @2016.05.31) B类地址以10开头，前两个字节作为网络号，地址范围是：128.0.0.0~191.255.255.255; C类地址以110开头，前三个字节作为网络号，地址范围是：192.0.0.0~223.255.255.255。 D类地址以1110开头，地址范围是224.0.0.0~239.255.255.255，D类地址作为组播地址（一对多的通信）； E类地址以1111开头，地址范围是240.0.0.0~255.255.255.255，E类地址为保留地址，供以后使用。 注：只有A,B,C有网络号和主机号之分，D类地址和E类地址没有划分网络号和主机号。 4）255.255.255.255 该IP地址指的是受限的广播地址。受限广播地址与一般广播地址（直接广播地址）的区别在于，受限广播地址只能用于本地网络，路由器不会转发以受限广播地址为目的地址的分组；一般广播地址既可在本地广播，也可跨网段广播。例如：主机192.168.1.1/30上的直接广播数据包后，另外一个网段192.168.1.5/30也能收到该数据报；若发送受限广播数据报，则不能收到。 注：一般的广播地址（直接广播地址）能够通过某些路由器（当然不是所有的路由器），而受限的广播地址不能通过路由器。 5）0.0.0.0 常用于寻找自己的IP地址，例如在我们的RARP，BOOTP和DHCP协议中，若某个未知IP地址的无盘机想要知道自己的IP地址，它就以255.255.255.255为目的地址，向本地范围（具体而言是被各个路由器屏蔽的范围内）的服务器发送IP请求分组。 6）回环地址 127.0.0.0/8被用作回环地址，回环地址表示本机的地址，常用于对本机的测试，用的最多的是127.0.0.1。 7）A、B、C类私有地址 私有地址(private address)也叫专用地址，它们不会在全球使用，只具有本地意义。 A类私有地址：10.0.0.0/8，范围是：10.0.0.0~10.255.255.255 B类私有地址：172.16.0.0/12，范围是：172.16.0.0~172.31.255.255 C类私有地址：192.168.0.0/16，范围是：192.168.0.0~192.168.255.255 4. 子网掩码及网络划分 随着互连网应用的不断扩大，原先的IPv4的弊端也逐渐暴露出来，即网络号占位太多，而主机号位太少，所以其能提供的主机地址也越来越稀缺，目前除了使用NAT在企业内部利用保留地址自行分配以外，通常都对一个高类别的IP地址进行再划分，以形成多个子网，提供给不同规模的用户群使用。这里主要是为了在网络分段情况下有效地利用IP地址，通过对主机号的高位部分取作为子网号，从通常的网络位界限中扩展或压缩子网掩码，用来创建某类地址的更多子网。但创建更多的子网时，在每个子网上的可用主机地址数目会比原先减少。什么是子网掩码？子网掩码是标志两个IP地址是否同属于一个子网的，也是32位二进制地址，其每一个为1代表该位是网络位，为0代表主机位。它和IP地址一样也是使用点式十进制来表示的。如果两个IP地址在子网掩码的按位与的计算下所得结果相同，即表明它们共属于同一子网中。在计算子网掩码时，我们要注意IP地址中的保留地址，即&quot; 0&quot;地址和广播地址，它们是指主机地址或网络地址全为&quot; 0&quot;或&quot; 1&quot;时的IP地址，它们代表着本网络地址和广播地址，一般是不能被计算在内的。子网掩码的计算：对于无须再划分成子网的IP地址来说，其子网掩码非常简单，即按照其定义即可写出：如某B类IP地址为 10.12.3.0，无须再分割子网，则该IP地址的子网掩码255.255.0.0。如果它是一个C类地址，则其子网掩码为 255.255.255.0。其它类推，不再详述。下面我们关键要介绍的是一个IP地址，还需要将其高位主机位再作为划分出的子网网络号，剩下的是每个子网的主机号，这时该如何进行每个子网的掩码计算。 下面总结一下有关子网掩码和网络划分常见的面试考题： 1）利用子网数来计算 · 在求子网掩码之前必须先搞清楚要划分的子网数目，以及每个子网内的所需主机数目。(1) 将子网数目转化为二进制来表示;如欲将B类IP地址168.195.0.0划分成27个子网：27=11011；(2) 取得该二进制的位数，为N；该二进制为五位数，N = 5(3) 取得该IP地址的类子网掩码，将其主机地址部分的的前N位置1即得出该IP地址划分子网的子网掩码。将B类地址的子网掩码255.255.0.0的主机地址前5位置 1，得到 255.255.248.0 2）利用主机数来计算 · 如欲将B类IP地址168.195.0.0划分成若干子网，每个子网内有主机700台：(1) 将主机数目转化为二进制来表示；700=1010111100(2) 如果主机数小于或等于254（注意去掉保留的两个IP地址），则取得该主机的二进制位数，为N，这里肯定 N&lt;8。如果大于254，则 N&gt;8，这就是说主机地址将占据不止8位；该二进制为十位数，N=10；(3) 使用255.255.255.255来将该类IP地址的主机地址位数全部置1，然后从后向前的将N位全部置为 0，即为子网掩码值。将该B类地址的子网掩码255.255.0.0的主机地址全部置1，得到255.255.255.255，然后再从后向前将后 10位置0,即为：11111111.11111111.11111100.00000000，即255.255.252.0。这就是该欲划分成主机为700台的B类IP地址 168.195.0.0的子网掩码。 3）根据每个网络的主机数量进行子网地址的规划和计算子网掩码 · 这也可按上述原则进行计算。比如一个子网有10台主机，那么对于这个子网需要的IP地址是：10＋1＋1＋1＝13注意：加的第一个1是指这个网络连接时所需的网关地址，接着的两个1分别是指网络地址和广播地址。因为13小于16（16等于2的4次方），所以主机位为4位。而256－16＝240，所以该子网掩码为255.255.255.240。如果一个子网有14台主机，不少人常犯的错误是：依然分配具有16个地址空间的子网，而忘记了给网关分配地址。这样就错误了，因为14＋1＋1＋1＝17，17大于16，所以我们只能分配具有32个地址（32等于2的5次方）空间的子网。这时子网掩码为：255.255.255.224。 5. ARP/RARP协议 地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。主机发送信息时将包含目标IP地址的ARP请求广播到网络上的所有主机，并接收返回消息，以此确定目标的物理地址；收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。地址解析协议是建立在网络中各个主机互相信任的基础上的，网络上的主机可以自主发送ARP应答消息，其他主机收到应答报文时不会检测该报文的真实性就会将其记入本机ARP缓存；由此攻击者就可以向某一主机发送伪ARP应答报文，使其发送的信息无法到达预期的主机或到达错误的主机，这就构成了一个ARP欺骗。ARP命令可用于查询本机ARP缓存中IP地址和MAC地址的对应关系、添加或删除静态对应关系等。 ARP工作流程举例： 主机A的IP地址为192.168.1.1，MAC地址为0A-11-22-33-44-01； 主机B的IP地址为192.168.1.2，MAC地址为0A-11-22-33-44-02； 当主机A要与主机B通信时，地址解析协议可以将主机B的IP地址（192.168.1.2）解析成主机B的MAC地址，以下为工作流程： · （1）根据主机A上的路由表内容，IP确定用于访问主机B的转发IP地址是192.168.1.2。然后A主机在自己的本地ARP缓存中检查主机B的匹配MAC地址。 · （2）如果主机A在ARP缓存中没有找到映射，它将询问192.168.1.2的硬件地址，从而将ARP请求帧广播到本地网络上的所有主机。源主机A的IP地址和MAC地址都包括在ARP请求中。本地网络上的每台主机都接收到ARP请求并且检查是否与自己的IP地址匹配。如果主机发现请求的IP地址与自己的IP地址不匹配，它将丢弃ARP请求。 · （3）主机B确定ARP请求中的IP地址与自己的IP地址匹配，则将主机A的IP地址和MAC地址映射添加到本地ARP缓存中。 · （4）主机B将包含其MAC地址的ARP回复消息直接发送回主机A。 · （5）当主机A收到从主机B发来的ARP回复消息时，会用主机B的IP和MAC地址映射更新ARP缓存。本机缓存是有生存期的，生存期结束后，将再次重复上面的过程。主机B的MAC地址一旦确定，主机A就能向主机B发送IP通信了。 逆地址解析协议，即RARP，功能和ARP协议相对，其将局域网中某个主机的物理地址转换为IP地址，比如局域网中有一台主机只知道物理地址而不知道IP地址，那么可以通过RARP协议发出征求自身IP地址的广播请求，然后由RARP服务器负责回答。 RARP协议工作流程： · （1）给主机发送一个本地的RARP广播，在此广播包中，声明自己的MAC地址并且请求任何收到此请求的RARP服务器分配一个IP地址； · （2）本地网段上的RARP服务器收到此请求后，检查其RARP列表，查找该MAC地址对应的IP地址； · （3）如果存在，RARP服务器就给源主机发送一个响应数据包并将此IP地址提供给对方主机使用； · （4）如果不存在，RARP服务器对此不做任何的响应； 6. 路由选择协议 常见的路由选择协议有：RIP协议、OSPF协议。RIP协议 ：底层是贝尔曼福特算法，它选择路由的度量标准（metric)是跳数，最大跳数是15跳，如果大于15跳，它就会丢弃数据包。OSPF协议 ：Open Shortest Path First开放式最短路径优先，底层是迪杰斯特拉算法，是链路状态路由选择协议，它选择路由的度量标准是带宽，延迟。 7. TCP/IP协议 TCP/IP协议是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。通俗而言：TCP负责发现传输的问题，一有问题就发出信号，要求重新传输，直到所有数据安全正确地传输到目的地。而IP是给因特网的每一台联网设备规定一个地址。IP层接收由更低层（网络接口层例如以太网设备驱动程序）发来的数据包，并把该数据包发送到更高层---TCP或UDP层；相反，IP层也把从TCP或UDP层接收来的数据包传送到更低层。IP数据包是不可靠的，因为IP并没有做任何事情来确认数据包是否按顺序发送的或者有没有被破坏，IP数据包中含有发送它的主机的地址（源地址）和接收它的主机的地址（目的地址）。TCP是面向连接的通信协议，通过三次握手建立连接，通讯完成时要拆除连接，由于TCP是面向连接的所以只能用于端到端的通讯。TCP提供的是一种可靠的数据流服务，采用&quot;带重传的肯定确认&quot;技术来实现传输的可靠性。TCP还采用一种称为&quot;滑动窗口&quot;的方式进行流量控制，所谓窗口实际表示接收能力，用以限制发送方的发送速度。 TCP报文首部格式：件）、HTTP协议等。 TCP协议的三次握手和四次挥手： 注：seq:&quot;sequance&quot;序列号；ack:&quot;acknowledge&quot;确认号；SYN:&quot;synchronize&quot;请求同步标志；；ACK:&quot;acknowledge&quot;确认标志&quot;；FIN：&quot;Finally&quot;结束标志。 TCP连接建立过程：首先Client端发送连接请求报文，Server段接受连接后回复ACK报文，并为这次连接分配资源。Client端接收到ACK报文后也向Server段发生ACK报文，并分配资源，这样TCP连接就建立了。TCP连接断开过程：假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说&quot;我Client端没有数据要发给你了&quot;，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，&quot;告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息&quot;。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，&quot;告诉Client端，好了，我这边数据发完了，准备好关闭连接了&quot;。Client端收到FIN报文后，&quot;就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。&quot;，Server端收到ACK后，&quot;就知道可以断开连接了&quot;。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！为什么要三次握手？在只有两次&quot;握手&quot;的情形下，假设Client想跟Server建立连接，但是却因为中途连接请求的数据报丢失了，故Client端不得不重新发送一遍；这个时候Server端仅收到一个连接请求，因此可以正常的建立连接。但是，有时候Client端重新发送请求不是因为数据报丢失了，而是有可能数据传输过程因为网络并发量很大在某结点被阻塞了，这种情形下Server端将先后收到2次请求，并持续等待两个Client请求向他发送数据...问题就在这里，Cient端实际上只有一次请求，而Server端却有2个响应，极端的情况可能由于Client端多次重新发送请求数据而导致Server端最后建立了N多个响应在等待，因而造成极大的资源浪费！所以，&quot;三次握手&quot;很有必要！为什么要四次挥手？试想一下，假如现在你是客户端你想断开跟Server的所有连接该怎么做？第一步，你自己先停止向Server端发送数据，并等待Server的回复。但事情还没有完，虽然你自身不往Server发送数据了，但是因为你们之前已经建立好平等的连接了，所以此时他也有主动权向你发送数据；故Server端还得终止主动向你发送数据，并等待你的确认。其实，说白了就是保证双方的一个合约的完整执行！使用TCP的协议：FTP（文件传输协议）、Telnet（远程登录协议）、SMTP（简单邮件传输协议）、POP3（和SMTP相对，用于接收邮 8. UDP协议 UDP用户数据报协议，是面向无连接的通讯协议，UDP数据包括目的端口号和源端口号信息，由于通讯不需要连接，所以可以实现广播发送。UDP通讯时不需要接收方确认，属于不可靠的传输，可能会出现丢包现象，实际应用中要求程序员编程验证。UDP与TCP位于同一层，但它不管数据包的顺序、错误或重发。因此，UDP不被应用于那些使用虚电路的面向连接的服务，UDP主要用于那些面向查询---应答的服务，例如NFS。相对于FTP或Telnet，这些服务需要交换的信息量较小。每个UDP报文分UDP报头和UDP数据区两部分。报头由四个16位长（2字节）字段组成，分别说明该报文的源端口、目的端口、报文长度以及校验值。UDP报头由4个域组成，其中每个域各占用2个字节，具体如下：（1）源端口号；（2）目标端口号；（3）数据报长度；（4）校验值。使用UDP协议包括：TFTP（简单文件传输协议）、SNMP（简单网络管理协议）、DNS（域名解析协议）、NFS、BOOTP。TCP 与 UDP 的区别：TCP是面向连接的，可靠的字节流服务；UDP是面向无连接的，不可靠的数据报服务。 9. DNS协议 DNS是域名系统(DomainNameSystem)的缩写，该系统用于命名组织到域层次结构中的计算机和网络服务，可以简单地理解为将URL转换为IP地址。域名是由圆点分开一串单词或缩写组成的，每一个域名都对应一个惟一的IP地址，在Internet上域名与IP地址之间是一一对应的，DNS就是进行域名解析的服务器。DNS命名用于Internet等TCP/IP网络中，通过用户友好的名称查找计算机和服务。 10. NAT协议 NAT网络地址转换(Network Address Translation)属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技术，它被广泛应用于各种类型Internet接入方式和各种类型的网络中。原因很简单，NAT不仅完美地解决了lP地址不足的问题，而且还能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。 11. DHCP协议 DHCP动态主机设置协议（Dynamic Host Configuration Protocol）是一个局域网的网络协议，使用UDP协议工作，主要有两个用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 12. HTTP协议 超文本传输协议（HTTP，HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议。所有的WWW文件都必须遵守这个标准。 HTTP 协议包括哪些请求？ GET：请求读取由URL所标志的信息。 POST：给服务器添加信息（如注释）。 PUT：在给定的URL下存储一个文档。 DELETE：删除给定的URL所标志的资源。 HTTP 中， POST 与 GET 的区别 1）Get是从服务器上获取数据，Post是向服务器传送数据。 2）Get是把参数数据队列加到提交表单的Action属性所指向的URL中，值和表单内各个字段一一对应，在URL中可以看到。 3）Get传送的数据量小，不能大于2KB；Post传送的数据量较大，一般被默认为不受限制。 4）根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的。 I. 所谓 安全的 意味着该操作用于获取信息而非修改信息。换句话说，GET请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 II. 幂等 的意味着对同一URL的多个请求应该返回同样的结果。 HTTP 协议的 8 种请求类型介绍 HTTP 协议中共定义了八种方法或者叫“动作”来表明对 Request-URI 指定的资源的不同操作方式，具体介绍如下： OPTIONS：返回服务器针对特定资源所支持的HTTP请求方法。也可以利用向Web服务器发送'*'的请求来测试服务器的功能性。 HEAD：向服务器索要与GET请求相一致的响应，只不过响应体将不会被返回。这一方法可以在不必传输整个响应内容的情况下，就可以获取包含在响应消息头中的元信息。 GET：向特定的资源发出请求。 POST：向指定资源提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的创建和/或已有资源的修改。 PUT：向指定资源位置上传其最新内容。 DELETE：请求服务器删除 Request-URI 所标识的资源。 TRACE：回显服务器收到的请求，主要用于测试或诊断。 CONNECT：HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。 虽然 HTTP 的请求方式有 8 种，但是我们在实际应用中常用的也就是 get 和 post，其他请求方式也都可以通过这两种方式间接的来实现。 网络常用参数 Content-Type 格式：Content-Type：type/subtype ;parameter type：主类型，任意的字符串，如text，如果是*号代表所有； subtype：子类型，任意的字符串，如html，如果是*号代表所有，用“/”与主类型隔开； parameter：可选参数，如charset，boundary等。 参考：1、2 常见的媒体格式类型如下： text/html ： HTML格式 text/plain ：纯文本格式 text/xml ： XML格式 image/gif ：gif图片格式 image/jpeg ：jpg图片格式 image/png：png图片格式 以application开头的媒体格式类型： application/xhtml+xml ：XHTML格式 application/xml： XML数据格式 application/atom+xml ：Atom XML聚合格式 application/json： JSON数据格式 application/pdf：pdf格式 application/msword ： Word文档格式 application/octet-stream ： 二进制流数据（如常见的文件下载） application/x-www-form-urlencoded ： &lt;-form encType=&quot;&quot;&gt;中默认的encType，form表单数据被编码为key1=val1&amp;key2=val2格式发送到服务器（表单默认的提交数据的格式） 另外一种常见的媒体格式是上传文件之时使用的： multipart/form-data ： 需要在表单中进行文件上传时，就需要使用该格式 13. 一个举例 在浏览器中输入 http://www.baidu.com/ 后执行的全部过程。现在假设如果我们在客户端（客户端）浏览器中输入 http://www.baidu.com， 而 baidu.com 为要访问的服务器（服务器），下面详细分析客户端为了访问服务器而执行的一系列关于协议的操作：1）客户端浏览器通过DNS解析到www.baidu.com的IP地址220.181.27.48，通过这个IP地址找到客户端到服务器的路径。客户端浏览器发起一个HTTP会话到220.161.27.48，然后通过TCP进行封装数据包，输入到网络层。2）在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。3）客户端的网络层不用关系应用层或者传输层的东西，主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，不作过多的描述，无非就是通过查找路由表决定通过那个路径到达服务器。4）客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换的IP数据包现在就可以传输了，然后发送IP数据包到达服务器的地址。 14.TCP三次握手/四次挥手 TCP TCP头部为20字节 源端口号（16位）和目的端口号（16位）：再加上Ip首部的源IP地址和目的IP地址可以唯一确定一个TCP连接 数据序号（16位）：表示在这个报文段中的第一个数据字节序号 确认序号：仅当ACK标志为1时有效，确认号表示期望收到的下一个字节的序号 偏移：就是头部长度，有4位，跟IP头部一样，以4字节为单位。最大是60个字节 保留位：6位，必须为0 6个标志位：URG-紧急指针有效；ACK-确认序号有效；PSH-接收方应尽快将这个报文交给应用层；RST-连接重置；SYN-同步序号用来发起一个连接；FIN-终止一个连接。 窗口字段：16位，代表的是窗口的字节容量，也就是TCP的标准窗口最大为2^16 - 1 = 65535个字节 校验和：源机器基于数据内容计算一个数值，收信息机要与源机器数值结果完全一样，从而证明数据的有效性。检验和覆盖了整个的TCP报文段：这是一个强制性的字段，一定是由发送端计算和存储，并由接收端进行验证的。 TCP 三次握手 为了保证数据能到达目标，TCP采用三次握手策略。 发送端首先发送一个带SYN（synchronize）标志的数据包给接收方【第一次的seq序列号是随机产生的，这样是为了网络安全，如果不是随机产生初始序列号，黑客将会以很容易的方式获取到你与其他主机之间的初始化序列号，并且伪造序列号进行攻击】 接收端收到后，回传一个带有SYN/ACK（acknowledgement）标志的数据包以示传达确认信息【SYN 是为了告诉发送端，发送方到接收方的通道没问题；ACK 用来验证接收方到发送方的通道没问题】 最后，发送端再回传一个带ACK标志的数据包，代表握手结束 若在握手某个过程中某个阶段莫名中断，TCP协议会再次以相同的顺序发送相同的数据包 Q：为什么要三次握手？ 三次握手的目的是建立可靠的通信信道，说到通讯，简单来说就是数据的发送与接收，而三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的 第一次握手，发送端：什么都确认不了；接收端：对方发送正常，自己接受正常 第二次握手，发送端：对方发送，接受正常，自己发送，接受正常 ；接收端：对方发送正常，自己接受正常 第三次握手，发送端：对方发送，接受正常，自己发送，接受正常；接收端：对方发送，接受正常，自己发送，接受正常 Q：两次握手不行吗？为什么TCP客户端最后还要发送一次确认呢？ 主要防止已经失效的连接请求报文突然又传送到了服务器，从而产生错误。 经典场景：客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了 由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。 此时此前滞留的那一次请求连接，网络通畅了到达服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。 如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。 Q：为什么三次握手，返回时，ack 值是 seq 加 1（ack = x+1） 假设对方接收到数据，比如sequence number = 1000，TCP Payload = 1000，数据第一个字节编号为1000，最后一个为1999，回应一个确认报文，确认号为2000，意味着编号2000前的字节接收完成，准备接收编号为2000及更多的数据 确认收到的序列，并且告诉发送端下一次发送的序列号从哪里开始（便于接收方对数据排序，便于选择重传） Q：SYN洪泛攻击(SYN Flood，半开放攻击)，怎么解决？ 什么是SYN洪范泛攻击？ SYN Flood利用TCP协议缺陷，发送大量伪造的TCP连接请求，常用假冒的IP或IP号段发来海量的请求连接的第一个握手包（SYN包），被攻击服务器回应第二个握手包（SYN+ACK包），因为对方是假冒IP，对方永远收不到包且不会回应第三个握手包。导致被攻击服务器保持大量SYN_RECV状态的“半连接”，并且会重试默认5次回应第二个握手包，大量随机的恶意syn占满了未完成连接队列，导致正常合法的syn排不上队列，让正常的业务请求连接不进来。【服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的，所以服务器容易受到SYN洪泛攻击】 检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击【在 Linux/Unix 上可以使用系统自带的 netstats 命令来检测 SYN 攻击】 怎么解决？ 缩短超时（SYN Timeout）时间 增加最大半连接数 过滤网关防护 SYN cookies技术： 当服务器接受到 SYN 报文段时，不直接为该 TCP 分配资源，而只是打开一个半开的套接字。接着会使用 SYN 报文段的源 Id，目的 Id，端口号以及只有服务器自己知道的一个秘密函数生成一个 cookie，并把 cookie 作为序列号响应给客户端。 如果客户端是正常建立连接，将会返回一个确认字段为 cookie + 1 的报文段。接下来服务器会根据确认报文的源 Id，目的 Id，端口号以及秘密函数计算出一个结果，如果结果的值 + 1 等于确认字段的值，则证明是刚刚请求连接的客户端，这时候才为该 TCP 分配资源 Q：TCP三次握手中，最后一次回复丢失，会发生什么？ 如果最后一次ACK在网络中丢失，那么Server端（服务端）该TCP连接的状态仍为SYN_RECV，并且根据 TCP的超时重传机制依次等待3秒、6秒、12秒后重新发送 SYN+ACK 包，以便 Client（客户端）重新发送ACK包 如果重发指定次数后，仍然未收到ACK应答，那么一段时间后，Server（服务端）自动关闭这个连接 但是Client（客户端）认为这个连接已经建立，如果Client（客户端）端向Server（服务端）发送数据，Server端（服务端）将以RST包（Reset，标示复位，用于异常的关闭连接）响应，此时，客户端知道第三次握手失败 TCP四次挥手 主动断开方（客户端/服务端）-发送一个 FIN，用来关闭主动断开方（客户端/服务端）到被动断开方（客户端/服务端）的数据传送 被动断开方（客户端/服务端）-收到这个 FIN，它发回一 个 ACK，确认序号为收到的序号加1 。和 SYN 一样，一个 FIN 将占用一个序号 被动点开方（客户端/服务端）-关闭与主动断开方（客户端/服务端）的连接，发送一个FIN给主动断开方（客户端/服务端） 主动断开方（客户端/服务端）-发回 ACK 报文确认，并将确认序号设置为收到序号加1 Q：为什么连接的时候是三次握手，关闭的时候却是四次握手？ 建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。 关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了,所以服务器可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接。因此，服务器ACK和FIN一般都会分开发送，从而导致多了一次。 Q：为什么TCP挥手每两次中间有一个 FIN-WAIT2等待时间？ 主动关闭的一端调用完close以后（即发FIN给被动关闭的一端， 并且收到其对FIN的确认ACK）则进入FIN_WAIT_2状态。如果这个时候因为网络突然断掉、被动关闭的一段宕机等原因，导致主动关闭的一端不能收到被动关闭的一端发来的FIN（防止对端不发送关闭连接的FIN包给本端），这个时候就需要FIN_WAIT_2定时器， 如果在该定时器超时的时候，还是没收到被动关闭一端发来的FIN，那么直接释放这个链接，进入CLOSE状态 Q：为什么客户端最后还要等待2MSL？为什么还有个TIME-WAIT的时间等待？ 保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，服务器已经发送了FIN+ACK报文，请求断开，客户端却没有回应，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。 防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，这样新的连接中不会出现旧连接的请求报文。 2MSL，最大报文生存时间，一个MSL 30 秒，2MSL = 60s Q：客户端 TIME-WAIT 状态过多会产生什么后果？怎样处理？ 作为服务器，短时间内关闭了大量的Client连接，就会造成服务器上出现大量的TIME_WAIT连接，占据大量的tuple /tApl/ ，严重消耗着服务器的资源，此时部分客户端就会显示连接不上 作为客户端，短时间内大量的短连接，会大量消耗的Client机器的端口，毕竟端口只有65535个，端口被耗尽了，后续就无法在发起新的连接了 在高并发短连接的TCP服务器上，当服务器处理完请求后立刻主动正常关闭连接。这个场景下会出现大量socket处于TIME_WAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上 高并发可以让服务器在短时间范围内同时占用大量端口，而端口有个0~65535的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了 短连接表示“业务处理+传输数据的时间 远远小于 TIMEWAIT超时的时间”的连接 解决方法： 用负载均衡来抗这些高并发的短请求； 服务器可以设置 SO_REUSEADDR 套接字选项来避免 TIME_WAIT状态，TIME_WAIT 状态可以通过优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的 强制关闭，发送 RST 包越过TIMEWAIT状态，直接进入CLOSED状态 Q：服务器出现了大量 CLOSE_WAIT 状态如何解决？ 大量 CLOSE_WAIT 表示程序出现了问题，对方的 socket 已经关闭连接，而我方忙于读或写没有及时关闭连接，需要检查代码，特别是释放资源的代码，或者是处理请求的线程配置。 Q：服务端会有一个TIME_WAIT状态吗？如果是服务端主动断开连接呢？ 发起链接的主动方基本都是客户端，但是断开连接的主动方服务器和客户端都可以充当，也就是说，只要是主动断开连接的，就会有 TIME_WAIT状态 四次挥手是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发 由于TCP连接时全双工的，因此，每个方向的数据传输通道都必须要单独进行关闭。","link":"/2021/02/25/Draft/2021/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"RabbitMQ","text":"RabbitMQ简介 MQ全称为Message Queue，消息队列是应用程序和应用程序之间的通信方法。 RabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。RabbitMQ官方地址：http://www.rabbitmq.com/ AMQP 一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。 AMQP是一个二进制协议，拥有一些现代化特点：多信道、协商式，异步，安全，扩平台，中立，高效。 RabbitMQ是AMQP协议的Erlang的实现。 概念 说明 连接Connection 一个网络连接，比如TCP/IP套接字连接。 会话Session 端点之间的命名对话。在一个会话上下文中，保证“恰好传递一次”。 信道Channel 多路复用连接中的一条独立的双向数据流通道。为会话提供物理传输介质。 客户端Client AMQP连接或者会话的发起者。AMQP是非对称的，客户端生产和消费消息，服务器存储和路由这些消息。 服务节点Broker 消息中间件的服务节点；一般情况下可以将一个RabbitMQ Broker看作一台RabbitMQ 服务器。 端点 AMQP对话的任意一方。一个AMQP连接包括两个端点（一个是客户端，一个是服务器）。 消费者Consumer 一个从消息队列里请求消息的客户端程序。 生产者Producer 一个向交换机发布消息的客户端应用程序。 6种工作模式： 简单模式，work模式，Publish/Subscribe发布与订阅模式，Routing路由模式，Topics主题模式，RPC远程调用模式（远程调用，不太算MQ；暂不作介绍）； 官网对应模式介绍：https://www.rabbitmq.com/getstarted.html 优势： 应用解耦，提升容错性和可维护性 异步提速，提升用户体验，系统吞吐量 削峰填谷，提高系统稳定性 劣势： 系统可用性降低（需保证MQ高可用） 系统复杂度提高（不被重复消费，保证消息顺序） 数据一致性问题 使用条件: 生产者不需要从消费者处获得反馈 容许短暂的不一致性 确实用了有效果 ，好处大于复杂性等劣势 常见MQ产品 JMS JMS即Java消息服务（JavaMessage Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 AMQP 与 JMS 区别 JMS是定义了统一的接口，来对消息操作进行统一；AMQP是通过规定协议来统一数据交互的格式 JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的。 JMS规定了两种消息模式；而AMQP的消息模式更加丰富 RabbitMQ 安装配置 MQ版本选择和erlang相关 使用资料里提供的CentOS-7-x86_64-DVD-1810.iso 安装虚拟机. 1. 安装依赖环境 在线安装依赖环境： 1yum install build-essential openssl openssl-devel unixODBC unixODBC-devel make gcc gcc-c++ kernel-devel m4 ncurses-devel tk tc xz 2. 安装Erlang 上传 alt+p进入sftp界面 erlang-18.3-1.el7.centos.x86_64.rpm socat-1.7.3.2-5.el7.lux.x86_64.rpm rabbitmq-server-3.6.5-1.noarch.rpm 12# 安装rpm -ivh erlang-18.3-1.el7.centos.x86_64.rpm 如果出现如下错误 说明gblic 版本太低。我们可以查看当前机器的gblic 版本 1strings /lib64/libc.so.6 | grep GLIBC 当前最高版本2.12，需要2.15.所以需要升级glibc 使用yum更新安装依赖 1sudo yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make -y 下载rpm包 1234567wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-utils-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-static-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-common-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-devel-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/glibc-headers-2.17-55.el6.x86_64.rpm &amp;wget http://copr-be.cloud.fedoraproject.org/results/mosquito/myrepo-el6/epel-6-x86_64/glibc-2.17-55.fc20/nscd-2.17-55.el6.x86_64.rpm &amp; 安装rpm包 1sudo rpm -Uvh *-2.17-55.el6.x86_64.rpm --force --nodeps 安装完毕后再查看glibc版本,发现glibc版本已经到2.17了 1strings /lib64/libc.so.6 | grep GLIBC 3. 安装RabbitMQ 123456# 安装rpm -ivh socat-1.7.3.2-5.el7.lux.x86_64.rpm# 安装rpm -ivh rabbitmq-server-3.6.5-1.noarch.rpm 12345678910111213[root@softbank126011009150 rabbitmq]# lserlang-18.3-1.el7.centos.x86_64.rpm rabbitmq-server-3.6.5-1.noarch.rpm socat-1.7.3.2-1.1.el7.x86_64.rpm[root@softbank126011009150 rabbitmq]# rpm -ivh erlang-18.3-1.el7.centos.x86_64.rpm 准备中... ################################# [100%] 软件包 erlang-18.3-1.el7.centos.x86_64 已经安装[root@softbank126011009150 rabbitmq]# rpm -ivh socat-1.7.3.2-1.1.el7.x86_64.rpm 警告：socat-1.7.3.2-1.1.el7.x86_64.rpm: 头V4 RSA/SHA1 Signature, 密钥 ID 87e360b8: NOKEY准备中... ################################# [100%] 软件包 socat-1.7.3.2-1.1.el7.x86_64 已经安装[root@softbank126011009150 rabbitmq]# rpm -ivh rabbitmq-server-3.6.5-1.noarch.rpm 警告：rabbitmq-server-3.6.5-1.noarch.rpm: 头V4 RSA/SHA1 Signature, 密钥 ID 6026dfca: NOKEY准备中... ################################# [100%] 软件包 rabbitmq-server-3.6.5-1.noarch 已经安装 4. 开启管界面及配置 123456# 开启管理界面rabbitmq-plugins enable rabbitmq_management# 修改默认配置信息vim /usr/lib/rabbitmq/lib/rabbitmq_server-3.6.5/ebin/rabbit.app # 比如修改密码、配置等等，例如：loopback_users 中的 &lt;&lt;&quot;guest&quot;&gt;&gt;,只保留guesthttp://ip:15672 访问 5. 启动 123456service rabbitmq-server start # 启动服务service rabbitmq-server stop # 停止服务service rabbitmq-server restart # 重启服务service iptables stop#关闭防火墙systemctl stop firewalld #conos7关闭防火墙方法 设置配置文件 1234cd /usr/share/doc/rabbitmq-server-3.6.5/cp rabbitmq.config.example /etc/rabbitmq/rabbitmq.config 6. 配置虚拟主机及用户 6.1. 用户角色 RabbitMQ在安装好后，可以访问http://ip地址:15672 ；其自带了guest/guest的用户名和密码；如果需要创建自定义用户；那么也可以登录管理界面后，如下操作： 角色说明： 1、 超级管理员(administrator) 可登陆管理控制台，可查看所有的信息，并且可以对用户，策略(policy)进行操作。 2、 监控者(monitoring) 可登陆管理控制台，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等) 3、 策略制定者(policymaker) 可登陆管理控制台, 同时可以对policy进行管理。但无法查看节点的相关信息(上图红框标识的部分)。 4、 普通管理者(management) 仅可登陆管理控制台，无法看到节点信息，也无法对策略进行管理。 5、 其他 无法登陆管理控制台，通常就是普通的生产者和消费者。 6.2. Virtual Hosts配置 像mysql拥有数据库的概念并且可以指定用户对库和表等操作的权限。RabbitMQ也有类似的权限管理；在RabbitMQ中可以虚拟消息服务器Virtual Host，每个Virtual Hosts相当于一个相对独立的RabbitMQ服务器，每个VirtualHost之间是相互隔离的。exchange、queue、message不能互通。 相当于mysql的db。Virtual Name一般以/开头。 6.2.1. 创建Virtual Hosts 6.2.2. 设置Virtual Hosts权限 解决配置文件未找到 1234567891011121314151617181920212223242526272829[root@softbank126011009150 rabbitmq]# cd /usr/share/doc/rabbitmq-server-3.6.5/[root@softbank126011009150 rabbitmq-server-3.6.5]# lsLICENSE LICENSE-APL2-Stomp-Websocket LICENSE-EPL-OTP LICENSE-MIT-jQuery164 LICENSE-MIT-Sammy060 LICENSE-MPL-RabbitMQ set_rabbitmq_policy.sh.exampleLICENSE-APACHE2-ExplorerCanvas LICENSE-BSD-base64js LICENSE-MIT-EJS10 LICENSE-MIT-Mochi LICENSE-MIT-SockJS rabbitmq.config.exampleLICENSE-APL2-Rebar LICENSE-BSD-glMatrix LICENSE-MIT-Flot LICENSE-MIT-Mochiweb LICENSE-MPL2 README[root@softbank126011009150 rabbitmq-server-3.6.5]# ll总用量 200-rw-r--r--. 1 root root 28945 8月 5 2016 LICENSE-rw-r--r--. 1 root root 11358 8月 5 2016 LICENSE-APACHE2-ExplorerCanvas-rw-r--r--. 1 root root 10175 8月 5 2016 LICENSE-APL2-Rebar-rw-r--r--. 1 root root 10851 8月 5 2016 LICENSE-APL2-Stomp-Websocket-rw-r--r--. 1 root root 1206 8月 5 2016 LICENSE-BSD-base64js-rw-r--r--. 1 root root 1304 8月 5 2016 LICENSE-BSD-glMatrix-rw-r--r--. 1 root root 14041 8月 5 2016 LICENSE-EPL-OTP-rw-r--r--. 1 root root 1087 8月 5 2016 LICENSE-MIT-EJS10-rw-r--r--. 1 root root 1069 8月 5 2016 LICENSE-MIT-Flot-rw-r--r--. 1 root root 1075 8月 5 2016 LICENSE-MIT-jQuery164-rw-r--r--. 1 root root 1087 3月 31 2016 LICENSE-MIT-Mochi-rw-r--r--. 1 root root 1087 8月 5 2016 LICENSE-MIT-Mochiweb-rw-r--r--. 1 root root 1076 8月 5 2016 LICENSE-MIT-Sammy060-rw-r--r--. 1 root root 1056 8月 5 2016 LICENSE-MIT-SockJS-rw-r--r--. 1 root root 16726 8月 5 2016 LICENSE-MPL2-rw-r--r--. 1 root root 24897 8月 5 2016 LICENSE-MPL-RabbitMQ-rw-r--r--. 1 root root 21023 4月 11 2016 rabbitmq.config.example-rw-r--r--. 1 root root 943 3月 31 2016 README-rw-r--r--. 1 root root 277 3月 31 2016 set_rabbitmq_policy.sh.example[root@softbank126011009150 rabbitmq-server-3.6.5]# cp ./rabbitmq.config.example /etc/rabbitmq/rabbitmq.config[root@softbank126011009150 rabbitmq-server-3.6.5]# service rabbitmq-server restartRestarting rabbitmq-server (via systemctl): [ 确定 ] 工作模式 简单模式 生产者 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.lxl.producer;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Producer_HelloWord { public static void main(String[] args) throws IOException, TimeoutException { //1.创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); //2. 设置参数 factory.setHost(&quot;126.11.41.128&quot;);//ip 默认值 localhost factory.setPort(5672); //端口 默认值 5672 factory.setVirtualHost(&quot;/lxlv&quot;);//虚拟机 默认值/ factory.setUsername(&quot;lxl&quot;);//用户名 默认 guest factory.setPassword(&quot;lxl&quot;);//密码 默认值 guest //3. 创建连接 Connection Connection connection = factory.newConnection(); //4. 创建Channel Channel channel = connection.createChannel(); //5. 创建队列Queue /* queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) 参数： 1. queue：队列名称 2. durable:是否持久化，当mq重启之后，还在 3. exclusive： * 是否独占。只能有一个消费者监听这队列 * 当Connection关闭时，是否删除队列 * 4. autoDelete:是否自动删除。当没有Consumer时，自动删除掉 5. arguments：参数。 */ //如果没有一个名字叫hello_world的队列，则会创建该队列，如果有则不会创建 channel.queueDeclare(&quot;hello_world&quot;,true,false,false,null); /* basicPublish(String exchange, String routingKey, BasicProperties props, byte[] body) 参数： 1. exchange：交换机名称。简单模式下交换机会使用默认的 &quot;&quot; 2. routingKey：路由名称 3. props：配置信息 4. body：发送消息数据 */ String body = &quot;hello rabbitmq~~~&quot;; //6. 发送消息 channel.basicPublish(&quot;&quot;,&quot;hello_world&quot;,null,body.getBytes()); //7.释放资源// channel.close();// connection.close(); }} 消费者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.lxl.consumer;import com.rabbitmq.client.*;import java.io.IOException;import java.util.concurrent.TimeoutException;public class Consumer_HelloWorld { public static void main(String[] args) throws IOException, TimeoutException { //1.创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); //2. 设置参数 factory.setHost(&quot;126.11.41.128&quot;);//ip 默认值 localhost factory.setPort(5672); //端口 默认值 5672 factory.setVirtualHost(&quot;/lxlv&quot;);//虚拟机 默认值/ factory.setUsername(&quot;lxl&quot;);//用户名 默认 guest factory.setPassword(&quot;lxl&quot;);//密码 默认值 guest //3. 创建连接 Connection Connection connection = factory.newConnection(); //4. 创建Channel Channel channel = connection.createChannel(); //5. 创建队列Queue /* queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) 参数： 1. queue：队列名称 2. durable:是否持久化，当mq重启之后，还在 3. exclusive： * 是否独占。只能有一个消费者监听这队列 * 当Connection关闭时，是否删除队列 * 4. autoDelete:是否自动删除。当没有Consumer时，自动删除掉 5. arguments：参数。 */ //如果没有一个名字叫hello_world的队列，则会创建该队列，如果有则不会创建 channel.queueDeclare(&quot;hello_world&quot;,true,false,false,null); /* basicConsume(String queue, boolean autoAck, Consumer callback) 参数： 1. queue：队列名称 2. autoAck：是否自动确认 3. callback：回调对象 */ // 接收消息 Consumer consumer = new DefaultConsumer(channel){ /* 回调方法，当收到消息后，会自动执行该方法 1. consumerTag：标识 2. envelope：获取一些信息，交换机，路由key... 3. properties:配置信息 4. body：数据 */ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { System.out.println(&quot;consumerTag：&quot;+consumerTag); System.out.println(&quot;Exchange：&quot;+envelope.getExchange()); System.out.println(&quot;RoutingKey：&quot;+envelope.getRoutingKey()); System.out.println(&quot;properties：&quot;+properties); System.out.println(&quot;body：&quot;+new String(body)); } }; channel.basicConsume(&quot;hello_world&quot;,true,consumer); //关闭资源？不要 }} RabbitMQ运转流程 在入门案例中： 生产者发送消息 生产者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker； 声明队列并设置属性；是否排它，是否持久化，是否自动删除； 将路由键（空字符串）与队列绑定起来； 发送消息至RabbitMQ Broker； 关闭信道； 关闭连接； 消费者接收消息 消费者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker 向Broker 请求消费相应队列中的消息，设置相应的回调函数； 等待Broker回应闭关投递响应队列中的消息，消费者接收消息； 确认（ack，自动确认）接收到的消息； RabbitMQ从队列中删除相应已经被确认的消息； 关闭信道； 关闭连接； 生产者流转过程说明 客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQPO-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start/.Start-OK 、Connection.Tune/.Tune-Ok ，Connection.Open/ .Open-Ok 这6 个命令的交互。 客户端调用connection.createChannel方法。此方法开启信道，其包装的channel.open命令发送给Broker,等待channel.basicPublish方法，对应的AMQP命令为Basic.Publish,这个命令包含了content Header 和content Body()。content Header 包含了消息体的属性，例如:投递模式，优先级等，content Body 包含了消息体本身。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 消费者流转过程说明 消费者客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQPO-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start/.Start-OK 、Connection.Tune/.Tune-Ok ，Connection.Open/ .Open-Ok 这6 个命令的交互。 消费者客户端调用connection.createChannel方法。和生产者客户端一样，协议涉及Channel . Open/Open-Ok命令。 在真正消费之前，消费者客户端需要向Broker 发送Basic.Consume 命令(即调用channel.basicConsume 方法〉将Channel 置为接收模式，之后Broker 回执Basic . Consume - Ok 以告诉消费者客户端准备好消费消息。 Broker 向消费者客户端推送(Push) 消息，即Basic.Deliver 命令，这个命令和Basic.Publish 命令一样会携带Content Header 和Content Body。 消费者接收到消息并正确消费之后，向Broker 发送确认，即Basic.Ack 命令。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 工作队列模式 模式说明（只有一盒糖你们几个抢） Work Queues与入门程序的简单模式相比，多了一个或一些消费端，多个消费端共同消费同一个队列中的消息。 应用场景：对于 任务过重或任务较多情况使用工作队列可以提高任务处理的速度。 代码 Work Queues与入门程序的简单模式的代码是几乎一样的；可以完全复制，并复制多一个消费者进行多个消费者同时消费消息的测试。 生产者 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.itheima.rabbitmq.work;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class Producer { static final String QUEUE_NAME = &quot;work_queue&quot;; public static void main(String[] args) throws Exception { //创建连接 Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(QUEUE_NAME, true, false, false, null); for (int i = 1; i &lt;= 30; i++) { // 发送信息 String message = &quot;你好；小兔子！work模式--&quot; + i; /** * 参数1：交换机名称，如果没有指定则使用默认Default Exchage * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot;已发送消息：&quot; + message); } // 关闭资源 channel.close(); connection.close(); }} 消费者1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.itheima.rabbitmq.work;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.*;import java.io.IOException;public class Consumer1 { public static void main(String[] args) throws Exception { Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //一次只能接收并处理一个消息 channel.basicQos(1); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel){ @Override /** * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { try { //路由key System.out.println(&quot;路由key为：&quot; + envelope.getRoutingKey()); //交换机 System.out.println(&quot;交换机为：&quot; + envelope.getExchange()); //消息id System.out.println(&quot;消息id为：&quot; + envelope.getDeliveryTag()); //收到的消息 System.out.println(&quot;消费者1-接收到的消息为：&quot; + new String(body, &quot;utf-8&quot;)); Thread.sleep(1000); //确认消息 channel.basicAck(envelope.getDeliveryTag(), false); } catch (InterruptedException e) { e.printStackTrace(); } } }; //监听消息 /** * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, false, consumer); }} 消费者2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.itheima.rabbitmq.work;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.*;import java.io.IOException;public class Consumer2 { public static void main(String[] args) throws Exception { Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //一次只能接收并处理一个消息 channel.basicQos(1); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel){ @Override /** * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { try { //路由key System.out.println(&quot;路由key为：&quot; + envelope.getRoutingKey()); //交换机 System.out.println(&quot;交换机为：&quot; + envelope.getExchange()); //消息id System.out.println(&quot;消息id为：&quot; + envelope.getDeliveryTag()); //收到的消息 System.out.println(&quot;消费者2-接收到的消息为：&quot; + new String(body, &quot;utf-8&quot;)); Thread.sleep(1000); //确认消息 channel.basicAck(envelope.getDeliveryTag(), false); } catch (InterruptedException e) { e.printStackTrace(); } } }; //监听消息 /** * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, false, consumer); }} 测试 启动两个消费者，然后再启动生产者发送消息；到IDEA的两个消费者对应的控制台查看是否竞争性的接收到消息。 小结 在一个队列中如果有多个消费者，那么消费者之间对于同一个消息的关系是竞争的关系。 Publish/Subscribe发布与订阅模式 模式说明（给喜欢的人一样的糖）【FANOUT】 在订阅模型中，多了一个 Exchange 角色，而且过程略有变化： P：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机） 123456789//1.创建连接工厂//2. 设置参数//3. 创建连接 Connection//4. 创建Channel//5. 创建交换机//6. 创建队列//7. 绑定队列和交换机//8. 发送消息//9. 释放资源 C：消费者，消息的接收者，会一直等待消息到来 Queue：消息队列，接收消息、缓存消息 Exchange：交换机（X）。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有常见以下3种类型： Fanout：广播，将消息交给所有绑定到交换机的队列 Direct：定向，把消息交给符合指定routing key 的队列 Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与 Exchange 绑定，或者没有符合路由规则的队列，那么消息会丢失！ Routing路由工作模式 模式说明：给更爱的人更甜的糖【DIRECT】 队列与交换机的绑定，不能是任意绑定了，而是要指定一个 RoutingKey（路由key） l消息的发送方在向 Exchange 发送消息时，也必须指定消息的 RoutingKey lExchange 不再把消息交给每一个绑定的队列，而是根据消息的 Routing Key 进行判断，只有队列的Routingkey 与消息的 Routing key 完全一致，才会接收到消息 lP：生产者，向 Exchange 发送消息，发送消息时，会指定一个routing key lX：Exchange（交换机），接收生产者的消息，然后把消息递交给与 routing key 完全匹配的队列 lC1：消费者，其所在队列指定了需要 routing key 为 error 的消息 lC2：消费者，其所在队列指定了需要 routing key 为 info、error、warning 的消息 Routing 模式要求队列在绑定交换机时要指定 routing key，消息会转发到符合 routing key 的队列 Topics通配符工作模式 模式说明：给姓李的人类棉花糖，姓梁的人类鸡屎麻糖【TOPIC】 Topic类型与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！ Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert 通配符规则： #：匹配一个或多个词 *：匹配不多不少恰好1个词 举例： item.#：能够匹配item.insert.abc 或者 item.insert item.*：只能匹配item.insert 图解： 红色Queue：绑定的是usa.# ，因此凡是以 usa.开头的routing key 都会被匹配到 黄色Queue：绑定的是#.news ，因此凡是以 .news结尾的 routing key 都会被匹配 Spring-RabbitMQ 详情见代码 SpringBoot-RabbitMQ RabbitMQ高级特性 发、收、限、时、达、 1.1消息可靠投递 【寄的糖你收到了吗】 confirm确认模式 【消息从 producer 到 exchange 无论如何会返回一个confirmCallback】 return退回模式 【消息从producer到 queue 失败则会返回一个returnCallback】 rabbitmq整各消息投递的路径为： profucer-----》rabbitmq broker----》exchange-----》queue----》consumer 设置ConnectionFactory的publisher-confirm=&quot;true&quot; 开启确认模式 使用rabbitTemplate.setConfirmCallback设置回调函数。当消息发送到exchange后回调confirm方法。在方法中判断true，则发送成功，反之失败，需要处理 设置ConnectionFactory的publisher-returns=&quot;true&quot; 开启退回模式 使用rabbitTemplate.setReturnCallback设置回调函数。当消息从exchange路由到queue失败后，如果设置了rabbitTemplate.setMandatory(true)参数，则会将消息退回给producer。并执行回调函数returnedMessage。 在RabbitMQ中也提供了事物机制，但是性能较差。 1.2Consumer ACK 【收到糖给你怎么回复】 ack指acknowledge，确认。表示消费端收到消息后的确认方式 三种确认方式： ​ 自动确认：acknowledge=&quot;none&quot;【业务出问题也会回复收到】 ​ 手动确认：acknowledge=&quot;manual&quot;【等业务处理没问题后回复收到，有问题可处理】 ​ 根据异常情况确认：acknowledge=&quot;auto&quot; ​ 自动确认消息被consumer接收到，则自动确认，并将响应message从缓存中移除，实际业务中小细节受到，业务出现异常，那么消息会丢失，如果是设置的手动确认方式，则需要业务处理成功后，调用channel.basicAck(),手动签收，如果出现异常，则调用channel.basicNack（）方法，让其自动重发消息。 1234* 1. 设置手动签收。acknowledge=&quot;manual&quot;* 2. 让监听器类实现ChannelAwareMessageListener接口* 3. 如果消息成功处理，则调用channel的 basicAck()签收* 4. 如果消息处理失败，则调用channel的basicNack()拒绝签收，broker重新发送给consumer 1.3消费端限流qos 【我每次只收1000颗糖】 12341. 确保ack机制为手动确认。2. listener-container配置属性 perfetch = 1,表示消费端每次从mq拉去一条消息来消费，直到手动确认消费完毕后，才会继续拉去下一条消息。 &lt;rabbit:listener-container connection-factory=&quot;connectionFactory&quot; acknowledge=&quot;manual&quot; prefetch=&quot;1&quot; &gt; 1.4TTL 【糖放快递站十年就不能吃了】 1234567* TTL:过期时间* 1. 队列统一过期* 2. 消息单独过期** 如果设置了消息的过期时间，也设置了队列的过期时间，它以时间短的为准。* 队列过期后，会将队列所有消息全部移除。* 消息过期后，只有消息在队列顶端，才会判断其是否过期(移除掉) 1.5死信队列 【不好吃的糖都给其他人吧】 死信队列（DXL）。Dead Letter Exchange（死信队列交换机），当消息成为Dead Message后，可以被重新发送到另外一个交换机，这个交换机就是DEX。死信队列和死信交换机和普通的没有区别。 问题： 1.消息什么时候成为死信? 消息成为死信的三种情况： 1.队列消息的长度达到限制 2.消费者拒收消费消息，basicNack/basicReject，并且不把消息重新放入原目标队列，requeue=false 3.原队列存在消息过期设置，消息达超时间未被消费； 2.队列如何与DEX绑定？ 1.6延迟队列 【我妈叫我三十分钟后才能吃这颗糖】 消息进入队列后不会被立即被消费，只有达到指定时间后，才会被消费。 需求： ​ 1.下单后三十分钟未支付，取消订单，回滚库存 ​ 2.新用户注册7天后，发送短信问候 实现方式 ​ 1.定时器（有延迟，对数据库有影响） ​ 2.延迟队列（MQ中未直接提供延迟队列，DLX+TTL实现） 1.7日志与监控 RabbitMQ默认日志存放路径： /var/log/rabbitmq/rabbit@xxx.log 日志包含了RabbitMQ的版本号、Erlang的版本号、RabbitMQ服务节点名称、cookie的hash值、RabbitMQ配置文件地址、内存限制、磁盘限制、默认账户guest的创建以及权限配置等等。 123456789101112131415161718查看队列# rabbitmqctl list_queues查看exchanges# rabbitmqctl list_exchanges查看用户# rabbitmqctl list_users查看连接# rabbitmqctl list_connections查看消费者信息# rabbitmqctl list_consumers查看环境变量# rabbitmqctl environment查看未被确认的队列# rabbitmqctl list_queues name messages_unacknowledged查看单个队列的内存使用# rabbitmqctl list_queues name memory查看准备就绪的队列# rabbitmqctl list_queues name messages_ready 1.8 消息追踪 在使用任何消息中间件的过程中，难免会出现某条消息异常丢失的情况。对于RabbitMQ而言，可能是因为生产者或消费者与RabbitMQ断开了连接，而它们与RabbitMQ又采用了不同的确认机制；也有可能是因为交换器与队列之间不同的转发策略；甚至是交换器并没有与任何队列进行绑定，生产者又不感知或者没有采取相应的措施；另外RabbitMQ本身的集群策略也可能导致消息的丢失。这个时候就需要有一个较好的机制跟踪记录消息的投递过程，以此协助开发和运维人员进行问题的定位。 在RabbitMQ中可以使用Firehose和rabbitmq_tracing插件功能来实现消息追踪。 Firehose firehose的机制是将生产者投递给rabbitmq的消息，rabbitmq投递给消费者的消息按照指定的格式发送到默认的exchange上。这个默认的exchange的名称为amq.rabbitmq.trace，它是一个topic类型的exchange。发送到这个exchange上的消息的routing key为 publish.exchangename 和 deliver.queuename。其中exchangename和queuename为实际exchange和queue的名称，分别对应生产者投递到exchange的消息，和消费者从queue上获取的消息。 注意：打开 trace 会影响消息写入功能，适当打开后请关闭。 rabbitmqctl trace_on：开启Firehose命令 rabbitmqctl trace_off：关闭Firehose命令 rabbitmq_tracing rabbitmq_tracing和Firehose在实现上如出一辙，只不过rabbitmq_tracing的方式比Firehose多了一层GUI的包装，更容易使用和管理。 启用插件：rabbitmq-plugins enable rabbitmq_tracing 应用问题 1.消息可靠性保障------消息补偿 2.消息幂等性保障-----乐观锁机制 幂等性指一次和多次请求某一个资源，对于资源本身应该具有同样的结果。也就是说，其任意多次执行对资源本身所产生的影响均与一次执行的影响相同。 在MQ中指，消费多条相同的消息，得到与消费该消息一次相同的结果。 3.RabbitMQ集群搭建 摘要：实际生产应用中都会采用消息队列的集群方案，如果选择RabbitMQ那么有必要了解下它的集群方案原理 一般来说，如果只是为了学习RabbitMQ或者验证业务工程的正确性那么在本地环境或者测试环境上使用其单实例部署就可以了，但是出于MQ中间件本身的可靠性、并发性、吞吐量和消息堆积能力等问题的考虑，在生产环境上一般都会考虑使用RabbitMQ的集群方案。 3.1 集群方案的原理 RabbitMQ这款消息队列中间件产品本身是基于Erlang编写，Erlang语言天生具备分布式特性（通过同步Erlang集群各节点的magic cookie来实现）。因此，RabbitMQ天然支持Clustering。这使得RabbitMQ本身不需要像ActiveMQ、Kafka那样通过ZooKeeper分别来实现HA方案和保存集群的元数据。集群是保证可靠性的一种方式，同时可以通过水平扩展以达到增加消息吞吐量能力的目的。 3.2 单机多实例部署 由于某些因素的限制，有时候你不得不在一台机器上去搭建一个rabbitmq集群，这个有点类似zookeeper的单机版。真实生成环境还是要配成多机集群的。有关怎么配置多机集群的可以参考其他的资料，这里主要论述如何在单机中配置多个rabbitmq实例。 主要参考官方文档：https://www.rabbitmq.com/clustering.html 首先确保RabbitMQ运行没有问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@super ~]# rabbitmqctl statusStatus of node rabbit@super ...[{pid,10232}, {running_applications, [{rabbitmq_management,&quot;RabbitMQ Management Console&quot;,&quot;3.6.5&quot;}, {rabbitmq_web_dispatch,&quot;RabbitMQ Web Dispatcher&quot;,&quot;3.6.5&quot;}, {webmachine,&quot;webmachine&quot;,&quot;1.10.3&quot;}, {mochiweb,&quot;MochiMedia Web Server&quot;,&quot;2.13.1&quot;}, {rabbitmq_management_agent,&quot;RabbitMQ Management Agent&quot;,&quot;3.6.5&quot;}, {rabbit,&quot;RabbitMQ&quot;,&quot;3.6.5&quot;}, {os_mon,&quot;CPO CXC 138 46&quot;,&quot;2.4&quot;}, {syntax_tools,&quot;Syntax tools&quot;,&quot;1.7&quot;}, {inets,&quot;INETS CXC 138 49&quot;,&quot;6.2&quot;}, {amqp_client,&quot;RabbitMQ AMQP Client&quot;,&quot;3.6.5&quot;}, {rabbit_common,[],&quot;3.6.5&quot;}, {ssl,&quot;Erlang/OTP SSL application&quot;,&quot;7.3&quot;}, {public_key,&quot;Public key infrastructure&quot;,&quot;1.1.1&quot;}, {asn1,&quot;The Erlang ASN1 compiler version 4.0.2&quot;,&quot;4.0.2&quot;}, {ranch,&quot;Socket acceptor pool for TCP protocols.&quot;,&quot;1.2.1&quot;}, {mnesia,&quot;MNESIA CXC 138 12&quot;,&quot;4.13.3&quot;}, {compiler,&quot;ERTS CXC 138 10&quot;,&quot;6.0.3&quot;}, {crypto,&quot;CRYPTO&quot;,&quot;3.6.3&quot;}, {xmerl,&quot;XML parser&quot;,&quot;1.3.10&quot;}, {sasl,&quot;SASL CXC 138 11&quot;,&quot;2.7&quot;}, {stdlib,&quot;ERTS CXC 138 10&quot;,&quot;2.8&quot;}, {kernel,&quot;ERTS CXC 138 10&quot;,&quot;4.2&quot;}]}, {os,{unix,linux}}, {erlang_version, &quot;Erlang/OTP 18 [erts-7.3] [source] [64-bit] [async-threads:64] [hipe] [kernel-poll:true]\\n&quot;}, {memory, [{total,56066752}, {connection_readers,0}, {connection_writers,0}, {connection_channels,0}, {connection_other,2680}, {queue_procs,268248}, {queue_slave_procs,0}, {plugins,1131936}, {other_proc,18144280}, {mnesia,125304}, {mgmt_db,921312}, {msg_index,69440}, {other_ets,1413664}, {binary,755736}, {code,27824046}, {atom,1000601}, {other_system,4409505}]}, {alarms,[]}, {listeners,[{clustering,25672,&quot;::&quot;},{amqp,5672,&quot;::&quot;}]}, {vm_memory_high_watermark,0.4}, {vm_memory_limit,411294105}, {disk_free_limit,50000000}, {disk_free,13270233088}, {file_descriptors, [{total_limit,924},{total_used,6},{sockets_limit,829},{sockets_used,0}]}, {processes,[{limit,1048576},{used,262}]}, {run_queue,0}, {uptime,43651}, {kernel,{net_ticktime,60}}] 停止rabbitmq服务 123[root@super sbin]# service rabbitmq-server stopStopping rabbitmq-server: rabbitmq-server. 启动第一个节点： 12345678910[root@super sbin]# RABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=rabbit1 rabbitmq-server start RabbitMQ 3.6.5. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit1.log ###### ## /var/log/rabbitmq/rabbit1-sasl.log ########## Starting broker... completed with 6 plugins. 启动第二个节点： web管理插件端口占用,所以还要指定其web插件占用的端口号。 1234567891011[root@super ~]# RABBITMQ_NODE_PORT=5674 RABBITMQ_SERVER_START_ARGS=&quot;-rabbitmq_management listener [{port,15674}]&quot; RABBITMQ_NODENAME=rabbit2 rabbitmq-server start RabbitMQ 3.6.5. Copyright (C) 2007-2016 Pivotal Software, Inc. ## ## Licensed under the MPL. See http://www.rabbitmq.com/ ## ## ########## Logs: /var/log/rabbitmq/rabbit2.log ###### ## /var/log/rabbitmq/rabbit2-sasl.log ########## Starting broker... completed with 6 plugins. 结束命令： 12rabbitmqctl -n rabbit1 stoprabbitmqctl -n rabbit2 stop rabbit1操作作为主节点： 1234567[root@super ~]# rabbitmqctl -n rabbit1 stop_app Stopping node rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit1 reset Resetting node rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit1 start_appStarting node rabbit1@super ...[root@super ~]# rabbit2操作为从节点： 123456789[root@super ~]# rabbitmqctl -n rabbit2 stop_appStopping node rabbit2@super ...[root@super ~]# rabbitmqctl -n rabbit2 resetResetting node rabbit2@super ...[root@super ~]# rabbitmqctl -n rabbit2 join_cluster rabbit1@'super' ###''内是主机名换成自己的Clustering node rabbit2@super with rabbit1@super ...[root@super ~]# rabbitmqctl -n rabbit2 start_appStarting node rabbit2@super ... 查看集群状态： 1234567[root@super ~]# rabbitmqctl cluster_status -n rabbit1Cluster status of node rabbit1@super ...[{nodes,[{disc,[rabbit1@super,rabbit2@super]}]}, {running_nodes,[rabbit2@super,rabbit1@super]}, {cluster_name,&lt;&lt;&quot;rabbit1@super&quot;&gt;&gt;}, {partitions,[]}, {alarms,[{rabbit2@super,[]},{rabbit1@super,[]}]}] web监控： 3.3 集群管理 rabbitmqctl join_cluster {cluster_node} [–ram] 将节点加入指定集群中。在这个命令执行前需要停止RabbitMQ应用并重置节点。 rabbitmqctl cluster_status 显示集群的状态。 rabbitmqctl change_cluster_node_type {disc|ram} 修改集群节点的类型。在这个命令执行前需要停止RabbitMQ应用。 rabbitmqctl forget_cluster_node [–offline] 将节点从集群中删除，允许离线执行。 rabbitmqctl update_cluster_nodes {clusternode} 在集群中的节点应用启动前咨询clusternode节点的最新信息，并更新相应的集群信息。这个和join_cluster不同，它不加入集群。考虑这样一种情况，节点A和节点B都在集群中，当节点A离线了，节点C又和节点B组成了一个集群，然后节点B又离开了集群，当A醒来的时候，它会尝试联系节点B，但是这样会失败，因为节点B已经不在集群中了。 rabbitmqctl cancel_sync_queue [-p vhost] {queue} 取消队列queue同步镜像的操作。 rabbitmqctl set_cluster_name {name} 设置集群名称。集群名称在客户端连接时会通报给客户端。Federation和Shovel插件也会有用到集群名称的地方。集群名称默认是集群中第一个节点的名称，通过这个命令可以重新设置。 3.4 RabbitMQ镜像集群配置 上面已经完成RabbitMQ默认集群模式，但并不保证队列的高可用性，尽管交换机、绑定这些可以复制到集群里的任何一个节点，但是队列内容不会复制。虽然该模式解决一项目组节点压力，但队列节点宕机直接导致该队列无法应用，只能等待重启，所以要想在队列节点宕机或故障也能正常应用，就要复制队列内容到集群里的每个节点，必须要创建镜像队列。 镜像队列是基于普通的集群模式的，然后再添加一些策略，所以你还是得先配置普通集群，然后才能设置镜像队列，我们就以上面的集群接着做。 设置的镜像队列可以通过开启的网页的管理端Admin-&gt;Policies，也可以通过命令。 rabbitmqctl set_policy my_ha &quot;^&quot; '{&quot;ha-mode&quot;:&quot;all&quot;}' Name:策略名称 Pattern：匹配的规则，如果是匹配所有的队列，是^. Definition:使用ha-mode模式中的all，也就是同步所有匹配的队列。问号链接帮助文档。 3.5 负载均衡-HAProxy HAProxy提供高可用性、负载均衡以及基于TCP和HTTP应用的代理，支持虚拟主机，它是免费、快速并且可靠的一种解决方案,包括Twitter，Reddit，StackOverflow，GitHub在内的多家知名互联网公司在使用。HAProxy实现了一种事件驱动、单一进程模型，此模型支持非常大的并发连接数。 3.5.1 安装HAProxy 12345678910111213141516//下载依赖包yum install gcc vim wget//上传haproxy源码包//解压tar -zxvf haproxy-1.6.5.tar.gz -C /usr/local//进入目录、进行编译、安装cd /usr/local/haproxy-1.6.5make TARGET=linux31 PREFIX=/usr/local/haproxymake install PREFIX=/usr/local/haproxymkdir /etc/haproxy//赋权groupadd -r -g 149 haproxyuseradd -g haproxy -r -s /sbin/nologin -u 149 haproxy//创建haproxy配置文件mkdir /etc/haproxyvim /etc/haproxy/haproxy.cfg 3.5.2 配置HAProxy 配置文件路径：/etc/haproxy/haproxy.cfg 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#logging optionsglobal log 127.0.0.1 local0 info maxconn 5120 chroot /usr/local/haproxy uid 99 gid 99 daemon quiet nbproc 20 pidfile /var/run/haproxy.piddefaults log global mode tcp option tcplog option dontlognull retries 3 option redispatch maxconn 2000 contimeout 5s clitimeout 60s srvtimeout 15s #front-end IP for consumers and producterslisten rabbitmq_cluster bind 0.0.0.0:5672 mode tcp #balance url_param userid #balance url_param session_id check_post 64 #balance hdr(User-Agent) #balance hdr(host) #balance hdr(Host) use_domain_only #balance rdp-cookie #balance leastconn #balance source //ip balance roundrobin server node1 127.0.0.1:5673 check inter 5000 rise 2 fall 2 server node2 127.0.0.1:5674 check inter 5000 rise 2 fall 2listen stats bind 172.16.98.133:8100 mode http option httplog stats enable stats uri /rabbitmq-stats stats refresh 5s 启动HAproxy负载 123456/usr/local/haproxy/sbin/haproxy -f /etc/haproxy/haproxy.cfg//查看haproxy进程状态ps -ef | grep haproxy访问如下地址对mq节点进行监控http://172.16.98.133:8100/rabbitmq-stats 代码中访问mq集群地址，则变为访问haproxy地址:5672 参考文献：黑马视频相关笔记","link":"/2021/03/24/Draft/2021/RabbitMQ/"},{"title":"魑魅先生 | 服务器","text":"Linux常用知识、服务器选择、搭建个人网盘、JAVA开发相关软件配置、博客搭建、域名配置、相关软件推荐。 Linux 常用命令 基本 1234567891011121314关机 shutdown -h now 立刻关机 shutdown -h 5 5分钟后关机 poweroff 立刻关机重启 shutdown -r now 立刻重启 shutdown -r 5 5分钟后重启 reboot 立刻重启--help命令 shutdown --help： ifconfig --help：查看网卡信息man命令（命令说明书） man shutdown 注意：man shutdown打开命令说明书之后，使用按键q退出 文件 新建 touch 文件名 删除 rm -rf 文件名 修改 打开：vi/vim 文件名 i:在光标所在字符前开始插入 a:在光标所在字符后开始插入 o:在光标所在行的下面另起一新行插入 保存文件： 第一步：ESC 进入命令行模式 第二步：: 进入底行模式 第三步：wq 保存并退出编辑 取消编辑： 第一步：ESC 进入命令行模式 第二步：: 进入底行模式 第三步：q! 撤销本次修改并退出编辑 1) 命令行模式command mode） 控制屏幕光标的移动，字符、字或行的删除，查找，移动复制某区段及进入Insert mode下，或者到 last line mode。 命令行模式下的常用命令： 【1】控制光标移动：↑，↓，j 【2】删除当前行：dd 【3】查找：/字符 【4】进入编辑模式：i o a 【5】进入底行模式：: 2) 编辑模式（Insert mode） 只有在Insert mode下，才可以做文字输入，按「ESC」键可回到命令行模式。 编辑模式下常用命令： 【1】ESC 退出编辑模式到命令行模式； 3) 底行模式（last line mode） 将文件保存或退出vi，也可以设置编辑环境，如寻找字符串、列出行号……等。 底行模式下常用命令： 【1】退出编辑： :q 【2】强制退出： :q! 【3】保存并退出： :wq 查看 cat：看最后一屏 示例：使用cat查看/etc/sudo.conf文件，只能显示最后一屏内容 cat sudo.conf more：百分比显示 示例：使用more查看/etc/sudo.conf文件，可以显示百分比，回车可以向下一行，空格可以向下一页，q可以退出查看 more sudo.conf less：翻页查看 示例：使用less查看/etc/sudo.conf文件，可以使用键盘上的PgUp和PgDn向上 和向下翻页，q结束查看 less sudo.conf tail：指定行数或者动态查看 示例：使用tail -10 查看/etc/sudo.conf文件的后10行，Ctrl+C结束 tail -10 sudo.conf 权限 目录 压缩 查找 权限 更改文件属性 进程 系统 网络 部署 Linux简介 Linux，全称GNU/Linux，是一种免费使用和自由传播的类UNIX操作系统 应用 Linux工具 SecureCRT 7.3 rz上传文件到当前目录 sz 文件名下载 yum -y install bash-completion自动补全 tab 启动过程 内核的引导。 电源 BIOS开机自检，按照BIOS中设置的启动设备（通常是硬盘）来启动 操作系统接管硬件以后，首先读入 /boot 目录下的内核文件。 运行 init。 init程序的类型 SysV: init, CentOS 5之前, 配置文件： /etc/inittab。 Upstart: init,CentOS 6, 配置文件： /etc/inittab, /etc/init/*.conf。 Systemd： systemd, CentOS 7,配置文件： /usr/lib/systemd/system、 /etc/systemd/system。 init 进程是系统所有进程的起点，你可以把它比拟成系统所有进程的老祖宗，没有这个进程，系统中任何进程都不会启动。 init 程序首先是需要读取配置文件 /etc/inittab。 运行级别 程序需要开机启动。它们在Windows叫做&quot;服务&quot;（service），在Linux就叫做&quot;守护进程&quot;（daemon）。 init进程的一大任务，就是去运行这些开机启动的程序。 但是，不同的场合需要启动不同的程序，比如用作服务器时，需要启动Apache，用作桌面就不需要。 Linux允许为不同的场合，分配不同的开机启动程序，这就叫做&quot;运行级别&quot;（runlevel）。也就是说，启动时根据&quot;运行级别&quot;，确定要运行哪些程序。 - 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 系统初始化。 si::sysinit:/etc/rc.d/rc.sysinit 它调用执行了/etc/rc.d/rc.sysinit，而rc.sysinit是一个bash shell的脚本，它主要是完成一些系统初始化的工作，rc.sysinit是每一个运行级别都要首先运行的重要脚本。 它主要完成的工作有：激活交换分区，检查磁盘，加载硬件模块以及其它一些需要优先执行任务。 l5:5:wait:/etc/rc.d/rc 5 这一行表示以5为参数运行/etc/rc.d/rc，/etc/rc.d/rc是一个Shell脚本，它接受5作为参数，去执行/etc/rc.d/rc5.d/目录下的所有的rc启动脚本，/etc/rc.d/rc5.d/目录中的这些启动脚本实际上都是一些连接文件，而不是真正的rc启动脚本，真正的rc启动脚本实际上都是放在/etc/rc.d/init.d/目录下。 而这些rc启动脚本有着类似的用法，它们一般能接受start、stop、restart、status等参数。 /etc/rc.d/rc5.d/中的rc启动脚本通常是K或S开头的连接文件，对于以 S 开头的启动脚本，将以start参数来运行。 而如果发现存在相应的脚本也存在K打头的连接，而且已经处于运行态了(以/var/lock/subsys/下的文件作为标志)，则将首先以stop为参数停止这些已经启动了的守护进程，然后再重新运行。 这样做是为了保证是当init改变运行级别时，所有相关的守护进程都将重启。 至于在每个运行级中将运行哪些守护进程，用户可以通过chkconfig或setup中的&quot;System Services&quot;来自行设定。 建立终端 。 rc执行完毕后，返回init。这时基本系统环境已经设置好了，各种守护进程也已经启动了。 init接下来会打开6个终端，以便用户登录系统。在inittab中的以下6行就是定义了6个终端： 1:2345:respawn:/sbin/mingetty tty1 2:2345:respawn:/sbin/mingetty tty2 3:2345:respawn:/sbin/mingetty tty3 4:2345:respawn:/sbin/mingetty tty4 5:2345:respawn:/sbin/mingetty tty5 6:2345:respawn:/sbin/mingetty tty6 从上面可以看出在2、3、4、5的运行级别中都将以respawn方式运行mingetty程序，mingetty程序能打开终端、设置模式。 同时它会显示一个文本登录界面，这个界面就是我们经常看到的登录界面，在这个登录界面中会提示用户输入用户名，而用户输入的用户将作为参数传给login程序来验证用户的身份。 用户登录系统。 一般来说，用户的登录方式有三种： （1）命令行登录 （2）ssh登录 （3）图形界面登录 对于运行级别为5的图形方式用户来说，他们的登录是通过一个图形化的登录界面。登录成功后可以直接进入 KDE、Gnome 等窗口管理器。 而本文主要讲的还是文本方式登录的情况：当我们看到mingetty的登录界面时，我们就可以输入用户名和密码来登录系统了。 Linux 的账号验证程序是 login，login 会接收 mingetty 传来的用户名作为用户名参数。 然后 login 会对用户名进行分析：如果用户名不是 root，且存在 /etc/nologin 文件，login 将输出 nologin 文件的内容，然后退出。 这通常用来系统维护时防止非root用户登录。只有/etc/securetty中登记了的终端才允许 root 用户登录，如果不存在这个文件，则 root 用户可以在任何终端上登录。 /etc/usertty文件用于对用户作出附加访问限制，如果不存在这个文件，则没有其他限制。 图形模式与文字模式的切换方式 Linux预设提供了六个命令窗口终端机让我们来登录。 默认我们登录的就是第一个窗口，也就是tty1，这个六个窗口分别为tty1,tty2 … tty6，你可以按下Ctrl + Alt + F1 ~ F6 来切换它们。 如果你安装了图形界面，默认情况下是进入图形界面的，此时你就可以按Ctrl + Alt + F1 ~ F6来进入其中一个命令窗口界面。 当你进入命令窗口界面后再返回图形界面只要按下Ctrl + Alt + F7 就回来了。 如果你用的vmware 虚拟机，命令窗口切换的快捷键为 Alt + Space + F1~F6. 如果你在图形界面下请按Alt + Shift + Ctrl + F1~F6 切换至命令窗口。 Linux 关机 在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机。 正确的关机流程为：sync &gt; shutdown &gt; reboot &gt; halt 关机指令为：shutdown ，你可以man shutdown 来看一下帮助文档。 例如你可以运行如下命令关机： sync 将数据由内存同步到硬盘中。 shutdown 关机指令，你可以man shutdown 来看一下帮助文档。例如你可以运行如下命令关机： shutdown –h 10 ‘This server will shutdown after 10 mins’ 这个命令告诉大家，计算机将在10分钟后关机，并且会显示在登陆用户的当前屏幕中。 shutdown –h now 立马关机 shutdown –h 20:25 系统会在今天20:25关机 shutdown –h +10 十分钟后关机 shutdown –r now 系统立马重启 shutdown –r +10 系统十分钟后重启 reboot 就是重启，等同于 shutdown –r now halt 关闭系统，等同于shutdown –h now 和 poweroff 最后总结一下，不管是重启系统还是关闭系统，首先要运行 sync 命令，把内存中的数据写到磁盘中。 关机的命令有 shutdown –h now halt poweroff 和 init 0 , 重启系统的命令有 shutdown –r now reboot init 6。 注意事项 严格区分大小写 所有内容以文件形式保存，包括硬件 硬件/dev/sd[a-p] 光盘文件/dec/sr0等 不靠扩展名区分文件类型 有扩展都是给管理员看的 所有存储设备都需要挂载之后才能使用 windows程序不能直接在linux安装运行 服务器不允许关机，只能重启 重启时应该关闭服务 服务器访问高峰不要使用高负载命令 远程配置防火墙时不要把自己踢出服务器 指定合理密码规范并定期更新 合理分配权限 定期备份重要数据和日志 系统目录结构 ls / 查看目录 /bin： bin 是 Binaries (二进制文件) 的缩写, 这个目录存放着最经常使用的命令。 /boot： 这里存放的是启动 Linux 时使用的一些核心文件，包括一些连接文件以及镜像文件。 /dev ： dev 是 Device(设备) 的缩写, 该目录下存放的是 Linux 的外部设备，在 Linux 中访问设备的方式和访问文件的方式是相同的。 /etc： etc 是 Etcetera(等等) 的缩写,这个目录用来存放所有的系统管理所需要的配置文件和子目录。 /home： 用户的主目录，在 Linux 中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的，如上图中的 alice、bob 和 eve。 /lib： lib 是 Library(库) 的缩写这个目录里存放着系统最基本的动态连接共享库，其作用类似于 Windows 里的 DLL 文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found： 这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /media： linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，Linux 会把识别的设备挂载到这个目录下。 /mnt： 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在 /mnt/ 上，然后进入该目录就可以查看光驱里的内容了。 /opt： opt 是 optional(可选) 的缩写，这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /proc： proc 是 Processes(进程) 的缩写，/proc 是一种伪文件系统（也即虚拟文件系统），存储的是当前内核运行状态的一系列特殊文件，这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。 这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器： echo 1 &gt; /proc/sys/net/ipv4/icmp_echo_ignore_all /root： 该目录为系统管理员，也称作超级权限者的用户主目录。 /sbin： s 就是 Super User 的意思，是 Superuser Binaries (超级用户的二进制文件) 的缩写，这里存放的是系统管理员使用的系统管理程序。 /selinux： 这个目录是 Redhat/CentOS 所特有的目录，Selinux 是一个安全机制，类似于 windows 的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。 /srv： 该目录存放一些服务启动之后需要提取的数据。 /sys： 这是 Linux2.6 内核的一个很大的变化。该目录下安装了 2.6 内核中新出现的一个文件系统 sysfs 。 sysfs 文件系统集成了下面3种文件系统的信息：针对进程信息的 proc 文件系统、针对设备的 devfs 文件系统以及针对伪终端的 devpts 文件系统。 该文件系统是内核设备树的一个直观反映。 当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。 /tmp： tmp 是 temporary(临时) 的缩写这个目录是用来存放一些临时文件的。 /usr： usr 是 unix shared resources(共享资源) 的缩写，这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于 windows 下的 program files 目录。 /usr/bin： 系统用户使用的应用程序。 /usr/sbin： 超级用户使用的比较高级的管理程序和系统守护程序。 /usr/src： 内核源代码默认的放置目录。 /var： var 是 variable(变量) 的缩写，这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。 /run： 是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。 在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。 /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。 /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在 /bin/ls 目录下的。 值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给 root 使用的指令。 /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在 /var/log 目录下，另外 mail 的预设放置也是在这里。 忘记密码解决方法https://www.runoob.com/linux/linux-forget-password.html 重启linux系统 3 秒之内要按一下回车 输入e 第二行最后边输入 single，有一个空格。具体方法为按向下尖头移动到第二行，按&quot;e&quot;进入编辑模式 在后边加上single 回车 最后按&quot;b&quot;启动，启动后就进入了单用户模式了 更改root密码了。更密码的命令为 passwd 光盘启动，按F5 进入rescue模式 输入linux rescue 回车 选择英语 选择us 键盘 问你是否启动网络，有时候可能会联网调试。我们选no 这里告诉我们，接下来会把系统挂载在/mnt/sysimage 中。 其中有三个选项: Continue 就是挂载后继续下一步。 Read-Only 挂载成只读，这样更安全，有时文件系统损坏时，只读模式会防止文件系统近一步损坏。 Skip就是不挂载，进入一个命令窗口模式。 这里我们选择Continue。 - 至此，系统已经挂载到了/mnt/sysimage中。接下来回车，输入chroot /mnt/sysimage 进入管理员环境。 远程登录 Linux 一般作为服务器使用，而服务器一般放在机房，你不可能在机房操作你的 Linux 服务器。这时我们就需要远程登录到Linux服务器来管理维护系统。 Linux 系统中是通过 ssh 服务实现的远程登录功能，默认 ssh 服务端口号为 22。 Window 系统上 Linux 远程登录客户端有 SecureCRT, Putty, SSH Secure Shell 等，本文以 Putty 为例来登录远程服务器。 Putty 下载地址：https://www.putty.org/ 在Host Name( or IP address) 下面的框中输入你要登录的远程服务器IP(可以通过ifconfig命令查看服务器ip)，然后回车。 输入要登录的用户名。 再输入密码，就能登录到远程的linux系统了 使用密钥认证机制远程登录linux SSH 为 Secure Shell 的缩写，由 IETF 的网络工作小组（Network Working Group）所制定。 SSH 为建立在应用层和传输层基础上的安全协议。 首先使用工具 PUTTYGEN.EXE 生成密钥对。打开工具 PUTTYGEN.EXE 后如下图所示： 该工具可以生成三种格式的key ：SSH-1(RSA) SSH-2(RSA) SSH-2(DSA) ，我们采用默认的格式即 SSH-2(RSA)。Number of bits in a generated key 这个是指生成的key的大小，这个数值越大，生成的key就越复杂，安全性就越高。这里我们写 2048。 Generate 过程中鼠标要来回的动，否则这个进度条是不会动的。 可以给你的密钥输入一个密码，（在Key Passphrase那里）也可以留空。然后点 Save public key 保存公钥，点 Save private Key 保存私钥。笔者建议你放到一个比较安全的地方，一来防止别人偷窥，二来防止误删除。接下来就该到远程 linux 主机上设置了。 1）创建目录 /root/.ssh 并设置权限 [root@localhost ~]# mkdir /root/.ssh mkdir 命令用来创建目录，以后会详细介绍，暂时只了解即可。 [root@localhost ~]# chmod 700 /root/.ssh chmod 命令是用来修改文件属性权限的，以后会详细介绍。 2）创建文件 / root/.ssh/authorized_keys [root@localhost ~]# vim /root/.ssh/authorized_keys vim 命令是编辑一个文本文件的命令，同样在后续章节详细介绍。 3）打开刚才生成的public key 文件，建议使用写字板打开，这样看着舒服一些，复制从AAAA开头至 &quot;---- END SSH2 PUBLIC KEY ----&quot; 该行上的所有内容，粘贴到/root/.ssh/authorized_keys 文件中，要保证所有字符在一行。（可以先把复制的内容拷贝至记事本，然后编辑成一行载粘贴到该文件中）。 在这里要简单介绍一下，如何粘贴，用vim打开那个文件后，该文件不存在，所以vim会自动创建。按一下字母&quot;i&quot;然后同时按shift + Insert 进行粘贴（或者单击鼠标右键即可），前提是已经复制到剪切板中了。粘贴好后，然后把光标移动到该行最前面输入 ssh-rsa ，然后按空格。再按ESC，然后输入冒号wq 即 :wq 就保存了。格式如下图： - 再设置putty选项，点窗口左侧的SSh –&gt; Auth ，单击窗口右侧的Browse… 选择刚刚生成的私钥， 再点Open ，此时输入root，就不用输入密码就能登录了。如果在前面你设置了Key Passphrase ，那么此时就会提示你输入密码的。为了更加安全建议大家要设置一个Key Passphrase。 文件基本属性 Linux 系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。 为了保护系统的安全性，Linux 系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。 在 Linux 中我们通常使用以下两个命令来修改文件或目录的所属用户与权限： chown (change ownerp) ： 修改所属用户与组。 chmod (change mode) ： 修改用户的权限。 使用 ll 或者 ls –l 命令来显示一个文件的属性以及文件所属的用户和组，如： [root@www /]# ls -l total 64 dr-xr-xr-x 2 root root 4096 Dec 14 2012 bin dr-xr-xr-x 4 root root 4096 Apr 19 2012 boot …… - 实例中，bin 文件的第一个属性用 d 表示。d 在 Linux 中代表该文件是一个目录文件。 在 Linux 中第一个字符代表这个文件是目录、文件或链接文件等等。 当为 d 则是目录 当为 - 则是文件； 若是 l 则表示为链接文档(link file)； 若是 b 则表示为装置文件里面的可供储存的接口设备(可随机存取装置)； 若是 c 则表示为装置文件里面的串行端口设备，例如键盘、鼠标(一次性读取装置)。 - 接下来的字符中，以三个为一组，且均为 rwx 的三个参数的组合。其中， r 代表可读(read)、 w 代表可写(write)、 x 代表可执行(execute)。 要注意的是，这三个权限的位置不会改变，如果没有权限，就会出现减号 - 而已。 - 每个文件的属性由左边第一部分的 10 个字符来确定（如图）。 363003_1227493859FdXT 从左至右用 0-9 这些数字来表示。 第 0 位确定文件类型，第 1-3 位确定属主（该文件的所有者）拥有该文件的权限。 第4-6位确定属组（所有者的同组用户）拥有该文件的权限，第7-9位确定其他用户拥有该文件的权限。 其中，第 1、4、7 位表示读权限，如果用 r 字符表示，则有读权限，如果用 - 字符表示，则没有读权限； 第 2、5、8 位表示写权限，如果用 w 字符表示，则有写权限，如果用 - 字符表示没有写权限；第 3、6、9 位表示可执行权限，如果用 x 字符表示，则有执行权限，如果用 - 字符表示，则没有执行权限。 Linux文件属主和属组 [root@www /]# ls -l total 64 drwxr-xr-x 2 root root 4096 Feb 15 14:46 cron drwxr-xr-x 3 mysql mysql 4096 Apr 21 2014 mysql …… 对于文件来说，它都有一个特定的所有者，也就是对该文件具有所有权的用户。 同时，在Linux系统中，用户是按组分类的，一个用户属于一个或多个组。 文件所有者以外的用户又可以分为文件所有者的同组用户和其他用户。 因此，Linux系统按文件所有者、文件所有者同组用户和其他用户来规定了不同的文件访问权限。 在以上实例中，mysql 文件是一个目录文件，属主和属组都为 mysql，属主有可读、可写、可执行的权限；与属主同组的其他用户有可读和可执行的权限；其他用户也有可读和可执行的权限。 对于 root 用户来说，一般情况下，文件的权限对其不起作用。 更改文件属性 chgrp：更改文件属组 语法：chgrp [-R] 属组名 文件名 参数选项 -R：递归更改文件属组，就是在更改某个目录文件的属组时，如果加上-R的参数，那么该目录下的所有文件的属组都会更改。 chown：更改文件属主，也可以同时更改文件属组 语法： chown [–R] 属主名 文件名 chown [-R] 属主名：属组名 文件名 进入 /root 目录（~）将install.log的拥有者改为bin这个账号： [root@www ~] cd ~ [root@www ~]# chown bin install.log [root@www ~]# ls -l -rw-r--r-- 1 bin users 68495 Jun 25 08:53 install.log 将install.log的拥有者与群组改回为root： [root@www ~]# chown root:root install.log [root@www ~]# ls -l -rw-r--r-- 1 root root 68495 Jun 25 08:53 install.log chmod：更改文件9个属性 Linux文件属性有两种设置方法，一种是数字，一种是符号。 Linux 文件的基本权限就有九个，分别是 owner/group/others(拥有者/组/其他) 三种身份各有自己的 read/write/execute 权限。 先复习一下刚刚上面提到的数据：文件的权限字符为： -rwxrwxrwx ， 这九个权限是三个三个一组的！其中，我们可以使用数字来代表各个权限，各权限的分数对照表如下： r:4 w:2 x:1 每种身份(owner/group/others)各自的三个权限(r/w/x)分数是需要累加的，例如当权限为： -rwxrwx--- 分数则是： owner = rwx = 4+2+1 = 7 group = rwx = 4+2+1 = 7 others= --- = 0+0+0 = 0 所以等一下我们设定权限的变更时，该文件的权限数字就是 770。变更权限的指令 chmod 的语法是这样的： chmod [-R] xyz 文件或目录 选项与参数： xyz : 就是刚刚提到的数字类型的权限属性，为 rwx 属性数值的相加。 -R : 进行递归(recursive)的持续变更，亦即连同次目录下的所有文件都会变更 举例来说，如果要将 .bashrc 这个文件所有的权限都设定启用，那么命令如下： [root@www ~]# ls -al .bashrc -rw-r--r-- 1 root root 395 Jul 4 11:45 .bashrc [root@www ~]# chmod 777 .bashrc [root@www ~]# ls -al .bashrc -rwxrwxrwx 1 root root 395 Jul 4 11:45 .bashrc 那如果要将权限变成 -rwxr-xr-- 呢？那么权限的分数就成为 [4+2+1][4+0+1][4+0+0]=754。 符号类型改变文件权限 还有一个改变权限的方法，从之前的介绍中我们可以发现，基本上就九个权限分别是： user：用户 group：组 others：其他 那么我们就可以使用 u, g, o 来代表三种身份的权限。 此外， a 则代表 all，即全部的身份。读写的权限可以写成 r, w, x，也就是可以使用下表的方式来看： chmod u g o a +(加入) -(除去) =(设定) r w x 文件或目录 如果我们需要将文件权限设置为 -rwxr-xr-- ，可以使用 chmod u=rwx,g=rx,o=r 文件名 来设定: touch test1 // 创建 test1 文件 ls -al test1 // 查看 test1 默认权限 -rw-r--r-- 1 root root 0 Nov 15 10:32 test1 chmod u=rwx,g=rx,o=r test1 // 修改 test1 权限 ls -al test1 -rwxr-xr-- 1 root root 0 Nov 15 10:32 test1 而如果是要将权限去掉而不改变其他已存在的权限呢？例如要拿掉全部人的可执行权限，则： chmod a-x test1 ls -al test1 -rw-r--r-- 1 root root 0 Nov 15 10:32 test1 文件与目录管理 Linux的目录结构为树状结构，最顶级的目录为根目录 /。 其他目录通过挂载可以将它们添加到树中，通过解除挂载可以移除它们。 在开始本教程前我们需要先知道什么是绝对路径与相对路径。 绝对路径： 路径的写法，由根目录 / 写起，例如： /usr/share/doc 这个目录。 相对路径： 路径的写法，不是由 / 写起，例如由 /usr/share/doc 要到 /usr/share/man 底下时，可以写成： cd ../man 这就是相对路径的写法。 处理目录的常用命令 ls（英文全拼：list files）: 列出目录及文件名 cd（英文全拼：change directory）：切换目录 pwd（英文全拼：print work directory）：显示目前的目录 mkdir（英文全拼：make directory）：创建一个新的目录 rmdir（英文全拼：remove directory）：删除一个空的目录 cp（英文全拼：copy file）: 复制文件或目录 rm（英文全拼：remove）: 移除文件或目录 mv（英文全拼：move file）: 移动文件与目录，或修改文件与目录的名称 你可以使用 man [命令] 来查看各个命令的使用文档，如 ：man cp。 ls (列出目录) 在Linux系统当中， ls 命令可能是最常被运行的。 语法： [root@www ~]# ls [-aAdfFhilnrRSt] 目录名称 [root@www ~]# ls [--color={never,auto,always}] 目录名称 [root@www ~]# ls [--full-time] 目录名称 选项与参数： -a ：全部的文件，连同隐藏文件( 开头为 . 的文件) 一起列出来(常用) -d ：仅列出目录本身，而不是列出目录内的文件数据(常用) -l ：长数据串列出，包含文件的属性与权限等等数据；(常用) 将家目录下的所有文件列出来(含属性与隐藏档) [root@www ~]# ls -al ~ cd (切换目录) cd是Change Directory的缩写，这是用来变换工作目录的命令。 语法： cd [相对路径或绝对路径] #使用 mkdir 命令创建 runoob 目录 [root@www ~]# mkdir runoob #使用绝对路径切换到 runoob 目录 [root@www ~]# cd /root/runoob/ #使用相对路径切换到 runoob 目录 [root@www ~]# cd ./runoob/ 表示回到自己的家目录，亦即是 /root 这个目录 [root@www runoob]# cd ~ 表示去到目前的上一级目录，亦即是 /root 的上一级目录的意思； [root@www ~]# cd .. 接下来大家多操作几次应该就可以很好的理解 cd 命令的。 - pwd (显示目前所在的目录) pwd 是 Print Working Directory 的缩写，也就是显示目前所在目录的命令。 [root@www ~]# pwd [-P] 选项与参数： -P ：显示出确实的路径，而非使用连结 (link) 路径。 实例：单纯显示出目前的工作目录： [root@www ~]# pwd /root &lt;== 显示出目录啦～ 实例显示出实际的工作目录，而非连结档本身的目录名而已。 [root@www ~]# cd /var/mail &lt;==注意，/var/mail是一个连结档 [root@www mail]# pwd /var/mail &lt;==列出目前的工作目录 [root@www mail]# pwd -P /var/spool/mail &lt;==怎么回事？有没有加 -P 差很多～ [root@www mail]# ls -ld /var/mail lrwxrwxrwx 1 root root 10 Sep 4 17:54 /var/mail -&gt; spool/mail 看到这里应该知道为啥了吧？因为 /var/mail 是连结档，连结到 /var/spool/mail 所以，加上 pwd -P 的选项后，会不以连结档的数据显示，而是显示正确的完整路径啊！ - mkdir (创建新目录) 如果想要创建新的目录的话，那么就使用mkdir (make directory)吧。 语法： mkdir [-mp] 目录名称 选项与参数： -m ：配置文件的权限喔！直接配置，不需要看默认权限 (umask) 的脸色～ -p ：帮助你直接将所需要的目录(包含上一级目录)递归创建起来！ 实例：请到/tmp底下尝试创建数个新目录看看： [root@www ~]# cd /tmp [root@www tmp]# mkdir test &lt;创建一名为 test 的新目录 [root@www tmp]# mkdir test1/test2/test3/test4 mkdir: cannot create directory `test1/test2/test3/test4': No such file or directory &lt; 没办法直接创建此目录啊！ [root@www tmp]# mkdir -p test1/test2/test3/test4 加了这个 -p 的选项，可以自行帮你创建多层目录！ 实例：创建权限为 rwx--x--x 的目录。 [root@www tmp]# mkdir -m 711 test2 [root@www tmp]# ls -l drwxr-xr-x 3 root root 4096 Jul 18 12:50 test drwxr-xr-x 3 root root 4096 Jul 18 12:53 test1 drwx--x--x 2 root root 4096 Jul 18 12:54 test2 上面的权限部分，如果没有加上 -m 来强制配置属性，系统会使用默认属性。 如果我们使用 -m ，如上例我们给予 -m 711 来给予新的目录 drwx--x--x 的权限。 - rmdir (删除空的目录) 语法： rmdir [-p] 目录名称 选项与参数： -p ：连同上一级『空的』目录也一起删除 删除 runoob 目录 [root@www tmp]# rmdir runoob/ 将 mkdir 实例中创建的目录(/tmp 底下)删除掉！ [root@www tmp]# ls -l &lt;==看看有多少目录存在？ drwxr-xr-x 3 root root 4096 Jul 18 12:50 test drwxr-xr-x 3 root root 4096 Jul 18 12:53 test1 drwx--x--x 2 root root 4096 Jul 18 12:54 test2 [root@www tmp]# rmdir test &lt;==可直接删除掉，没问题 [root@www tmp]# rmdir test1 &lt;==因为尚有内容，所以无法删除！ rmdir: `test1': Directory not empty [root@www tmp]# rmdir -p test1/test2/test3/test4 [root@www tmp]# ls -l &lt;==您看看，底下的输出中test与test1不见了！ drwx--x--x 2 root root 4096 Jul 18 12:54 test2 利用 -p 这个选项，立刻就可以将 test1/test2/test3/test4 一次删除。 不过要注意的是，这个 rmdir 仅能删除空的目录，你可以使用 rm 命令来删除非空目录。 - cp (复制文件或目录) cp 即拷贝文件和目录。 语法: [root@www ~]# cp [-adfilprsu] 来源档(source) 目标档(destination) [root@www ~]# cp [options] source1 source2 source3 .... directory 选项与参数： -a：相当於 -pdr 的意思，至於 pdr 请参考下列说明；(常用) -d：若来源档为连结档的属性(link file)，则复制连结档属性而非文件本身； -f：为强制(force)的意思，若目标文件已经存在且无法开启，则移除后再尝试一次； -i：若目标档(destination)已经存在时，在覆盖时会先询问动作的进行(常用) -l：进行硬式连结(hard link)的连结档创建，而非复制文件本身； -p：连同文件的属性一起复制过去，而非使用默认属性(备份常用)； -r：递归持续复制，用於目录的复制行为；(常用) -s：复制成为符号连结档 (symbolic link)，亦即『捷径』文件； -u：若 destination 比 source 旧才升级 destination ！ 用 root 身份，将 root 目录下的 .bashrc 复制到 /tmp 下，并命名为 bashrc [root@www ~]# cp ~/.bashrc /tmp/bashrc [root@www ~]# cp -i ~/.bashrc /tmp/bashrc cp: overwrite `/tmp/bashrc'? n &lt;==n不覆盖，y为覆盖 - rm (移除文件或目录) 语法： rm [-fir] 文件或目录 选项与参数： -f ：就是 force 的意思，忽略不存在的文件，不会出现警告信息； -i ：互动模式，在删除前会询问使用者是否动作 -r ：递归删除啊！最常用在目录的删除了！这是非常危险的选项！！！ 将刚刚在 cp 的实例中创建的 bashrc 删除掉！ [root@www tmp]# rm -i bashrc rm: remove regular file `bashrc'? y 如果加上 -i 的选项就会主动询问喔，避免你删除到错误的档名！ - mv (移动文件与目录，或修改名称) 语法： [root@www ~]# mv [-fiu] source destination [root@www ~]# mv [options] source1 source2 source3 .... directory 选项与参数： -f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖； -i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！ -u ：若目标文件已经存在，且 source 比较新，才会升级 (update) 复制一文件，创建一目录，将文件移动到目录中 [root@www ~]# cd /tmp [root@www tmp]# cp ~/.bashrc bashrc [root@www tmp]# mkdir mvtest [root@www tmp]# mv bashrc mvtest 将某个文件移动到某个目录去，就是这样做！ 将刚刚的目录名称更名为 mvtest2 [root@www tmp]# mv mvtest mvtest2 - Linux 文件内容查看 Linux系统中使用以下命令来查看文件的内容： cat 由第一行开始显示文件内容 tac 从最后一行开始显示，可以看出 tac 是 cat 的倒着写！ nl 显示的时候，顺道输出行号！ more 一页一页的显示文件内容 less 与 more 类似，但是比 more 更好的是，他可以往前翻页！ head 只看头几行 tail 只看尾巴几行 你可以使用 man [命令]来查看各个命令的使用文档，如 ：man cp。 - cat 由第一行开始显示文件内容 语法： cat [-AbEnTv] 选项与参数： -A ：相当於 -vET 的整合选项，可列出一些特殊字符而不是空白而已； -b ：列出行号，仅针对非空白行做行号显示，空白行不标行号！ -E ：将结尾的断行字节 $ 显示出来； -n ：列印出行号，连同空白行也会有行号，与 -b 的选项不同； -T ：将 [tab] 按键以 ^I 显示出来； -v ：列出一些看不出来的特殊字符 检看 /etc/issue 这个文件的内容： [root@www ~]# cat /etc/issue CentOS release 6.4 (Final) Kernel \\r on an \\m - tac tac与cat命令刚好相反，文件内容从最后一行开始显示，可以看出 tac 是 cat 的倒着写！如： [root@www ~]# tac /etc/issue Kernel \\r on an \\m CentOS release 6.4 (Final) - nl 显示行号 语法： nl [-bnw] 文件 选项与参数： -b ：指定行号指定的方式，主要有两种： -b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)； -b t ：如果有空行，空的那一行不要列出行号(默认值)； -n ：列出行号表示的方法，主要有三种： -n ln ：行号在荧幕的最左方显示； -n rn ：行号在自己栏位的最右方显示，且不加 0 ； -n rz ：行号在自己栏位的最右方显示，且加 0 ； -w ：行号栏位的占用的位数。 实例一：用 nl 列出 /etc/issue 的内容 [root@www ~]# nl /etc/issue 1 CentOS release 6.4 (Final) 2 Kernel \\r on an \\m - more 一页一页翻动 [root@www ~]# more /etc/man_db.config Generated automatically from man.conf.in by the configure script. man.conf from man-1.6d ....(中间省略).... --More--(28%) &lt;== 重点在这一行喔！你的光标也会在这里等待你的命令 在 more 这个程序的运行过程中，你有几个按键可以按的： 空白键 (space)：代表向下翻一页； Enter ：代表向下翻『一行』； /字串 ：代表在这个显示的内容当中，向下搜寻『字串』这个关键字； :f ：立刻显示出档名以及目前显示的行数； q ：代表立刻离开 more ，不再显示该文件内容。 b 或 [ctrl]-b ：代表往回翻页，不过这动作只对文件有用，对管线无用。 - less 一页一页翻动，以下实例输出/etc/man.config文件的内容： [root@www ~]# less /etc/man.config Generated automatically from man.conf.in by the configure script. man.conf from man-1.6d ....(中间省略).... &lt;== 这里可以等待你输入命令！ less运行时可以输入的命令有： 空白键 ：向下翻动一页； [pagedown]：向下翻动一页； [pageup] ：向上翻动一页； /字串 ：向下搜寻『字串』的功能； ?字串 ：向上搜寻『字串』的功能； n ：重复前一个搜寻 (与 / 或 ? 有关！) N ：反向的重复前一个搜寻 (与 / 或 ? 有关！) q ：离开 less 这个程序； - head 取出文件前面几行 语法： head [-n number] 文件 选项与参数： -n ：后面接数字，代表显示几行的意思 [root@www ~]# head /etc/man.config 默认的情况中，显示前面 10 行！若要显示前 20 行，就得要这样： [root@www ~]# head -n 20 /etc/man.config - tail 取出文件后面几行 语法： tail [-n number] 文件 选项与参数： -n ：后面接数字，代表显示几行的意思 -f ：表示持续侦测后面所接的档名，要等到按下[ctrl]-c才会结束tail的侦测 [root@www ~]# tail /etc/man.config 默认的情况中，显示最后的十行！若要显示最后的 20 行，就得要这样： [root@www ~]# tail -n 20 /etc/man.config 用户和用户组管理 Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 每个用户账号都拥有一个唯一的用户名和各自的口令。 用户在登录时键入正确的用户名和口令后，就能够进入系统和自己的主目录。 实现用户账号的管理，要完成的工作主要有如下几个方面： 用户账号的添加、删除与修改。 用户口令的管理。 用户组的管理。 一、Linux系统用户账号的管理 用户账号的管理工作主要涉及到用户账号的添加、修改和删除。 添加用户账号就是在系统中创建一个新账号，然后为新账号分配用户号、用户组、主目录和登录Shell等资源。刚添加的账号是被锁定的，无法使用。 1、添加新的用户账号使用useradd命令，其语法如下： useradd 选项 用户名 参数说明： 选项: -c comment 指定一段注释性描述。 -d 目录 指定用户主目录，如果此目录不存在，则同时使用-m选项，可以创建主目录。 -g 用户组 指定用户所属的用户组。 -G 用户组，用户组 指定用户所属的附加组。 -s Shell文件 指定用户的登录Shell。 -u 用户号 指定用户的用户号，如果同时有-o选项，则可以重复使用其他用户的标识号。 用户名: 指定新账号的登录名。 实例1 useradd –d /home/sam -m sam 此命令创建了一个用户sam，其中-d和-m选项用来为登录名sam产生一个主目录 /home/sam（/home为默认的用户主目录所在的父目录）。 实例2 useradd -s /bin/sh -g group –G adm,root gem 此命令新建了一个用户gem，该用户的登录Shell是 /bin/sh，它属于group用户组，同时又属于adm和root用户组，其中group用户组是其主组。 这里可能新建组：#groupadd group及groupadd adm 增加用户账号就是在/etc/passwd文件中为新用户增加一条记录，同时更新其他系统文件如/etc/shadow, /etc/group等。 Linux提供了集成的系统管理工具userconf，它可以用来对用户账号进行统一管理。 2、删除帐号 如果一个用户的账号不再使用，可以从系统中删除。删除用户账号就是要将/etc/passwd等系统文件中的该用户记录删除，必要时还删除用户的主目录。 删除一个已有的用户账号使用userdel命令，其格式如下： userdel 选项 用户名 常用的选项是 -r，它的作用是把用户的主目录一起删除。 例如： userdel -r sam 此命令删除用户sam在系统文件中（主要是/etc/passwd, /etc/shadow, /etc/group等）的记录，同时删除用户的主目录。 3、修改帐号 修改用户账号就是根据实际情况更改用户的有关属性，如用户号、主目录、用户组、登录Shell等。 修改已有用户的信息使用usermod命令，其格式如下： usermod 选项 用户名 常用的选项包括-c, -d, -m, -g, -G, -s, -u以及-o等，这些选项的意义与useradd命令中的选项一样，可以为用户指定新的资源值。 另外，有些系统可以使用选项：-l 新用户名 这个选项指定一个新的账号，即将原来的用户名改为新的用户名。 例如： usermod -s /bin/ksh -d /home/z –g developer sam 此命令将用户sam的登录Shell修改为ksh，主目录改为/home/z，用户组改为developer。 4、用户口令的管理 用户管理的一项重要内容是用户口令的管理。用户账号刚创建时没有口令，但是被系统锁定，无法使用，必须为其指定口令后才可以使用，即使是指定空口令。 指定和修改用户口令的Shell命令是passwd。超级用户可以为自己和其他用户指定口令，普通用户只能用它修改自己的口令。命令的格式为： passwd 选项 用户名 可使用的选项： -l 锁定口令，即禁用账号。 -u 口令解锁。 -d 使账号无口令。 -f 强迫用户下次登录时修改口令。 如果默认用户名，则修改当前用户的口令。 例如，假设当前用户是sam，则下面的命令修改该用户自己的口令： $ passwd Old password:****** New password:******* Re-enter new password:******* 如果是超级用户，可以用下列形式指定任何用户的口令： passwd sam New password:******* Re-enter new password:******* 普通用户修改自己的口令时，passwd命令会先询问原口令，验证后再要求用户输入两遍新口令，如果两次输入的口令一致，则将这个口令指定给用户；而超级用户为用户指定口令时，就不需要知道原口令。 为了系统安全起见，用户应该选择比较复杂的口令，例如最好使用8位长的口令，口令中包含有大写、小写字母和数字，并且应该与姓名、生日等不相同。 为用户指定空口令时，执行下列形式的命令： passwd -d sam 此命令将用户 sam 的口令删除，这样用户 sam 下一次登录时，系统就不再允许该用户登录了。 passwd 命令还可以用 -l(lock) 选项锁定某一用户，使其不能登录，例如： passwd -l sam 二、Linux系统用户组的管理 每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 1、增加一个新的用户组使用groupadd命令。其格式如下： groupadd 选项 用户组 可以使用的选项有： -g GID 指定新用户组的组标识号（GID）。 -o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。 实例1： groupadd group1 此命令向系统中增加了一个新组group1，新组的组标识号是在当前已有的最大组标识号的基础上加1。 实例2： groupadd -g 101 group2 此命令向系统中增加了一个新组group2，同时指定新组的组标识号是101。 2、如果要删除一个已有的用户组，使用groupdel命令，其格式如下： groupdel 用户组 例如： groupdel group1 此命令从系统中删除组group1。 3、修改用户组的属性使用groupmod命令。其语法如下： groupmod 选项 用户组 常用的选项有： -g GID 为用户组指定新的组标识号。 -o 与-g选项同时使用，用户组的新GID可以与系统已有用户组的GID相同。 -n新用户组 将用户组的名字改为新名字 实例1： groupmod -g 102 group2 此命令将组group2的组标识号修改为102。 实例2： groupmod –g 10000 -n group3 group2 此命令将组group2的标识号改为10000，组名修改为group3。 4、如果一个用户同时属于多个用户组，那么用户可以在用户组之间切换，以便具有其他用户组的权限。 用户可以在登录后，使用命令newgrp切换到其他用户组，这个命令的参数就是目的用户组。例如： $ newgrp root 这条命令将当前用户切换到root用户组，前提条件是root用户组确实是该用户的主组或附加组。类似于用户账号的管理，用户组的管理也可以通过集成的系统管理工具来完成。 三、与用户账号有关的系统文件 完成用户管理的工作有许多种方法，但是每一种方法实际上都是对有关的系统文件进行修改。 与用户和用户组相关的信息都存放在一些系统文件中，这些文件包括/etc/passwd, /etc/shadow, /etc/group等。 下面分别介绍这些文件的内容。 1、/etc/passwd文件是用户管理工作涉及的最重要的一个文件。 Linux系统中的每个用户都在/etc/passwd文件中有一个对应的记录行，它记录了这个用户的一些基本属性。 这个文件对所有用户都是可读的。它的内容类似下面的例子： ＃ cat /etc/passwd root❌0:0:Superuser:/: daemon❌1:1:System daemons:/etc: bin❌2:2:Owner of system commands:/bin: sys❌3:3:Owner of system files:/usr/sys: adm❌4:4:System accounting:/usr/adm: uucp❌5:5:UUCP administrator:/usr/lib/uucp: auth❌7:21:Authentication administrator:/tcb/files/auth: cron❌9:16:Cron daemon:/usr/spool/cron: listen❌37:4:Network daemon:/usr/net/nls: lp❌71:18:Printer administrator:/usr/spool/lp: sam❌200:50:Sam san:/home/sam:/bin/sh 从上面的例子我们可以看到，/etc/passwd中一行记录对应着一个用户，每行记录又被冒号(:)分隔为7个字段，其格式和具体含义如下： 用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell 1）&quot;用户名&quot;是代表用户账号的字符串。 通常长度不超过8个字符，并且由大小写字母和/或数字组成。登录名中不能有冒号(😃，因为冒号在这里是分隔符。 为了兼容起见，登录名中最好不要包含点字符(.)，并且不使用连字符(-)和加号(+)打头。 2）“口令”一些系统中，存放着加密后的用户口令字。 虽然这个字段存放的只是用户口令的加密串，不是明文，但是由于/etc/passwd文件对所有用户都可读，所以这仍是一个安全隐患。因此，现在许多Linux 系统（如SVR4）都使用了shadow技术，把真正的加密后的用户口令字存放到/etc/shadow文件中，而在/etc/passwd文件的口令字段中只存放一个特殊的字符，例如“x”或者“*”。 3）“用户标识号”是一个整数，系统内部用它来标识用户。 一般情况下它与用户名是一一对应的。如果几个用户名对应的用户标识号是一样的，系统内部将把它们视为同一个用户，但是它们可以有不同的口令、不同的主目录以及不同的登录Shell等。 通常用户标识号的取值范围是0～65 535。0是超级用户root的标识号，1～99由系统保留，作为管理账号，普通用户的标识号从100开始。在Linux系统中，这个界限是500。 4）“组标识号”字段记录的是用户所属的用户组。 它对应着/etc/group文件中的一条记录。 5)“注释性描述”字段记录着用户的一些个人情况。 例如用户的真实姓名、电话、地址等，这个字段并没有什么实际的用途。在不同的Linux 系统中，这个字段的格式并没有统一。在许多Linux系统中，这个字段存放的是一段任意的注释性描述文字，用做finger命令的输出。 6)“主目录”，也就是用户的起始工作目录。 它是用户在登录到系统之后所处的目录。在大多数系统中，各用户的主目录都被组织在同一个特定的目录下，而用户主目录的名称就是该用户的登录名。各用户对自己的主目录有读、写、执行（搜索）权限，其他用户对此目录的访问权限则根据具体情况设置。 7)用户登录后，要启动一个进程，负责将用户的操作传给内核，这个进程是用户登录到系统后运行的命令解释器或某个特定的程序，即Shell。 Shell是用户与Linux系统之间的接口。Linux的Shell有许多种，每种都有不同的特点。常用的有sh(Bourne Shell), csh(C Shell), ksh(Korn Shell), tcsh(TENEX/TOPS-20 type C Shell), bash(Bourne Again Shell)等。 系统管理员可以根据系统情况和用户习惯为用户指定某个Shell。如果不指定Shell，那么系统使用sh为默认的登录Shell，即这个字段的值为/bin/sh。 用户的登录Shell也可以指定为某个特定的程序（此程序不是一个命令解释器）。 利用这一特点，我们可以限制用户只能运行指定的应用程序，在该应用程序运行结束后，用户就自动退出了系统。有些Linux 系统要求只有那些在系统中登记了的程序才能出现在这个字段中。 8)系统中有一类用户称为伪用户（pseudo users）。 这些用户在/etc/passwd文件中也占有一条记录，但是不能登录，因为它们的登录Shell为空。它们的存在主要是方便系统管理，满足相应的系统进程对文件属主的要求。 常见的伪用户如下所示： 伪 用 户 含 义 bin 拥有可执行的用户命令文件 sys 拥有系统文件 adm 拥有帐户文件 uucp UUCP使用 lp lp或lpd子系统使用 nobody NFS使用 拥有帐户文件 1、除了上面列出的伪用户外，还有许多标准的伪用户，例如：audit, cron, mail, usenet等，它们也都各自为相关的进程和文件所需要。 由于/etc/passwd文件是所有用户都可读的，如果用户的密码太简单或规律比较明显的话，一台普通的计算机就能够很容易地将它破解，因此对安全性要求较高的Linux系统都把加密后的口令字分离出来，单独存放在一个文件中，这个文件是/etc/shadow文件。 有超级用户才拥有该文件读权限，这就保证了用户密码的安全性。 2、/etc/shadow中的记录行与/etc/passwd中的一一对应，它由pwconv命令根据/etc/passwd中的数据自动产生 它的文件格式与/etc/passwd类似，由若干个字段组成，字段之间用&quot;:&quot;隔开。这些字段是： 登录名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志 &quot;登录名&quot;是与/etc/passwd文件中的登录名相一致的用户账号 &quot;口令&quot;字段存放的是加密后的用户口令字，长度为13个字符。如果为空，则对应用户没有口令，登录时不需要口令；如果含有不属于集合 { ./0-9A-Za-z }中的字符，则对应的用户不能登录。 &quot;最后一次修改时间&quot;表示的是从某个时刻起，到用户最后一次修改口令时的天数。时间起点对不同的系统可能不一样。例如在SCO Linux 中，这个时间起点是1970年1月1日。 &quot;最小时间间隔&quot;指的是两次修改口令之间所需的最小天数。 &quot;最大时间间隔&quot;指的是口令保持有效的最大天数。 &quot;警告时间&quot;字段表示的是从系统开始警告用户到用户密码正式失效之间的天数。 &quot;不活动时间&quot;表示的是用户没有登录活动但账号仍能保持有效的最大天数。 &quot;失效时间&quot;字段给出的是一个绝对的天数，如果使用了这个字段，那么就给出相应账号的生存期。期满后，该账号就不再是一个合法的账号，也就不能再用来登录了。 下面是/etc/shadow的一个例子： ＃ cat /etc/shadow root:Dnakfw28zf38w:8764:0:168:7::: daemon:::0:0:::: bin:::0:0:::: sys:::0:0:::: adm:::0:0:::: uucp:::0:0:::: nuucp:::0:0:::: auth:::0:0:::: cron:::0:0:::: listen:::0:0:::: lp:::0:0:::: sam:EkdiSECLWPdSa:9740:0:0:::: 3、用户组的所有信息都存放在/etc/group文件中。 将用户分组是Linux 系统中对用户进行管理及控制访问权限的一种手段。 每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不同的组。 当一个用户同时是多个组中的成员时，在/etc/passwd文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。 用户要访问属于附加组的文件时，必须首先使用newgrp命令使自己成为所要访问的组中的成员。 用户组的所有信息都存放在/etc/group文件中。此文件的格式也类似于/etc/passwd文件，由冒号(:)隔开若干个字段，这些字段有： 组名:口令:组标识号:组内用户列表 &quot;组名&quot;是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。 &quot;口令&quot;字段存放的是用户组加密后的口令字。一般Linux 系统的用户组都没有口令，即这个字段一般为空，或者是*。 &quot;组标识号&quot;与用户标识号类似，也是一个整数，被系统内部用来标识组。 &quot;组内用户列表&quot;是属于这个组的所有用户的列表/b]，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。 /etc/group文件的一个例子如下： root::0:root bin::2:root,bin sys::3:root,uucp adm::4:root,adm daemon::5:root,daemon lp::7:root,lp users::20:root,sam 四、添加批量用户 添加和删除用户对每位Linux系统管理员都是轻而易举的事，比较棘手的是如果要添加几十个、上百个甚至上千个用户时，我们不太可能还使用useradd一个一个地添加，必然要找一种简便的创建大量用户的方法。Linux系统提供了创建大量用户的工具，可以让您立即创建大量用户，方法如下： （1）先编辑一个文本用户文件。 每一列按照/etc/passwd密码文件的格式书写，要注意每个用户的用户名、UID、宿主目录都不可以相同，其中密码栏可以留做空白或输入x号。一个范例文件user.txt内容如下： user001::600💯user:/home/user001:/bin/bash user002::601💯user:/home/user002:/bin/bash user003::602💯user:/home/user003:/bin/bash user004::603💯user:/home/user004:/bin/bash user005::604💯user:/home/user005:/bin/bash user006::605💯user:/home/user006:/bin/bash （2）以root身份执行命令 /usr/sbin/newusers，从刚创建的用户文件user.txt中导入数据，创建用户： newusers &lt; user.txt 然后可以执行命令 vipw 或 vi /etc/passwd 检查 /etc/passwd 文件是否已经出现这些用户的数据，并且用户的宿主目录是否已经创建。 （3）执行命令/usr/sbin/pwunconv。 将 /etc/shadow 产生的 shadow 密码解码，然后回写到 /etc/passwd 中，并将/etc/shadow的shadow密码栏删掉。这是为了方便下一步的密码转换工作，即先取消 shadow password 功能。 pwunconv （4）编辑每个用户的密码对照文件。 格式为： 用户名:密码 实例文件 passwd.txt 内容如下： user001:123456 user002:123456 user003:123456 user004:123456 user005:123456 user006:123456 （5）以 root 身份执行命令 /usr/sbin/chpasswd。 创建用户密码，chpasswd 会将经过 /usr/bin/passwd 命令编码过的密码写入 /etc/passwd 的密码栏。 chpasswd &lt; passwd.txt （6）确定密码经编码写入/etc/passwd的密码栏后。 执行命令 /usr/sbin/pwconv 将密码编码为 shadow password，并将结果写入 /etc/shadow。 pwconv 这样就完成了大量用户的创建了，之后您可以到/home下检查这些用户宿主目录的权限设置是否都正确，并登录验证用户密码是否正确。 Linux磁盘管理 Linux磁盘管理好坏直接关系到整个系统的性能问题。 Linux磁盘管理常用三个命令为df，du和fdisk。 df：列出文件系统的整体磁盘使用量 du：检查磁盘空间使用量 fdisk：用于磁盘分区 df df命令参数功能：检查文件系统的磁盘空间占用情况。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 语法： df [ -ahikHTm ] [目录或文件名] 选项与参数： -a：列出所有的文件系统，包括系统特有的/ proc等文件系统； -k：以KBytes的容量显示各文件系统； -m：以MBytes的容量显示各文件系统； -h：以人们较易阅读的GB，MB，KB等格式自行显示； -H：以M = 1000K取代M = 1024K的进位方式； -T：显示文件系统类型，并合并该分区的文件系统名称（例如ext3）也列出； -i：不用硬盘容量，而以inode的数量来显示 实例1 将系统内所有的文件系统列出来！ [根@ WWW 〜]＃DF 文件系统1K -块 用于推介使用％安装上 / dev的/ HDC2 9920624 3823112 5585444 41 ％/的/ dev / hdc3上 4956316 141376 4559108 4 ％/家 / dev的/ hdc1分区 101086 11126 84741 12 ％/引导 tmpfs 371332 0 371332 0 ％/ dev / shm 在Linux底下如果df没有加任何选项，那么替换重定向系统内部的所有（排除特殊内存内部的文件系统与swap）都以1 KB的容量来列出来！ 实例2 将容量结果以易读的容量格式显示出来 [根@ WWW 〜]＃DF - ħ 文件系统大小用于库存状况使用％安装上 / dev的/ HDC2 9.5克3.7G 5.4G 41 ％/的/ dev / hdc3上 4.8G 139M 4.4G 4 ％/家 / dev的/ hdc1分区 99M 11M 83M 12 ％/开机 tmpfs 363M 0 363M 0 ％/ dev / shm 实例3 将系统内的所有特殊文件格式及名称都列出来 [根@ WWW 〜]＃DF -在 文件系统类型1K -块 用于推介使用％安装上 / dev的/ HDC2 EXT3 9920624 3823112 5585444 41 ％/ PROC PROC 0 0 0 - / PROC sysfs中的sysfs 0 0 0 - / SYS devpts devpts 0 0 0 -的/ dev / PTS / dev的/位于hdc3 EXT3 4956316 141376 4559108 4 ％/家 / dev的/ hdc1分区EXT3 101086 11126 84741 12 ％/引导 tmpfs tmpfs 371332 0 371332 0 ％/ dev / shm 没有binfmt_misc 0 0 0 -的/ proc / SYS / FS / binfmt_misc sunrpc rpc_pipefs 0 0 0 -在/ var / lib中/ NFS / rpc_pipefs 实例4 将/ etc底下的可用的磁盘容量以易读的容量格式显示 [根@ WWW 〜]＃DF - ħ /等 文件系统大小用于库存状况使用％安装上 / dev的/ HDC2 9.5克3.7G 5.4G 41 ％/ du Linux du命令也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的，这里介绍Linux du命令。 语法： du [ -ahskm ]文件或目录名称 选项与参数： -a：列出所有的文件与目录容量，因为唯一仅统计目录底下的文件量而已。 -h：以人们较易读的容量格式（G / M）显示； -s：列出总数而已，而不列出每个各别的目录占用容量； -S：不包括子目录下的总计，与-s有点区别。 -k：以KBytes列出容量显示； -m：以MBytes列出容量显示； 实例1 只列出当前目录下的所有文件夹容量（包括隐藏文件夹）： [根@ WWW 〜]＃杜 8 ./ TEST4 &lt;==每个目录都会列出来8 ./ TEST2 ....中间省略.... 12 ./。gconfd &lt;==包括隐藏文件的目录220 。&lt;==这个目录（。）所占用的大量 直接输入du没有加任何选项时，则du会分析当前所在目录的文件与目录所占用的硬盘空间。 实例2 将文件的容量也列出来 [根@ WWW 〜]＃杜-一个 12 ./安装。日志。syslog &lt;==有文件的列表了8 ./。bash_logout 8 ./ test4 8 ./ test2 ....中间省略.... 12 ./。gconfd 220 。 实例3 检查根目录底下每个目录所占用的容量 [ root @ www〜]＃du - sm / * 7 /箱 6 /启动 .....中间省略.... 0 /进程 .....中间省略.... 1 /转 3859 / usr &lt;==系统初期最大就是他了啦！ 77 / var 通配符*来代表每个目录。 与df不一样的是，du这个命令实际上会直接到文件系统内去搜寻所有的文件数据。 fdisk fdisk是Linux的磁盘分区表操作工具。 语法： fdisk [ -l ]装置名称 选项与参数： -l：输出后面接的装置所有的分区内容。若仅有fdisk -l时，则系统将会把整个系统内部能够搜寻到的装置的分区均列出来。 实例1 列出所有分区信息 [ root @ AY120919111755c246621 tmp ]＃fdisk - l 磁盘/ dev / xvda ：21.5 GB ，21474836480字节 255磁头，63扇区/磁道，2610个柱面 单位=柱面16065 * 512 = 8225280字节 扇区大小（逻辑/物理）：512字节/ 512字节 I / O大小（最小/最佳）：512字节/ 512字节 磁盘标识符：0x00000000 设备引导启动结束块ID系统/ dev / xvda1 * 1 2550 20480000 83 Linux / dev / xvda2 2550 2611 490496 82 Linux swap / Solaris ​ 磁盘/ dev / xvdb ：21.5 GB ，21474836480字节 255磁头，63扇区/磁道，2610个柱面 单位=柱面16065 * 512 = 8225280字节 扇区大小（逻辑/物理）：512字节/ 512字节 I / O大小（最小/最佳）：512字节/ 512字节 磁盘标识符：0x56f40944 设备启动开始端块ID系统/ dev / xvdb2 1 2610 20964793 + 83 Linux 实例2 找出您系统中的根目录所在磁盘，并查阅该硬盘内的相关信息 [根@ WWW 〜]＃DF / &lt;==注意：重点在找出磁盘文件名而已文件系统1K -块 用于推介使用％安装上 / dev的/ HDC2 9920624 3823168 5585388 41 ％/ [根@ WWW 〜]＃的fdisk /开发/ HDC &lt;==仔细看，不要加上数字喔！的汽缸数为这盘被设置到5005有是没有错与那个，但是这是大于1024 ，并且可以在某些设置会导致问题与：1 ）软件，在系统启动时运行（ē 。摹。老版本的LILO ）2 ）引导和分区软件从其它操作系统（É 。克。，DOS FDISK ，OS / 2 FDISK ） 命令（m为帮助）：&lt;==等待你的输入！ 输入m后，就会看到底下这些命令介绍 命令（m为帮助）：m &lt;==输入m后，就会看到底下这些命令介绍Command action 切换可启动标志 b编辑bsd disklabel c切换dos兼容性标志 d 删除分区 &lt;==删除一个分区 l列出已知的分区类型 m 打印此菜单 n添加一个新分区 &lt;==添加一个分区 o创建一个新的空DOS分区表 p 打印分区表 &lt;==在屏幕上显示分区表 q退出而不保存更改&lt;==不保存 离开fdisk程序 创建一个新的空Sun disklabel t更改分区的系统ID u更改显示/输入单位 v验证分区表 w将表写入磁盘并退出&lt;==将刚刚的动作写入分割表 x额外功能（仅限专家） 离开fdisk时点击q，那么所有的动作都不会生效！相反的，按下w就是动作生效的意思。 命令（m为帮助）：p &lt;==这里可以输出当前磁盘的状态 磁盘/ dev / hdc ：41.1 GB ，41174138880字节 &lt;==这个磁盘的文件名与容量255个磁头，63个扇区/磁道，5005个磁柱 &lt;==磁头，增大与磁柱大小单位= 16065 * 512磁柱= 8225280字节&lt;==每个磁柱的大小 ​ 设备引导启动结束块ID系统/ dev / hdc1 * 1 13 104391 83 Linux / dev / hdc2 14 1288 10241437 + 83 Linux / dev / hdc3 1289 1925 5116702 + 83 Linux / dev / hdc4 1926 5005 24740100 5扩展/ dev / hdc5 1926 2052 1020096 82 Linux swap / Solaris ＃装置文件名启动区否开始磁柱结束磁柱1K大小容量磁盘分区槽内的系统 命令（m表示帮助）：q 想要不储存离开吗？点击q就对了！不要随便按w啊！ 使用p可以列出目前这颗磁盘的分割表信息，这个信息的上半部在显示整体磁盘的状态。 磁盘格式化 磁盘分割完成后自然就是要进行文件系统的格式化，格式化的命令非常的简单，使用mkfs（使文件系统）命令。 语法： mkfs [ -t文件系统格式]装置文件名 选项与参数： -t：可以接文件系统格式，例如ext3，ext2，vfat等（系统有支持才会生效） 实例1 查看mkfs支持的文件格式 [ root @ www〜] ＃mkfs [标签] [标签] mkfs mkfs 。cramfs mkfs 。ext2 mkfs 。ext3 mkfs 。msdos mkfs 。胖子 按下两个[tab]，会发现mkfs支持的文件格式如上所示。 实例2 将分区/ dev / hdc6（可指定你自己的分区）格式化为ext3文件系统： [ root @ www〜]＃mkfs - t ext3 / dev / hdc6 mke2fs 1.39 （29 - May - 2006 ）文件系统标签= &lt;==这里指的是分割槽的名称（label ） 操作系统类型：Linux块大小= 4096 （log = 2 ）&lt;==块的大小配置为4K片段大小= 4096 （日志= 2 ）251392个i节点，502023块 &lt;==由此配置决定的索引节点/块数量 25101块（5.00 ％）保留用于该超级用户 首先数据块= 0最大文件系统块= 515899392个16块组 32768个每块组，32768个每片段组15712个每索引节点组的超级块存储在块的备份：32768 ，98304 ，163840 ，229376 ，294912 ​ 编写inode表：完成创建日志（8192个块）：完成&lt;==有日志记录编写超级块和文件系统记帐信息：完成 该文件系统将每34坐骑或180天自动检查一次，以先到者为准。使用tune2fs - ç或-我要重写。＃这样就创建了我们所需要的Ext3文件系统了！简单明了！ 磁盘检验 fsck（文件系统检查）用来检查和维护二者的文件系统。 若系统掉电或磁盘发生问题，可利用fsck命令对文件系统进行检查。 语法： fsck [ -t文件系统] [ -ACay ]装置名称 选项与参数： -t：给定档案系统的型式，若在/ etc / fstab中已有定义或kernel本身已支持的则不需加上此参数 -s：依序一个一个地执行fsck的指令来检查 -A：对/ etc / fstab中所有列出来的分区（partition）做检查 -C：显示完整的检查进度 -d：打印出e2fsck的调试结果 -p：同时有-A条件时，同时有多个fsck的检查一起执行 -R：同时有-A条件时，省略/不检查 -V：详细显示模式 -a：如果检查有错则自动修复 -r：如果检查有错则由使用者回答是否恢复 -y：选项指定检测每个文件是自动输入yes，在不确定那些是不正常的时候，可以执行＃fsck -y全部检查修复。 实例1 查看系统有多少文件系统支持的fsck命令： [ root @ www〜] ＃fsck [标签] [标签] fsck fsck 。cramfs fsck 。ext2 fsck 。ext3 fsck 。msdos fsck 。胖子 实例2 强制检测/ dev / hdc6分区： [ root @ www〜]＃fsck - C - f - t ext3 / dev / hdc6 fsck的1.39 （29 -月- 2006年） 用e2fsck 1.39 （29 -月- 2006年）通1 ：检查索引节点，块，和尺寸 通行证2 ：检查目录结构 通行证3 ：检查目录的连接 通4 ：检查引用计数 传递5 ：检查组 摘要信息 vbird_logical ：11 /二十五万一千九百六十八文件（9.1 ％非-连续的），36926 /一百万四千零四十六块 如果没有加上-f的选项，则由于该文件系统不曾出现问题，检查的经过非常快速！若加上-f强制检查，只会有一项的显示过程。 磁盘挂载与卸除 Linux的磁盘挂载使用mount命令，卸载使用umount命令。 磁盘挂载语法： mount [ -t文件系统] [ -L标签名] [ -o其他选项] [ -n ]装置文件名挂载点 实例1 用最小的方式，将刚刚创建的/ dev / hdc6挂载到/ mnt / hdc6上面！ [根@ WWW 〜]＃MKDIR / MNT / hdc6 [根@ WWW 〜]＃安装/ dev的/ hdc6 / MNT / hdc6 [根@ WWW 〜]＃DF 文件系统1K -块 用于推介使用％安装上 .... 。中间省略..... / dev / hdc6 1976312 42072 1833836 3 ％/ mnt / hdc6 磁盘卸载命令umount语法： 卸除[ - FN ]装置文件名或挂载点 选项与参数： -f：强制卸除！可用在类似网络文件系统（NFS）无法读取到的情况下； -n：不升级/ etc / mtab情况下卸除。 卸载/ dev / hdc6 [根@ WWW 〜]＃卸除/ dev的/ hdc6 vi/vim 所有的 Unix Like 系统都会内建 vi 文书编辑器，其他的文书编辑器则不一定会存在。但是目前我们使用比较多的是 vim 编辑器。vim 具有程序编辑的能力，可以主动的以字体颜色辨别语法的正确性，方便程序设计。 什么是 vim？ Vim是从 vi 发展出来的一个文本编辑器。代码补完、编译及错误跳转等方便编程的功能特别丰富，在程序员中被广泛使用。 简单的来说， vi 是老式的字处理器，不过功能已经很齐全了，但是还是有可以进步的地方。 vim 则可以说是程序开发者的一项很好用的工具。 连 vim 的官方网站 (http://www.vim.org) 自己也说 vim 是一个程序开发工具而不是文字处理软件。 vim 键盘图： vi/vim 的使用 基本上 vi/vim 共分为三种模式，分别是命令模式（Command mode），输入模式（Insert mode）和底线命令模式（Last line mode）。 这三种模式的作用分别是： 命令模式： 用户刚刚启动 vi/vim，便进入了命令模式。 此状态下敲击键盘动作会被Vim识别为命令，而非输入字符。比如我们此时按下i，并不会输入一个字符，i被当作了一个命令。 以下是常用的几个命令： i 切换到输入模式，以输入字符。 x 删除当前光标所在处的字符。 : 切换到底线命令模式，以在最底一行输入命令。 若想要编辑文本：启动Vim，进入了命令模式，按下i，切换到输入模式。 命令模式只有一些最基本的命令，因此仍要依靠底线命令模式输入更多命令。 输入模式 在命令模式下按下i就进入了输入模式。 在输入模式中，可以使用以下按键： 字符按键以及Shift组合，输入字符 ENTER，回车键，换行 BACK SPACE，退格键，删除光标前一个字符 DEL，删除键，删除光标后一个字符 方向键，在文本中移动光标 HOME/END，移动光标到行首/行尾 Page Up/Page Down，上/下翻页 Insert，切换光标为输入/替换模式，光标将变成竖线/下划线 ESC，退出输入模式，切换到命令模式 底线命令模式 在命令模式下按下:（英文冒号）就进入了底线命令模式。 底线命令模式可以输入单个或多个字符的命令，可用的命令非常多。 在底线命令模式中，基本的命令有（已经省略了冒号）： q 退出程序 w 保存文件 按ESC键可随时退出底线命令模式。 简单的说，我们可以将这三个模式想成底下的图标来表示 vi/vim 使用实例 使用 vi/vim 进入一般模式 如果你想要使用 vi 来建立一个名为 runoob.txt 的文件时，你可以这样做： $ vim runoob.txt 直接输入 vi 文件名 就能够进入 vi 的一般模式了。请注意，记得 vi 后面一定要加文件名，不管该文件存在与否！ - 按下 i 进入输入模式(也称为编辑模式)，开始编辑文字 在一般模式之中，只要按下 i, o, a 等字符就可以进入输入模式了！ 在编辑模式当中，你可以发现在左下角状态栏中会出现 –INSERT- 的字样，那就是可以输入任意字符的提示。 这个时候，键盘上除了 Esc 这个按键之外，其他的按键都可以视作为一般的输入按钮了，所以你可以进行任何的编辑。 - 按下 ESC 按钮回到一般模式 好了，假设我已经按照上面的样式给他编辑完毕了，那么应该要如何退出呢？是的！没错！就是给他按下 Esc 这个按钮即可！马上你就会发现画面左下角的 – INSERT – 不见了！ 在一般模式中按下 :wq 储存后离开 vi OK，我们要存档了，存盘并离开的指令很简单，输入 :wq 即可保存离开！ - Linux vi_vim _ 菜鸟教程.pdf Linux yum命令 yum（Yellow dog Updater，Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端连接管理器。 基于RPM包管理，能够从指定的服务器自动下载RPM包和安装，可以自动处理相对关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载，安装。 yum提供了查找，安装，删除某人，多个甚至全部完全的命令，而且命令简洁而又好记。 yum语法 yum [选项] [命令] [包...] 选项：任选，选项包括-h（帮助），-y（当安装过程提示选择全部为“ yes”），-q（不显示安装的过程）等等。 命令：要进行的操作。 package：安装的包名。 yum常用命令 1.列出所有可更新的软件清单命令：yum check-update 2.更新所有软件命令：yum update 3.仅安装指定的软件命令：yum install &lt;程序包名称&gt; 4.仅更新指定的软件命令：yum update &lt;程序包名称&gt; 5.列出所有可安装的软件清单命令：百胜名单 6.删除删除命令：yum remove &lt;package_name&gt; 7.查找普通命令：yum search &lt;关键字&gt; 8.清除缓存命令： 百胜清洁包：清除缓存目录下的双重 yum clean headers：清除缓存目录下的headers yum clean oldheaders：清除缓存目录下旧的标题 yum clean，yum clean all（= yum clean package; yum clean oldheaders）：清除缓存目录下的副本及旧的标题 实例1 安装pam-devel [根@ WWW 〜]＃百胜安装PAM - devel的 设置了安装过程解析包安装参数 解析依赖性&lt;==先检查软件的属性相依问题- &gt;运行的事务检查 ---&gt;封装PAM - devel的。i386 0 ：0.99 。6.2 - 4.el5集被更新 处理相关：PAM = 0.99 。6.2 - 4.el5的包：PAM - devel的 运行交易检查 ---&gt;包装PAM 。i386 0 ：0.99 。6.2 - 4.el5集进行更新 文件列表。xml 。gz 100 ％| ======================== | 1.6 MB 00 ：05 的文件列表。xml 。gz 100 ％| ======================== | 138 kB 00 : 00- &gt;完成的依赖性解析……（省略） 实例2 可拆卸pam-devel [根@ WWW 〜]＃荫删除PAM - devel的 设置了删除过程解决依赖&lt;==同样的，先解决属性相依的问题- &gt;运行的事务检查 ---&gt;封装PAM - devel的。i386 0 ：0.99 。6.2 - 4.el5集被擦除 成品依赖决议 解决依赖性 ================================================== =软件包Arch版本库大小 ================================================== =删除： pam - devel i386 0.99 。6.2 - 4.el5 安装 495 ķ 交易摘要================================ =============================安装0包（小号）更新0包（小号）中取出1包（小号）&lt;==还好，并没有属性相依的问题，单纯可移除一个软件 是该行[ ÿ / Ñ ]：Ÿ 下载软件包：运行rpm_check_debug 运行交易测试已完成的交易测试的交易测试成功运行交易擦除：PAM - devel的 ################### ###### [1/1] 删除：pam - devel 。i386 0 ：0.99 。6.2 - 4.el5完成！ 实例3 利用yum的功能，发现以pam为开头的软件名称有什么？ [ root @ www〜] ＃yum list pam *已安装的软件包 pam 。i386 0.99 。6.2 - 3.27 。已安装el5 pam_ccreds 。I386 3 - 5 安装 pam_krb5 。i386 2.2 。14 - 1 安装 pam_passwdqc 。i386 1.0 。2 - 1.2 。2 安装 pam_pkcs11 。i386 0.5 。3 - 23 安装 pam_smb 。i386 1.1 。7 - 7.2 。1个 已安装的 可用软件包&lt;==底下则是『可升级』的或『未安装』的 pam 。i386 0.99 。6.2 - 4.el5基 PAM - devel的。i386 0.99 。6.2 - 4.el5基 pam_krb5的。i386 2.2 。14 - 10基 国内yum源 网易（163）yum源是国内最好的yum源之一，无论是速度还是软件版本，都非常的不错。 将yum源设置为163 yum，可以提高扩展安装和更新的速度，同时避免一些常见软件版本无法找到。 安装步骤 首先备份/etc/yum.repos.d/CentOS-Base.repo mv / etc / yum 。回购。d / CentOS的-基础。回购/ etc / yum 。回购。d / CentOS的-基础。回购。后备 下载对应版本repo文件，加入/etc/yum.repos.d/（操作前请做好相应备份） CentOS5：http：//mirrors.163.com/.help/CentOS5-Base-163.repo CentOS6：http：//mirrors.163.com/.help/CentOS6-Base-163.repo CentOS7：http：//mirrors.163.com/.help/CentOS7-Base-163.repo wget的HTTP ：//mirrors.163.com/.help/CentOS6-Base-163.repo MV CentOS6 -基地- 163.repo的CentOS -基地。回购 运行以下命令生成缓存 百胜清理所有 yum makecache 除了网易之外，国内还有其他不错的yum源，某些中科大和搜狐。 中科大的yum源，安装方法查看：https 😕/lug.ustc.edu.cn/wiki/mirrors/help/centos sohu的yum源安装方法查看：http : //mirrors.sohu.com/help/centos.html Linux apt 命令 apt（Advanced Packaging Tool）是一个在 Debian 和 Ubuntu 中的 Shell 前端软件包管理器。 apt 命令提供了查找、安装、升级、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 apt 命令执行需要超级管理员权限(root)。 apt 语法 apt [options] [command] [package ...] options：可选，选项包括 -h（帮助），-y（当安装过程提示选择全部为&quot;yes&quot;），-q（不显示安装的过程）等等。 command：要进行的操作。 package：安装的包名。 apt 常用命令 列出所有可更新的软件清单命令：sudo apt update 升级软件包：sudo apt upgrade 列出可更新的软件包及版本信息：apt list --upgradeable 升级软件包，升级前先删除需要更新软件包：sudo apt full-upgrade 安装指定的软件命令：sudo apt install &lt;package_name&gt; 安装多个软件包：sudo apt install &lt;package_1&gt; &lt;package_2&gt; &lt;package_3&gt; 更新指定的软件命令：sudo apt update &lt;package_name&gt; 显示软件包具体信息,例如：版本号，安装大小，依赖关系等等：sudo apt show &lt;package_name&gt; 删除软件包命令：sudo apt remove &lt;package_name&gt; 清理不再使用的依赖和库文件: sudo apt autoremove 移除软件包及配置文件: sudo apt purge &lt;package_name&gt; 查找软件包命令： sudo apt search 列出所有已安装的包：apt list --installed 列出所有已安装的包的版本信息：apt list --all-versions 实例 查看一些可更新的包： sudo apt update 升级安装包： sudo apt upgrade 在以上交互式输入字母 Y 即可开始升级。 可以将以下两个命令组合起来，一键升级： sudo apt update &amp;&amp; sudo apt upgrade -y 安装 mplayer 包： sudo apt install mplayer 如过不太记得完整的包名，我们可以只输入前半部分的包名，然后按下 Tab 键，会列出相关的包名： 以上实例我们输入来 reds，然后按下 Tab 键，输出来四个相关的包。 如果我们想安装一个软件包，但如果软件包已经存在，则不要升级它，可以使用 –no-upgrade 选项: sudo apt install &lt;package_name&gt; --no-upgrade 安装 mplayer 如果存在则不要升级： sudo apt install mplayer --no-upgrade 如果只想升级，不要安装可以使用 --only-upgrade 参数： sudo apt install &lt;package_name&gt; --only-upgrade 只升级 mplayer，如果不存在就不要安装它： sudo apt install mplayer --only-upgrade 如果需要设置指定版本，语法格式如下： sudo apt install &lt;package_name&gt;=&lt;version_number&gt; package_name 为包名，version_number 为版本号。 移除包可以使用 remove 命令： sudo apt remove mplayer 查找名为 libimobile 的相关包： apt search libimobile 查看 pinta 包的相关信息： apt show pinta 列出可更新的软件包： apt list --upgradeable 清理不再使用的依赖和库文件： sudo apt autoremove 在以上交互式输入字母 Y 即可开始清理。 应用场景 离线/在线 安装各种包 后台 JAVA相关 Redis Mysql Tomcat 查看tomcat实时运行日志：cd tomcat/log —&gt; tail -f catalina.out 程序部署 前台 程序部署 其他 Nginx 文件操作 清理速度优化 进程管理 权限管理","link":"/2021/03/01/Draft/2021/Linux/"},{"title":"Python 学习","text":"Python3学习 简介 Python is powerful... and fast; plays well with others; runs everywhere; is friendly &amp; easy to learn; is Open. Python 发展历史 Python 特点 Python 应用 工具 开发平台：PYCharm 语法基础 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719'''=============基本语法============='''import keyword# from modname import name1[, name2[, ... nameN]]# Python 提供了一个办法，把这些定义存放在文件中，为一些脚本或者交互式的解释器实例使用，这个文件被称为模块。# 模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py。模块可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 python 标准库的方法。# 从某个模块中导入多个函数,格式为： from somemodule import firstfunc, secondfunc, thirdfunc# ====__name__属性====# 一个模块被另一个程序第一次引入时，其主程序将运行。如果我们想在模块被引入时，模块中的某一程序块不执行，我们可以用__name__属性来使该程序块仅在该模块自身运行时执行。# #!/usr/bin/python3# # Filename: using_name.py# if __name__ == '__main__':# print('程序自身在运行')# else:# print('我来自另一模块')# ====dir() 函数====# 内置的函数 dir() 可以找到模块内定义的所有名称。以一个字符串列表的形式返回:'''输入和输出'''# Python两种输出值的方式: 表达式语句和 print() 函数。# 第三种方式是使用文件对象的 write() 方法，标准输出文件可以用 sys.stdout 引用。# 如果你希望输出的形式更加多样，可以使用 str.format() 函数来格式化输出值。# 如果你希望将输出的值转成字符串，可以使用 repr() 或 str() 函数来实现。# str()： 函数返回一个用户易读的表达形式。# repr()： 产生一个解释器易读的表达形式。print(&quot;Hello word!&quot;)print('{}网址： &quot;{}!&quot;'.format('菜鸟教程', 'www.runoob.com'))print('{0} 和 {1}'.format('Google', 'Runoob'))print('站点列表 {0}, {1}, 和 {other}。'.format('Google', 'Runoob', other='Taobao'))# 读取键盘输入str = input(&quot;请输入：&quot;);print (&quot;你输入的内容是: &quot;, str)# 读和写文件# open(filename, mode)# filename：包含了你要访问的文件名称的字符串值。# mode：决定了打开文件的模式：只读，写入，追加等。所有可取值见如下的完全列表。这个参数是非强制的，默认文件访问模式为只读(r)。# 模式 描述# r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。# rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。# r+ 打开一个文件用于读写。文件指针将会放在文件的开头。# rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。# w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。# ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。# 打开一个文件f = open(&quot;/tmp/foo.txt&quot;, &quot;w&quot;)f.write( &quot;Python 是一个非常好的语言。\\n是的，的确非常好!!\\n&quot; )# f.read(size), 这将读取一定数目的数据, 然后作为字符串或字节对象返回。size 是一个可选的数字类型的参数。 当 size 被忽略了或者为负, 那么该文件的所有内容都将被读取并且返回。print(f.read())# 返回一个空字符串, 说明已经已经读取到最后一行。print(f.readline())# 返回所有行。print(f.readlines())# 将 string 写入到文件中, 然后返回写入的字符数。f.write(&quot;string&quot;)# 返回文件对象当前所处的位置, 它是从文件开头开始算起的字节数。f.tell()# 改变文件当前的位置, 可以使用 f.seek(offset, from_what) 函数。# from_what 的值, 如果是 0 表示开头, 如果是 1 表示当前位置, 2 表示文件的结尾，例如：# seek(x,0) ： 从起始位置即文件首行首字符开始移动 x 个字符# seek(x,1) ： 表示从当前位置往后移动x个字符# seek(-x,2)：表示从文件的结尾往前移动x个字符# from_what 值为默认为0，即文件开头# 关闭打开的文件f.close()# pickle 模块 实现了基本的数据序列和反序列化。import pickle# 使用pickle模块将数据对象保存到文件data1 = {'a': [1, 2.0, 3, 4+6j], 'b': ('string', u'Unicode string'), 'c': None}selfref_list = [1, 2, 3]selfref_list.append(selfref_list)output = open('data.pkl', 'wb')# Pickle dictionary using protocol 0.pickle.dump(data1, output)# Pickle the list using the highest protocol available.pickle.dump(selfref_list, output, -1)output.close()''' 注释 '''&quot;&quot;&quot;注释&quot;&quot;&quot;# 保留字 关键字end可以用于将结果输出到同一行，或者在输出的末尾添加不同的字符print(b, end=',')print(&quot;保留字&quot;, keyword.kwlist)# 缩进的空格数是可变的，但是同一个代码块的语句必须包含相同的缩进空格数if True: print(&quot;ture&quot;)else: print('false')# 多行语句，语句很长，我们可以使用反斜杠(\\)来实现多行语句，在 [], {}, 或 () 中的多行语句，不需要使用反斜杠(\\)total = 'item_one' + \\ 'item_two' + \\ 'item_three'total1 = ['item_one', 'item_two', 'item_three', 'item_four', 'item_five']print(total)print(total1)''' ================基本数据类型===================Python3 中有六个标准的数据类型：Number（数字）：int、float、bool、complex（复数）String（字符串）List（列表）Tuple（元组）Set（集合）Dictionary（字典）Python3 的六个标准数据类型中：不可变数据（3 个）：Number（数字）、String（字符串）、Tuple（元组）；可变数据（3 个）：List（列表）、Dictionary（字典）、Set（集合）。 type()查看数据类型，isinstance(a, int)进行判断'''# Number（数字）------------------------------------------------------------------------# int(x) 将x转换为一个整数。# float(x) 将x转换到一个浮点数。# complex(x) 将x转换到一个复数，实数部分为 x，虚数部分为 0。# complex(x, y) 将 x 和 y 转换到一个复数，实数部分为 x，虚数部分为 y。x 和 y 是数字表达式。# ==数学函数==# 函数 返回值 ( 描述 )# abs(x) 返回数字的绝对值，如abs(-10) 返回 10# ceil(x) 返回数字的上入整数，如math.ceil(4.1) 返回 5# cmp(x, y) 如果 x &lt; y 返回 -1, 如果 x == y 返回 0, 如果 x &gt; y 返回 1。 Python 3 已废弃，使用 (x&gt;y)-(x&lt;y) 替换。# exp(x) 返回e的x次幂(ex),如math.exp(1) 返回2.718281828459045# fabs(x) 返回数字的绝对值，如math.fabs(-10) 返回10.0# floor(x) 返回数字的下舍整数，如math.floor(4.9)返回 4# log(x) 如math.log(math.e)返回1.0,math.log(100,10)返回2.0# log10(x) 返回以10为基数的x的对数，如math.log10(100)返回 2.0# max(x1, x2,...) 返回给定参数的最大值，参数可以为序列。# min(x1, x2,...) 返回给定参数的最小值，参数可以为序列。# modf(x) 返回x的整数部分与小数部分，两部分的数值符号与x相同，整数部分以浮点型表示。# pow(x, y) x**y 运算后的值。# round(x [,n]) 返回浮点数 x 的四舍五入值，如给出 n 值，则代表舍入到小数点后的位数。其实准确的说是保留值将保留到离上一位更近的一端。# sqrt(x) 返回数字x的平方根。# ==随机数函数==# choice(seq) 从序列的元素中随机挑选一个元素，比如random.choice(range(10))，从0到9中随机挑选一个整数。# randrange ([start,] stop [,step]) 从指定范围内，按指定基数递增的集合中获取一个随机数，基数默认值为 1# random() 随机生成下一个实数，它在[0,1)范围内。# seed([x]) 改变随机数生成器的种子seed。如果你不了解其原理，你不必特别去设定seed，Python会帮你选择seed。# shuffle(lst) 将序列的所有元素随机排序# uniform(x, y) 随机生成下一个实数，它在[x,y]范围内# ==三角函数==# Python包括以下三角函数：# 函数 描述# acos(x) 返回x的反余弦弧度值。# asin(x) 返回x的反正弦弧度值。# atan(x) 返回x的反正切弧度值。# atan2(y, x) 返回给定的 X 及 Y 坐标值的反正切值。# cos(x) 返回x的弧度的余弦值。# hypot(x, y) 返回欧几里德范数 sqrt(x*x + y*y)。# sin(x) 返回的x弧度的正弦值。# tan(x) 返回x弧度的正切值。# degrees(x) 将弧度转换为角度,如degrees(math.pi/2) ， 返回90.0# radians(x) 将角度转换为弧度# ==数学常量==# 常量 描述# pi 数学常量 pi（圆周率，一般以π来表示）# e 数学常量 e，e即自然常数（自然常数）。a = b = c = counter = 100 # 同时为多个变量赋值整型变量d, e, f = 1, 2, &quot;runoob&quot; # 同时为多个变量赋值不同的值miles = 1000.0 # 浮点型变量name = &quot;runoob&quot; # 字符串# del语句删除一些对象引用del a# 数值运算Division = 2 / 4 # 除法，得到一个浮点数Division2 = 2 // 4 # 除法，得到一个整数remainder = 17 % 3 # 取余involution = 2 ** 5 # 乘方print(Division, 'Division')print(Division2, 'Division2')# String（字符串）------------------------------------------------------------------------------# 从后面索引01234567# 从前面索引-8-7-6-5-4-3-2-1# Good boy# 从前面截取:123456:# 从后面截取:-6-5-4-3-2-1:# 反斜杠(\\)可以作为续行符，表示下一行是上一行的延续。也可以使用 &quot;&quot;&quot;...&quot;&quot;&quot; 或者 '''...''' 跨越多行# 用+运算符连接在一起，用*运算符重复# ==转义字符==# \\(在行尾时) 续行符# &gt;&gt;&gt; print(&quot;line1 \\# ... line2 \\# ... line3&quot;)# line1 line2 line3# \\\\ 反斜杠符号# &gt;&gt;&gt; print(&quot;\\\\&quot;)# \\# \\' 单引号# &gt;&gt;&gt; print('\\'')# '# \\&quot; 双引号# &gt;&gt;&gt; print(&quot;\\&quot;&quot;)# &quot;# \\a 响铃# &gt;&gt;&gt; print(&quot;\\a&quot;)执行后电脑有响声。# \\b 退格(Backspace)# &gt;&gt;&gt; print(&quot;Hello \\b World!&quot;)# Hello World!# \\000 空# &gt;&gt;&gt; print(&quot;\\000&quot;)## &gt;&gt;&gt;# \\n 换行# &gt;&gt;&gt; print(&quot;\\n&quot;)## &gt;&gt;&gt;# \\v 纵向制表符# &gt;&gt;&gt; print(&quot;Hello \\v World!&quot;)# Hello# World!# &gt;&gt;&gt;# \\t 横向制表符# &gt;&gt;&gt; print(&quot;Hello \\t World!&quot;)# Hello World!# &gt;&gt;&gt;# \\r 回车，将 \\r 后面的内容移到字符串开头，并逐一替换开头部分的字符，直至将 \\r 后面的内容完全替换完成。# &gt;&gt;&gt; print(&quot;Hello\\rWorld!&quot;)# World!# &gt;&gt;&gt; print('google runoob taobao\\r123456')# 123456 runoob taobao# \\f 换页# &gt;&gt;&gt; print(&quot;Hello \\f World!&quot;)# Hello# World!# &gt;&gt;&gt;# \\yyy 八进制数，y 代表 0~7 的字符，例如：\\012 代表换行。# &gt;&gt;&gt; print(&quot;\\110\\145\\154\\154\\157\\40\\127\\157\\162\\154\\144\\41&quot;)# Hello World!# \\xyy 十六进制数，以 \\x 开头，y 代表的字符，例如：\\x0a 代表换行# &gt;&gt;&gt; print(&quot;\\x48\\x65\\x6c\\x6c\\x6f\\x20\\x57\\x6f\\x72\\x6c\\x64\\x21&quot;)# Hello World!# \\other 其它的字符以普通格式输出# ==字符串运算符==# + 字符串连接 a + b 输出结果： HelloPython# * 重复输出字符串 a*2 输出结果：HelloHello# [] 通过索引获取字符串中字符 a[1] 输出结果 e# [ : ] 截取字符串中的一部分，遵循左闭右开原则，str[0:2] 是不包含第 3 个字符的。 a[1:4] 输出结果 ell# in 成员运算符 - 如果字符串中包含给定的字符返回 True 'H' in a 输出结果 True# not in 成员运算符 - 如果字符串中不包含给定的字符返回 True 'M' not in a 输出结果 True# r/R 原始字符串 - 原始字符串：所有的字符串都是直接按照字面的意思来使用，没有转义特殊或不能打印的字符。 原始字符串除在字符串的第一个引号前加上字母 r（可以大小写）以外，与普通字符串有着几乎完全相同的语法。 print( r'\\n' ) print( R'\\n' )# % 格式字符串# ==字符串格式化==# % c 格式化字符及其ASCII码# % s 格式化字符串# % d 格式化整数# % u 格式化无符号整型# % o 格式化无符号八进制数# % x 格式化无符号十六进制数# % X 格式化无符号十六进制数（大写）# % f 格式化浮点数字，可指定小数点后的精度# % e 用科学计数法格式化浮点数# % E 作用同 % e，用科学计数法格式化浮点数# % g % f和 % e的简写# % G % f 和 % E 的简写# % p 用十六进制数格式化变量的地址# 格式化操作符辅助指令:# 符号 功能# * 定义宽度或者小数点精度# - 用做左对齐# + 在正数前面显示加号( + )# &lt;sp&gt; 在正数前面显示空格# # 在八进制数前面显示零('0')，在十六进制前面显示'0x'或者'0X'(取决于用的是'x'还是'X')# 0 显示的数字前面填充'0'而不是默认的空格# % '%%'输出一个单一的'%'# (var) 映射变量(字典参数)# m.n. m 是显示的最小总宽度,n 是小数点后的位数(如果可用的话)# ==f-string==name = 'Runoob'f'Hello {name}' # 替换变量'Hello Runoob'f'{1 + 2}' # 使用表达式'3'w = {'name': 'Runoob', 'url': 'www.runoob.com'}f'{w[&quot;name&quot;]}: {w[&quot;url&quot;]}''Runoob: www.runoob.com'para_str = &quot;&quot;&quot;这是一个多行字符串的实例多行字符串可以使用制表符TAB ( \\t )。也可以使用换行符 [ \\n ]。&quot;&quot;&quot;# ==字符串内建函数==# capitalize()# 将字符串的第一个字符转换为大写# center(width, fillchar)# 返回一个指定的宽度 width 居中的字符串，fillchar 为填充的字符，默认为空格。# count(str, beg= 0,end=len(string))# 返回 str 在 string 里面出现的次数，如果 beg 或者 end 指定则返回指定范围内 str 出现的次数# bytes.decode(encoding=&quot;utf-8&quot;, errors=&quot;strict&quot;)# Python3 中没有 decode 方法，但我们可以使用 bytes 对象的 decode() 方法来解码给定的 bytes 对象，这个 bytes 对象可以由 str.encode() 来编码返回。# encode(encoding='UTF-8',errors='strict')# 以 encoding 指定的编码格式编码字符串，如果出错默认报一个ValueError 的异常，除非 errors 指定的是'ignore'或者'replace'# endswith(suffix, beg=0, end=len(string))# 检查字符串是否以 obj 结束，如果beg 或者 end 指定则检查指定的范围内是否以 obj 结束，如果是，返回 True,否则返回 False.# expandtabs(tabsize=8)# 把字符串 string 中的 tab 符号转为空格，tab 符号默认的空格数是 8 。# find(str, beg=0, end=len(string))# 检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含返回开始的索引值，否则返回-1# index(str, beg=0, end=len(string))# 跟find()方法一样，只不过如果str不在字符串中会报一个异常。# isalnum()# 如果字符串至少有一个字符并且所有字符都是字母或数字则返 回 True，否则返回 False# isalpha()# 如果字符串至少有一个字符并且所有字符都是字母或中文字则返回 True, 否则返回 False# isdigit()# 如果字符串只包含数字则返回 True 否则返回 False..# islower()# 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True，否则返回 False# isnumeric()# 如果字符串中只包含数字字符，则返回 True，否则返回 False# isspace()# 如果字符串中只包含空白，则返回 True，否则返回 False.# istitle()# 如果字符串是标题化的(见 title())则返回 True，否则返回 False# isupper()# 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是大写，则返回 True，否则返回 False# join(seq)# 以指定字符串作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串# len(string)# 返回字符串长度# ljust(width[, fillchar])# 返回一个原字符串左对齐,并使用 fillchar 填充至长度 width 的新字符串，fillchar 默认为空格。# lower()# 转换字符串中所有大写字符为小写.# lstrip()# 截掉字符串左边的空格或指定字符。# maketrans()# 创建字符映射的转换表，对于接受两个参数的最简单的调用方式，第一个参数是字符串，表示需要转换的字符，第二个参数也是字符串表示转换的目标。# max(str)# 返回字符串 str 中最大的字母。# min(str)# 返回字符串 str 中最小的字母。# replace(old, new [, max])# 把 将字符串中的 old 替换成 new,如果 max 指定，则替换不超过 max 次。# rfind(str, beg=0,end=len(string))# 类似于 find()函数，不过是从右边开始查找.# rindex( str, beg=0, end=len(string))# 类似于 index()，不过是从右边开始.# rjust(width,[, fillchar])# 返回一个原字符串右对齐,并使用fillchar(默认空格）填充至长度 width 的新字符串# rstrip()# 删除字符串末尾的空格或指定字符。# split(str=&quot;&quot;, num=string.count(str))# 以 str 为分隔符截取字符串，如果 num 有指定值，则仅截取 num+1 个子字符串# splitlines([keepends])# 按照行('\\r', '\\r\\n', \\n')分隔，返回一个包含各行作为元素的列表，如果参数 keepends 为 False，不包含换行符，如果为 True，则保留换行符。# startswith(substr, beg=0,end=len(string))# 检查字符串是否是以指定子字符串 substr 开头，是则返回 True，否则返回 False。如果beg 和 end 指定值，则在指定范围内检查。# strip([chars])# 在字符串上执行 lstrip()和 rstrip()# swapcase()# 将字符串中大写转换为小写，小写转换为大写# title()# 返回&quot;标题化&quot;的字符串,就是说所有单词都是以大写开始，其余字母均为小写(见 istitle())# translate(table, deletechars=&quot;&quot;)# 根据 str 给出的表(包含 256 个字符)转换 string 的字符, 要过滤掉的字符放到 deletechars 参数中# upper()# 转换字符串中的小写字母为大写# zfill (width)# 返回长度为 width 的字符串，原字符串右对齐，前面填充0# isdecimal()# 检查字符串是否只包含十进制字符，如果是返回 true，否则返回 false。str = 'Lxl is a good boy'print(str) # 输出字符串print(str[0:-1]) # 输出第一个到倒数第二个的所有字符print(str[0]) # 输出字符串第一个字符print(str[2:5]) # 输出从第三个开始到第五个的字符print(str[2:]) # 输出从第三个开始的后的所有字符print(str * 2) # 输出字符串两次，也可以写成 print (2 * str)print(str + &quot;TEST&quot;) # 连接字符串# 转义print('Ru\\noob')# 不转义字符串前加rprint(2 * r'Ru\\noob')# List（列表）------------------------------------------------------------------------------# 创建列表只需要方括号括起来，里面数据类型可不一致，索引正0反-1，# 列表中的元素是可以改变的# append() 追加 del 删除# ==列表脚本操作符==# Python 表达式 结果 描述# len([1, 2, 3]) 3 长度# [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6] 组合# ['Hi!'] * 4 ['Hi!', 'Hi!', 'Hi!', 'Hi!'] 重复# 3 in [1, 2, 3] True 元素是否存在于列表中# for x in [1, 2, 3]: print(x, end=&quot; &quot;) 1 2 3 迭代# ==函数&amp;方法==# len(list) 列表元素个数# max(list) 返回列表元素最大值# min(list) 返回列表元素最小值# list(seq) 将元组转换为列表# 1 list.append(obj) 在列表末尾添加新的对象# 2 list.count(obj) 统计某个元素在列表中出现的次数# 3 list.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）# 4 list.index(obj) 从列表中找出某个值第一个匹配项的索引位置# 5 list.insert(index, obj) 将对象插入列表# 6 list.pop([index=-1]) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值# 7 list.remove(obj) 移除列表中某个值的第一个匹配项# 8 list.reverse() 反向列表中元素# 9 list.sort( key=None, reverse=False) 对原列表进行排序# 10 list.clear() 清空列表# 11 list.copy() 复制列表lists = ['1', '2', '3', '4', '5']print(lists[:])print(lists[:3])print(lists[1:3])print(lists[4:])print(lists[-1:])print(lists[:-2])print(lists * 3) # 输出多次列表print(lists + lists) # 连接列表for i, v in enumerate(['tic', 'tac', 'toe']): print(i, v)def reverseWords(input): # 通过空格将字符串分隔符，把各个单词分隔为列表 inputWords = input.split(&quot; &quot;) # 翻转字符串 # 假设列表 list = [1,2,3,4], # list[0]=1, list[1]=2 ，而 -1 表示最后一个元素 list[-1]=4 ( 与 list[3]=4 一样) # inputWords[-1::-1] 有三个参数 # 第一个参数 -1 表示最后一个元素 # 第二个参数为空，表示移动到列表末尾 # 第三个参数为步长，-1 表示逆向 inputWords = inputWords[-1::-1] # 重新组合字符串 output = ' '.join(inputWords) return outputif __name__ == &quot;__main__&quot;: input = 'I like runoob' rw = reverseWords(input) print(rw)# Tuple（元组）--------------------------------------------------------------------# 元组（tuple）与列表类似，不同之处在于元组的元素不能修改，其使用小括号（）或不要括号，# 但它可以包含可变的对象，可以连接组合成新元祖，元素不允许删除但可以使用del删除整个元祖。# string、list 和 tuple 都属于 sequence（序列）。# 当元祖只有一个元素时，后面添加逗号，否则括号会被当做运算符使用，tup1 = (50)为整型，tup1 = (50，)为元祖# ==元祖运算符==# Python 表达式 结果 描述# len((1, 2, 3)) 3 计算元素个数# (1, 2, 3) + (4, 5, 6) (1, 2, 3, 4, 5, 6) 连接# ('Hi!',) * 4 ('Hi!', 'Hi!', 'Hi!', 'Hi!') 复制# 3 in (1, 2, 3) True 元素是否存在# for x in (1, 2, 3): print (x,) 1 2 3 迭代# ==内置函数==# len(tuple) 计算元组元素个数。# max(tuple) 返回元组中元素最大值。# min(tuple) 返回元组中元素最小值。# tuple(iterable) 将可迭代系列转换为元组。tuple = ('abcd', 786, 2.23, 'runoob', 70.2)tinytuple = (123, 'runoob')print(tuple) # 输出完整元组print(tuple[0]) # 输出元组的第一个元素print(tuple[1:3]) # 输出从第二个元素开始到第三个元素print(tuple[2:]) # 输出从第三个元素开始的所有元素print(tinytuple * 2) # 输出两次元组print(tuple + tinytuple) # 连接元组# Set（集合）----------------------------------------------------------------------# 创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典sites = {'Google', 'Taobao', 'Runoob', 'Facebook', 'Zhihu', 'Baidu'}sites1 = {}sites2 = ()set()# # 添加元素，重复不操作# sites1.add(&quot;1&quot;)# # 可以添加列表，元祖字典等# sites1.update(&quot;1&quot;)# print(sites) # 输出集合，重复的元素被自动去掉# # 移除，不存在报错# sites1.remove( x )# # 移除，不存在不报错# sites1.discard( x )# # 随机删除# sites1.pop()# # 长度# len(sites1)# # 清空# sites1.clear()# # 判断是否存在# x in s# ==内置方法完整列表==# add() 为集合添加元素# clear() 移除集合中的所有元素# copy() 拷贝一个集合# difference() 返回多个集合的差集# difference_update() 移除集合中的元素，该元素在指定的集合也存在。# discard() 删除集合中指定的元素# intersection() 返回集合的交集# intersection_update() 返回集合的交集。# isdisjoint() 判断两个集合是否包含相同的元素，如果没有返回 True，否则返回 False。# issubset() 判断指定集合是否为该方法参数集合的子集。# issuperset() 判断该方法的参数集合是否为指定集合的子集# pop() 随机移除元素# remove() 移除指定元素# symmetric_difference() 返回两个集合中不重复的元素集合。# symmetric_difference_update() 移除当前集合中在另外一个指定集合相同的元素，并将另外一个指定集合中不同的元素插入到当前集合中。# union() 返回两个集合的并集# update() 给集合添加元素# 成员测试if 'Runoob' in sites: print('Runoob 在集合中')else: print('Runoob 不在集合中')# set可以进行集合运算a = set('abracadabra')b = set('alacazam')print(a)print(a - b) # a 和 b 的差集print(a | b) # a 和 b 的并集print(a &amp; b) # a 和 b 的交集print(a ^ b) # a 和 b 中不同时存在的元素# Dictionary（字典）------------------------------------------------------------------# 字典（dictionary）是Python中另一个非常有用的内置数据类型。# 列表是有序的对象集合，字典是无序的对象集合。两者之间的区别在于：字典当中的元素是通过键来存取的，而不是通过偏移存取。# 字典是一种映射类型，字典用 { } 标识，它是一个无序的 键(key) : 值(value) 的集合。# 键(key)可以是任意类型，但必须使用不可变类型。# 在同一个字典中，键(key)必须是唯一的。# ==内置函数==# len(dict) 计算字典元素个数，即键的总数。# str(dict) 输出字典，以可打印的字符串表示。# type(variable) 返回输入的变量类型，如果变量是字典就返回字典类型。# ==内置方法==# 1 radiansdict.clear() 删除字典内所有元素# 2 radiansdict.copy() 返回一个字典的浅复制# 3 radiansdict.fromkeys() 创建一个新字典，以序列seq中元素做字典的键，val为字典所有键对应的初始值# 4 radiansdict.get(key, default=None) 返回指定键的值，如果键不在字典中返回 default 设置的默认值# 5 key in dict 如果键在字典dict里返回true，否则返回false# 6 radiansdict.items() 以列表返回一个视图对象# 7 radiansdict.keys() 返回一个视图对象# 8 radiansdict.setdefault(key, default=None) 和get()类似, 但如果键不存在于字典中，将会添加键并将值设为default# 9 radiansdict.update(dict2) 把字典dict2的键/值对更新到dict里# 10 radiansdict.values() 返回一个视图对象# 11 pop(key[,default]) 删除字典给定键 key 所对应的值，返回值为被删除的值。key值必须给出。 否则，返回default值。# 12 popitem() 随机返回并删除字典中的最后一对键和值dict = {}dict['one'] = &quot;1 - 菜鸟教程&quot;dict[2] = &quot;2 - 菜鸟工具&quot;tinydict = {'name': 'runoob', 'code': 1, 'site': 'www.runoob.com'}for k, v in knights.items(): print(k, v)print(dict['one']) # 输出键为 'one' 的值print(dict[2]) # 输出键为 2 的值print(tinydict) # 输出完整的字典print(tinydict.keys()) # 输出所有键print(tinydict.values()) # 输出所有值del tinydict['name'] # 删除键 'name'dict.clear() # 清空字典del tinydict # 删除字典# 构造函数 dict() 可以直接从键值对序列中构建字典如下'''============数据类型转换==============='''# int(x [,projects]) 将x转换为一个整数# float(x)将x转换到一个浮点数# complex(real [,imag])创建一个复数# str(x)将对象 x 转换为字符串# repr(x)将对象 x 转换为表达式字符串# eval(str)用来计算在字符串中的有效Python表达式,并返回一个对象# tuple(s)将序列 s 转换为一个元组# list(s)将序列 s 转换为一个列表# set(s)转换为可变集合# dict(d)创建一个字典。d 必须是一个 (key, value)元组序列。# frozenset(s)转换为不可变集合# chr(x)将一个整数转换为一个字符# ord(x)将一个字符转换为它的整数值# hex(x)将一个整数转换为一个十六进制字符串# oct(x)将一个整数转换为一个八进制字符串'''============Python3运算符==============='''# 算术运算符【a为10，b为21】----------------------------------# + 加 - 两个对象相加 a + b 输出结果 31# - 减 - 得到负数或是一个数减去另一个数 a - b 输出结果 -11# * 乘 - 两个数相乘或是返回一个被重复若干次的字符串 a * b 输出结果 210# / 除 - x 除以 y b / a 输出结果 2.1# % 取模 - 返回除法的余数 b % a 输出结果 1# ** 幂 - 返回x的y次幂 a**b 为10的21次方# // 取整除 - 向下取接近商的整数，除后取小 9//2=4 -9//2=-5# 比较（关系）运算符【a为10，b为20】-----------------------------# == 等于 - 比较对象是否相等 (a == b) 返回 False。# != 不等于 - 比较两个对象是否不相等 (a != b) 返回 True。# &gt; 大于 - 返回x是否大于y (a &gt; b) 返回 False。# &lt; 小于 - 返回x是否小于y。所有比较运算符返回1表示真，返回0表示假。这分别与特殊的变量True和False等价。注意，这些变量名的大写。 (a &lt; b) 返回 True。# &gt;= 大于等于 - 返回x是否大于等于y。 (a &gt;= b) 返回 False。# &lt;= 小于等于 - 返回x是否小于等于y。 (a &lt;= b) 返回 True。# 赋值运算符【a为10，b为20】----------------------------------# = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c# += 加法赋值运算符 c += a 等效于 c = c + a# -= 减法赋值运算符 c -= a 等效于 c = c - a# *= 乘法赋值运算符 c *= a 等效于 c = c * a# /= 除法赋值运算符 c /= a 等效于 c = c / a# %= 取模赋值运算符 c %= a 等效于 c = c % a# **= 幂赋值运算符 c **= a 等效于 c = c ** a# //= 取整除赋值运算符 c //= a 等效于 c = c // a# := 海象运算符，可在表达式内部为变量赋值。Python3.8 版本新增运算符。# 在这个示例中，赋值表达式可以避免调用 len() 两次:# if (n := len(a)) &gt; 10:# print(f&quot;List is too long ({n} elements, expected &lt;= 10)&quot;)# 逻辑运算符-----------------------------------------------------# and x and y 布尔&quot;与&quot; - 如果 x 为 False，x and y 返回 x 的值，否则返回 y 的计算值。 (a and b) 返回 20。# or x or y 布尔&quot;或&quot; - 如果 x 是 True，它返回 x 的值，否则它返回 y 的计算值。 (a or b) 返回 10。# not not x 布尔&quot;非&quot; - 如果 x 为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False# 位运算符-------------------------------------------------------# &amp; 按位与运算符：参与运算的两个值,如果两个相应位都为1,则该位的结果为1,否则为0 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100# | 按位或运算符：只要对应的二个二进位有一个为1时，结果位就为1。 (a | b) 输出结果 61 ，二进制解释： 0011 1101# ^ 按位异或运算符：当两对应的二进位相异时，结果为1 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001# ~ 按位取反运算符：对数据的每个二进制位取反,即把1变为0,把0变为1。~x 类似于 -x-1 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。# &lt;&lt; 左移动运算符：运算数的各二进位全部左移若干位，由&quot;&lt;&lt;&quot;右边的数指定移动的位数，高位丢弃，低位补0。 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000# &gt;&gt; 右移动运算符：把&quot;&gt;&gt;&quot;左边的运算数的各二进位全部右移若干位，&quot;&gt;&gt;&quot;右边的数指定移动的位数 a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111# 成员运算符----------------------------------------------------------# in 如果在指定的序列中找到值返回 True，否则返回 False。 x 在 y 序列中 , 如果 x 在 y 序列中返回 True。# not in 如果在指定的序列中没有找到值返回 True，否则返回 False。 x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。# 身份运算符------------------------------------------------------------# is is 是判断两个标识符是不是引用自一个对象 x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False# is not is not 是判断两个标识符是不是引用自不同对象 x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。# 运算符优先级# ** 指数 (最高优先级)# ~ + - 按位翻转, 一元加号和减号 (最后两个的方法名为 +@ 和 -@)# * / % // 乘，除，求余数和取整除# + - 加法减法# &gt;&gt; &lt;&lt; 右移，左移运算符# &amp; 位 'AND'# ^ | 位运算符# &lt;= &lt; &gt; &gt;= 比较运算符# == != 等于运算符# = %= /= //= -= += *= **= 赋值运算符# is is not 身份运算符# in not in 成员运算符# not and or 逻辑运算符 流程控制 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485'''流程控制'''# ifif condition_1: statement_block_1elif condition_2: statement_block_2else: statement_block_3# while whlie后面为false时则执行else语句块while counter &lt;= n: sum = sum + counter counter += 1else: additional_statement(s)# for range()函数。它会生成数列# break 语句可以跳出 for 和 while 的循环体。如果你从 for 或 while 循环中终止，任何对应的循环 else 块将不执行。# continue 语句被用来告诉 Python 跳过当前循环块中的剩余语句，然后继续进行下一轮循环。for i in range(5,9) : print(i)# pass 不做任何事情，一般用做占位语句for letter in 'Runoob': if letter == 'o': pass print('执行 pass 块') print('当前字母 :', letter)print(&quot;Good bye!&quot;)'''迭代器与生成器 iter()创建迭代器对象 和 next()输出迭代器下一个元素 Python 的构造函数为 __init__(),__iter__() 方法返回一个特殊的迭代器对象， 这个迭代器对象实现了 __next__() 方法并通过 StopIteration 异常标识迭代的完成。__next__() 方法（Python 2 里是 next()）会返回下一个迭代器对象。'''# StopIteration 异常用于标识迭代的完成，防止出现无限循环的情况，在 __next__() 方法中我们可以设置在完成指定循环次数后触发 StopIteration 异常来结束迭代。class MyNumbers: def __iter__(self): self.a = 1 return self def __next__(self): if self.a &lt;= 20: x = self.a self.a += 1 return x else: raise StopIterationmyclass = MyNumbers()myiter = iter(myclass)for x in myiter: print(x)# 使用了 yield 的函数被称为生成器（generator）。# 跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。## 在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。## 调用一个生成器函数，返回的是一个迭代器对象。import sysdef fibonacci(n): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b counter += 1f = fibonacci(10) # f 是一个迭代器，由生成器返回生成while True: try: print(next(f), end=&quot; &quot;) except StopIteration: sys.exit() 时间 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&quot;&quot;&quot;时间&quot;&quot;&quot;# Python 提供了一个 time 和 calendar 模块可以用于格式化日期和时间。# 时间间隔是以秒为单位的浮点小数。# 每个时间戳都以自从 1970 年 1 月 1 日午夜（历元）经过了多长时间来表示。import time # 引入time模块ticks = time.time()print (&quot;当前时间戳为:&quot;, ticks)# 时间元组# 很多Python函数用一个元组装起来的9组数字处理时间:## 序号 字段 值# 0 4位数年 2008# 1 月 1 到 12# 2 日 1到31# 3 小时 0到23# 4 分钟 0到59# 5 秒 0到61 (60或61 是闰秒)# 6 一周的第几日 0到6 (0是周一)# 7 一年的第几日 1到366 (儒略历)# 8 夏令时 -1, 0, 1, -1是决定是否为夏令时的旗帜# 上述也就是struct_time元组。这种结构具有如下属性：# 序号 属性 值# 0 tm_year 2008# 1 tm_mon 1 到 12# 2 tm_mday 1 到 31# 3 tm_hour 0 到 23# 4 tm_min 0 到 59# 5 tm_sec 0 到 61 (60或61 是闰秒)# 6 tm_wday 0到6 (0是周一)# 7 tm_yday 一年中的第几天，1 到 366# 8 tm_isdst 是否为夏令时，值有：1(夏令时)、0(不是夏令时)、-1(未知)，默认 -1# 获取当前时间localtime = time.asctime( time.localtime(time.time()) )print (&quot;本地时间为 :&quot;, localtime)# 格式化# %y 两位数的年份表示（00-99）# %Y 四位数的年份表示（000-9999）# %m 月份（01-12）# %d 月内中的一天（0-31）# %H 24小时制小时数（0-23）# %I 12小时制小时数（01-12）# %M 分钟数（00=59）# %S 秒（00-59）# %a 本地简化星期名称# %A 本地完整星期名称# %b 本地简化的月份名称# %B 本地完整的月份名称# %c 本地相应的日期表示和时间表示# %j 年内的一天（001-366）# %p 本地A.M.或P.M.的等价符# %U 一年中的星期数（00-53）星期天为星期的开始# %w 星期（0-6），星期天为星期的开始# %W 一年中的星期数（00-53）星期一为星期的开始# %x 本地相应的日期表示# %X 本地相应的时间表示# %Z 当前时区的名称# %% %号本身print(time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()))# 格式化成Sat Mar 28 22:24:24 2016形式print(time.strftime(&quot;%a %b %d %H:%M:%S %Y&quot;, time.localtime()))# 将格式字符串转换为时间戳a = &quot;Sat Mar 28 22:24:24 2016&quot;print(time.mktime(time.strptime(a, &quot;%a %b %d %H:%M:%S %Y&quot;)))# 获取某月日历import calendarcal = calendar.month(2016, 1)print (&quot;以下输出2016年1月份的日历:&quot;)print (cal) JSON 123456789101112131415161718192021222324252627282930313233343536373839404142434445&quot;&quot;&quot;JSON&quot;&quot;&quot;# json.dumps(): ==编码==# dict object# list, tuple array# str string# int, float, int- &amp; float-derived Enums number# True true# False false# None null# json.loads(): ==解码==# object dict# array list# string str# number (int) int# number (real) float# true True# false False# null Noneimport json# Python 字典类型转换为 JSON 对象data = { 'no': 1, 'name': 'Runoob', 'url': 'http://www.runoob.com'}json_str = json.dumps(data)print(&quot;Python 原始数据：&quot;, repr(data))print(&quot;JSON 对象：&quot;, json_str)# 将 JSON 对象转换为 Python 字典data2 = json.loads(json_str)print (&quot;data2['name']: &quot;, data2['name'])print (&quot;data2['url']: &quot;, data2['url'])# 文件处理# 写入 JSON 数据with open('data.json', 'w') as f: json.dump(data, f)# 读取数据with open('data.json', 'r') as f: data = json.load(f) 标准库概览 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&quot;&quot;&quot;标准库概览&quot;&quot;&quot;# 操作系统接口-----------------------------import osos.getcwd() # 返回当前的工作目录os.chdir('/server/accesslogs') # 修改当前的工作目录os.system('mkdir today') # 执行系统命令 mkdirimport shutilshutil.copyfile('data.db', 'archive.db')shutil.move('/build/executables', 'installdir')# 文件通配符 ---------------------------------------------# 用于从目录通配符搜索中生成文件列表import globglob.glob('*.py')# 命令行参数------------------------------------import sysprint(sys.argv)# 错误输出重定向和程序终止-------------------------------# sys 还有 stdin，stdout 和 stderr 属性，即使在 stdout 被重定向时，后者也可以用于显示警告和错误信息。sys.stderr.write('Warning, log file not found starting a new one\\n')# 字符串正则匹配-----------------------------------------------import rere.findall(r'\\bf[a-z]*', 'which foot or hand fell fastest')re.sub(r'(\\b[a-z]+) \\1', r'\\1', 'cat in the the hat')# 数学-----------------------------------------------import mathmath.cos(math.pi / 4)math.log(1024, 2)import randomrandom.choice(['apple', 'pear', 'banana'])random.sample(range(100), 10) # sampling without replacementrandom.random() # random floatrandom.randrange(6)# 互联网-----------------------------------------------from urllib.request import urlopenfor line in urlopen('http://tycho.usno.navy.mil/cgi-bin/timer.pl'): line = line.decode('utf-8') # Decoding the binary data to text. if 'EST' in line or 'EDT' in line: # look for Eastern Time print(line)import smtplibserver = smtplib.SMTP('localhost')server.sendmail('soothsayer@example.org', 'jcaesar@example.org',&quot;&quot;&quot;To: jcaesar@example.org...From: soothsayer@example.org...... Beware the Ides of March.... &quot;&quot;&quot;)server.quit()# 日期和时间-----------------------------------------------from datetime import datenow = date.today()print(now)now.strftime(&quot;%m-%d-%y. %d %b %Y is a %A on the %d day of %B.&quot;)# dates support calendar arithmeticbirthday = date(1964, 7, 31)age = now - birthdayage.days# 数据压缩import zlibs = b'witch which has which witches wrist watch'len(s)t = zlib.compress(s)len(t)zlib.decompress(t)zlib.crc32(s)# 性能度量from timeit import TimerTimer('t=a; a=b; b=t', 'a=1; b=2').timeit()Timer('a,b = b,a', 'a=1; b=2').timeit()# 测试模块def average(values): &quot;&quot;&quot;Computes the arithmetic mean of a list of numbers. &gt;&gt;&gt; print(average([20, 30, 70])) 40.0 &quot;&quot;&quot; return sum(values) / len(values)import doctestdoctest.testmod() # 自动验证嵌入测试# unittest模块不像 doctest模块那么容易使用，不过它可以在一个独立的文件里提供一个更全面的测试集:import unittestclass TestStatisticalFunctions(unittest.TestCase): def test_average(self): self.assertEqual(average([20, 30, 70]), 40.0) self.assertEqual(round(average([1, 5, 7]), 1), 4.3) self.assertRaises(ZeroDivisionError, average, []) self.assertRaises(TypeError, average, 20, 30, 70)unittest.main() # Calling from the command line invokes all tests OOP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129&quot;&quot;&quot;OOP&quot;&quot;&quot;# ===类===# class ClassName:# &lt;statement-1&gt;# .# .# &lt;statement-N&gt;# ===类对象===# 属性引用和实例化class MyClass: &quot;&quot;&quot;一个简单的类实例&quot;&quot;&quot; i = 12345 def f(self): return 'hello world'# ===实例化类===x = MyClass()# ===访问类的属性和方法===print(&quot;MyClass 类的属性 i 为：&quot;, x.i)print(&quot;MyClass 类的方法 f 输出为：&quot;, x.f())# 类有一个名为 __init__() 的特殊方法（构造方法），该方法在类实例化时会自动调用，像下面这样：# __init__() 方法可以有参数，参数通过 __init__() 传递到类的实例化操作上，self代表类的实例，而非类def __init__(self): self.data = []class Test: def prt(self): print(self) print(self.__class__)t = Test()t.prt()# ===方法===# def定义方法,与一般函数定义不同，类方法必须包含self参数且作为第一个参数，self代表类的实例# 类定义class people: # 定义基本属性 name = '' age = 0 # 定义私有属性,私有属性在类外部无法直接进行访问 __weight = 0 # 定义构造方法 def __init__(self, n, a, w): self.name = n self.age = a self.__weight = w def speak(self): print(&quot;%s 说: 我 %d 岁。&quot; % (self.name, self.age))# 实例化类p = people('runoob', 10, 30)p.speak()# ===继承===# 子类（派生类 DerivedClassName）会继承父类（基类 BaseClassName）的属性和方法。# class DerivedClassName(BaseClassName):# &lt;statement-1&gt;# .# .# .# &lt;statement-N&gt;# ===多继承===# class DerivedClassName(Base1, Base2, Base3):# &lt;statement-1&gt;# .# .# .# &lt;statement-N&gt;# 若是父类中有相同的方法名，而在子类使用时未指定，python从左至右搜索 即方法在子类中未找到时，从左到右查找父类中是否包含方法# ===方法重写===class Parent: # 定义父类 def myMethod(self): print('调用父类方法')class Child(Parent): # 定义子类 def myMethod(self): print('调用子类方法')c = Child() # 子类实例c.myMethod() # 子类调用重写方法super(Child, c).myMethod() # 用子类对象调用父类已被覆盖的方法# ===类属性与方法===# 类的私有属性# __private_attrs：两个下划线开头，声明该属性为私有，不能在类的外部被使用或直接访问。在类内部的方法中使用时 self.__private_attrs。## 类的方法# 在类的内部，使用 def 关键字来定义一个方法，与一般函数定义不同，类方法必须包含参数 self，且为第一个参数，self 代表的是类的实例。# self 的名字并不是规定死的，也可以使用 this，但是最好还是按照约定使用 self。## 类的私有方法# __private_method：两个下划线开头，声明该方法为私有方法，只能在类的内部调用 ，不能在类的外部调用。self.__private_methods。# ==重载==class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return 'Vector (%d, %d)' % (self.a, self.b) def __add__(self, other): return Vector(self.a + other.a, self.b + other.b)v1 = Vector(2, 10)v2 = Vector(5, -2)print(v1 + v2) 函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859'''函数'''# 你可以定义一个由自己想要功能的函数，以下是简单的规则：## 函数代码块以 def 关键词开头，后接函数标识符名称和圆括号 ()。# 任何传入参数和自变量必须放在圆括号中间，圆括号之间可以用于定义参数。# 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。# 函数内容以冒号 : 起始，并且缩进。# return [表达式] 结束函数，选择性地返回一个值给调用方，不带表达式的 return 相当于返回 None。def max(a, b): if a &gt; b: return a else: return b# python 函数的参数传递：# 不可变类型：类似 C++ 的值传递，如整数、字符串、元组。如 fun(a)，传递的只是 a 的值，没有影响 a 对象本身。如果在 fun(a) 内部修改 a 的值，则是新生成一个 a 的对象。# 可变类型：类似 C++ 的引用传递，如 列表，字典。如 fun(la)，则是将 la 真正的传过去，修改后 fun 外部的 la 也会受影响# python 中一切都是对象，严格意义我们不能说值传递还是引用传递，我们应该说传不可变对象和传可变对象。# 可通过id()函数查看对象内存地址# 参数# 以下是调用函数时可使用的正式参数类型：# 必需参数【调用时的数量必须和声明时的一样】 def printme( str ):# 关键字参数【允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值】def printinfo( name, age ):# 默认参数【如果没有传递参数，则会使用默认参数】def printinfo( name, age = 35 ):# 不定长参数 【加了星号 * 的参数会以元组(tuple)的形式导入，存放所有未命名的变量参数，没有指定参数，它就是一个空元组。我们也可以不向函数传递未命名的变量，加了两个星号 ** 的参数会以字典的形式导入，如果单独出现星号 * 后的参数必须用关键字传入】def functionname([formal_args,] *var_args_tuple ):'''匿名函数'''# lambda [arg1 [,arg2,.....argn]]:expression# 可写函数说明sum = lambda arg1, arg2: arg1 + arg2# 调用sum函数print(&quot;相加后的值为 : &quot;, sum(10, 20))print(&quot;相加后的值为 : &quot;, sum(20, 20))'''强制位置参数'''# 在以下的例子中，形参 a 和 b 必须使用指定位置参数，c 或 d 可以是位置形参或关键字形参，而 e 或 f 要求为关键字形参:def f(a, b, /, c, d, *, e, f): print(a, b, c, d, e, f)# 以下使用方法是正确的:f(10, 20, 30, d=40, e=50, f=60)# 以下使用方法会发生错误:f(10, b=20, c=30, d=40, e=50, f=60) # b 不能使用关键字参数的形式f(10, 20, 30, 40, 50, f=60) # e 必须使用关键字参数的形式 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&quot;&quot;&quot;文件&quot;&quot;&quot;# 打开文件返回文件对象# file: 必需，文件路径（相对或者绝对路径）。# mode: 可选，文件打开模式# buffering: 设置缓冲# encoding: 一般使用utf8# errors: 报错级别# newline: 区分换行符# closefd: 传入的file参数类型# opener: 设置自定义开启器，开启器的返回值必须是一个打开的文件描述符。# mode 参数有：# 模式 描述# t 文本模式 (默认)。# x 写模式，新建一个文件，如果该文件已存在则会报错。# b 二进制模式。# + 打开一个文件进行更新(可读可写)。# U 通用换行模式（Python 3 不支持）。# r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。# rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。# r+ 打开一个文件用于读写。文件指针将会放在文件的开头。# rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。# w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。# w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。# wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。# a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。# a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。# ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。file = open('filetest.txt', mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)# file 对象常用的函数：# file.close() 关闭文件。关闭后文件不能再进行读写操作。# file.flush() 刷新文件内部缓冲，直接把内部缓冲区的数据立刻写入文件, 而不是被动的等待输出缓冲区写入。# file.fileno() 返回一个整型的文件描述符(file descriptor FD 整型), 可以用在如os模块的read方法等一些底层操作上。# file.isatty() 如果文件连接到一个终端设备返回 True，否则返回 False。# file.next() Python 3 中的 File 对象不支持 next() 方法。 返回文件下一行。# file.read([size]) 从文件读取指定的字节数，如果未给定或为负则读取所有。# file.readline([size]) 读取整行，包括 &quot;\\n&quot; 字符。# file.readlines([sizeint]) 读取所有行并返回列表，若给定sizeint&gt;0，返回总和大约为sizeint字节的行, 实际读取值可能比 sizeint 较大, 因为需要填充缓冲区。# file.seek(offset[, whence]) 移动文件读取指针到指定位置# file.tell() 返回文件当前位置。# file.truncate([size]) 从文件的首行首字符开始截断，截断文件为 size 个字符，无 size 表示从当前位置截断；截断之后后面的所有字符被删除，其中 windows 系统下的换行代表2个字符大小。# file.write(str) 将字符串写入文件，返回的是写入的字符长度。# file.writelines(sequence) 向文件写入一个序列字符串列表，如果需要换行则要自己加入每行的换行符。&quot;&quot;&quot;文件和目录&quot;&quot;&quot;# os 模块提供了非常丰富的方法用来处理文件和目录。常用的方法如下表所示：## 序号 方法及描述# os.access(path, mode) 检验权限模式# os.chdir(path) 改变当前工作目录# os.chflags(path, flags) 设置路径的标记为数字标记。# os.chmod(path, mode) 更改权限# os.chown(path, uid, gid) 更改文件所有者# os.chroot(path) 改变当前进程的根目录# os.close(fd) 关闭文件描述符 fd# os.closerange(fd_low, fd_high) 关闭所有文件描述符，从 fd_low (包含) 到 fd_high (不包含), 错误会忽略# os.dup(fd) 复制文件描述符 fd# os.dup2(fd, fd2) 将一个文件描述符 fd 复制到另一个 fd2# os.fchdir(fd) 通过文件描述符改变当前工作目录# os.fchmod(fd, mode) 改变一个文件的访问权限，该文件由参数fd指定，参数mode是Unix下的文件访问权限。# os.fchown(fd, uid, gid) 修改一个文件的所有权，这个函数修改一个文件的用户ID和用户组ID，该文件由文件描述符fd指定。# os.fdatasync(fd) 强制将文件写入磁盘，该文件由文件描述符fd指定，但是不强制更新文件的状态信息。# os.fdopen(fd[, mode[, bufsize]]) 通过文件描述符 fd 创建一个文件对象，并返回这个文件对象# os.fpathconf(fd, name) 返回一个打开的文件的系统配置信息。name为检索的系统配置的值，它也许是一个定义系统值的字符串，这些名字在很多标准中指定（POSIX.1, Unix 95, Unix 98, 和其它）。# os.fstat(fd) 返回文件描述符fd的状态，像stat()。# os.fstatvfs(fd) 返回包含文件描述符fd的文件的文件系统的信息，Python 3.3 相等于 statvfs()。# os.fsync(fd) 强制将文件描述符为fd的文件写入硬盘。# os.ftruncate(fd, length) 裁剪文件描述符fd对应的文件, 所以它最大不能超过文件大小。# os.getcwd() 返回当前工作目录# os.getcwdb() 返回一个当前工作目录的Unicode对象# os.isatty(fd) 如果文件描述符fd是打开的，同时与tty(-like)设备相连，则返回true, 否则False。# os.lchflags(path, flags) 设置路径的标记为数字标记，类似 chflags()，但是没有软链接# os.lchmod(path, mode) 修改连接文件权限# os.lchown(path, uid, gid) 更改文件所有者，类似 chown，但是不追踪链接。# os.link(src, dst) 创建硬链接，名为参数 dst，指向参数 src# os.listdir(path) 返回path指定的文件夹包含的文件或文件夹的名字的列表。# os.lseek(fd, pos, how) 设置文件描述符 fd当前位置为pos, how方式修改: SEEK_SET 或者 0 设置从文件开始的计算的pos; SEEK_CUR或者 1 则从当前位置计算; os.SEEK_END或者2则从文件尾部开始. 在unix，Windows中有效# os.lstat(path) 像stat(),但是没有软链接# os.major(device) 从原始的设备号中提取设备major号码 (使用stat中的st_dev或者st_rdev field)。# os.makedev(major, minor) 以major和minor设备号组成一个原始设备号# os.makedirs(path[, mode]) 递归文件夹创建函数。像mkdir(), 但创建的所有intermediate-level文件夹需要包含子文件夹。# os.minor(device) 从原始的设备号中提取设备minor号码 (使用stat中的st_dev或者st_rdev field )。# os.mkdir(path[, mode]) 以数字mode的mode创建一个名为path的文件夹.默认的 mode 是 0777 (八进制)。# os.mkfifo(path[, mode]) 创建命名管道，mode 为数字，默认为 0666 (八进制)# os.mknod(filename[, mode=0600, device]) 创建一个名为filename文件系统节点（文件，设备特别文件或者命名pipe）。# os.open(file, flags[, mode]) 打开一个文件，并且设置需要的打开选项，mode参数是可选的# os.openpty() 打开一个新的伪终端对。返回 pty 和 tty的文件描述符。# os.pathconf(path, name) 返回相关文件的系统配置信息。# os.pipe() 创建一个管道. 返回一对文件描述符(r, w) 分别为读和写# os.popen(command[, mode[, bufsize]]) 从一个 command 打开一个管道# os.read(fd, n) 从文件描述符 fd 中读取最多 n 个字节，返回包含读取字节的字符串，文件描述符 fd对应文件已达到结尾, 返回一个空字符串。# os.readlink(path) 返回软链接所指向的文件# os.remove(path) 删除路径为path的文件。如果path 是一个文件夹，将抛出OSError; 查看下面的rmdir()删除一个 directory。# os.removedirs(path) 递归删除目录。# os.rename(src, dst) 重命名文件或目录，从 src 到 dst# os.renames(old, new) 递归地对目录进行更名，也可以对文件进行更名。# os.rmdir(path) 删除path指定的空目录，如果目录非空，则抛出一个OSError异常。# os.stat(path) 获取path指定的路径的信息，功能等同于C API中的stat()系统调用。# os.stat_float_times([newvalue]) 决定stat_result是否以float对象显示时间戳# os.statvfs(path) 获取指定路径的文件系统统计信息# os.symlink(src, dst) 创建一个软链接# os.tcgetpgrp(fd) 返回与终端fd（一个由os.open()返回的打开的文件描述符）关联的进程组# os.tcsetpgrp(fd, pg) 设置与终端fd（一个由os.open()返回的打开的文件描述符）关联的进程组为pg。# os.tempnam([dir[, prefix]]) Python3 中已删除。返回唯一的路径名用于创建临时文件。# os.tmpfile() Python3 中已删除。返回一个打开的模式为(w+b)的文件对象 .这文件对象没有文件夹入口，没有文件描述符，将会自动删除。# os.tmpnam() Python3 中已删除。为创建一个临时文件返回一个唯一的路径# os.ttyname(fd) 返回一个字符串，它表示与文件描述符fd 关联的终端设备。如果fd 没有与终端设备关联，则引发一个异常。# os.unlink(path) 删除文件路径# os.utime(path, times) 返回指定的path文件的访问和修改的时间。# os.walk(top[, topdown=True[, onerror=None[, followlinks=False]]]) 输出在文件夹中的文件名通过在树中游走，向上或者向下。# os.write(fd, str) 写入字符串到文件描述符 fd中. 返回实际写入的字符串长度# os.path 模块 获取文件的属性信息。# os.pardir() 获取当前目录的父目录，以字符串形式显示目录名。 语法错误和异常 1234567891011121314151617181920212223242526&quot;&quot;&quot;语法错误和异常&quot;&quot;&quot;# 语法错误 SyntaxError# 异常 ZeroDivisionError，NameError 和 TypeError# 异常处理# ---try/except---# 一个except子句可以同时处理多个异常，这些异常将被放在一个括号里成为一个元组，例如:# except (RuntimeError, TypeError, NameError):while True: try: x = int(input(&quot;请输入一个数字: &quot;)) break except ValueError: print(&quot;您输入的不是数字，请再次尝试输入！&quot;)# ----try/except...else---else 子句将在 try 子句没有发生任何异常的时候执行# ---try-finally----# 抛出异常 raise [Exception [, args [, traceback]]]x = 10if x &gt; 5: raise Exception('x 不能大于 5。x 的值为: {}'.format(x))# 用户自定义异常 通过创建一个新的异常类来拥有自己的异常。异常类继承自 Exception 类，可以直接继承，或者间接继承# with 语句就可以保证诸如文件之类的对象在使用完之后一定会正确的执行他的清理方法with open(&quot;myfile.txt&quot;) as f: for line in f: print(line, end=&quot;&quot;) Mysql 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import mysql.connector# python -m pip install mysql-connector 安装连接驱动# python -m pip install mysql-connector-python# pip uninstall mysql-connector 卸载mydb = mysql.connector.connect( host=&quot;localhost&quot;, # 数据库主机地址 user=&quot;root&quot;, # 数据库用户名 passwd=&quot;root&quot; # 数据库密码 # auth_plugin='mysql_native_password' # database=&quot;runoob_db&quot; #有则直接连接数据库)print(mydb)# 创建数据库mycursor = mydb.cursor()mycursor.execute(&quot;CREATE DATABASE runoob_db&quot;)# 输出所有数据库列表mycursor.execute(&quot;SHOW DATABASES&quot;)for x in mycursor: print(x)# 创建表mycursor.execute(&quot;CREATE TABLE sites (name VARCHAR(255), url VARCHAR(255))&quot;)# mycursor.execute(sql语句)# 增sql = &quot;INSERT INTO sites (name, url) VALUES (%s, %s)&quot;val = (&quot;RUNOOB&quot;, &quot;https://www.runoob.com&quot;)mycursor.execute(sql, val)mydb.commit() # 数据表内容有更新，必须使用到该语句print(mycursor.rowcount, &quot;记录插入成功。&quot;)# 批量增sql = &quot;INSERT INTO sites (name, url) VALUES (%s, %s)&quot;val = [ ('Google', 'https://www.google.com'), ('Github', 'https://www.github.com'), ('Taobao', 'https://www.taobao.com'), ('stackoverflow', 'https://www.stackoverflow.com/')]mycursor.executemany(sql, val)mydb.commit() # 数据表内容有更新，必须使用到该语句print(mycursor.rowcount, &quot;记录插入成功, ID:&quot;, mycursor.lastrowid)# 查mycursor.execute(&quot;SELECT * FROM sites&quot;)myresult = mycursor.fetchall() # fetchall() 获取所有记录myresult = mycursor.fetchone() #获取一条for x in myresult: print(x)# 防sql注入sql = &quot;SELECT * FROM sites WHERE name = %s&quot;na = (&quot;RUNOOB&quot;,)mycursor.execute(sql, na)myresult = mycursor.fetchall()for x in myresult: print(x)#删sql = &quot;DELETE FROM sites WHERE name = 'stackoverflow'&quot;mycursor.execute(sql)mydb.commit()print(mycursor.rowcount, &quot; 条记录删除&quot;)# 改sql = &quot;UPDATE sites SET name = 'ZH' WHERE name = 'Zhihu'&quot;mycursor.execute(sql)mydb.commit()print(mycursor.rowcount, &quot; 条记录被修改&quot;) 网络编程 123456789101112131415161718192021222324252627282930313233343536373839&quot;&quot;&quot;网络编程&quot;&quot;&quot;# 低级别的网络服务支持基本的 Socket，它提供了标准的 BSD Sockets API，可以访问底层操作系统 Socket 接口的全部方法。# 高级别的网络服务模块 SocketServer， 它提供了服务器中心类，可以简化网络服务器的开发。# socket()# socket.socket([family[, type[, proto]]])# family: 套接字家族可以使 AF_UNIX 或者 AF_INET。# type:套接字类型可以根据是面向连接的还是非连接分为SOCK_STREAM或SOCK_DGRAM。# protocol: 一般不填默认为 0。# Socket 对象(内建)方法# 函数 描述# 服务器端套接字-----------# s.bind() 绑定地址（host,port）到套接字， 在 AF_INET下，以元组（host,port）的形式表示地址。# s.listen() 开始 TCP 监听。backlog 指定在拒绝连接之前，操作系统可以挂起的最大连接数量。该值至少为 1，大部分应用程序设为 5 就可以了。# s.accept() 被动接受TCP客户端连接,(阻塞式)等待连接的到来# 客户端套接字-----------# s.connect() 主动初始化TCP服务器连接，。一般address的格式为元组（hostname,port），如果连接出错，返回socket.error错误。# s.connect_ex() connect()函数的扩展版本,出错时返回出错码,而不是抛出异常# 公共用途的套接字函数----------# s.recv() 接收 TCP 数据，数据以字符串形式返回，bufsize 指定要接收的最大数据量。flag 提供有关消息的其他信息，通常可以忽略。# s.send() 发送 TCP 数据，将 string 中的数据发送到连接的套接字。返回值是要发送的字节数量，该数量可能小于 string 的字节大小。# s.sendall() 完整发送 TCP 数据。将 string 中的数据发送到连接的套接字，但在返回之前会尝试发送所有数据。成功返回 None，失败则抛出异常。# s.recvfrom() 接收 UDP 数据，与 recv() 类似，但返回值是（data,address）。其中 data 是包含接收数据的字符串，address 是发送数据的套接字地址。# s.sendto() 发送 UDP 数据，将数据发送到套接字，address 是形式为（ipaddr，port）的元组，指定远程地址。返回值是发送的字节数。# s.close() 关闭套接字# s.getpeername() 返回连接套接字的远程地址。返回值通常是元组（ipaddr,port）。# s.getsockname() 返回套接字自己的地址。通常是一个元组(ipaddr,port)# s.setsockopt(level,optname,value) 设置给定套接字选项的值。# s.getsockopt(level,optname[.buflen]) 返回套接字选项的值。# s.settimeout(timeout) 设置套接字操作的超时期，timeout是一个浮点数，单位是秒。值为None表示没有超时期。一般，超时期应该在刚创建套接字时设置，因为它们可能用于连接的操作（如connect()）# s.gettimeout() 返回当前超时期的值，单位是秒，如果没有设置超时期，则返回None。# s.fileno() 返回套接字的文件描述符。# s.setblocking(flag) 如果flag为0，则将套接字设为非阻塞模式，否则将套接字设为阻塞模式（默认值）。非阻塞模式下，如果调用recv()没有发现任何数据，或send()调用无法立即发送数据，那么将引起socket.error异常。# s.makefile() 创建一个与该套接字相关连的文件 123456789101112131415161718192021import socketimport sys# 创建 socket 对象s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()# 设置端口号port = 9999# 连接服务，指定主机和端口s.connect((host, port))# 接收小于 1024 字节的数据msg = s.recv(1024)s.close()print (msg.decode('utf-8')) 123456789101112131415161718192021222324252627import socketimport sys# 创建 socket 对象serversocket = socket.socket( socket.AF_INET, socket.SOCK_STREAM)# 获取本地主机名host = socket.gethostname()port = 9999# 绑定端口号serversocket.bind((host, port))# 设置最大连接数，超过后排队serversocket.listen(5)while True: # 建立客户端连接 clientsocket, addr = serversocket.accept() print(&quot;连接地址: %s&quot; % str(addr)) msg = '欢迎访问菜鸟教程！' + &quot;\\r\\n&quot; clientsocket.send(msg.encode('utf-8')) clientsocket.close() SMTP 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101&quot;&quot;&quot;SMTP&quot;&quot;&quot;# SMTP（Simple Mail Transfer Protocol）即简单邮件传输协议,它是一组用于由源地址到目的地址传送邮件的规则，由它来控制信件的中转方式。# python的smtplib提供了一种很方便的途径发送电子邮件。它对smtp协议进行了简单的封装。# Python创建 SMTP 对象语法如下：# import smtplib# smtpObj = smtplib.SMTP( [host [, port [, local_hostname]]] )# 参数说明：# host: SMTP 服务器主机。 你可以指定主机的ip地址或者域名如:runoob.com，这个是可选参数。# port: 如果你提供了 host 参数, 你需要指定 SMTP 服务使用的端口号，一般情况下SMTP端口号为25。# local_hostname: 如果SMTP在你的本机上，你只需要指定服务器地址为 localhost 即可。# Python SMTP对象使用sendmail方法发送邮件，语法如下：# SMTP.sendmail(from_addr, to_addrs, msg[, mail_options, rcpt_options]# 参数说明：# from_addr: 邮件发送者地址。# to_addrs: 字符串列表，邮件发送地址。# msg: 发送消息# 这里要注意一下第三个参数，msg是字符串，表示邮件。我们知道邮件一般由标题，发信人，收件人，邮件内容，附件等构成，发送邮件的时候，要注意msg的格式。这个格式就是smtp协议中定义的格式。import smtplibfrom email.mime.image import MIMEImagefrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextfrom email.header import Headermy_sender='714416426@qq.com' # 发件人邮箱账号my_pass = '11111111' # 发件人邮箱密码my_user='714416426@qq.com' # 收件人邮箱账号，我这边发送给自己sender = 'from@runoob.com'receivers = ['714416426@qq.com'] # 接收邮件，可设置为你的QQ邮箱或者其他邮箱msgRoot = MIMEMultipart('related')msgRoot['From'] = Header(&quot;菜鸟教程&quot;, 'utf-8')msgRoot['To'] = Header(&quot;测试&quot;, 'utf-8')subject = 'Python SMTP 邮件测试'msgRoot['Subject'] = Header(subject, 'utf-8')# 发送html内容mail_msg = &quot;&quot;&quot;&lt;p&gt;Python 邮件发送测试...&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;http://www.runoob.com&quot;&gt;这是一个链接&lt;/a&gt;&lt;/p&gt;&quot;&quot;&quot;message = MIMEText(mail_msg, 'html', 'utf-8')# 三个参数：第一个为文本内容，第二个 plain 设置文本格式，第三个 utf-8 设置编码message = MIMEText('Python 邮件发送测试...', 'plain', 'utf-8')message['From'] = Header(&quot;菜鸟教程&quot;, 'utf-8') # 发送者message['To'] = Header(&quot;测试&quot;, 'utf-8') # 接收者subject = 'Python SMTP 邮件测试'message['Subject'] = Header(subject, 'utf-8')# 发送带附件内容# # 构造附件1，传送当前目录下的 test.txt 文件# att1 = MIMEText(open('test.txt', 'rb').read(), 'base64', 'utf-8')# att1[&quot;Content-Type&quot;] = 'application/octet-stream'# # 这里的filename可以任意写，写什么名字，邮件中显示什么名字# att1[&quot;Content-Disposition&quot;] = 'attachment; filename=&quot;test.txt&quot;'# message.attach(att1)## # 构造附件2，传送当前目录下的 runoob.txt 文件# att2 = MIMEText(open('runoob.txt', 'rb').read(), 'base64', 'utf-8')# att2[&quot;Content-Type&quot;] = 'application/octet-stream'# att2[&quot;Content-Disposition&quot;] = 'attachment; filename=&quot;runoob.txt&quot;'# message.attach(att2)# 文本中添加图片# msgAlternative = MIMEMultipart('alternative')# msgRoot.attach(msgAlternative)# mail_msg = &quot;&quot;&quot;# &lt;p&gt;Python 邮件发送测试...&lt;/p&gt;# &lt;p&gt;&lt;a href=&quot;http://www.runoob.com&quot;&gt;菜鸟教程链接&lt;/a&gt;&lt;/p&gt;# &lt;p&gt;图片演示：&lt;/p&gt;# &lt;p&gt;&lt;img src=&quot;cid:image1&quot;&gt;&lt;/p&gt;# &quot;&quot;&quot;# msgAlternative.attach(MIMEText(mail_msg, 'html', 'utf-8'))## # 指定图片为当前目录# fp = open('test.png', 'rb')# msgImage = MIMEImage(fp.read())# fp.close()## # 定义图片 ID，在 HTML 文本中引用# msgImage.add_header('Content-ID', '&lt;image1&gt;')# msgRoot.attach(msgImage)try: smtpObj = smtplib.SMTP('localhost') smtpObj.sendmail(my_sender, receivers, message.as_string()) print(&quot;邮件发送成功&quot;)except smtplib.SMTPException: print(&quot;Error: 无法发送邮件&quot;) 多线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101&quot;&quot;&quot;多线程：函数或者用类来包装线程对象&quot;&quot;&quot;# 函数式：调用thread模块中的start_new_thread()函数来产生新线程。语法如下:# thread.start_new_thread ( function, args[, kwargs] )# 参数说明:# function - 线程函数。# args - 传递给线程函数的参数,他必须是个tuple类型。# kwargs - 可选参数。# 两个模块# _thread# threading(推荐使用)import _threadimport time# 为线程定义一个函数def print_time( threadName, delay): count = 0 while count &lt; 5: time.sleep(delay) count += 1 print (&quot;%s: %s&quot; % ( threadName, time.ctime(time.time()) ))# 创建两个线程try: _thread.start_new_thread( print_time, (&quot;Thread-1&quot;, 2, ) ) _thread.start_new_thread( print_time, (&quot;Thread-2&quot;, 4, ) )except: print (&quot;Error: 无法启动线程&quot;)while 1: pass# _thread 提供了低级别的、原始的线程以及一个简单的锁，它相比于 threading 模块的功能还是比较有限的。## threading 模块除了包含 _thread 模块中的所有方法外，还提供的其他方法：## threading.currentThread(): 返回当前的线程变量。# threading.enumerate(): 返回一个包含正在运行的线程的list。正在运行指线程启动后、结束前，不包括启动前和终止后的线程。# threading.activeCount(): 返回正在运行的线程数量，与len(threading.enumerate())有相同的结果。# 除了使用方法外，线程模块同样提供了Thread类来处理线程，Thread类提供了以下方法:## run(): 用以表示线程活动的方法。# start():启动线程活动。# join([time]): 等待至线程中止。这阻塞调用线程直至线程的join() 方法被调用中止-正常退出或者抛出未处理的异常-或者是可选的超时发生。# isAlive(): 返回线程是否活动的。# getName(): 返回线程名。# setName(): 设置线程名。import threadingexitFlag = 0class myThread (threading.Thread): def __init__(self, threadID, name, counter): threading.Thread.__init__(self) self.threadID = threadID self.name = name self.counter = counter def run(self): print (&quot;开始线程：&quot; + self.name) print_time(self.name, self.counter, 5) print (&quot;退出线程：&quot; + self.name)def print_time(threadName, delay, counter): while counter: if exitFlag: threadName.exit() time.sleep(delay) print (&quot;%s: %s&quot; % (threadName, time.ctime(time.time()))) counter -= 1# 创建新线程thread1 = myThread(1, &quot;Thread-1&quot;, 1)thread2 = myThread(2, &quot;Thread-2&quot;, 2)# 开启新线程thread1.start()thread2.start()thread1.join()thread2.join()print (&quot;退出主线程&quot;)# 线程同步# 使用 Thread 对象的 Lock 和 Rlock 可以实现简单的线程同步，这两个对象都有 acquire 方法和 release 方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到 acquire 和 release 方法之间# 线程优先级队列（ Queue）# Python 的 Queue 模块中提供了同步的、线程安全的队列类，包括FIFO（先入先出)队列Queue，LIFO（后入先出）队列LifoQueue，和优先级队列 PriorityQueue。# 这些队列都实现了锁原语，能够在多线程中直接使用，可以使用队列来实现线程间的同步。# Queue 模块中的常用方法:# Queue.qsize() 返回队列的大小# Queue.empty() 如果队列为空，返回True,反之False# Queue.full() 如果队列满了，返回True,反之False# Queue.full 与 maxsize 大小对应# Queue.get([block[, timeout]])获取队列，timeout等待时间# Queue.get_nowait() 相当Queue.get(False)# Queue.put(item) 写入队列，timeout等待时间# Queue.put_nowait(item) 相当Queue.put(item, False)# Queue.task_done() 在完成一项工作之后，Queue.task_done()函数向任务已经完成的队列发送一个信号# Queue.join() 实际上意味着等到队列为空，再执行别的操作 urllib 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&quot;&quot;&quot;urllib 库用于操作网页 URL，并对网页的内容进行抓取处理。&quot;&quot;&quot;# urllib.request - 打开和读取 URL。# urllib.error - 包含 urllib.request 抛出的异常。# urllib.parse - 解析 URL。# urllib.robotparser - 解析 robots.txt 文件# urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)# url：url 地址。# data：发送到服务器的其他数据对象，默认为 None。# timeout：设置访问超时时间。# cafile 和 capath：cafile 为 CA 证书， capath 为 CA 证书的路径，使用 HTTPS 需要用到。# cadefault：已经被弃用。# context：ssl.SSLContext类型，用来指定 SSL 设置。from urllib.request import urlopenmyURL = urlopen(&quot;https://www.runoob.com/&quot;)print(myURL.read())print(myURL.read(600))print(myURL.readline())lines = myURL.readlines()for line in lines: print(line)print(myURL.getcode())f = open(&quot;runoob_urllib_test.html&quot;, &quot;wb&quot;)content = myURL.read() # 读取网页内容f.write(content)f.close()import urllib.requestencode_url = urllib.request.quote(&quot;https://www.runoob.com/&quot;) # 编码print(encode_url)unencode_url = urllib.request.unquote(encode_url) # 解码print(unencode_url)# 模拟头部信息# class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)# url：url 地址。# data：发送到服务器的其他数据对象，默认为 None。# headers：HTTP 请求的头部信息，字典格式。# origin_req_host：请求的主机地址，IP 或域名。# unverifiable：很少用整个参数，用于设置网页是否需要验证，默认是False。。# method：请求方法， 如 GET、POST、DELETE、PUT等。import urllib.parseurl = 'https://www.runoob.com/?s=' # 菜鸟教程搜索页面keyword = 'Python 教程'key_code = urllib.request.quote(keyword) # 对请求进行编码url_all = url+key_codeheader = { 'User-Agent':'Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'} #头部信息request = urllib.request.Request(url_all,headers=header)reponse = urllib.request.urlopen(request).read()fh = open(&quot;./urllib_test_runoob_search.html&quot;,&quot;wb&quot;) # 将文件写入到当前目录中fh.write(reponse)fh.close() XML解析 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143&quot;&quot;&quot;XML解析&quot;&quot;&quot;# SAX 是一种基于事件驱动的API。# 利用 SAX 解析 XML 文档牵涉到两个部分: 解析器和事件处理器。# 解析器负责读取 XML 文档，并向事件处理器发送事件，如元素开始跟元素结束事件。# 而事件处理器则负责对事件作出响应，对传递的 XML 数据进行处理。# 1、对大型文件进行处理；# 2、只需要文件的部分内容，或者只需从文件中得到特定信息。# 3、想建立自己的对象模型的时候。# 在 Python 中使用 sax 方式处理 xml 要先引入 xml.sax 中的 parse 函数，还有 xml.sax.handler 中的 ContentHandler。# ContentHandler 类方法介绍# characters(content) 方法# 调用时机：# 从行开始，遇到标签之前，存在字符，content 的值为这些字符串。# 从一个标签，遇到下一个标签之前， 存在字符，content 的值为这些字符串。# 从一个标签，遇到行结束符之前，存在字符，content 的值为这些字符串。# 标签可以是开始标签，也可以是结束标签。# startDocument() 方法# 文档启动的时候调用。# endDocument() 方法# 解析器到达文档结尾时调用。# startElement(name, attrs) 方法# 遇到XML开始标签时调用，name 是标签的名字，attrs 是标签的属性值字典。# endElement(name) 方法# 遇到XML结束标签时调用。# make_parser 方法# 以下方法创建一个新的解析器对象并返回。# xml.sax.make_parser( [parser_list] )# 参数说明:# parser_list - 可选参数，解析器列表# parser 方法# 以下方法创建一个 SAX 解析器并解析xml文档：# xml.sax.parse( xmlfile, contenthandler[, errorhandler])# 参数说明:# xmlfile - xml文件名# contenthandler - 必须是一个 ContentHandler 的对象# errorhandler - 如果指定该参数，errorhandler 必须是一个 SAX ErrorHandler 对象# parseString 方法# parseString 方法创建一个 XML 解析器并解析 xml 字符串：# xml.sax.parseString(xmlstring, contenthandler[, errorhandler])# 参数说明:# xmlstring - xml字符串# contenthandler - 必须是一个 ContentHandler 的对象# errorhandler - 如果指定该参数，errorhandler 必须是一个 SAX ErrorHandler对象import xml.sax# from xml.dom.minidom import parse# import xml.dom.minidom# 使用xml.dom解析xml# 文件对象模型（Document Object Model，简称DOM），是W3C组织推荐的处理可扩展置标语言的标准编程接口。# 一个 DOM 的解析器在解析一个 XML 文档时，一次性读取整个文档，把文档中所有元素保存在内存中的一个树结构里，之后你可以利用DOM 提供的不同的函数来读取或修改文档的内容和结构，也可以把修改过的内容写入xml文件。# python中用xml.dom.minidom来解析xml文件# 使用minidom解析器打开 XML 文档# DOMTree = xml.dom.minidom.parse(&quot;movies.xml&quot;)# collection = DOMTree.documentElement# if collection.hasAttribute(&quot;shelf&quot;):# print (&quot;Root element : %s&quot; % collection.getAttribute(&quot;shelf&quot;))## # 在集合中获取所有电影# movies = collection.getElementsByTagName(&quot;movie&quot;)## # 打印每部电影的详细信息# for movie in movies:# print (&quot;*****Movie*****&quot;)# if movie.hasAttribute(&quot;title&quot;):# print (&quot;Title: %s&quot; % movie.getAttribute(&quot;title&quot;))## type = movie.getElementsByTagName('type')[0]# print (&quot;Type: %s&quot; % type.childNodes[0].data)# format = movie.getElementsByTagName('format')[0]# print (&quot;Format: %s&quot; % format.childNodes[0].data)# rating = movie.getElementsByTagName('rating')[0]# print (&quot;Rating: %s&quot; % rating.childNodes[0].data)# description = movie.getElementsByTagName('description')[0]# print (&quot;Description: %s&quot; % description.childNodes[0].data)class MovieHandler(xml.sax.ContentHandler): def __init__(self): self.CurrentData = &quot;&quot; self.type = &quot;&quot; self.format = &quot;&quot; self.year = &quot;&quot; self.rating = &quot;&quot; self.stars = &quot;&quot; self.description = &quot;&quot; # 元素开始调用 def startElement(self, tag, attributes): self.CurrentData = tag if tag == &quot;movie&quot;: print(&quot;*****Movie*****&quot;) title = attributes[&quot;title&quot;] print(&quot;Title:&quot;, title) # 元素结束调用 def endElement(self, tag): if self.CurrentData == &quot;type&quot;: print(&quot;Type:&quot;, self.type) elif self.CurrentData == &quot;format&quot;: print(&quot;Format:&quot;, self.format) elif self.CurrentData == &quot;year&quot;: print(&quot;Year:&quot;, self.year) elif self.CurrentData == &quot;rating&quot;: print(&quot;Rating:&quot;, self.rating) elif self.CurrentData == &quot;stars&quot;: print(&quot;Stars:&quot;, self.stars) elif self.CurrentData == &quot;description&quot;: print(&quot;Description:&quot;, self.description) self.CurrentData = &quot;&quot; # 读取字符时调用 def characters(self, content): if self.CurrentData == &quot;type&quot;: self.type = content elif self.CurrentData == &quot;format&quot;: self.format = content elif self.CurrentData == &quot;year&quot;: self.year = content elif self.CurrentData == &quot;rating&quot;: self.rating = content elif self.CurrentData == &quot;stars&quot;: self.stars = content elif self.CurrentData == &quot;description&quot;: self.description = contentif (__name__ == &quot;__main__&quot;): # 创建一个 XMLReader parser = xml.sax.make_parser() # 关闭命名空间 parser.setFeature(xml.sax.handler.feature_namespaces, 0) # 重写 ContextHandler Handler = MovieHandler() parser.setContentHandler(Handler) parser.parse(&quot;movies.xml&quot;) 应用领域 Web and Internet Development Database Access Desktop GUIs Scientific &amp; Numeric Education Network Programming Software &amp; Game Development 爬虫 WEB开发 图形处理 深度学习 数据分析","link":"/2021/07/06/Draft/2021/Python3%E5%AD%A6%E4%B9%A0/"},{"title":"Spring","text":"Spring5 轻量级的开源的 JavaEE 框架 进度【55，剩余Webflux暂时未学】 源码笔记加入 进度【】 Spring5框架概述 1、Spring 是轻量级的开源的 JavaEE 框架 2、Spring 可以解决企业应用开发的复杂性 3、Spring 有两个核心部分：IOC 和AOP （1） IOC【Inversion of Control】：控制反转，把创建对象过程交给 Spring 进行管理 （2） Aop【Aspect Oriented Programming】：面向切面，不修改源代码进行功能增强 4、Spring 特点 （1） 方便解耦，简化开发 （2） Aop 编程支持 （3） 方便程序测试 （4） 方便和其他框架进行整合 （5） 方便进行事务操作 （6） 降低 API 开发难度 相关资源 官网 版本说明： snapshot 快照 alpha 内测 beta 公测 release 稳定版本 GA 最稳定版本 Final 正式版 Pro(professional) 专业版 Plus 加强版 Retail 零售版 DEMO 演示版 Build 内部标号 Delux 豪华版 (deluxe：豪华的，华丽的) Corporation或Enterpraise 企业版 M1 M2 M3 M是milestone的简写 里程碑的意思 RC 版本RC:(Release Candidate)，几乎就不会加入新的功能了，而主要着重于除错 SR 修正版 Trial 试用版 Shareware 共享版 Full 完全版 下载： https://repo.spring.io/ui/native/release https://repo.spring.io/ui/native/release/org/springframework/spring 注：本文档使用5.2.6 创建普通工程 新建项目 引入依赖 对应 Core中四个包引入，以及commons-logging-1.1.1.jar 新建lib文件夹并引入jar包 创建测试类 12345public class User { public void Add(){ System.out.println(&quot;add.......&quot;); }} 创建XML 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--配置User对象创建--&gt; &lt;bean id=&quot;user&quot; class=&quot;com.lxl.spring5.User&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 测试代码 123456789101112131415161718192021222324252627282930package com.lxl.spring5.testdemo;import com.lxl.spring5.User;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.context.support.FileSystemXmlApplicationContext;public class TestSpring5 { @Test public void testAdd(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定盘符路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean1.xml&quot;); //2 获取配置创建的对象，user为配置文件中id User user = context.getBean(&quot;user&quot;, User.class); System.out.println(user); user.add(); }}结果：com.intellij.rt.junit.JUnitStarter -ideVersion5 com.lxl.spring5.testdemo.TestSpring5,testAddcom.lxl.spring5.User@46daef40add.......Process finished with exit code 0 IOC【Inversion of Control】 概念原理 概念 （1） 控制反转，把对象创建和对象之间的调用过程，交给 Spring 进行管理 （2） 使用 IOC 目的：为了耦合度降低（改） （3） 上面入门案例就是 IOC 实现 原理 xml 解析、工厂模式、反射 底层 优化1 new对象---》工厂模式创建（让创建类与被创建类不耦合，比如被创建类地址改变创建类必须改变） 优化2 解析XML--》反射创建对象 IOC容器 1、IOC思想基于IOC容器完成，IOC容器底层就是对象工厂 2、Spring提供IOC容器实现两种方式：（两个接口） （1）BeanFactory：IOC容器基本实现，是Spring内部的使用接口，不提供开发人员进行使用。加载配置文件时候不创建对象，在获取对象（使用）才去创建对象 （2）ApplicationContext：BeanFactory接口的子接口，提供更多更强大的功能，一般由开发人员进行使用。加载配置文件时候就会把在配置文件对象进行创建，服务器启动时，对web运行时效果更好。 查看实现类（Ctrl+H） ApplicationContext： IOC操作Bean管理 1、什么是Bean管理 （0）Bean管理指的是两个操作 （1）Spring创建对象 （2）Spirng注入属性 2、Bean管理操作有两种方式 （1）xml配置文件方式 1、基于xml方式创建对象 12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--配置User对象创建 （1）在spring配置文件中，使用bean标签，标签里面添加对应属性，就可以实现对象创建 （2）在bean标签有很多属性，介绍常用的属性 * id属性：唯一标识，不可加特殊字符 * name :唯一表示，可加特殊字符 * class属性：类全路径（包类路径） （3）创建对象时候，默认也是执行无参数构造方法完成对象创建 --&gt; &lt;bean id=&quot;user&quot; class=&quot;com.lxl.spring5.User&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 2、基于xml方式注入属性 ​ DI：依赖注入，就是注入属性,IOC的一种具体实现，注入属性需要在创建对象的基础之上完成。 2.1set方法进行注入 创建类，定义属性和对应的set方法 123456789101112131415public class Book { //创建属性 private String bname; private String bauthor; //set方法进行注入属性 public void setBname(String bname) { this.bname = bname; } public void setBauthor(String bauthor) { this.bauthor = bauthor; }} 在spring配置文件配置对象创建，配置属性注入 1234567&lt;!--=========================set方法注入属性=============================================--&gt; &lt;!--2 set方法注入属性--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.Book&quot;&gt; &lt;!--使用property完成属性注入 name：类里面属性名称 value：向属性注入的值 --&gt; &lt;property name=&quot;bname&quot; value=&quot;易筋经&quot;&gt;&lt;/property&gt; &lt;property name=&quot;bauthor&quot; value=&quot;达摩老祖&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 2.2有参数构造进行注入 创建类，定义属性，创建属性对应有参数构造方法 123456789101112131415161718/** * 使用有参数构造注入 */public class Orders { //属性 private String oname; private String address; //有参数构造 public Orders(String oname, String address) { this.oname = oname; this.address = address; } public void ShowOrder() { System.out.println(oname+&quot;::&quot;+address); }} 配置文件创建对象，有参注入属性 12345678&lt;!--==========================有参构造注入===========================================--&gt; &lt;!--3 有参构造注入--&gt; &lt;bean id=&quot;orders&quot; class=&quot;com.lxl.spring5.Orders&quot;&gt;&lt;!-- &lt;constructor-arg index=&quot;0&quot; value=&quot;&quot;/&gt; 效果同下--&gt; &lt;constructor-arg name=&quot;oname&quot; value=&quot;电脑&quot;&gt;&lt;/constructor-arg&gt;&lt;!-- &lt;constructor-arg index=&quot;1&quot; value=&quot;&quot;/&gt;--&gt; &lt;constructor-arg name=&quot;address&quot; value=&quot;China&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; 2.3p名称空间注入 使用p名称空间注入，可以简化基于xml配置方式 第一步 添加p名称空间在配置文件中 1234&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; 第二步 进行属性注入，在bean标签里面进行操作 1234567891011&lt;!--=========================set方法注入属性=============================================--&gt; &lt;!--2 set方法注入属性--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.Book&quot;&gt; &lt;!--使用property完成属性注入 name：类里面属性名称 value：向属性注入的值 --&gt; &lt;property name=&quot;bname&quot; value=&quot;易筋经&quot;&gt;&lt;/property&gt; &lt;property name=&quot;bauthor&quot; value=&quot;达摩老祖&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;!--2.1 p名称空间注入方式，已达到简化目的--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.Book&quot; p:bname=&quot;易筋经&quot; p:bauthor=&quot;达摩老祖&quot;&gt;&lt;/bean&gt; 2.4xml注入其他类型属性 **字面量：**属性设置的固定值。 null值 ： &lt;property name=&quot;bname&quot; value=&quot;易筋经&quot;&gt; 包含特殊符号： 解决：转义、特殊内容写到 即： 注入属性-外部bean （1）创建两个类 service类和dao类 （2）在service调用dao里面的方法 123456789101112131415161718192021222324package com.lxl.spring5.service;import com.lxl.spring5.dao.UserDao;import com.lxl.spring5.dao.UserDaoImpl;public class UserService { //创建UserDao类型属性，生成set方法 private UserDao userDao; public void setUserDao(UserDao userDao) { this.userDao = userDao; } public void add() { System.out.println(&quot;service add...............&quot;); // 原始创建对象// UserDao userDao = new UserDaoImpl();// userDao.update(); }} 123456789package com.lxl.spring5.dao;/** * @author Administrator */public interface UserDao { public void update();} 12345678package com.lxl.spring5.dao;public class UserDaoImpl implements UserDao{ @Override public void update() { System.out.println(&quot;dao update--------&quot;); }} （3）在spring配置文件中进行配置 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!--1 service和dao对象创建--&gt; &lt;bean id=&quot;userService&quot; class=&quot;com.lxl.spring5.service.UserService&quot;&gt; &lt;!--注入userDao对象 name属性：类里面属性名称 ref属性：创建userDao对象bean标签id值 --&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDaoImpl&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;userDaoImpl&quot; class=&quot;com.lxl.spring5.dao.UserDaoImpl&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; （4）测试 12345678910111213 @Test public void testAdd(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean2.xml&quot;); //2 获取配置创建的对象，user为配置文件中id UserService userService = context.getBean(&quot;userService&quot;, UserService.class); System.out.println(userService); userService.add(); } 注入属性-内部bean （1）一对多两个类： 12345678910111213141516171819202122232425package com.lxl.spring5.bean;//员工类public class Emp { private String ename; private String gender; //一对多 //员工属于某一个部门，使用对象形式表示 private Dept dept; public void setDept(Dept dept) { this.dept = dept; } public void setEname(String ename) { this.ename = ename; } public void setGender(String gender) { this.gender = gender; }public void add(){ System.out.println(ename+&quot;::&quot;+gender+&quot;::&quot;+dept);}} 1234567891011121314151617package com.lxl.spring5.bean;//部门类public class Dept { private String dname; public void setDname(String dname) { this.dname = dname; } @Override public String toString() { return &quot;Dept{&quot; + &quot;dname='&quot; + dname + '\\'' + '}'; }} （2）xml配置 1234567891011&lt;!--+++++++++++++++++++注入属性-内部bean：一对多+++++++++++++++++++++++--&gt; &lt;!--内部bean--&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.bean.Emp&quot;&gt; &lt;!--设置两个普通属性--&gt; &lt;property name=&quot;ename&quot; value=&quot;lucy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;gender&quot; value=&quot;女&quot;&gt;&lt;/property&gt; &lt;!--设置对象类型属性--&gt; &lt;property name=&quot;dept&quot;&gt; &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.bean.Dept&quot;&gt; &lt;property name=&quot;dname&quot; value=&quot;安保部&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; （3）测试： 12345678910111213 @Test public void testOneToMore(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean3.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Emp emp = context.getBean(&quot;emp&quot;, Emp.class); System.out.println(emp); emp.add(); } 注入属性-级联赋值两种方式 1234567891011121314151617181920212223 &lt;!-- +++++++++++++++++++ 级联赋值两种方式 +++++++++++++++++++++++--&gt; &lt;!--级联赋值 方式一 --&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.bean.Emp&quot;&gt; &lt;!--设置两个普通属性--&gt; &lt;property name=&quot;ename&quot; value=&quot;lucy&quot;&gt;&lt;/property&gt; &lt;property name=&quot;gender&quot; value=&quot;女&quot;&gt;&lt;/property&gt; &lt;!--级联赋值--&gt; &lt;property name=&quot;dept&quot; ref=&quot;dept&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.bean.Dept&quot;&gt; &lt;property name=&quot;dname&quot; value=&quot;财务部&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--级联赋值 方式二 --&gt;&lt;!-- 设置两个普通属性 dept.dname方式需要添加dept的get方法 --&gt;&lt;!-- &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.bean.Emp&quot;&gt; --&gt;&lt;!-- &lt;property name=&quot;ename&quot; value=&quot;lucy&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;gender&quot; value=&quot;女&quot;&gt;&lt;/property&gt; --&gt;&lt;!-- &lt;property name=&quot;dept&quot; ref=&quot;dept&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;dept.dname&quot; value=&quot;技术部&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;/bean&gt;--&gt;&lt;!-- &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.bean.Dept&quot;&gt;--&gt;&lt;!-- &lt;property name=&quot;dname&quot; value=&quot;财务部&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;/bean&gt;--&gt; 2.5xml注入集合类属性 （1）创建类，定义数组、list、map、set类型属性，生成对应set方法 12345678910111213141516171819202122232425262728293031323334353637383940package com.lxl.spring5.collectiontype;import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.Set;public class Stu { //1 数组类型属性 private String[] courses; //2 list集合类型属性 private List&lt;String&gt; list; //3 map集合类型属性 private Map&lt;String, String&gt; maps; //4 set集合类型属性 private Set&lt;String&gt; sets; public void setSets(Set&lt;String&gt; sets) { this.sets = sets; } public void setCourses(String[] courses) { this.courses = courses; } public void setList(List&lt;String&gt; list) { this.list = list; } public void setMaps(Map&lt;String, String&gt; maps) { this.maps = maps; } public void test(){ System.out.println(Arrays.toString(courses)); System.out.println(list); System.out.println(maps); System.out.println(sets); }} （2）xml配置 12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- +++++++++++++++++++ 集合类属性注入 +++++++++++++++++++++++--&gt; &lt;!--1 集合类型属性注入--&gt; &lt;bean id=&quot;stu&quot; class=&quot;com.lxl.spring5.collectiontype.Stu&quot;&gt; &lt;!--数组类型属性注入--&gt; &lt;property name=&quot;courses&quot;&gt; &lt;array&gt; &lt;value&gt;java课程&lt;/value&gt; &lt;value&gt;数据库课程&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!--list类型属性注入--&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;张三&lt;/value&gt; &lt;value&gt;小三&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--map类型属性注入--&gt; &lt;property name=&quot;maps&quot;&gt; &lt;map&gt; &lt;entry key=&quot;JAVA&quot; value=&quot;java&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;PHP&quot; value=&quot;php&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--set类型属性注入--&gt; &lt;property name=&quot;sets&quot;&gt; &lt;set&gt; &lt;value&gt;MySQL&lt;/value&gt; &lt;value&gt;Redis&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; （3）测试 12345678910111213141516171819202122package com.lxl.spring5.testdemo;import com.lxl.spring5.bean.Emp;import com.lxl.spring5.collectiontype.Stu;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestCollection { @Test public void testOneToMore(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean5.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Stu stu = context.getBean(&quot;stu&quot;, Stu.class); stu.test(); }} 2.6在集合里面设置对象类型值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.lxl.spring5.collectiontype;import java.util.Arrays;import java.util.List;import java.util.Map;import java.util.Set;public class Stu { //1 数组类型属性 private String[] courses; //2 list集合类型属性 private List&lt;String&gt; list; //3 map集合类型属性 private Map&lt;String, String&gt; maps; //4 set集合类型属性 private Set&lt;String&gt; sets; //5 List集合中为对象类型属性 private List&lt;Course&gt; courseList; public void setCourseList(List&lt;Course&gt; courseList) { this.courseList = courseList; } public void setSets(Set&lt;String&gt; sets) { this.sets = sets; } public void setCourses(String[] courses) { this.courses = courses; } public void setList(List&lt;String&gt; list) { this.list = list; } public void setMaps(Map&lt;String, String&gt; maps) { this.maps = maps; } public void test(){ System.out.println(Arrays.toString(courses)); System.out.println(list); System.out.println(maps); System.out.println(sets); System.out.println(courseList); }} 12345678910111213141516package com.lxl.spring5.collectiontype;public class Course { private String cname; public void setCname(String cname) { this.cname = cname; } @Override public String toString() { return &quot;Course{&quot; + &quot;cname='&quot; + cname + '\\'' + '}'; }} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;!-- +++++++++++++++++++ 集合类属性注入、集合内为对象情况注入 +++++++++++++++++++++++--&gt; &lt;!--1 集合类型属性注入--&gt; &lt;bean id=&quot;stu&quot; class=&quot;com.lxl.spring5.collectiontype.Stu&quot;&gt; &lt;!--数组类型属性注入--&gt; &lt;property name=&quot;courses&quot;&gt; &lt;array&gt; &lt;value&gt;java课程&lt;/value&gt; &lt;value&gt;数据库课程&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;!--list类型属性注入--&gt; &lt;property name=&quot;list&quot;&gt; &lt;list&gt; &lt;value&gt;张三&lt;/value&gt; &lt;value&gt;小三&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--map类型属性注入--&gt; &lt;property name=&quot;maps&quot;&gt; &lt;map&gt; &lt;entry key=&quot;JAVA&quot; value=&quot;java&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;PHP&quot; value=&quot;php&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--set类型属性注入--&gt; &lt;property name=&quot;sets&quot;&gt; &lt;set&gt; &lt;value&gt;MySQL&lt;/value&gt; &lt;value&gt;Redis&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;!--注入list集合类型，值是对象--&gt; &lt;property name=&quot;courseList&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;course1&quot;&gt;&lt;/ref&gt; &lt;ref bean=&quot;course2&quot;&gt;&lt;/ref&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;course1&quot; class=&quot;com.lxl.spring5.collectiontype.Course&quot;&gt; &lt;property name=&quot;cname&quot; value=&quot;Spring5框架&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;course2&quot; class=&quot;com.lxl.spring5.collectiontype.Course&quot;&gt; &lt;property name=&quot;cname&quot; value=&quot;MyBatis框架&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 把集合注入部分提取出来 1234567891011121314package com.lxl.spring5.collectiontype;import java.util.List;public class Book { private List&lt;String&gt; list; public void setList(List&lt;String&gt; list) { this.list = list; } public void test() { System.out.println(list); }} 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; &lt;!-- +++++++++++++++++++ 集合内为对象情况注入 +++++++++++++++++++++++--&gt; &lt;!--1 加util元空间 xmlns:util=&quot;http://www.springframework.org/schema/util&quot; http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd --&gt; &lt;!--2 提取list集合类型属性注入--&gt; &lt;util:list id=&quot;bookList&quot;&gt; &lt;value&gt;易筋经&lt;/value&gt; &lt;value&gt;九阴真经&lt;/value&gt; &lt;value&gt;九阳神功&lt;/value&gt; &lt;/util:list&gt; &lt;!--3 提取list集合类型属性注入使用--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.collectiontype.Book&quot;&gt; &lt;property name=&quot;list&quot; ref=&quot;bookList&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 测试类 1234567891011121314151617181920212223242526272829303132333435package com.lxl.spring5.testdemo;import com.lxl.spring5.collectiontype.Book;import com.lxl.spring5.bean.Emp;import com.lxl.spring5.collectiontype.Stu;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestCollection { @Test public void testOneToMore(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean5.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Stu stu = context.getBean(&quot;stu&quot;, Stu.class); stu.test(); } @Test public void testOneToMore1(){//1 加载 spring 配置文件// ClassPathXmlApplicationContext() web src 下配置文件// FileSystemXmlApplicationContext() 指定路径配置文件 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean6.xml&quot;); //2 获取配置创建的对象，user为配置文件中id Book book = context.getBean(&quot;book&quot;, Book.class); book.test(); }} 2.7FactoryBean 1、Spring有两种类型bean，一种普通bean，另外一种工厂bean（FactoryBean） 2、普通bean：在配置文件中定义bean类型就是返回类型 3、工厂bean：在配置文件定义bean类型可以和返回类型不一样 第一步 创建类，让这个类作为工厂bean，实现接口 FactoryBean 第二步 实现接口里面的方法，在实现的方法中定义返回的bean类型 1234567891011121314151617181920212223242526package com.lxl.spring5.factorybean;import com.lxl.spring5.collectiontype.Course;import org.springframework.beans.factory.FactoryBean;/**FactoryBean*/public class MyBean implements FactoryBean&lt;Course&gt; { /**定义返回bean类型*/ @Override public Course getObject() throws Exception { Course course=new Course(); course.setCname(&quot;体育课&quot;); return course; } @Override public Class&lt;?&gt; getObjectType() { return null; } @Override public boolean isSingleton() { return FactoryBean.super.isSingleton(); }} 12&lt;bean id=&quot;myBean&quot; class=&quot;com.lxl.spring5.factorybean.MyBean&quot;&gt;&lt;/bean&gt; 12345678910 @Test public void testFactoryBean(){ ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Course myBean = context.getBean(&quot;myBean&quot;, Course.class); System.out.println(myBean); } 2.8bean 作用域 1、在Spring里面，设置创建bean实例是单实例还是多实例 2、在Spring里面，默认情况下，bean是单实例对象 3、如何设置单实例还是多实例 （1）在spring配置文件bean标签里面有属性（scope）用于设置单实例还是多实例 （2）scope属性值 第一个值 默认值，singleton，表示是单实例对象 第二个值 prototype，表示是多实例对象 其他：request、session （3）singleton和prototype区别 第一 singleton单实例，prototype多实例。 第二 设置scope值是singleton时候，加载spring配置文件时候就会创建单实例对象。 设置scope值是prototype时候，不是在加载spring配置文件时候创建对象，而是在在调用getBean方法时候创建多实例对象。 2.9Bean生命周期 （1）通过构造器创建bean实例（无参数构造） （2）为bean的属性设置值和对其他bean引用（调用set方法） （3）调用bean的初始化的方法（需要进行配置初始化的方法） （4）bean可以使用了（对象获取到了） （5）当容器关闭时候，调用bean的销毁的方法（需要进行配置销毁的方法） 123456789101112131415161718192021222324package com.lxl.spring5.lifecyclebean;public class Orders { //无参数构造 public Orders() { System.out.println(&quot;第一步 执行无参数构造创建bean实例&quot;); } private String oname; public void setOname(String oname) { this.oname = oname; System.out.println(&quot;第二步 调用set方法设置属性值&quot;); } //创建执行的初始化的方法 public void initMethod() { System.out.println(&quot;第三步 执行初始化的方法&quot;); } //创建执行的销毁的方法 public void destroyMethod() { System.out.println(&quot;第五步 执行销毁的方法&quot;); }} 1234&lt;!-- +++++++++++++++++++bean生命周期+++++++++++++++++++++++--&gt;&lt;bean id=&quot;orders&quot; class=&quot;com.lxl.spring5.lifecyclebean.Orders&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;oname&quot; value=&quot;键盘&quot;&gt;&lt;/property&gt;&lt;/bean&gt; 1234567891011121314 @Test public void TestLifeCycle(){// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;);// ClassPathXmlApplicationContext 为ApplicationContext子实现类 ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean8.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Orders myBean = context.getBean(&quot;orders&quot;, Orders.class); System.out.println(&quot;第四步 获取创建的bean实例&quot;); System.out.println(myBean); context.close(); } 2.10后置处理器 123456789101112131415161718package com.lxl.spring5.lifecyclebean;import org.springframework.beans.BeansException;import org.springframework.beans.factory.config.BeanPostProcessor;public class BeforeAfter implements BeanPostProcessor { @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;后置处理器：在初始化之前执行的方法&quot;); return BeanPostProcessor.super.postProcessBeforeInitialization(bean, beanName); } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { System.out.println(&quot;后置处理器：在初始化之后执行的方法&quot;); return BeanPostProcessor.super.postProcessAfterInitialization(bean, beanName); }} 12345678&lt;!--+++++++++++++++++++bean生命周期、后置处理器+++++++++++++++++++++++--&gt;&lt;bean id=&quot;orders&quot; class=&quot;com.lxl.spring5.lifecyclebean.Orders&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt; &lt;property name=&quot;oname&quot; value=&quot;键盘&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--后置处理器 ,在此配置文件中的所有bean添加后置处理器--&gt;&lt;bean id=&quot;myBeanPost&quot; class=&quot;com.lxl.spring5.lifecyclebean.BeforeAfter&quot;&gt;&lt;/bean&gt; 2.12自动装配 1234567891011package com.lxl.spring5.autowire;/** * @author Administrator */public class Dept { @Override public String toString() { return &quot;Dept{}&quot;; }} 1234567891011121314151617181920package com.lxl.spring5.testdemo;import com.lxl.spring5.autowire.Emp;import com.lxl.spring5.lifecyclebean.Orders;import org.junit.Test;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAutoWire { @Test public void TestLifeCycle(){// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;);// ClassPathXmlApplicationContext 为ApplicationContext子实现类 ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean9.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Emp emp = context.getBean(&quot;emp&quot;, Emp.class); System.out.println(emp); }} 1234567891011121314151617181920&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd&quot;&gt; &lt;!--+++++++++++++++++++自动装配 autowire=&quot;byName/byType&quot;+++++++++++++++++++++++--&gt; &lt;!--实现自动装配 bean标签属性autowire，配置自动装配 autowire属性常用两个值： byName根据属性名称注入 ，注入值bean的id值和类属性名称一样，如下dept1不会注入。 byType根据属性类型注入 --&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.autowire.Emp&quot; autowire=&quot;byName&quot;&gt;&lt;!-- Could not autowire. There is more than one bean of 'Dept' type. Beans: dept,dept1. Properties: 'dept'--&gt;&lt;!-- &lt;bean id=&quot;emp&quot; class=&quot;com.lxl.spring5.autowire.Emp&quot; autowire=&quot;byType&quot;&gt;--&gt;&lt;!-- 原始模式--&gt;&lt;!-- &lt;property name=&quot;dept&quot; ref=&quot;dept&quot;&gt;&lt;/property&gt;--&gt; &lt;/bean&gt; &lt;!--后置处理器 ,在此配置文件中的所有bean添加后置处理器--&gt; &lt;bean id=&quot;dept&quot; class=&quot;com.lxl.spring5.autowire.Dept&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;dept1&quot; class=&quot;com.lxl.spring5.autowire.Dept&quot;&gt;&lt;/bean&gt;&lt;/beans&gt; 测试 1234567891011121314151617181920package com.lxl.spring5.testdemo;import com.lxl.spring5.autowire.Emp;import com.lxl.spring5.lifecyclebean.Orders;import org.junit.Test;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAutoWire { @Test public void TestLifeCycle(){// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean7.xml&quot;);// ClassPathXmlApplicationContext 为ApplicationContext子实现类 ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean9.xml&quot;); //2 获取配置创建的对象，user为配置文件中id// MyBean myBean = context.getBean(&quot;myBean&quot;, MyBean.class); Emp emp = context.getBean(&quot;emp&quot;, Emp.class); System.out.println(emp); }} 2.13引入外部属性文件 1、直接配置数据库信息 （1）配置德鲁伊连接池 （2）引入德鲁伊连接池依赖jar包 ​ 复制到lib文件夹并添加项目依赖 1234567891011121314151617181920212223242526272829303132&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context =&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--+++++++++++++++++++引入外部属性文件，数据库连接池示例+++++++++++++++++++++++--&gt; &lt;!-- 原始形式，固定值--&gt;&lt;!-- &lt;bean id=&quot;dept1&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt;--&gt;&lt;!-- &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://localhost:3306/userDb&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;username&quot; value=&quot;root&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- &lt;property name=&quot;password&quot; value=&quot;root&quot;&gt;&lt;/property&gt;--&gt;&lt;!-- 引入外部文件--&gt;&lt;!-- &lt;/bean&gt;--&gt;&lt;!-- 引入外部文件形式--&gt;&lt;!-- 引入外部文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt;&lt;!-- 配置连接池--&gt; &lt;bean id=&quot;dept1&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${prop.driverClass}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;${prop.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;${prop.userName}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${prop.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; jdbc.properties 1234prop.driverClass=com.mysql.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/userDbprop.userName=rootprop.password=root (2)注解方式 1、什么是注解 （1）注解是代码特殊标记，格式：@注解名称(属性名称=属性值, 属性名称=属性值..) （2）使用注解，注解作用在类上面，方法上面，属性上面 （3）使用注解目的：简化xml配置 2、Spring针对Bean管理中创建对象提供注解 （1）@Component （2）@Service （3）@Controller （4）@Repository 上面四个注解功能是一样的，都可以用来创建bean实例，但是习惯在不同层使用不同注解。 引入aop的jar包 类注解 12345678910111213141516171819202122232425package com.lxl.spring5.annotation;import org.springframework.stereotype.Component;import org.springframework.stereotype.Controller;import org.springframework.stereotype.Repository;import org.springframework.stereotype.Service;//在注解里面value属性值可以省略不写，// 默认值是类名称，首字母小写// UserService -- userService@Component(value = &quot;userService&quot;) //等同&lt;bean id=&quot;userService&quot; class=&quot;..&quot;/&gt;/** * @author Administrator*///@Component//@Service//@Controller//@Repository // 四个注解效果相同，习惯用在不同层public class UserService { public void add() { System.out.println(&quot;service add ...&quot;); }} 开启组件扫描 1234567891011121314151617181920212223242526272829&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!--+++++++++++++++++++注解方式，扫描包+++++++++++++++++++++++--&gt; &lt;!--添加context命名空间--&gt; &lt;!-- 开启组件扫描 1.扫描多个包，可逗号隔开 2.扫描多个包的上层目录 --&gt; &lt;context:component-scan base-package=&quot;com.lxl&quot;&gt;&lt;/context:component-scan&gt; &lt;!--示例1 use-default-filters=&quot;false&quot; 表示现在不使用默认filter，自己配置filter context:include-filter ，设置扫描哪些内容 --&gt; &lt;context:component-scan base-package=&quot;com.lxl&quot; use-default-filters=&quot;false&quot;&gt; &lt;context:include-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt; &lt;!--示例2 下面配置扫描包所有内容 context:exclude-filter： 设置哪些内容不进行扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl&quot;&gt; &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt; &lt;/context:component-scan&gt;&lt;/beans&gt; 测试 12345678910111213141516171819package com.lxl.spring5.testdemo;import com.lxl.spring5.annotation.UserService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;/** * @author Administrator */public class TestAnnotation { @Test public void testAnnotation() {// spring-aop-4.3.10.RELEASE 包未导入会报错：Unexpected exception parsing XML document from class path resource ApplicationContext applicationContext=new ClassPathXmlApplicationContext(&quot;bean11.xml&quot;); UserService userService = applicationContext.getBean(&quot;userService&quot;, UserService.class); userService.add(); }} 3.纯注解开发 添加配置类扫描包 12345678910package com.lxl.spring5.annotation.allbyannotation.config;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;@Configuration@ComponentScan(basePackages = {&quot;com.lxl.spring5.annotation.allbyannotation&quot;})public class SpringConfig {} 测试 123456789101112131415161718package com.lxl.spring5.testdemo;import com.lxl.spring5.annotation.allbyannotation.config.SpringConfig;import com.lxl.spring5.annotation.allbyannotation.service.UserService1;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAllByAnnotation { @Test public void testAllByAnnotation(){ //加载配置类 ApplicationContext context = new AnnotationConfigApplicationContext(SpringConfig.class); UserService1 userService1 = context.getBean(&quot;userService1&quot;, UserService1.class); userService1.add(); }} AOP【Aspect Oriented Programming】 AOP介绍 1、什么是AOP （1）面向切面编程（方面），利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率。 （2）通俗描述：不通过修改源代码方式，在主干功能里面添加新功能、比如登录的权限过滤。 AOP底层原理 1、AOP底层使用动态代理的两种情况： 第一种 有接口情况，使用JDK动态代理 ⚫ 创建接口实现类代理对象，增强类的方法 第二种 没有接口情况，使用CGLIB动态代理 ⚫ 创建子类的代理对象，增强类的方法 AOP JDK 动态代理 1、使用JDK动态代理，使用Proxy类里面的方法创建代理对象 （1）创建接口，定义方法 12345678package com.lxl.spring5.aop.jdkdynamicproxy;public interface UserDao { public int add(int a, int b); public String update(String id);} （2）创建接口实现类，实现方法 12345678910111213package com.lxl.spring5.aop.jdkdynamicproxy;public class UserDaoImpl implements UserDao { @Override public int add(int a, int b) { return a + b; } @Override public String update(String id) { return id; }} （3）使用Proxy类创建接口代理对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.lxl.spring5.aop.jdkdynamicproxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.util.Arrays;/** * 使用Proxy创建代理对象 */public class JDKProxy { public static void main(String[] args) { //创建接口实现类代理对象 Class[] interfaces = {UserDao.class};// getClassLoader类加载器// interfaces要实现的接口// Proxy.newProxyInstance(JDKProxy.class.getClassLoader(), interfaces, new InvocationHandler() {// @Override// public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {// return null;// }// }); UserDaoImpl userDao = new UserDaoImpl(); UserDao dao = (UserDao) Proxy.newProxyInstance(JDKProxy.class.getClassLoader(), interfaces, new UserDaoProxy(userDao)); int result = dao.add(1, 2); System.out.println(&quot;result:&quot; + result); }}//创建代理对象代码class UserDaoProxy implements InvocationHandler { //1 把创建的是谁的代理对象，把谁传递过来 // 有参数构造传递代理对象 private Object obj; public UserDaoProxy(Object obj) { this.obj = obj; } //增强的逻辑// invoke方法类对象创建后即被调用 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //方法之前 System.out.println(&quot;方法之前执行....&quot; + method.getName() + &quot; :传递的参数...&quot; + Arrays.toString(args)); // 被增强的方法执行 Object res = method.invoke(obj, args); // 方法之后 System.out.println(&quot;方法之后执行....&quot; + obj); return res; }} AOP主要术语 1.连接点 可以被增强的方法 2.切入点 实际被增强的方法 3.通知（增强） 实际增强的部分。通知类型：前置通知、后置通知、环绕通知、异常通知、最终通知 4.切面 把通知应用到切入点的过程动作 AOP操作（准备） 1、Spring框架一般都是基于AspectJ实现AOP操作 （1）AspectJ不是Spring组成部分，独立AOP框架，一般把AspectJ和Spirng框架一起使用，进行AOP操作 2、基于AspectJ实现AOP操作 （1）基于xml配置文件实现 （2）基于注解方式实现（使用） 3、在项目工程里面引入AOP相关依赖 4、切入点表达式 （1）切入点表达式作用：知道对哪个类里面的哪个方法进行增强 （2）语法结构： execution([权限修饰符] [返回类型] [类全路径] [方法名称 ]-([参数列表])) 举例1：对com.atguigu.dao.BookDao类里面的add进行增强 execution(* com.atguigu.dao.BookDao.add(..)) 举例2：对com.atguigu.dao.BookDao类里面的所有的方法进行增强 execution(* com.atguigu.dao.BookDao.* (..)) 举例3：对com.atguigu.dao包里面所有类，类里面所有方法进行增强 execution(* com.atguigu.dao.. (..)) AOP操作（ 注解） 1、创建类，在类里面定义方法 12345678import org.springframework.stereotype.Component;@Componentpublic class User { public void add() { System.out.println(&quot;add.......&quot;); }} 2、创建增强类（编写增强逻辑） ​ 在增强类里面，创建方法，让不同方法代表不同通知类型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.lxl.spring5.aop.aspectjanno;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.stereotype.Component;//增强的类@Component@Aspect// 生成代理对象public class UserProxy { // 前置通知 //@Before注解表示作为前置通知 @Before(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void before() { System.out.println(&quot;before.........&quot;); } //后置通知（返回通知）// 在方法返回结果之后执行// 异常时不执行 @AfterReturning(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void afterReturning() { System.out.println(&quot;afterReturning.........&quot;); } //最终通知// 方法之后执行// 异常时也执行 @After(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void after() { System.out.println(&quot;after.........&quot;); } //异常通知// 出现异常执行 @AfterThrowing(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void afterThrowing() { System.out.println(&quot;afterThrowing.........&quot;); } //环绕通知 @Around(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable { System.out.println(&quot;环绕之前.........&quot;);// 被增强的方法执行 proceedingJoinPoint.proceed(); System.out.println(&quot;环绕之后.........&quot;); }} 3、进行通知的配置 （1）在spring配置文件中，开启注解扫描 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!--=====================AOP注解使用======================--&gt; &lt;!-- 添加命名空间：aop、context--&gt; &lt;!-- 开启注解扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl.spring5.aop.aspectjanno&quot;&gt;&lt;/context:component-scan&gt; &lt;!-- 开启Aspect生成代理对象--&gt; &lt;aop:aspectj-autoproxy&gt;&lt;/aop:aspectj-autoproxy&gt;&lt;/beans&gt; 4.测试 12345678@Testpublic void TestAopAnno() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;aop1.xml&quot;); User user= context.getBean(&quot;user&quot;,User.class); user.add();} 5.细节：共同切入点抽取 1234567891011 //共同切入点抽取 @Pointcut(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void samepoint() { } // 前置通知 //@Before注解表示作为前置通知 @Before(value = &quot;samepoint()&quot;)// @Before(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void before() { System.out.println(&quot;before.........&quot;); } 6、细节：设置增强类优先级 ​ 有多个增强类多同一个方法进行增强，设置增强类优先级，在增强类上面添加注解 @Order(数字类型值)，数字类型值越小优先级越高。 1234567891011121314151617181920212223242526import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.*;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;//增强的类@Component@Aspect@Order(1)// 生成代理对象public class PersonProxy { //共同切入点抽取 @Pointcut(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void samepoint() { } // 前置通知 //@Before注解表示作为前置通知 @Before(value = &quot;samepoint()&quot;)// @Before(value = &quot;execution(* com.lxl.spring5.aop.aspectjanno.User.add(..))&quot;) public void before() { System.out.println(&quot;PersonProxy before.........&quot;); }} 7.完全使用注解开发 （1）创建配置类，不需要创建xml配置文件 @Configuration @ComponentScan(basePackages = {&quot;com.lxl&quot;}) @EnableAspectJAutoProxy(proxyTargetClass = true) public class ConfigAop AOP操作（XML） 1.创建类与增强类 12345public class Book { public void buy() { System.out.println(&quot;buy--&quot;); }} 1234567package com.lxl.spring5.aop.aopxml;public class BookProxy { public void before() { System.out.println(&quot;before----&quot;); }} 2.配置切入点 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt; &lt;!--=====================AOP xml操作======================--&gt; &lt;!--创建对象--&gt; &lt;bean id=&quot;book&quot; class=&quot;com.lxl.spring5.aop.aopxml.Book&quot;/&gt; &lt;bean id=&quot;bookProxy&quot; class=&quot;com.lxl.spring5.aop.aopxml.BookProxy&quot;/&gt; &lt;!--配置aop增强--&gt; &lt;aop:config&gt; &lt;!--切入点--&gt; &lt;aop:pointcut id=&quot;p&quot; expression=&quot;execution(* com.lxl.spring5.aop.aopxml.Book.buy(..))&quot;/&gt; &lt;!--配置切面，即把通知（增强的部分bookProxy）ref应用到切入点（buy()）的过程动作--&gt; &lt;aop:aspect ref=&quot;bookProxy&quot;&gt; &lt;!--增强作用在具体的方法上--&gt; &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;p&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 3.测试 1234567891011121314151617package com.lxl.spring5.aop.aopxml;import com.lxl.spring5.aop.aopanno.User;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class TestAopXml { @Test public void TestAopAnno() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;aop2.xml&quot;); Book book= context.getBean(&quot;book&quot;,Book.class); book.buy(); }} JdbcTemplate 依赖准备 配置连接池 jdbc.properties 1234prop.driverClass=com.mysql.jdbc.Driverprop.url=jdbc:mysql://localhost:3306/learn?useUnicode=true&amp;characterEncoding=utf8prop.userName=rootprop.password=root 1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 引入外部文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;!-- 配置连接池--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${prop.driverClass}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;${prop.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;${prop.userName}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${prop.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!--注入dataSource--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 组件扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl.spring5.jdbctemplatel&quot;&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 所有数据库基本操作 创建实体类、service类，创建dao类，在dao注入jdbcTemplate对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185//=======================Dao=============================import com.lxl.spring5.jdbctemplatel.entity.Book;import java.util.List;public interface BookDao { public void add(Book book); public void delete(String id); public void updateBook(Book book); public int selectCount(); public Book selectOneBook(String bookid); public List&lt;Book&gt; selectAllBooks(); public void batchInsert(List&lt;Object[]&gt; books); public void batchUpdateBook(List&lt;Object[]&gt; batchArgs); public void batchDeleteBook(List&lt;Object[]&gt; batchArgs);}//=======================DaoImpl=============================import com.lxl.spring5.jdbctemplatel.entity.Book;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;import java.util.Arrays;import java.util.List;@Repositorypublic class BookDaoImpl implements BookDao { //注入JdbcTemplate @Autowired private JdbcTemplate jdbcTemplate; @Override public void add(Book book) { String sql = &quot;insert into book values(?,?,?) &quot;; String args[] = {book.getBookId(), book.getBookName(), book.getBookStatus()}; int update = jdbcTemplate.update(sql, args); System.out.println(&quot;更新了&quot; + update + &quot;条数据&quot;); } @Override public void updateBook(Book book) { String sql = &quot;update book set bookname=?,bookstatus=? where bookid=?&quot;; Object[] args = {book.getBookName(), book.getBookStatus(), book.getBookId()}; int update = jdbcTemplate.update(sql, args); System.out.println(update); } @Override public void delete(String id) { String sql = &quot;delete from book where bookid=?&quot;; int update = jdbcTemplate.update(sql, id); System.out.println(update); } //查询表记录数 @Override public int selectCount() { String sql = &quot;select count(*) from book&quot;; Integer count = jdbcTemplate.queryForObject(sql, Integer.class); return count; } //查询单个对象 @Override public Book selectOneBook(String bookid) { String sql = &quot;select * from book where bookid=?&quot;; Book book = jdbcTemplate.queryForObject(sql, new BeanPropertyRowMapper&lt;Book&gt;(Book.class), bookid); return book; } //查询多个对象 @Override public List&lt;Book&gt; selectAllBooks() { String sql = &quot;select * from book&quot;; List&lt;Book&gt; query = jdbcTemplate.query(sql, new BeanPropertyRowMapper&lt;Book&gt;(Book.class)); return query; } @Override public void batchInsert(List&lt;Object[]&gt; books) { String sql = &quot;insert into book values(?,?,?)&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, books); System.out.println(Arrays.toString(ints)); }// 批量修改 @Override public void batchUpdateBook(List&lt;Object[]&gt; batchArgs) { String sql = &quot;update book set bookname=?,bookstatus=? where bookid=?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); }// 批量删除 @Override public void batchDeleteBook(List&lt;Object[]&gt; batchArgs) { String sql = &quot;delete from book where bookid=?&quot;; int[] ints = jdbcTemplate.batchUpdate(sql, batchArgs); System.out.println(Arrays.toString(ints)); }}//======================Service==============================import com.lxl.spring5.jdbctemplatel.dao.BookDao;import com.lxl.spring5.jdbctemplatel.entity.Book;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class BookService { //注入dao @Autowired private BookDao bookDao; public void addBook(Book book){ bookDao.add(book); } public void updateBook(Book book){ bookDao.updateBook(book); } public void delete(String id){ bookDao.delete(id); } public int selectCount(){ return bookDao.selectCount(); } public Book selectOneBook(String bookid){ return bookDao.selectOneBook(bookid); } public List&lt;Book&gt; selectAllBooks(){ return bookDao.selectAllBooks(); } public void batchInsert(List&lt;Object[]&gt; books){ bookDao.batchInsert(books); }; public void batchUpdateBook(List&lt;Object[]&gt; books){ bookDao.batchUpdateBook(books); }; public void batchDeleteBook(List&lt;Object[]&gt; books){ bookDao.batchDeleteBook(books); };}//========================entity============================public class Book { private String BookId; private String BookName; private String BookStatus; @Override public String toString() { return &quot;Book{&quot; + &quot;BookId='&quot; + BookId + '\\'' + &quot;, BookName='&quot; + BookName + '\\'' + &quot;, BookStatus='&quot; + BookStatus + '\\'' + '}'; } public String getBookId() { return BookId; } public void setBookId(String bookId) { BookId = bookId; } public String getBookName() { return BookName; } public void setBookName(String bookName) { BookName = bookName; } public String getBookStatus() { return BookStatus; } public void setBookStatus(String bookStatus) { BookStatus = bookStatus; }} 新建对应数据库字段后测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import com.lxl.spring5.jdbctemplatel.entity.Book;import com.lxl.spring5.jdbctemplatel.service.BookService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import java.util.ArrayList;import java.util.List;public class BookTest { @Test public void jdbcTemplateTest() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;jdbctemplate1.xml&quot;);// 增加// Book book = new Book();// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// book.setBookId(&quot;1&quot;);// book.setBookName(&quot;Java宝典&quot;);// book.setBookStatus(&quot;完好&quot;);// bookService.addBook(book);// 修改，无值改为null// Book book = new Book();// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// book.setBookId(&quot;1&quot;);// book.setBookStatus(&quot;改了&quot;);// bookService.updateBook(book);// 根据id删除// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// bookService.delete(&quot;1&quot;);// 查询数目// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// int i = bookService.selectCount();// System.out.println(i);// 查询单个对象// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// Book book = bookService.selectOneBook(&quot;1&quot;);// System.out.println(book);// 查询一组对象// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// List&lt;Book&gt; book = bookService.selectAllBooks();// System.out.println(book);// 批量新增一组对象// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// List&lt;Object[]&gt; books = new ArrayList&lt;&gt;();// Object[] o1={&quot;3&quot;,&quot;java&quot;,&quot;a&quot;};// Object[] o2={&quot;4&quot;,&quot;orcle&quot;,&quot;b&quot;};// Object[] o3={&quot;5&quot;,&quot;net&quot;,&quot;v&quot;};// books.add(o1);// books.add(o2);// books.add(o3);// bookService.batchInsert(books);// 批量修改// BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class);// List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;();//// sql参数顺序// Object[] o1 = {&quot;java1&quot;, &quot;a3&quot;, &quot;3&quot;};// Object[] o2 = {&quot;orcle1&quot;, &quot;b4&quot;, &quot;4&quot;};// batchArgs.add(o1);// batchArgs.add(o2);//// 调用方法实现批量修改// bookService.batchUpdateBook(batchArgs); //批量删除 BookService bookService = context.getBean(&quot;bookService&quot;, BookService.class); List&lt;Object[]&gt; batchArgs = new ArrayList&lt;&gt;(); Object[] o1 = {&quot;3&quot;}; Object[] o2 = {&quot;4&quot;}; batchArgs.add(o1); batchArgs.add(o2); //调用方法实现批量删除 bookService.batchDeleteBook(batchArgs); }} 可能BUG Client does not support authentication protocol requested by server; 123mysql -hlocalhost -uroot -pALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'root';FLUSH PRIVILEGES; Unknown initial character set index '255' received from server. Initial client character set can be forced via the 'characterEncoding' property. 12String url后添加?useUnicode=true&amp;characterEncoding=utf8 mybatis url后添加?useUnicode=true&amp;characterEncoding=utf8 事务 1.什么是事务 （1）事务是数据库操作最基本单元，逻辑上一组操作，要么都成功，如果有一个失败所有操作都失败 （2）典型场景：银行转账 lucy 转账100元 给mary lucy少100，mary多100 2.事务四个特性（ACID） （1）原子性 （2）一致性 （3）隔离性 （4）持久性 3.事物测试环境搭建 创建数据库 123456789101112131415161718192021SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;-- ------------------------------ Table structure for t_account-- ----------------------------DROP TABLE IF EXISTS `t_account`;CREATE TABLE `t_account` ( `id` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `username` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL, `money` int NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of t_account-- ----------------------------INSERT INTO `t_account` VALUES ('1', 'lucy', 900);INSERT INTO `t_account` VALUES ('2', 'mary', 1100);SET FOREIGN_KEY_CHECKS = 1; 创建service，搭建dao，完成对象创建和注入关系 123456package com.lxl.spring5.transactionl.dao;public interface UserDao { public void reduceMoney(); public void addMoney();} 1234567891011121314151617181920212223242526package com.lxl.spring5.transactionl.dao;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;@Repositorypublic class UserDaoImpl implements UserDao { @Autowired private JdbcTemplate jdbcTemplate; //lucy转账100给mary // 少钱 @Override public void reduceMoney() { String sql = &quot;update t_account set money=money-? where username=?&quot;; jdbcTemplate.update(sql, 100, &quot;lucy&quot;); } //多钱 @Override public void addMoney() { String sql = &quot;update t_account set money=money+? where username=?&quot;; jdbcTemplate.update(sql, 100, &quot;mary&quot;); }} 1234567891011121314151617181920package com.lxl.spring5.transactionl.service;import com.lxl.spring5.transactionl.dao.UserDao;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;@Servicepublic class UserService { // 注入dao @Autowired private UserDao userDao; //转账的方法 public void accountMoney() { // lucy少100 userDao.reduceMoney(); // mary多100 userDao.addMoney(); }} 1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 引入外部文件--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;!-- 配置连接池--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;${prop.driverClass}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;url&quot; value=&quot;${prop.url}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;username&quot; value=&quot;${prop.userName}&quot;&gt;&lt;/property&gt; &lt;property name=&quot;password&quot; value=&quot;${prop.password}&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- JdbcTemplate对象 --&gt; &lt;bean id=&quot;jdbcTemplate&quot; class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;!--注入dataSource--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- 组件扫描 --&gt; &lt;context:component-scan base-package=&quot;com.lxl.spring5.transactionl&quot;&gt;&lt;/context:component-scan&gt;&lt;/beans&gt; 测试 12345678910111213141516package com.lxl.spring5.transactionl.test;import com.lxl.spring5.transactionl.service.UserService;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class transactionTest {@Test public void transactionTest() { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;transaction1.xml&quot;); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); userService.accountMoney();}} 4.出现异常 出现异常后，lucy少100，mary不会变导致数据更改错误。 解决1：try检测异常，catch手动回滚 解决2：使用Spring声明式事务，如下5所示。 1234567public void accountMoney() { // lucy少100 userDao.reduceMoney(); int i=10/0; // mary多100 userDao.addMoney();} 5.事务操作（半注解） 事务添加到JavaEE三层结构里面Service层（业务逻辑层） 在Spring进行事务管理操作 （1）有两种方式：编程式事务管理和声明式事务管理（使用） 声明式事务管理 （1）基于注解方式（使用） （2）基于xml配置文件方式 在Spring进行声明式事务管理，底层使用AOP原理 Spring事务管理API （1）提供一个接口，代表事务管理器，这个接口针对不同的框架提供不同的实现类 （2）配置事物管理器，引入名称空间 tx，开启事物注解 12345678910111213&lt;!--1.引入名称空间tx--&gt;xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot;http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&lt;!--2.创建事务管理器--&gt;&lt;bean id=&quot;transactionManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;!--3.注入数据源--&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;&gt;&lt;/property&gt;&lt;/bean&gt;&lt;!--4.开启事务注解--&gt;&lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;&gt;&lt;/tx:annotation-driven&gt;&lt;!--5.添加@Transactional注解到需要的类或者方法上，其位置决定起作用域。--&gt; 6.事务参数 propagation：事务传播行为 ​ （1）事务传播行为用来描述由某一个事务传播行为修饰的方法被嵌套进另一个方法的时事务如何传播。 用伪代码说明： 123456789public void methodA(){ methodB(); //doSomething}@Transaction(Propagation=XXX)public void methodB(){ //doSomething} ​ 代码中methodA()方法嵌套调用了methodB()方法，methodB()的事务传播行为由@Transaction(Propagation=XXX)设置决定。这里需要注意的是methodA()并没有开启事务，某一个事务传播行为修饰的方法并不是必须要在开启事务的外围方法中调用。 ​ （2）Spring中七种事务传播行为 事务传播行为类型 说明 PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。 PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 ioslation：事务隔离级别 ​ 1.首先说明一下事务并发引起的三种情况： 1) Dirty Reads 脏读 一个事务正在对数据进行更新操作，但是更新还未提交，另一个事务这时也来操作这组数据，并且读取了前一个事务还未提交的数据，而前一个事务如果操作失败进行了回滚，后一个事务读取的就是错误数据，这样就造成了脏读。 2) Non-Repeatable Reads 不可重复读 一个事务多次读取同一数据，在该事务还未结束时，另一个事务也对该数据进行了操作，而且在第一个事务两次次读取之间，第二个事务对数据进行了更新，那么第一个事务前后两次读取到的数据是不同的，这样就造成了不可重复读。 3) Phantom Reads 幻像读 第一个数据正在查询符合某一条件的数据，这时，另一个事务又插入了一条符合条件的数据，第一个事务在第二次查询符合同一条件的数据时，发现多了一条前一次查询时没有的数据，仿佛幻觉一样，这就是幻像读。 非重复度和幻像读的区别： 非重复读是指同一查询在同一事务中多次进行，由于其他提交事务所做的修改或删除，每次返回不同的结果集，此时发生非重复读。幻像读是指同一查询在同一事务中多次进行，由于其他提交事务所做的插入操作，每次返回不同的结果集，此时发生幻像读。表面上看，区别就在于非重复读能看见其他事务提交的修改和删除，而幻像能看见其他事务提交的插入。 ​ 2.隔离级别： 1) DEFAULT （默认） 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别。另外四个与JDBC的隔离级别相对应。 2) READ_UNCOMMITTED （读未提交） 这是事务最低的隔离级别，它允许另外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。 3) READ_COMMITTED （读已提交） 保证一个事务修改的数据提交后才能被另外一个事务读取，另外一个事务不能读取该事务未提交的数据。这种事务隔离级别可以避免脏读出现，但是可能会出现不可重复读和幻像读。 4) REPEATABLE_READ （可重复读） 这种事务隔离级别可以防止脏读、不可重复读，但是可能出现幻像读。它除了保证一个事务不能读取另一个事务未提交的数据外，还保证了不可重复读。 5) SERIALIZABLE（串行化） 这是花费最高代价但是最可靠的事务隔离级别，事务被处理为顺序执行。除了防止脏读、不可重复读外，还避免了幻像读。 timeout：超时时间 （1）事务需要在一定时间内进行提交，如果不提交进行回滚 （2）默认值是 -1 ，设置时间以秒单位进行计算 readOnly：是否只读 （1）读：查询操作，写：添加修改删除操作 （2）readOnly默认值false，表示可以查询，可以添加修改删除操作 （3）设置readOnly值是true，设置成true之后，只能查询 rollbackFor：回滚 （1）设置出现哪些异常进行事务回滚 noRollbackFor：不回滚 （1）设置出现哪些异常不进行事务回滚 7.事务操作（XML） 续用上面示例，添加一下配置实现以XML方式实现事物操作，去掉@Transaction注解后复制测试方法进行测试。 1234567891011121314&lt;!--添加aop命名空间--&gt; &lt;!--无需事务注解配置--&gt;&lt;!-- &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;&gt;&lt;/tx:annotation-driven&gt;--&gt; &lt;!--配置通知--&gt; &lt;tx:advice id=&quot;txadvice&quot;&gt; &lt;!--配置事务参数--&gt; &lt;tx:attributes&gt; &lt;!--指定哪种规则的方法上面添加事务--&gt; &lt;tx:method name=&quot;accountMoney&quot; propagation=&quot;REQUIRED&quot;/&gt; &lt;!--&lt;tx:method name=&quot;account*&quot;/&gt;--&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!--3 配置切入点和切面--&gt; &lt;aop:config&gt; &lt;!--配置切入点--&gt; &lt;aop:pointcut id=&quot;pt&quot; expression=&quot;execution(* com.lxl.spring5.transactionl.service.UserService.*(..))&quot;/&gt; &lt;!--配置切面--&gt; &lt;aop:advisor advice-ref=&quot;txadvice&quot; pointcut-ref=&quot;pt&quot;/&gt; &lt;/aop:config&gt; 8.事务操作（完全注解） 添加配置类 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.lxl.spring5.transactionl;import com.alibaba.druid.pool.DruidDataSource;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.datasource.DataSourceTransactionManager;import org.springframework.transaction.annotation.EnableTransactionManagement;import javax.sql.DataSource;@Configuration//配置类@ComponentScan(basePackages = &quot;com.lxl.spring5.transactionl&quot;)//组件扫描@EnableTransactionManagement// 开启事务public class TxConfig { // 创建数据库连接池 @Bean public DruidDataSource getDruidDataSource() { DruidDataSource dataSource = new DruidDataSource(); dataSource.setDriverClassName(&quot;com.mysql.jdbc.Driver&quot;); dataSource.setUrl(&quot;jdbc:mysql://localhost:3306/learn?useUnicode=true&amp;characterEncoding=utf8&quot;); dataSource.setUsername(&quot;root&quot;); dataSource.setPassword(&quot;root&quot;); return dataSource; } //创建JdbcTemplate对象 @Bean public JdbcTemplate getJdbcTemplate(DataSource dataSource) { //到ioc容器中根据类型找到dataSource JdbcTemplate jdbcTemplate = new JdbcTemplate(); // 注入dataSource jdbcTemplate.setDataSource(dataSource);return jdbcTemplate;} //创建事务管理器 @Bean public DataSourceTransactionManager getDataSourceTransactionManager(DataSource dataSource) { DataSourceTransactionManager transactionManager = new DataSourceTransactionManager(); transactionManager.setDataSource(dataSource); return transactionManager; }} 测试 123456789 //全注解方式测试// 先去掉@Transaction注解 @Test public void transactionAllAnnoTest() { ApplicationContext context = new AnnotationConfigApplicationContext(TxConfig.class); UserService userService = context.getBean(&quot;userService&quot;, UserService.class); userService.accountMoney(); } Spring5新功能 Log4j2 整个Spring5框架的代码基于Java8，运行时兼容JDK9，许多不建议使用的类和方法在代码库中删除Spring 5.0框架自带了通用的日志封装 （1）Spring5已经移除Log4jConfigListener，官方建议使用Log4j2 （2）Spring5框架整合Log4j2 第一步 引入jar包 准备 导入jar包并导入依赖【2021.12 Log4j2.17.0 RCE(CVE-2021-44832)漏洞】生产建议使用2.17.1以上 创建配置文件log4j2 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!--日志级别以及优先级排序: OFF &gt; FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE &gt; ALL --&gt;&lt;!--Configuration后面的status用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，可以看到log4j2内部各种详细输出--&gt;&lt;!--设置INFO会输出前面级别所有日志--&gt;&lt;configuration status=&quot;INFO&quot;&gt; &lt;!--先定义所有的appender--&gt; &lt;appenders&gt; &lt;!--输出日志信息到控制台--&gt; &lt;console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt; &lt;!--控制日志输出的格式--&gt; &lt;PatternLayout pattern=&quot;%d{yyyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot;/&gt; &lt;/console&gt; &lt;/appenders&gt; &lt;!--然后定义logger，只有定义了logger并引入的appender，appender才会生效--&gt; &lt;!--root：用于指定项目的根日志，如果没有单独指定Logger，则会使用root作为默认的日志输出--&gt; &lt;loggers&gt; &lt;root level=&quot;info&quot;&gt; &lt;appender-ref ref=&quot;Console&quot;/&gt; &lt;/root&gt; &lt;/loggers&gt;&lt;/configuration&gt; 任意运行程序可见日志，更改 查看不同级别日志。 手动加日志 12345@Testpublic void manualLogTest(){ final Logger log = LoggerFactory.getLogger(transactionTest.class); log.warn(&quot;hello log4j==============&quot;);} @Nullable Spring5框架核心容器支持@Nullable注解 （1）@Nullable注解可以使用在方法上面，属性上面，参数上面，表示方法返回可以为空，属性值可以为空，参数值可以为空 （2）注解用在方法上面，方法返回值可以为空 （3）注解使用在方法参数里面，方法参数可以为空 （4）注解使用在属性上面，属性值可以为空 GenericApplicationContext Spring5核心容器支持函数式风格GenericApplicationContext,把自己new的对象加入搜spring管理中。 123456789101112//函数式风格创建对象，交给spring进行管理 @Test public void testGenericApplicationContext() { //1 创建GenericApplicationContext对象 GenericApplicationContext context = new GenericApplicationContext(); //2 调用context的方法对象注册 context.refresh(); context.registerBean(&quot;user1&quot;,User.class,() -&gt; new User()); //3 获取在spring注册的对象 // User user = (User)context.getBean(&quot;com.atguigu.spring5.test.User&quot;); User user = (User)context.getBean(&quot;user1&quot;); System.out.println(user); } Spring5支持整合JUnit5 （1）整合JUnit4 第一步 引入Spring相关针对测试依赖 测试代码 1234567891011121314151617181920212223package com.lxl.spring5.JunitTest;import com.lxl.spring5.transactionl.service.UserService;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)//单元测试框架 @ContextConfiguration(&quot;classpath:transaction1.xml&quot;)//加载配置文件public class JunitTest4 { @Autowired private UserService userService; @Test public void test1() { userService.accountMoney(); }} （2）Junit5 idea点击@Test导入包 1234567891011121314151617181920212223package com.lxl.spring5.JunitTest;import com.lxl.spring5.transactionl.service.UserService;import org.junit.jupiter.api.Test;import org.junit.jupiter.api.extension.ExtendWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit.jupiter.SpringExtension;import org.springframework.test.context.junit.jupiter.SpringJUnitConfig;//@ExtendWith(SpringExtension.class)//@ContextConfiguration(&quot;classpath:transaction1.xml&quot;)//SpringJUnitConfig效果同上面两个同时使用@SpringJUnitConfig(locations = &quot;classpath:transaction1.xml&quot;)public class JunitTest5 { @Autowired private UserService userService; @Test public void test1() { userService.accountMoney(); }} Webflux 1、SpringWebflux介绍 （1）是Spring5添加新的模块，用于web开发的，功能和SpringMVC类似的，Webflux使用当前一种比较流程响应式编程出现的框架。 （2）使用传统web框架，比如SpringMVC，这些基于Servlet容器，Webflux是一种异步非阻塞的框架，异步非阻塞的框架在Servlet3.1以后才支持，核心是基于Reactor的相关API实现的。 （3）解释什么是异步非阻塞 异步和同步 非阻塞和阻塞 上面都是针对对象不一样 异步和同步针对调用者，调用者发送请求，如果等着对方回应之后才去做其他事情就是同步，如果发送请求之后不等着对方回应就去做其他事情就是异步阻塞和非阻塞针对被调用者，被调用者受到请求之后，做完请求任务之后才给出反馈就是阻塞，受到请求之后马上给出反馈然后再去做事情就是非阻塞 （4）Webflux特点： 第一 非阻塞式：在有限资源下，提高系统吞吐量和伸缩性，以Reactor为基础实现响应式编程 第二 函数式编程：Spring5框架基于java8，Webflux使用Java8函数式编程方式实现路由请求 （5）比较SpringMVC 第一 两个框架都可以使用注解方式，都运行在Tomet等容器中 第二 SpringMVC采用命令式编程，Webflux采用异步响应式编程 时间紧迫，未完待续","link":"/2021/02/24/Draft/2021/Spring/"},{"title":"魑魅先生 | 前端","text":"VUE、HTML、JS、、、、 VUE 1.1. 基础 1.1.1. 安装 浏览器插件 Vue Devtools 12345&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue/dist/vue.js&quot;&gt;&lt;/script&gt;&lt;!--尽量明确版号--&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14&quot;&gt;&lt;/script&gt;&lt;!--原生ES--&gt;&lt;script type=&quot;module&quot;&gt; import Vue from 'https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.esm.browser.js' &lt;/script&gt; NPM • NPM 能很好地和诸如 webpack 或 Browserify 模块打包器配合使用。同时 Vue 也提供配套工具来开发单文件组件。 # 最新稳定版 $ npm install vue · 命令行工具 (CLI) • Vue 提供了一个官方的 CLI，为单页面应用 (SPA) 快速搭建繁杂的脚手架。它为现代前端工作流提供了 batteries-included 的构建设置。只需要几分钟的时间就可以运行起来并带有热重载、保存时 lint 校验，以及生产环境可用的构建版本。 1.1.2. 简介 · Vue (读音 /vjuː/，类似于 view) 是一套用于构建用户界面的渐进式框架。与其它大型框架不同的是，Vue 被设计为可以自底向上逐层应用。Vue 的核心库只关注视图层，不仅易于上手，还便于与第三方库或既有项目整合。另一方面，当与现代化的工具链以及各种支持类库结合使用时，Vue 也完全能够为复杂的单页应用提供驱动。 · 声明式渲染 • 指令 • v-bind • 把数据绑定到 DOM 文本或 attribute或DOM 结构 · 条件与循环 • 指令 • v-for • 绑定数组的数据来渲染一个项目列表： • v-if • 控制切换一个元素是否显示 · 处理用户输入 • 指令 • v-on · 组件化应用构建 • // 定义名为 todo-item 的新组件 Vue.component('todo-item', { template: '这是个待办项' }) var app = new Vue(...) · 与自定义元素的关系 · MVVM（Model-View-ViewModel） • 数据-试图-Vue实例化对象 · 特点 • 响应式 • 组件化 • 传统是js或jq操作DOM，数据和界面在一起 • VUE是数据层和视图层分离开的 1.1.3. Vue 实例 · 创建一个 Vue 实例 • var vm = new Vue({ // 选项 }) · 数据与方法 • 当一个 Vue 实例被创建时，它将 data 对象中的所有的 property 加入到 Vue 的响应式系统中。当这些 property 的值发生改变时，视图将会产生“响应”，即匹配更新为新的值。 • Object.freeze(对象)，这会阻止修改现有的 property，也意味着响应系统无法再追踪变化。 · 实例生命周期钩子 • created • 在一个实例被创建之后执行代码 • mounted、updated 和 destroyed。生命周期钩子的 this 上下文指向调用它的 Vue 实例 • 不要在选项 property 或回调上使用箭头函数，比如 created: () =&gt; console.log(this.a) 或 vm.$watch('a', newValue =&gt; this.myMethod())。因为箭头函数并没有 this，this 会作为变量一直向上级词法作用域查找，直至找到为止，经常导致 Uncaught TypeError: Cannot read property of undefined 或 Uncaught TypeError: this.myMethod is not a function 之类的错误。 · 生命周期图示 1.1.4. 模板语法 · Vue.js 使用了基于 HTML 的模板语法，允许开发者声明式地将 DOM 绑定至底层 Vue 实例的数据。所有 Vue.js 的模板都是合法的 HTML，所以能被遵循规范的浏览器和 HTML 解析器解析。 在底层的实现上，Vue 将模板编译成虚拟 DOM 渲染函数。结合响应系统，Vue 能够智能地计算出最少需要重新渲染多少组件，并把 DOM 操作次数减到最少。 如果你熟悉虚拟 DOM 并且偏爱 JavaScript 的原始力量，你也可以不用模板，直接写渲染 (render) 函数，使用可选的 JSX 语法。 · 插值 • 文本 • “Mustache”语法 (双大括号) • v-once 指令，你也能执行一次性地插值，当数据改变时，插值处的内容不会更新。但请留心这会影响到该节点上的其它数据绑定： 这个将不会改变: • 指令 • v-once • 原始 HTML • 指令 • v-html • 双大括号会将数据解释为普通文本，而非 HTML 代码。为了输出真正的 HTML，你需要使用 v-html 指令 • span 的内容将会被替换成为 property 值 rawHtml，直接作为 HTML——会忽略解析 property 值中的数据绑定。注意，你不能使用 v-html 来复合局部模板，因为 Vue 不是基于字符串的模板引擎。反之，对于用户界面 (UI)，组件更适合作为可重用和可组合的基本单位。 • 你的站点上动态渲染的任意 HTML 可能会非常危险，因为它很容易导致 XSS 攻击。请只对可信内容使用 HTML 插值，绝不要对用户提供的内容使用插值。 • Attribute • 指令 • v-bind • Mustache 语法不能作用在 HTML attribute 上，遇到这种情况应该使用 v-bind 指令： 对于布尔 attribute (它们只要存在就意味着值为 true)，v-bind 工作起来略有不同，在这个例子中： Button 如果 isButtonDisabled 的值是 null、undefined 或 false，则 disabled attribute 甚至不会被包含在渲染出来的 元素中。 • 使用 JavaScript 表达式 · 指令 • 指令 (Directives) 是带有 v- 前缀的特殊 attribute。指令 attribute 的值预期是单个 JavaScript 表达式 (v-for 是例外情况，稍后我们再讨论)。指令的职责是，当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM。 • 参数 • 一些指令能够接收一个“参数”，在指令名称之后以冒号表示。例如，v-bind 指令可以用于响应式地更新 HTML attribute： ... 在这里 href 是参数，告知 v-bind 指令将该元素的 href attribute 与表达式 url 的值绑定。 另一个例子是 v-on 指令，它用于监听 DOM 事件： ... 在这里参数是监听的事件名。我们也会更详细地讨论事件处理。 • 动态参数 • 从 2.6.0 开始，可以用方括号括起来的 JavaScript 表达式作为一个指令的参数： &lt;a v-bind:[attributeName]=&quot;url&quot;&gt; ... 这里的 attributeName 会被作为一个 JavaScript 表达式进行动态求值，求得的值将会作为最终的参数来使用。例如，如果你的 Vue 实例有一个 data property attributeName，其值为 &quot;href&quot;，那么这个绑定将等价于 v-bind:href。 同样地，你可以使用动态参数为一个动态的事件名绑定处理函数： &lt;a v-on:[eventName]=&quot;doSomething&quot;&gt; ... 在这个示例中，当 eventName 的值为 &quot;focus&quot; 时，v-on:[eventName] 将等价于 v-on:focus。 • 对动态参数的值的约束 动态参数预期会求出一个字符串，异常情况下值为 null。这个特殊的 null 值可以被显性地用于移除绑定。任何其它非字符串类型的值都将会触发一个警告。 • 对动态参数表达式的约束 动态参数表达式有一些语法约束，因为某些字符，如空格和引号，放在 HTML attribute 名里是无效的。例如： &lt;a v-bind:['foo' + bar]=&quot;value&quot;&gt; ... 变通的办法是使用没有空格或引号的表达式，或用计算属性替代这种复杂表达式。 在 DOM 中使用模板时 (直接在一个 HTML 文件里撰写模板)，还需要避免使用大写字符来命名键名，因为浏览器会把 attribute 名全部强制转为小写： &lt;a v-bind:[someAttr]=&quot;value&quot;&gt; ... • 修饰符 • 修饰符 (modifier) 是以半角句号 . 指明的特殊后缀，用于指出一个指令应该以特殊方式绑定。例如，.prevent 修饰符告诉 v-on 指令对于触发的事件调用 event.preventDefault()： ... 在接下来对 v-on 和 v-for 等功能的探索中，你会看到修饰符的其它例子。 · 缩写 • v- 前缀作为一种视觉提示，用来识别模板中 Vue 特定的 attribute。当你在使用 Vue.js 为现有标签添加动态行为 (dynamic behavior) 时，v- 前缀很有帮助，然而，对于一些频繁用到的指令来说，就会感到使用繁琐。同时，在构建由 Vue 管理所有模板的单页面应用程序 (SPA - single page application) 时，v- 前缀也变得没那么重要了。因此，Vue 为 v-bind 和 v-on 这两个最常用的指令，提供了特定简写： • v-bind 缩写 ... ... &lt;a :[key]=&quot;url&quot;&gt; ... • v-on 缩写 ... &lt;a @click=&quot;doSomething&quot;&gt;... &lt;a @[event]=&quot;doSomething&quot;&gt; ... 它们看起来可能与普通的 HTML 略有不同，但 : 与 @ 对于 attribute 名来说都是合法字符，在所有支持 Vue 的浏览器都能被正确地解析。而且，它们不会出现在最终渲染的标记中。缩写语法是完全可选的，但随着你更深入地了解它们的作用，你会庆幸拥有它们。 1.1.5. 计算属性和侦听器 · 计算属性 • 模板内的表达式非常便利，但是设计它们的初衷是用于简单运算的。在模板中放入太多的逻辑会让模板过重且难以维护。例如： 1234{{ message.split('').reverse().join('') }} &lt;div id=&quot;example&quot;&gt; {{ message.split('').reverse().join('') }} &lt;/div&gt; 在这个地方，模板不再是简单的声明式逻辑。你必须看一段时间才能意识到，这里是想要显示变量 message 的翻转字符串。当你想要在模板中多包含此处的翻转字符串时，就会更加难以处理。 所以，对于任何复杂逻辑，你都应当使用计算属性。 • 基础例子 12345678910111213var vm = new Vue({ el: '#example', data: { message: 'Hello' }, computed: { // 计算属性的 getter reversedMessage: function () { // `this` 指向 vm 实例 return this.message.split('').reverse().join('') } } }) • 计算属性缓存 vs 方法 • 你可能已经注意到我们可以通过在表达式中调用方法来达到同样的效果： 12345678&lt;p&gt;Reversed message: &quot;{{ reversedMessage() }}&quot;&lt;/p&gt;// 在组件中methods: { reversedMessage: function () { return this.message.split('').reverse().join('') }} 我们可以将同一函数定义为一个方法而不是一个计算属性。两种方式的最终结果确实是完全相同的。然而，不同的是计算属性是基于它们的响应式依赖进行缓存的。只在相关响应式依赖发生改变时它们才会重新求值。这就意味着只要 message 还没有发生改变，多次访问 reversedMessage 计算属性会立即返回之前的计算结果，而不必再次执行函数。 这也同样意味着下面的计算属性将不再更新，因为 Date.now() 不是响应式依赖： computed: { now: function () { return Date.now() } } 相比之下，每当触发重新渲染时，调用方法将总会再次执行函数。 我们为什么需要缓存？假设我们有一个性能开销比较大的计算属性 A，它需要遍历一个巨大的数组并做大量的计算。然后我们可能有其他的计算属性依赖于 A。如果没有缓存，我们将不可避免的多次执行 A 的 getter！如果你不希望有缓存，请用方法来替代。 • 计算属性 vs 侦听属性 • Vue 提供了一种更通用的方式来观察和响应 Vue 实例上的数据变动：侦听属性。当你有一些数据需要随着其它数据变动而变动时，你很容易滥用 watch——特别是如果你之前使用过 AngularJS。然而，通常更好的做法是使用计算属性而不是命令式的 watch 回调。细想一下这个例子： var vm = new Vue({ el: '#demo', data: { firstName: 'Foo', lastName: 'Bar', fullName: 'Foo Bar' }, watch: { firstName: function (val) { this.fullName = val + ' ' + this.lastName }, lastName: function (val) { this.fullName = this.firstName + ' ' + val } } }) 上面代码是命令式且重复的。将它与计算属性的版本进行比较： var vm = new Vue({ el: '#demo', data: { firstName: 'Foo', lastName: 'Bar' }, computed: { fullName: function () { return this.firstName + ' ' + this.lastName } } }) 好得多了，不是吗？ • 计算属性的 setter • 计算属性默认只有 getter，不过在需要时你也可以提供一个 setter： // ... computed: { fullName: { // getter get: function () { return this.firstName + ' ' + this.lastName }, // setter set: function (newValue) { var names = newValue.split(' ') this.firstName = names[0] this.lastName = names[names.length - 1] } } } // ... 现在再运行 vm.fullName = 'John Doe' 时，setter 会被调用，vm.firstName 和 vm.lastName 也会相应地被更新。 · 侦听器 • 虽然计算属性在大多数情况下更合适，但有时也需要一个自定义的侦听器。这就是为什么 Vue 通过 watch 选项提供了一个更通用的方法，来响应数据的变化。当需要在数据变化时执行异步或开销较大的操作时，这个方式是最有用的。 例如： Ask a yes/no question: var watchExampleVM = new Vue({ el: '#watch-example', data: { question: '', answer: 'I cannot give you an answer until you ask a question!' }, watch: { // 如果 `question` 发生改变，这个函数就会运行 question: function (newQuestion, oldQuestion) { this.answer = 'Waiting for you to stop typing...' this.debouncedGetAnswer() } }, created: function () { // `_.debounce` 是一个通过 Lodash 限制操作频率的函数。 // 在这个例子中，我们希望限制访问 yesno.wtf/api 的频率 // AJAX 请求直到用户输入完毕才会发出。想要了解更多关于 // `_.debounce` 函数 (及其近亲 `_.throttle`) 的知识， // 请参考：https://lodash.com/docs#debounce this.debouncedGetAnswer = _.debounce(this.getAnswer, 500) }, methods: { getAnswer: function () { if (this.question.indexOf('?') === -1) { this.answer = 'Questions usually contain a question mark. ;-)' return } this.answer = 'Thinking...' var vm = this axios.get('https://yesno.wtf/api') .then(function (response) { vm.answer = _.capitalize(response.data.answer) }) .catch(function (error) { vm.answer = 'Error! Could not reach the API. ' + error }) } } }) 结果： Ask a yes/no question: I cannot give you an answer until you ask a question! 在这个示例中，使用 watch 选项允许我们执行异步操作 (访问一个 API)，限制我们执行该操作的频率，并在我们得到最终结果前，设置中间状态。这些都是计算属性无法做到的。 除了 watch 选项之外，您还可以使用命令式的 vm.$watch API。 1.1.6. Class 与 Style 绑定 · 操作元素的 class 列表和内联样式是数据绑定的一个常见需求。因为它们都是 attribute，所以我们可以用 v-bind 处理它们：只需要通过表达式计算出字符串结果即可。不过，字符串拼接麻烦且易错。因此，在将 v-bind 用于 class 和 style 时，Vue.js 做了专门的增强。表达式结果的类型除了字符串之外，还可以是对象或数组。 · 绑定 HTML Class • 对象语法 • 我们可以传给 v-bind:class 一个对象，以动态地切换 class： 上面的语法表示 active 这个 class 存在与否将取决于数据 property isActive 的 truthiness。 你可以在对象中传入更多字段来动态切换多个 class。此外，v-bind:class 指令也可以与普通的 class attribute 共存。当有如下模板： 和如下 data： data: { isActive: true, hasError: false } 结果渲染为： 当 isActive 或者 hasError 变化时，class 列表将相应地更新。例如，如果 hasError 的值为 true，class 列表将变为 \"static active text-danger\"。 绑定的数据对象不必内联定义在模板里： data: { classObject: { active: true, 'text-danger': false } } 渲染的结果和上面一样。我们也可以在这里绑定一个返回对象的计算属性。这是一个常用且强大的模式： data: { isActive: true, error: null }, computed: { classObject: function () { return { active: this.isActive && !this.error, 'text-danger': this.error && this.error.type === 'fatal' } } } • 数组语法 • 我们可以把一个数组传给 v-bind:class，以应用一个 class 列表： data: { activeClass: 'active', errorClass: 'text-danger' } 渲染为： 如果你也想根据条件切换列表中的 class，可以用三元表达式： 这样写将始终添加 errorClass，但是只有在 isActive 是 truthy[1] 时才添加 activeClass。 不过，当有多个条件 class 时这样写有些繁琐。所以在数组语法中也可以使用对象语法： • 用在组件上 • 这个章节假设你已经对 Vue 组件有一定的了解。当然你也可以先跳过这里，稍后再回过头来看。 当在一个自定义组件上使用 class property 时，这些 class 将被添加到该组件的根元素上面。这个元素上已经存在的 class 不会被覆盖。 例如，如果你声明了这个组件： Vue.component('my-component', { template: 'Hi' }) 然后在使用它的时候添加一些 class： HTML 将被渲染为： Hi 对于带数据绑定 class 也同样适用： 当 isActive 为 truthy[1] 时，HTML 将被渲染成为： Hi · 绑定内联样式 • 对象语法 • v-bind:style 的对象语法十分直观——看着非常像 CSS，但其实是一个 JavaScript 对象。CSS property 名可以用驼峰式 (camelCase) 或短横线分隔 (kebab-case，记得用引号括起来) 来命名： data: { activeColor: 'red', fontSize: 30 } 直接绑定到一个样式对象通常更好，这会让模板更清晰： data: { styleObject: { color: 'red', fontSize: '13px' } } 同样的，对象语法常常结合返回对象的计算属性使用。 • 数组语法 • v-bind:style 的数组语法可以将多个样式对象应用到同一个元素上： • 自动添加前缀 • 当 v-bind:style 使用需要添加浏览器引擎前缀的 CSS property 时，如 transform，Vue.js 会自动侦测并添加相应的前缀。 • 多重值 • 从 2.3.0 起你可以为 style 绑定中的 property 提供一个包含多个值的数组，常用于提供多个带前缀的值，例如： 这样写只会渲染数组中最后一个被浏览器支持的值。在本例中，如果浏览器支持不带浏览器前缀的 flexbox，那么就只会渲染 display: flex。 译者注 [1] truthy 不是 true，详见 MDN 的解释。 1.1.7. 条件渲染 · v-if • v-if 指令用于条件性地渲染一块内容。这块内容只会在指令的表达式返回 truthy 值的时候被渲染。 Vue is awesome! 也可以用 v-else 添加一个“else 块”： Vue is awesome! Oh no 😢 • 在 元素上使用 v-if 条件渲染分组 • 因为 v-if 是一个指令，所以必须将它添加到一个元素上。但是如果想切换多个元素呢？此时可以把一个 元素当做不可见的包裹元素，并在上面使用 v-if。最终的渲染结果将不包含 元素。 Title Paragraph 1 Paragraph 2 • v-else • 你可以使用 v-else 指令来表示 v-if 的“else 块”： Now you see me Now you don't v-else 元素必须紧跟在带 v-if 或者 v-else-if 的元素的后面，否则它将不会被识别。 • v-else-if • 2.1.0 新增 v-else-if，顾名思义，充当 v-if 的“else-if 块”，可以连续使用： A B C Not A/B/C 类似于 v-else，v-else-if 也必须紧跟在带 v-if 或者 v-else-if 的元素之后。 • 用 key 管理可复用的元素 • Vue 会尽可能高效地渲染元素，通常会复用已有元素而不是从头开始渲染。这么做除了使 Vue 变得非常快之外，还有其它一些好处。例如，如果你允许用户在不同的登录方式之间切换： Username Email 那么在上面的代码中切换 loginType 将不会清除用户已经输入的内容。因为两个模板使用了相同的元素， 不会被替换掉——仅仅是替换了它的 placeholder。 自己动手试一试，在输入框中输入一些文本，然后按下切换按钮： 这样也不总是符合实际需求，所以 Vue 为你提供了一种方式来表达“这两个元素是完全独立的，不要复用它们”。只需添加一个具有唯一值的 key attribute 即可： Username Email 现在，每次切换时，输入框都将被重新渲染。请看： 注意， 元素仍然会被高效地复用，因为它们没有添加 key attribute。 · v-show • 另一个用于根据条件展示元素的选项是 v-show 指令。用法大致一样： Hello! 不同的是带有 v-show 的元素始终会被渲染并保留在 DOM 中。v-show 只是简单地切换元素的 CSS property display。 注意，v-show 不支持 元素，也不支持 v-else。 v-if vs v-show v-if 是“真正”的条件渲染，因为它会确保在切换过程中条件块内的事件监听器和子组件适当地被销毁和重建。 v-if 也是惰性的：如果在初始渲染时条件为假，则什么也不做——直到条件第一次变为真时，才会开始渲染条件块。 相比之下，v-show 就简单得多——不管初始条件是什么，元素总是会被渲染，并且只是简单地基于 CSS 进行切换。 一般来说，v-if 有更高的切换开销，而 v-show 有更高的初始渲染开销。因此，如果需要非常频繁地切换，则使用 v-show 较好；如果在运行时条件很少改变，则使用 v-if 较好。 v-if 与 v-for 一起使用 不推荐同时使用 v-if 和 v-for。请查阅风格指南以获取更多信息。 当 v-if 与 v-for 一起使用时，v-for 具有比 v-if 更高的优先级。请查阅列表渲染指南以获取详细信息。 1.1.8. 列表渲染 · 用 v-for 把一个数组对应为一组元素 • 我们可以用 v-for 指令基于一个数组来渲染一个列表。v-for 指令需要使用 item in items 形式的特殊语法，其中 items 是源数据数组，而 item 则是被迭代的数组元素的别名。 var example1 = new Vue({ el: '#example-1', data: { items: [ { message: 'Foo' }, { message: 'Bar' } ] } }) 结果： 在 v-for 块中，我们可以访问所有父作用域的 property。v-for 还支持一个可选的第二个参数，即当前项的索引。 - - var example2 = new Vue({ el: '#example-2', data: { parentMessage: 'Parent', items: [ { message: 'Foo' }, { message: 'Bar' } ] } }) 结果： 你也可以用 of 替代 in 作为分隔符，因为它更接近 JavaScript 迭代器的语法： · 在 v-for 里使用对象 • 你也可以用 v-for 来遍历一个对象的 property。 new Vue({ el: '#v-for-object', data: { object: { title: 'How to do lists in Vue', author: 'Jane Doe', publishedAt: '2016-04-10' } } }) 结果： 你也可以提供第二个的参数为 property 名称 (也就是键名)： : 还可以用第三个参数作为索引： . : 在遍历对象时，会按 Object.keys() 的结果遍历，但是不能保证它的结果在不同的 JavaScript 引擎下都一致。 · 维护状态 • 当 Vue 正在更新使用 v-for 渲染的元素列表时，它默认使用“就地更新”的策略。如果数据项的顺序被改变，Vue 将不会移动 DOM 元素来匹配数据项的顺序，而是就地更新每个元素，并且确保它们在每个索引位置正确渲染。这个类似 Vue 1.x 的 track-by=&quot;$index&quot;。 这个默认的模式是高效的，但是只适用于不依赖子组件状态或临时 DOM 状态 (例如：表单输入值) 的列表渲染输出。 为了给 Vue 一个提示，以便它能跟踪每个节点的身份，从而重用和重新排序现有元素，你需要为每项提供一个唯一 key attribute： 建议尽可能在使用 v-for 时提供 key attribute，除非遍历输出的 DOM 内容非常简单，或者是刻意依赖默认行为以获取性能上的提升。 因为它是 Vue 识别节点的一个通用机制，key 并不仅与 v-for 特别关联。后面我们将在指南中看到，它还具有其它用途。 不要使用对象或数组之类的非基本类型值作为 v-for 的 key。请用字符串或数值类型的值。 更多 key attribute 的细节用法请移步至 key 的 API 文档。 · 数组更新检测 • 变更方法 • Vue 将被侦听的数组的变更方法进行了包裹，所以它们也将会触发视图更新。这些被包裹过的方法包括： push() pop() shift() unshift() splice() sort() reverse() 你可以打开控制台，然后对前面例子的 items 数组尝试调用变更方法。比如 example1.items.push({ message: 'Baz' })。 • 替换数组 • 变更方法，顾名思义，会变更调用了这些方法的原始数组。相比之下，也有非变更方法，例如 filter()、concat() 和 slice()。它们不会变更原始数组，而总是返回一个新数组。当使用非变更方法时，可以用新数组替换旧数组： example1.items = example1.items.filter(function (item) { return item.message.match(/Foo/) }) 你可能认为这将导致 Vue 丢弃现有 DOM 并重新渲染整个列表。幸运的是，事实并非如此。Vue 为了使得 DOM 元素得到最大范围的重用而实现了一些智能的启发式方法，所以用一个含有相同元素的数组去替换原来的数组是非常高效的操作。 注意事项 由于 JavaScript 的限制，Vue 不能检测数组和对象的变化。深入响应式原理中有相关的讨论。 · 显示过滤/排序后的结果 • 有时，我们想要显示一个数组经过过滤或排序后的版本，而不实际变更或重置原始数据。在这种情况下，可以创建一个计算属性，来返回过滤或排序后的数组。 例如： data: { numbers: [ 1, 2, 3, 4, 5 ] }, computed: { evenNumbers: function () { return this.numbers.filter(function (number) { return number % 2 === 0 }) } } 在计算属性不适用的情况下 (例如，在嵌套 v-for 循环中) 你可以使用一个方法： data: { sets: [[ 1, 2, 3, 4, 5 ], [6, 7, 8, 9, 10]] }, methods: { even: function (numbers) { return numbers.filter(function (number) { return number % 2 === 0 }) } } · 在 v-for 里使用值范围 • 在 v-for 里使用值范围 v-for 也可以接受整数。在这种情况下，它会把模板重复对应次数。 结果：12345678910 · 在 上使用 v-for • 类似于 v-if，你也可以利用带有 v-for 的 来循环渲染一段包含多个元素的内容。比如： · v-for 与 v-if 一同使用 • 注意我们不推荐在同一元素上使用 v-if 和 v-for。更多细节可查阅风格指南。 当它们处于同一节点，v-for 的优先级比 v-if 更高，这意味着 v-if 将分别重复运行于每个 v-for 循环中。当你只想为部分项渲染节点时，这种优先级的机制会十分有用，如下： 上面的代码将只渲染未完成的 todo。 而如果你的目的是有条件地跳过循环的执行，那么可以将 v-if 置于外层元素 (或 ) 上。如： No todos left! · 在组件上使用 v-for • 这部分内容假定你已经了解组件相关知识。你也完全可以先跳过它，以后再回来查看。 在自定义组件上，你可以像在任何普通元素上一样使用 v-for。 2.2.0+ 的版本里，当在组件上使用 v-for 时，key 现在是必须的。 然而，任何数据都不会被自动传递到组件里，因为组件有自己独立的作用域。为了把迭代数据传递到组件里，我们要使用 prop： 不自动将 item 注入到组件里的原因是，这会使得组件与 v-for 的运作紧密耦合。明确组件数据的来源能够使组件在其他场合重复使用。 下面是一个简单的 todo 列表的完整例子： Add a todo Add 注意这里的 is=&quot;todo-item&quot; attribute。这种做法在使用 DOM 模板时是十分必要的，因为在 元素内只有 元素会被看作有效内容。这样做实现的效果与 相同，但是可以避开一些潜在的浏览器解析错误。查看 DOM 模板解析说明 来了解更多信息。 Vue.component('todo-item', { template: '\\ \\ 魑魅先生 | 前端\\ Remove\\ \\ ', props: ['title'] }) new Vue({ el: '#todo-list-example', data: { newTodoText: '', todos: [ { id: 1, title: 'Do the dishes', }, { id: 2, title: 'Take out the trash', }, { id: 3, title: 'Mow the lawn' } ], nextTodoId: 4 }, methods: { addNewTodo: function () { this.todos.push({ id: this.nextTodoId++, title: this.newTodoText }) this.newTodoText = '' } } }) 1.1.9. 事件处理 · 监听事件 • 可以用 v-on 指令监听 DOM 事件，并在触发时运行一些 JavaScript 代码。 示例： Add 1 The button above has been clicked times. var example1 = new Vue({ el: '#example-1', data: { counter: 0 } }) · 事件处理方法 • 然而许多事件处理逻辑会更为复杂，所以直接把 JavaScript 代码写在 v-on 指令中是不可行的。因此 v-on 还可以接收一个需要调用的方法名称。 示例： Greet var example2 = new Vue({ el: '#example-2', data: { name: 'Vue.js' }, // 在 `methods` 对象中定义方法 methods: { greet: function (event) { // `this` 在方法里指向当前 Vue 实例 alert('Hello ' + this.name + '!') // `event` 是原生 DOM 事件 if (event) { alert(event.target.tagName) } } } }) // 也可以用 JavaScript 直接调用方法 example2.greet() // =&gt; 'Hello Vue.js!' · 内联处理器中的方法 • 除了直接绑定到一个方法，也可以在内联 JavaScript 语句中调用方法： Say hi Say what new Vue({ el: '#example-3', methods: { say: function (message) { alert(message) } } }) 结果： 有时也需要在内联语句处理器中访问原始的 DOM 事件。可以用特殊变量 $event 把它传入方法： Submit // ... methods: { warn: function (message, event) { // 现在我们可以访问原生事件对象 if (event) { event.preventDefault() } alert(message) } } · 事件修饰符 • 在事件处理程序中调用 event.preventDefault() 或 event.stopPropagation() 是非常常见的需求。尽管我们可以在方法中轻松实现这点，但更好的方式是：方法只有纯粹的数据逻辑，而不是去处理 DOM 事件细节。 为了解决这个问题，Vue.js 为 v-on 提供了事件修饰符。之前提过，修饰符是由点开头的指令后缀来表示的。 .stop .prevent .capture .self .once .passive ... ... 使用修饰符时，顺序很重要；相应的代码会以同样的顺序产生。因此，用 v-on:click.prevent.self 会阻止所有的点击，而 v-on:click.self.prevent 只会阻止对元素自身的点击。 2.1.4 新增 不像其它只能对原生的 DOM 事件起作用的修饰符，.once 修饰符还能被用到自定义的组件事件上。如果你还没有阅读关于组件的文档，现在大可不必担心。 2.3.0 新增 Vue 还对应 addEventListener 中的 passive 选项提供了 .passive 修饰符。 ... 这个 .passive 修饰符尤其能够提升移动端的性能。 不要把 .passive 和 .prevent 一起使用，因为 .prevent 将会被忽略，同时浏览器可能会向你展示一个警告。请记住，.passive 会告诉浏览器你不想阻止事件的默认行为。 · 按键修饰符 • 在监听键盘事件时，我们经常需要检查详细的按键。Vue 允许为 v-on 在监听键盘事件时添加按键修饰符： 你可以直接将 KeyboardEvent.key 暴露的任意有效按键名转换为 kebab-case 来作为修饰符。 在上述示例中，处理函数只会在 $event.key 等于 PageDown 时被调用。 • 按键码 • keyCode 的事件用法已经被废弃了并可能不会被最新的浏览器支持。 使用 keyCode attribute 也是允许的： 为了在必要的情况下支持旧浏览器，Vue 提供了绝大多数常用的按键码的别名： .enter .tab .delete (捕获“删除”和“退格”键) .esc .space .up .down .left .right 有一些按键 (.esc 以及所有的方向键) 在 IE9 中有不同的 key 值, 如果你想支持 IE9，这些内置的别名应该是首选。 你还可以通过全局 config.keyCodes 对象自定义按键修饰符别名： // 可以使用 v-on:keyup.f1 Vue.config.keyCodes.f1 = 112 · 系统修饰键 • 2.1.0 新增 • 可以用如下修饰符来实现仅在按下相应按键时才触发鼠标或键盘事件的监听器。 .ctrl .alt .shift .meta 注意：在 Mac 系统键盘上，meta 对应 command 键 (⌘)。在 Windows 系统键盘 meta 对应 Windows 徽标键 (⊞)。在 Sun 操作系统键盘上，meta 对应实心宝石键 (◆)。在其他特定键盘上，尤其在 MIT 和 Lisp 机器的键盘、以及其后继产品，比如 Knight 键盘、space-cadet 键盘，meta 被标记为“META”。在 Symbolics 键盘上，meta 被标记为“META”或者“Meta”。 例如： Do something 请注意修饰键与常规按键不同，在和 keyup 事件一起用时，事件触发时修饰键必须处于按下状态。换句话说，只有在按住 ctrl 的情况下释放其它按键，才能触发 keyup.ctrl。而单单释放 ctrl 也不会触发事件。如果你想要这样的行为，请为 ctrl 换用 keyCode：keyup.17。 • .exact 修饰符 • 2.5.0 新增 .exact 修饰符允许你控制由精确的系统修饰符组合触发的事件。 A A A • 鼠标按钮修饰符 • .left .right .middle 这些修饰符会限制处理函数仅响应特定的鼠标按钮。 • 为什么在 HTML 中监听事件？ • 你可能注意到这种事件监听的方式违背了关注点分离 (separation of concern) 这个长期以来的优良传统。但不必担心，因为所有的 Vue.js 事件处理方法和表达式都严格绑定在当前视图的 ViewModel 上，它不会导致任何维护上的困难。实际上，使用 v-on 有几个好处： 扫一眼 HTML 模板便能轻松定位在 JavaScript 代码里对应的方法。 因为你无须在 JavaScript 里手动绑定事件，你的 ViewModel 代码可以是非常纯粹的逻辑，和 DOM 完全解耦，更易于测试。 当一个 ViewModel 被销毁时，所有的事件处理器都会自动被删除。你无须担心如何清理它们。 · 表单输入绑定 • 基础用法 • 你可以用 v-model 指令在表单 、 及 元素上创建双向数据绑定。它会根据控件类型自动选取正确的方法来更新元素。尽管有些神奇，但 v-model 本质上不过是语法糖。它负责监听用户的输入事件以更新数据，并对一些极端场景进行一些特殊处理。 v-model 会忽略所有表单元素的 value、checked、selected attribute 的初始值而总是将 Vue 实例的数据作为数据来源。你应该通过 JavaScript 在组件的 data 选项中声明初始值。 v-model 在内部为不同的输入元素使用不同的 property 并抛出不同的事件： text 和 textarea 元素使用 value property 和 input 事件； checkbox 和 radio 使用 checked property 和 change 事件； select 字段将 value 作为 prop 并将 change 作为事件。 对于需要使用输入法 (如中文、日文、韩文等) 的语言，你会发现 v-model 不会在输入法组合文字过程中得到更新。如果你也想处理这个过程，请使用 input 事件。 • 文本 • Message is: • 多行文本 • Multiline message is: • 在文本区域插值 () 并不会生效，应用 v-model 来代替。 • 复选框 • 单个复选框，绑定到布尔值： • 多个复选框，绑定到同一个数组： Jack John Mike Checked names: new Vue({ el: '...', data: { checkedNames: [] } }) • 单选按钮 • One Two Picked: new Vue({ el: '#example-4', data: { picked: '' } }) • 选择框 • 单选时： 请选择 A B C Selected: new Vue({ el: '...', data: { selected: '' } }) 请选择 Selected: 如果 v-model 表达式的初始值未能匹配任何选项， 元素将被渲染为“未选中”状态。在 iOS 中，这会使用户无法选择第一个选项。因为这样的情况下，iOS 不会触发 change 事件。因此，更推荐像上面这样提供一个值为空的禁用选项。 • 多选时 (绑定到一个数组)： A B C Selected: new Vue({ el: '#example-6', data: { selected: [] } }) ABC Selected: [] 用 v-for 渲染的动态选项： Selected: new Vue({ el: '...', data: { selected: 'A', options: [ { text: 'One', value: 'A' }, { text: 'Two', value: 'B' }, { text: 'Three', value: 'C' } ] } }) • 值绑定 • 对于单选按钮，复选框及选择框的选项，v-model 绑定的值通常是静态字符串 (对于复选框也可以是布尔值)： ABC 但是有时我们可能想把值绑定到 Vue 实例的一个动态 property 上，这时可以用 v-bind 实现，并且这个 property 的值可以不是字符串。 • 复选框 • &lt;input type=&quot;checkbox&quot; v-model=&quot;toggle&quot; true-value=&quot;yes&quot; false-value=&quot;no&quot; &gt; // 当选中时 vm.toggle === 'yes' // 当没有选中时 vm.toggle === 'no' 这里的 true-value 和 false-value attribute 并不会影响输入控件的 value attribute，因为浏览器在提交表单时并不会包含未被选中的复选框。如果要确保表单中这两个值中的一个能够被提交，(即“yes”或“no”)，请换用单选按钮。 • 单选按钮 • // 当选中时 vm.pick === vm.a • 选择框的选项 • 123 // 当选中时 typeof vm.selected // => 'object' vm.selected.number // => 123 • 修饰符 • .lazy • 在默认情况下，v-model 在每次 input 事件触发后将输入框的值与数据进行同步 (除了上述输入法组合文字时)。你可以添加 lazy 修饰符，从而转为在 change 事件_之后_进行同步： • .number • 如果想自动将用户的输入值转为数值类型，可以给 v-model 添加 number 修饰符： 这通常很有用，因为即使在 type=\"number\" 时，HTML 输入元素的值也总会返回字符串。如果这个值无法被 parseFloat() 解析，则会返回原始的值。 • .trim • 如果要自动过滤用户输入的首尾空白字符，可以给 v-model 添加 trim 修饰符： • 在组件上使用 v-model • 如果你还不熟悉 Vue 的组件，可以暂且跳过这里。 HTML 原生的输入元素类型并不总能满足需求。幸好，Vue 的组件系统允许你创建具有完全自定义行为且可复用的输入组件。这些输入组件甚至可以和 v-model 一起使用！ 1.1.10. 组件基础 · 基本示例 • // 定义一个名为 button-counter 的新组件 Vue.component('button-counter', { data: function () { return { count: 0 } }, template: 'You clicked me times.' }) 组件是可复用的 Vue 实例，且带有一个名字：在这个例子中是 。我们可以在一个通过 new Vue 创建的 Vue 根实例中，把这个组件作为自定义元素来使用： new Vue({ el: '#components-demo' }) 因为组件是可复用的 Vue 实例，所以它们与 new Vue 接收相同的选项，例如 data、computed、watch、methods 以及生命周期钩子等。仅有的例外是像 el 这样根实例特有的选项。 · 组件的复用 • 你可以将组件进行任意次数的复用： 注意当点击按钮时，每个组件都会各自独立维护它的 count。因为你每用一次组件，就会有一个它的新实例被创建。 • data 必须是一个函数 • 当我们定义这个 组件时，你可能会发现它的 data 并不是像这样直接提供一个对象： data: { count: 0 } 取而代之的是，一个组件的 data 选项必须是一个函数，因此每个实例可以维护一份被返回对象的独立的拷贝： data: function () { return { count: 0 } } 如果 Vue 没有这条规则，点击一个按钮就可能会像如下代码一样影响到其它所有实例： · 组件的组织 • 通常一个应用会以一棵嵌套的组件树的形式来组织： Component Tree 例如，你可能会有页头、侧边栏、内容区等组件，每个组件又包含了其它的像导航链接、博文之类的组件。 为了能在模板中使用，这些组件必须先注册以便 Vue 能够识别。这里有两种组件的注册类型：全局注册和局部注册。至此，我们的组件都只是通过 Vue.component 全局注册的： Vue.component('my-component-name', { // ... options ... }) 全局注册的组件可以用在其被注册之后的任何 (通过 new Vue) 新创建的 Vue 根实例，也包括其组件树中的所有子组件的模板中。 到目前为止，关于组件注册你需要了解的就这些了，如果你阅读完本页内容并掌握了它的内容，我们会推荐你再回来把组件注册读完。 · 通过 Prop 向子组件传递数据 • 早些时候，我们提到了创建一个博文组件的事情。问题是如果你不能向这个组件传递某一篇博文的标题或内容之类的我们想展示的数据的话，它是没有办法使用的。这也正是 prop 的由来。 Prop 是你可以在组件上注册的一些自定义 attribute。当一个值传递给一个 prop attribute 的时候，它就变成了那个组件实例的一个 property。为了给博文组件传递一个标题，我们可以用一个 props 选项将其包含在该组件可接受的 prop 列表中： Vue.component('blog-post', { props: ['title'], template: '魑魅先生 | 前端' }) 一个组件默认可以拥有任意数量的 prop，任何值都可以传递给任何 prop。在上述模板中，你会发现我们能够在组件实例中访问这个值，就像访问 data 中的值一样。 一个 prop 被注册之后，你就可以像这样把数据作为一个自定义 attribute 传递进来： 然而在一个典型的应用中，你可能在 data 里有一个博文的数组： new Vue({ el: '#blog-post-demo', data: { posts: [ { id: 1, title: 'My journey with Vue' }, { id: 2, title: 'Blogging with Vue' }, { id: 3, title: 'Why Vue is so fun' } ] } }) 并想要为每篇博文渲染一个组件： &lt;blog-post v-for=&quot;post in posts&quot; v-bind:key=&quot;post.id&quot; v-bind:title=&quot;post.title&quot; &gt; 如上所示，你会发现我们可以使用 v-bind 来动态传递 prop。这在你一开始不清楚要渲染的具体内容，比如从一个 API 获取博文列表的时候，是非常有用的。 到目前为止，关于 prop 你需要了解的大概就这些了，如果你阅读完本页内容并掌握了它的内容，我们会推荐你再回来把 prop 读完。 · 单个根元素 • 当构建一个 组件时，你的模板最终会包含的东西远不止一个标题： 魑魅先生 | 前端 最最起码，你会包含这篇博文的正文： 魑魅先生 | 前端 然而如果你在模板中尝试这样写，Vue 会显示一个错误，并解释道 every component must have a single root element (每个组件必须只有一个根元素)。你可以将模板的内容包裹在一个父元素内，来修复这个问题，例如： 魑魅先生 | 前端 看起来当组件变得越来越复杂的时候，我们的博文不只需要标题和内容，还需要发布日期、评论等等。为每个相关的信息定义一个 prop 会变得很麻烦： 所以是时候重构一下这个 组件了，让它变成接受一个单独的 post prop： Vue.component('blog-post', { props: ['post'], template: &lt;div class=&quot;blog-post&quot;&gt; &lt;h3&gt;{{ post.title }}&lt;/h3&gt; &lt;div v-html=&quot;post.content&quot;&gt;&lt;/div&gt; &lt;/div&gt; }) 上述的这个和一些接下来的示例使用了 JavaScript 的模板字符串来让多行的模板更易读。它们在 IE 下并没有被支持，所以如果你需要在不 (经过 Babel 或 TypeScript 之类的工具) 编译的情况下支持 IE，请使用折行转义字符取而代之。 现在，不论何时为 post 对象添加一个新的 property，它都会自动地在 内可用。 · 监听子组件事件 • 在我们开发 组件时，它的一些功能可能要求我们和父级组件进行沟通。例如我们可能会引入一个辅助功能来放大博文的字号，同时让页面的其它部分保持默认的字号。 在其父组件中，我们可以通过添加一个 postFontSize 数据 property 来支持这个功能： new Vue({ el: '#blog-posts-events-demo', data: { posts: [/* ... */], postFontSize: 1 } }) 它可以在模板中用来控制所有博文的字号： 现在我们在每篇博文正文之前添加一个按钮来放大字号： Vue.component('blog-post', { props: ['post'], template: &lt;div class=&quot;blog-post&quot;&gt; &lt;h3&gt;{{ post.title }}&lt;/h3&gt; &lt;button&gt; Enlarge text &lt;/button&gt; &lt;div v-html=&quot;post.content&quot;&gt;&lt;/div&gt; &lt;/div&gt; }) 问题是这个按钮不会做任何事： Enlarge text 当点击这个按钮时，我们需要告诉父级组件放大所有博文的文本。幸好 Vue 实例提供了一个自定义事件的系统来解决这个问题。父级组件可以像处理 native DOM 事件一样通过 v-on 监听子组件实例的任意事件： &lt;blog-post ... v-on:enlarge-text=&quot;postFontSize += 0.1&quot; &gt; 同时子组件可以通过调用内建的 $emit 方法并传入事件名称来触发一个事件： Enlarge text 有了这个 v-on:enlarge-text=&quot;postFontSize += 0.1&quot; 监听器，父级组件就会接收该事件并更新 postFontSize 的值。 • 使用事件抛出一个值 • 有的时候用一个事件来抛出一个特定的值是非常有用的。例如我们可能想让 组件决定它的文本要放大多少。这时可以使用 $emit 的第二个参数来提供这个值： Enlarge text 然后当在父级组件监听这个事件的时候，我们可以通过 $event 访问到被抛出的这个值： &lt;blog-post ... v-on:enlarge-text=&quot;postFontSize += $event&quot; &gt; 或者，如果这个事件处理函数是一个方法： &lt;blog-post ... v-on:enlarge-text=&quot;onEnlargeText&quot; &gt; 那么这个值将会作为第一个参数传入这个方法： methods: { onEnlargeText: function (enlargeAmount) { this.postFontSize += enlargeAmount } } • 在组件上使用 v-model • 自定义事件也可以用于创建支持 v-model 的自定义输入组件。记住： 等价于： &lt;input v-bind:value=&quot;searchText&quot; v-on:input=&quot;searchText = $event.target.value&quot; &gt; 当用在组件上时，v-model 则会这样： &lt;custom-input v-bind:value=&quot;searchText&quot; v-on:input=&quot;searchText = $event&quot; &gt; 为了让它正常工作，这个组件内的 必须： 将其 value attribute 绑定到一个名叫 value 的 prop 上 在其 input 事件被触发时，将新的值通过自定义的 input 事件抛出 写成代码之后是这样的： Vue.component('custom-input', { props: ['value'], template: &lt;input v-bind:value=&quot;value&quot; v-on:input=&quot;$emit('input', $event.target.value)&quot; \\&gt; }) 现在 v-model 就应该可以在这个组件上完美地工作起来了： 到目前为止，关于组件自定义事件你需要了解的大概就这些了，如果你阅读完本页内容并掌握了它的内容，我们会推荐你再回来把自定义事件读完。 · 通过插槽分发内容 • 和 HTML 元素一样，我们经常需要向一个组件传递内容，像这样： Something bad happened. 可能会渲染出这样的东西： 幸好，Vue 自定义的 元素让这变得非常简单： Vue.component('alert-box', { template: ` Error! ` }) 如你所见，我们只要在需要的地方加入插槽就行了——就这么简单！ 到目前为止，关于插槽你需要了解的大概就这些了，如果你阅读完本页内容并掌握了它的内容，我们会推荐你再回来把插槽读完。 · 动态组件 • 有的时候，在不同组件之间进行动态切换是非常有用的，比如在一个多标签的界面里： 上述内容可以通过 Vue 的 元素加一个特殊的 is attribute 来实现： 在上述示例中，currentTabComponent 可以包括 已注册组件的名字，或 一个组件的选项对象 你可以在这里查阅并体验完整的代码，或在这个版本了解绑定组件选项对象，而不是已注册组件名的示例。 请留意，这个 attribute 可以用于常规 HTML 元素，但这些元素将被视为组件，这意味着所有的 attribute 都会作为 DOM attribute 被绑定。对于像 value 这样的 property，若想让其如预期般工作，你需要使用 .prop 修饰器。 到目前为止，关于动态组件你需要了解的大概就这些了，如果你阅读完本页内容并掌握了它的内容，我们会推荐你再回来把动态和异步组件读完。 · 解析 DOM 模板时的注意事项 • 有些 HTML 元素，诸如 、、 和 ，对于哪些元素可以出现在其内部是有严格限制的。而有些元素，诸如 、 和 ，只能出现在其它某些特定的元素内部。 这会导致我们使用这些有约束条件的元素时遇到一些问题。例如： 这个自定义组件 会被作为无效的内容提升到外部，并导致最终渲染结果出错。幸好这个特殊的 is attribute 给了我们一个变通的办法： 需要注意的是如果我们从以下来源使用模板的话，这条限制是不存在的： 字符串 (例如：template: '...') 单文件组件 (.vue) 到这里，你需要了解的解析 DOM 模板时的注意事项——实际上也是 Vue 的全部必要内容，大概就是这些了。恭喜你！接下来还有很多东西要去学习，不过首先，我们推荐你先休息一下，试用一下 Vue，自己随意做些好玩的东西。 如果你感觉已经掌握了这些知识，我们推荐你再回来把完整的组件指南，包括侧边栏中组件深入章节的所有页面读完。 ## 1.2. 深入了解组件 ## 1.3. 过渡 & 动画 ## 1.4. 可复用性 & 组合 ## 1.5. 工具 ### 1.5.1. CLI 官方脚手架，快速创建项目 安装 • 安装node.js 安装淘宝镜像（不然会慢的一X） 全局安装webpack cnpm install webpack -g 全局安装vue-cli (cnpm install -g vue-cli) (用webpack模板来创建一个vue项目)vue init webpack my-project 安装项目依赖 cd my-project / cnpm install 启动项目 编译vue为静态文件(npm run build)c • https://nodejs.org/en/ • node/npm -v • 安装cnpm • npm install -g cnpm --registry=http://registry.npm.taobao.org • cnpm -v • npm install -g @vue/cli \\# OR yarn global add @vue/cli • vue --version • 项目依赖 上面列出来的命令是用于升级全局的 Vue CLI。如需升级项目中的 Vue CLI 相关模块（以 @vue/cli-plugin- 或 vue-cli-plugin- 开头），请在项目目录下运行 vue upgrade： 用法： upgrade [options] [plugin-name] （试用）升级 Vue CLI 服务及插件 选项： -t, --to 升级 到指定的版本 -f, --from 跳过本地版本检测，默认插件是从此处指定的版本升级上来 -r, --registry 使用指定的 registry 地址安装依赖 --all 升级所有的插件 --next 检查插件新版本时，包括 alpha/beta/rc 版本在内 -h, --help 输出帮助内容 • vue init webpack my-first-demo(项目文件夹名)。 • 通过输入cd my-first-demo就可以进入目录具体文件夹命令行中输入：cnmp install或者npm install安装依赖包。重新打开路径下的文件夹，我们可以看到文件夹中比之前的文件夹多了一个node_modules文件夹，到此脚手架安装完成。 • npm run dev · 快速原型开发 • 安装一个全局的扩展： npm install -g @vue/cli-service-global • vue serve Usage: serve [options] [entry] 在开发环境模式下零配置为 .js 或 .vue 文件启动一个服务器 Options: -o, --open 打开浏览器 -c, --copy 将本地 URL 复制到剪切板 -h, --help 输出用法信息 • App.vue 文件所在的目录下运行： vue serve • vue build Usage: build [options] [entry] 在生产环境模式下零配置构建一个 .js 或 .vue 文件 Options: -t, --target 构建目标 (app | lib | wc | wc-async, 默认值：app) -n, --name 库的名字或 Web Components 组件的名字 (默认值：入口文件名) -d, --dest 输出目录 (默认值：dist) -h, --help 输出用法信息 你也可以使用 vue build 将目标文件构建成一个生产环境的包并用来部署： vue build MyComponent.vue · 创建一个项目 • 安装node node -v检测是否成功 • npm install -g @vue/cli • yarn global add @vue/cli • vue create hello-world • vue create --help • vue ui进入图形化界面 · 插件和 Preset • vue add eslint · CLI 服务 • vue-cli-service serve 用法：vue-cli-service serve [options] [entry] 选项： --open 在服务器启动时打开浏览器 --copy 在服务器启动时将 URL 复制到剪切版 --mode 指定环境模式 (默认值：development) --host 指定 host (默认值：0.0.0.0) --port 指定 port (默认值：8080) --https 使用 https (默认值：false) • vue-cli-service build 用法：vue-cli-service build [options] [entry|pattern] 选项： --mode 指定环境模式 (默认值：production) --dest 指定输出目录 (默认值：dist) --modern 面向现代浏览器带自动回退地构建应用 --target app | lib | wc | wc-async (默认值：app) --name 库或 Web Components 模式下的名字 (默认值：package.json 中的 \"name\" 字段或入口文件名) --no-clean 在构建项目之前不清除目标目录 --report 生成 report.html 以帮助分析包内容 --report-json 生成 report.json 以帮助分析包内容 --watch 监听文件变化 ### 1.5.2. npm ### 1.5.3. webpak · js应用程序模块打包工具 · 任何文件，图片，css，字体成为模块（Model），彼此存在依赖关系，webpack就是来处理模块间的关系，并对其打包 · npm install webpack --save-dev局部安装 · npm install webpack-dev-server --save-dev可以提供很多服务，比如 · webpack --progress --hide-modules打包 · 核心概念 • webpack就是一个js文件： • package.json • { \"name\": \"bootvue\", \"version\": \"1.0.0\", \"description\": \"A Vue.js project\", \"author\": \"lxl \", \"private\": true, //scripts dev:快速启动webpack-dev-server服务脚本 \"scripts\": { \"dev\": \"webpack-dev-server --inline --progress --config build/webpack.dev.conf.js\", \"start\": \"npm run dev\", \"build\": \"node build/build.js\" }, \"dependencies\": { }, \"devDependencies\": { }, \"engines\": { \"node\": \">= 6.0.0\", \"npm\": \">= 3.0.0\" }, \"browserslist\": [ \"> 1%\", \"last 2 versions\", \"not ie","link":"/2021/03/01/Draft/2021/%E5%89%8D%E7%AB%AF/"},{"title":"MongoDB","text":"最像关系型数据库的非关系型数据库 新学四问 WHY【与前代优化了什么，弥补了什么空白】： WHAT【框架，思维导图，主题框架】： HOW【如何记忆，学习资源】：菜鸟、官方文档 LEVEL【不是每个都学精】： 进度：【一天】 快查 命令 解释 备注 show dbs 查看所有数据库 db 显示当前数据库对象或集合 use 连接到一个指定的数据库 db.createCollection(name, options) 创建集合 show collections 或 show tables 查看集合 关系图 一、简介 C++语言编写的，是一个基于分布式文件存储的开源数据库系统。 在高负载的情况下，添加更多的节点，可以保证服务器性能。 MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。 MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 环境：win10 二、基础概念 SQL术语/概念 MongoDB术语/概念 解释/说明 database database 数据库 table collection 数据库表/集合 row document 数据记录行/文档 column field 数据字段/域 index index 索引 table joins 表连接,MongoDB不支持 primary key primary key 主键,MongoDB自动将_id字段设置为主键 默认数据库为db文件夹需要自己创建，数据放在data文件夹中 文档(Document) ​ 文档是一组键值(key-value)对(即 BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。一个简单的文档例子如下： 1{&quot;site&quot;:&quot;www.runoob.com&quot;, &quot;name&quot;:&quot;菜鸟教程&quot;} 下表列出了 RDBMS 与 MongoDB 对应的术语： RDBMS MongoDB 数据库 数据库 表格 集合 行 文档 列 字段 表联合 嵌入文档 主键 主键 (MongoDB 提供了 key 为 _id ) 数据库服务和客户端 Mysqld/Oracle mongod mysql/sqlplus mongo 需要注意的是： 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型（甚至可以是整个嵌入的文档)。 MongoDB区分类型和大小写。 MongoDB的文档不能有重复的键。 文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 文档键命名规范： 键不能含有\\0 (空字符)。这个字符用来表示键的结尾。 .和$有特别的意义，只有在特定环境下才能使用。 以下划线&quot;_&quot;开头的键是保留的(不是严格要求的)。 集合 / 表 集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。 集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。 比如，我们可以将以下不同数据结构的文档插入到集合中： 123{&quot;site&quot;:&quot;www.baidu.com&quot;}{&quot;site&quot;:&quot;www.google.com&quot;,&quot;name&quot;:&quot;Google&quot;}{&quot;site&quot;:&quot;www.runoob.com&quot;,&quot;name&quot;:&quot;菜鸟教程&quot;,&quot;num&quot;:5} 当第一个文档插入时，集合就会被创建。 合法的集合名 集合名不能是空字符串&quot;&quot;。 集合名不能含有\\0字符（空字符)，这个字符表示集合名的结尾。 集合名不能以&quot;system.&quot;开头，这是为系统集合保留的前缀。 用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。 如下实例： 1db.col.findOne() capped collections Capped collections 就是固定大小的collection。 它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 &quot;RRD&quot; 概念类似。 Capped collections 是高性能自动的维护对象的插入顺序。它非常适合类似记录日志的功能，和标准的 collection 不同，你必须要显式的创建一个capped collection，指定一个 collection 的大小，单位是字节。collection 的数据存储空间值提前分配的。 Capped collections 可以按照文档的插入顺序保存到集合中，而且这些文档在磁盘上存放位置也是按照插入顺序来保存的，所以当我们更新Capped collections 中文档的时候，更新后的文档不可以超过之前文档的大小，这样话就可以确保所有文档在磁盘上的位置一直保持不变。 由于 Capped collection 是按照文档的插入顺序而不是使用索引确定插入位置，这样的话可以提高增添数据的效率。MongoDB 的操作日志文件 oplog.rs 就是利用 Capped Collection 来实现的。 要注意的是指定的存储大小包含了数据库的头信息。 1db.createCollection(&quot;mycoll&quot;, {capped:true, size:100000}) 在 capped collection 中，你能添加新的对象。 能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。 使用 Capped Collection 不能删除一个文档，可以使用 drop() 方法删除 collection 所有的行。 删除之后，你必须显式的重新创建这个 collection。 在32bit机器中，capped collection 最大存储为 1e9( 1X109)个字节。 元数据 数据库的信息是存储在集合中。它们使用了系统的命名空间： 1dbname.system.* 在MongoDB数据库中名字空间 .system.* 是包含多种系统信息的特殊集合(Collection)，如下: 集合命名空间 描述 dbname.system.namespaces 列出所有名字空间。 dbname.system.indexes 列出所有索引。 dbname.system.profile 包含数据库概要(profile)信息。 dbname.system.users 列出所有可访问数据库的用户。 dbname.local.sources 包含复制对端（slave）的服务器信息和状态。 对于修改系统集合中的对象有如下限制。 在插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的drop index命令将自动更新相关信息)。 是可修改的。 是可删除的。 MongoDB 数据类型 下表为MongoDB中常用的几种数据类型。 数据类型 描述 String 字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 Integer 整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 Boolean 布尔值。用于存储布尔值（真/假）。 Double 双精度浮点值。用于存储浮点值。 Min/Max keys 将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。 Array 用于将数组或列表或多个值存储为一个键。 Timestamp 时间戳。记录文档修改或添加的具体时间。 Object 用于内嵌文档。 Null 用于创建空值。 Symbol 符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 Date 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 Object ID 对象 ID。用于创建文档的 ID。 Binary Data 二进制数据。用于存储二进制数据。 Code 代码类型。用于在文档中存储 JavaScript 代码。 Regular expression 正则表达式类型。用于存储正则表达式。 下面说明下几种重要的数据类型。 ObjectId ObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是： 前 4 个字节表示创建 unix 时间戳,格林尼治时间 UTC 时间，比北京时间晚了 8 个小时 接下来的 3 个字节是机器标识码 紧接的两个字节由进程 id 组成 PID 最后三个字节是随机数 MongoDB 中存储的文档必须有一个 _id 键。这个键的值可以是任何类型的，默认是个 ObjectId 对象 由于 ObjectId 中保存了创建的时间戳，所以你不需要为你的文档保存时间戳字段，你可以通过 getTimestamp 函数来获取文档的创建时间: 123&gt; var newObject = ObjectId()&gt; newObject.getTimestamp()ISODate(&quot;2017-11-25T07:21:10Z&quot;) ObjectId 转为字符串 12&gt; newObject.str5a1919e63df83ce79df8b38f 字符串 BSON 字符串都是 UTF-8 编码。 时间戳 BSON 有一个特殊的时间戳类型用于 MongoDB 内部使用，与普通的 日期 类型不相关。 时间戳值是一个 64 位的值。其中： 前32位是一个 time_t 值（与Unix新纪元相差的秒数） 后32位是在某秒中操作的一个递增的序数 在单个 mongod 实例中，时间戳值通常是唯一的。 在复制集中， oplog 有一个 ts 字段。这个字段中的值使用BSON时间戳表示了操作时间。 BSON 时间戳类型主要用于 MongoDB 内部使用。在大多数情况下的应用开发中，你可以使用 BSON 日期类型。 日期 表示当前距离 Unix新纪元（1970年1月1日）的毫秒数。日期类型是有符号的, 负数表示 1970 年之前的日期。 12345678910&gt; var mydate1 = new Date() //格林尼治时间&gt; mydate1ISODate(&quot;2018-03-04T14:58:51.233Z&quot;)&gt; typeof mydate1object&gt; var mydate2 = ISODate() //格林尼治时间&gt; mydate2ISODate(&quot;2018-03-04T15:00:45.479Z&quot;)&gt; typeof mydate2object 这样创建的时间是日期类型，可以使用 JS 中的 Date 类型的方法。 返回一个时间类型的字符串： 12345&gt; var mydate1str = mydate1.toString()&gt; mydate1strSun Mar 04 2018 14:58:51 GMT+0000 (UTC) &gt; typeof mydate1strstring 或者 12&gt; Date()Sun Mar 04 2018 15:02:59 GMT+0000 (UTC) 三、安装（win） 下载 安装 创建默认数据文件夹 \\data\\db 与 \\data\\log \\MongoDB.log文件 手动运行MongoDB服务器 12345678#安装目录\\bin\\mongod --dbpath 自己定义的数据文件夹（c:\\data\\db）F:\\Softs\\Installed\\MongoDB\\bin\\mongod --dbpath F:\\Softs\\Installed\\MongoDB\\data\\db#成功结果：{&quot;t&quot;:{&quot;$date&quot;:&quot;2022-01-18T14:50:04.760+08:00&quot;},&quot;s&quot;:&quot;I&quot;, &quot;c&quot;:&quot;CONTROL&quot;, &quot;id&quot;:23285, &quot;ctx&quot;:&quot;-&quot;,&quot;msg&quot;:&quot;Automatically disabling TLS 1.0, to force-enable TLS 1.0 specify --sslDisabledProtocols 'none'&quot;}{&quot;t&quot;:{&quot;$date&quot;:&quot;2022-01-18T14:50:04.761+08:00&quot;},&quot;s&quot;:&quot;I&quot;, &quot;c&quot;:&quot;NETWORK&quot;, &quot;id&quot;:4915701, &quot;ctx&quot;:&quot;-&quot;,&quot;msg&quot;:&quot;Initialized wire specification&quot;,&quot;attr&quot;:{&quot;spec&quot;:{&quot;incomingExternalClient&quot;:{&quot;minWireVersion&quot;:0,&quot;maxWireVersion&quot;:13},&quot;incomingInternalClient&quot;:{&quot;minWireVersion&quot;:0,&quot;maxWireVersion&quot;:13},&quot;outgoing&quot;:{&quot;minWireVersion&quot;:0,&quot;maxWireVersion&quot;:13},&quot;isInternalClient&quot;:true}}}。。。 四、连接 12345678910111213141516171819202122232425#一#将其配置成服务后启动服务mongod --dbpath &quot;F:\\Softs\\Installed\\MongoDB\\data\\db&quot; --logpath &quot;F:\\Softs\\Installed\\MongoDB\\data\\log\\MongoDB.log&quot; --install --serviceName &quot;MongoDB&quot;PS F:\\Softs\\Installed\\MongoDB\\bin&gt; net start mongodb请求的服务已经启动。#二#配置bin目录环境变量后，用mongo连接C:\\Users\\Administrator&gt;mongoMongoDB shell version v5.0.6-rc0connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session { &quot;id&quot; : UUID(&quot;4f5de15b-0c12-4dde-8cc4-325ae6b23114&quot;) }MongoDB server version: 5.0.6-rc0#关闭#不为系统服务时关闭mongo命令窗口#系统服务NET stop MongoDB (关闭服务)#删除进程mongod --dbpath &quot;d:\\mongodb\\data\\db&quot; --logpath &quot;d:\\mongodb\\data\\log\\MongoDB.log&quot; --remove --serviceName &quot;MongoDB&quot; 五、数据库 1234567891011121314151617181920212223242526272829303132#创建数据库，没有创建，有则切换，#数据命名规则：#不能是空字符串（&quot;&quot;)。#不得含有' '（空格)、.、$、/、\\和\\0 (空字符)。#应全部小写。#最多64字节。#有一些数据库名是保留的，可以直接访问这些有特殊作用的数据库。#admin： 从权限的角度来看，这是&quot;root&quot;数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。#local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合#config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。&gt; use userswitched to db user#查看当前数据库名&gt; dbuser#插入数据&gt; db.user.insert({&quot;name&quot;:&quot;lxl&quot;})WriteResult({ &quot;nInserted&quot; : 1 })#查看数据库列表，有数据的数据库才会显示&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GBuser 0.000GB#删除数据库&gt; use userswitched to db user&gt; db.dropDatabase(){ &quot;ok&quot; : 1 } 六、集合 1234567891011121314151617181920#查看所有数据库&gt; show dbsadmin 0.000GBconfig 0.000GBlocal 0.000GB&gt; use studentswitched to db student#创建集合数据&gt; db.createCollection(&quot;students&quot;){ &quot;ok&quot; : 1 }#查看集合show collections 或 show tables&gt; show tablesstudents&gt; db.student.drop()false#删除集合数据&gt; db.students.drop()true&gt; show tables&gt; options 参数： 字段 类型 描述 capped 布尔 （可选）如果为 true，则创建固定集合。固定集合是指有着固定大小的集合，当达到最大值时，它会自动覆盖最早的文档。 当该值为 true 时，必须指定 size 参数。 autoIndexId 布尔 3.2 之后不再支持该参数。（可选）如为 true，自动在 _id 字段创建索引。默认为 false。 size 数值 （可选）为固定集合指定一个最大值，即字节数。 如果 capped 为 true，也需要指定该字段。 max 数值 （可选）指定固定集合中包含文档的最大数量。 七、文档 增 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&gt; show tables&gt; dbstudent&gt; show collections&gt; dbstudent#插入文档 insert（）&gt; db.student.insert({name:&quot;lxl&quot;})WriteResult({ &quot;nInserted&quot; : 1 })#查看文档 find()&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }# 将文档声明成变量再插入&gt; document=({age:18}){ &quot;age&quot; : 18 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 18 }&gt; document=({&quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;),age:19}){ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }# 插入文档 save（），有_id时重复则修改&gt; db.student.save(document)WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; document=({&quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;),age:20}){ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 20 }#插入文档 insetOne（）id重复报错 db.collection.insertMany()一次插入多个文档&gt; db.student.insertOne(document)WriteError({ &quot;index&quot; : 0, &quot;code&quot; : 11000, &quot;errmsg&quot; : &quot;E11000 duplicate key error collection: student.student index: _id_ dup key: { _id: ObjectId('61e7ae456e48f4bf534731d7') }&quot;, &quot;op&quot; : { &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 20 }}) :WriteError({ &quot;index&quot; : 0, &quot;code&quot; : 11000, &quot;errmsg&quot; : &quot;E11000 duplicate key error collection: student.student index: _id_ dup key: { _id: ObjectId('61e7ae456e48f4bf534731d7') }&quot;, &quot;op&quot; : { &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 20 }})WriteError@src/mongo/shell/bulk_api.js:465:48mergeBatchResults@src/mongo/shell/bulk_api.js:871:49executeBatch@src/mongo/shell/bulk_api.js:940:13Bulk/this.execute@src/mongo/shell/bulk_api.js:1182:21DBCollection.prototype.insertOne@src/mongo/shell/crud_api.js:264:9@(shell):1:1#插入文档replaceOne（）&gt; db.student.replaceOne(document)uncaught exception: TypeError: can't convert undefined to object :DBCollection.prototype.replaceOne@src/mongo/shell/crud_api.js:477:16@(shell):1:1&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; 改 模板 123456789db.collection.update( &lt;query&gt;, &lt;update&gt;, { upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; }) 参数说明： query : update的查询条件，类似sql update查询内where后面的。 update : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的 upsert : 可选，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。 multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 writeConcern :可选，抛出异常的级别。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lxl&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }#更新第一条文档&gt; db.student.update({&quot;name&quot;:&quot;lxl&quot;},{$set:{&quot;name&quot;:&quot;lll&quot;}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; document=({age:21}){ &quot;age&quot; : 21 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 21 }&gt; document=({age:21}){ &quot;age&quot; : 21 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 21 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 21 }#更新类型必须相同&gt; db.student.update({&quot;age&quot;:&quot;21&quot;},{$set:{&quot;age&quot;:&quot;22&quot;}})WriteResult({ &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 0 })#多条文档相同只更新第一条&gt; db.student.update({&quot;age&quot;:21},{$set:{&quot;age&quot;:22}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 22 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 21 }&gt; db.student.update({&quot;age&quot;:22},{$set:{&quot;age&quot;:21}})WriteResult({ &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 21 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 21 }#,{multi:true}将全部符合条件的文档都更新&gt; db.student.update({&quot;age&quot;:21},{$set:{&quot;age&quot;:22}},{multi:true})WriteResult({ &quot;nMatched&quot; : 2, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 2 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 22 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 22 }&gt; 删 1234567db.collection.remove( &lt;query&gt;, { justOne: &lt;boolean&gt;, writeConcern: &lt;document&gt; }) 参数说明： query :（可选）删除的文档的条件。 justOne : （可选）如果设为 true 或 1，则只删除一个文档，如果不设置该参数，或使用默认值 false，则删除所有匹配条件的文档。 writeConcern :（可选）抛出异常的级别 12345678910111213141516171819202122232425262728293031323334353637&gt; db.student.find({&quot;age&quot;:22}){ &quot;_id&quot; : ObjectId(&quot;61e7b1736e48f4bf534731d8&quot;), &quot;age&quot; : 22 }{ &quot;_id&quot; : ObjectId(&quot;61e7b19a6e48f4bf534731d9&quot;), &quot;age&quot; : 22 }#删除所有复合条件的文档&gt; db.student.remove({&quot;age&quot;:22})WriteResult({ &quot;nRemoved&quot; : 2 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b3d76e48f4bf534731da&quot;), &quot;age&quot; : 21 }{ &quot;_id&quot; : ObjectId(&quot;61e7b3d96e48f4bf534731db&quot;), &quot;age&quot; : 21 }#设置justOne为true或1，删除符合条件的第一条文档&gt; db.student.remove({&quot;age&quot;:22},1)WriteResult({ &quot;nRemoved&quot; : 0 })&gt; db.student.remove({&quot;age&quot;:21},1)WriteResult({ &quot;nRemoved&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7add36e48f4bf534731d6&quot;), &quot;name&quot; : &quot;lll&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7ae456e48f4bf534731d7&quot;), &quot;age&quot; : 19 }{ &quot;_id&quot; : ObjectId(&quot;61e7b3d96e48f4bf534731db&quot;), &quot;age&quot; : 21 }#删除所有文档&gt; db.student.remove()uncaught exception: Error: remove needs a query :DBCollection.prototype._parseRemove@src/mongo/shell/collection.js:364:15DBCollection.prototype.remove@src/mongo/shell/collection.js:394:18@(shell):1:1&gt; db.student.remove({})WriteResult({ &quot;nRemoved&quot; : 3 })&gt; db.student.find()&gt; 查 1db.collection.find(query, projection) query ：可选，使用查询操作符指定查询条件 projection ：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值， 只需省略该参数即可（默认省略）。 如果你需要以易读的方式来读取数据，可以使用 pretty() 方法，语法格式如下： 1&gt;db.col.find().pretty() pretty() 方法以格式化的方式来显示所有文档。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&gt; db.student.remove({})WriteResult({ &quot;nRemoved&quot; : 2 })&gt; db.student.insert({&quot;age&quot;:22,name:&quot;a1&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.insert({&quot;age&quot;:11,name:&quot;a2&quot;}) WriteResult({ &quot;nInserted&quot; : 1 })#查询第一个&gt; db.student.findOne(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }&gt; db.student.find().pretty(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#查询全部&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 小于$lt&gt; db.student.find({age:{$lt:20}}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#查询小于$lt的指定字段&gt; db.student.find({age:{$lt:20}}，{&quot;_id&quot;:1}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;)}#查询小于$lt的指定外字段&gt; db.student.find({age:{$lt:20}}，{&quot;_id&quot;:0}){ &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 小于或等于$lt&gt; db.student.find({age:{$lte:22}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 大于$lt&gt; db.student.find({age:{$gt:20}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }#条件查询 大于或等于$lt&gt; db.student.find({age:{$gte:11}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#条件查询 不等于$lt&gt; db.student.find({age:{$ne:11}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }&gt; db.student.insert({&quot;age&quot;:11,name:&quot;a3&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }#条件查询 and $ne&gt; db.student.find({age:{$ne:22},name:&quot;a2&quot;}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; db.student.insert({&quot;age&quot;:33,name:&quot;a3&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }#条件查询 or $or 缺少冒号&gt; db.student.find({$or[age:{$ne:11},name:&quot;a2&quot;]})uncaught exception: SyntaxError: missing : after property id :@(shell):1:20#条件查询 or $or 缺少括号&gt; db.student.find({$or:[age:{$ne:11},name:&quot;a2&quot;]})uncaught exception: SyntaxError: missing ] after element list :@(shell):1:25#条件查询 or $or&gt; db.student.find({$or:[{age:{$ne:11}},{name:&quot;a2&quot;}]}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }#条件查询 or + and&gt; db.student.find({name:&quot;a3&quot;,$or:[{age:{$ne:11}},{name:&quot;a2&quot;}]}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }&gt; $type 操作符 类型 数字 备注 Double 1 String 2 Object 3 Array 4 Binary data 5 Undefined 6 已废弃。 Object id 7 Boolean 8 Date 9 Null 10 Regular Expression 11 JavaScript 13 Symbol 14 JavaScript (with scope) 15 32-bit integer 16 Timestamp 17 64-bit integer 18 Min key 255 Query with -1. Max key 127 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&gt; db.student.insert({&quot;age&quot;:33,name:1})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find({&quot;name&quot;:{$type:'32-bit integer'}})Error: error: { &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;Unknown type name alias: 32-bit integer&quot;, &quot;code&quot; : 2, &quot;codeName&quot; : &quot;BadValue&quot;}&gt; db.student.find({&quot;name&quot;:{$type:'integer'}})Error: error: { &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;Unknown type name alias: integer&quot;, &quot;code&quot; : 2, &quot;codeName&quot; : &quot;BadValue&quot;}&gt; db.student.find({&quot;name&quot;:{$type:16}})&gt; db.student.find({&quot;name&quot;:{$type:2}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }&gt; db.student.find({&quot;name&quot;:{$type:18}})#类型数字默认为double 即1 类型&gt; db.student.find({&quot;name&quot;:{$type:1}}){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find({&quot;name&quot;:{$type:'Double'}})Error: error: { &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;Unknown type name alias: Double&quot;, &quot;code&quot; : 2, &quot;codeName&quot; : &quot;BadValue&quot;}&gt; db.student.find({&quot;name&quot;:{$type:Double}})uncaught exception: ReferenceError: Double is not defined :@(shell):1:26&gt; db.student.find({&quot;name&quot;:{$type:double}})uncaught exception: ReferenceError: double is not defined :@(shell):1:26#类型小写引号&gt; db.student.find({&quot;name&quot;:{$type:'double'}}){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; Limit() 方法 12345678910111213141516&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; db.student.find({name:{$type:1}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#条件筛选后限制查询数量&gt; db.student.find({name:{$type:2}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; Skip() 方法 12345678910111213141516171819&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }&gt; db.student.find({name:{$type:1}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find({name:{$type:2}}).limit(2){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }#跳过对应数量的限制查询的数据&gt; db.student.find({name:{$type:2}}).limit(2).skip(1){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }&gt; sort() 方法 12345678910111213&gt; db.student.find().sort({age:1}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().sort({age:-1}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }&gt; 八、索引 createIndex() 方法` 1234567891011121314151617181920212223242526272829303132333435&gt; db.student.find().sort({age:1}){ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }&gt; db.student.find().sort({age:-1}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }{ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }&gt; db.student.createIndex({&quot;age&quot;:1}){ &quot;numIndexesBefore&quot; : 1, &quot;numIndexesAfter&quot; : 2, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}&gt; db.student.createIndex({&quot;age&quot;:-1}){ &quot;numIndexesBefore&quot; : 2, &quot;numIndexesAfter&quot; : 3, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}#创建索引 1为正序，可使用多个字段创建索引&gt; db.student.createIndex({&quot;age&quot;:-1,&quot;name&quot;:1}){ &quot;numIndexesBefore&quot; : 3, &quot;numIndexesAfter&quot; : 4, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}&gt; Parameter Type Description background Boolean 建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 &quot;background&quot; 可选参数。 &quot;background&quot; 默认值为false。 unique Boolean 建立的索引是否唯一。指定为true创建唯一索引。默认值为false. name string 索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。 dropDups Boolean **3.0+版本已废弃。**在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 false. sparse Boolean 对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 false. expireAfterSeconds integer 指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。 v index version 索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。 weights document 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。 default_language string 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语 language_override string 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language. 九、聚合 aggregate() 方法 处理数据(诸如统计平均值，求和等) 12345678910111213&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#通过name判断相同name有多少数据&gt; db.student.aggregate([{$group:{_id:&quot;$name&quot;,num_tutorial:{$sum:1}}}]){ &quot;_id&quot; : &quot;a2&quot;, &quot;num_tutorial&quot; : 1 }{ &quot;_id&quot; : &quot;a3&quot;, &quot;num_tutorial&quot; : 2 }{ &quot;_id&quot; : &quot;a1&quot;, &quot;num_tutorial&quot; : 1 }{ &quot;_id&quot; : 1, &quot;num_tutorial&quot; : 1 }&gt; 聚合的表达式: 表达式 描述 实例 $sum 计算总和。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$sum : &quot;$likes&quot;}}}]) $avg 计算平均值 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$avg : &quot;$likes&quot;}}}]) $min 获取集合中所有文档对应值得最小值。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$min : &quot;$likes&quot;}}}]) $max 获取集合中所有文档对应值得最大值。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$max : &quot;$likes&quot;}}}]) $push 将值加入一个数组中，不会判断是否有重复的值。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, url : {$push: &quot;$url&quot;}}}]) $addToSet 将值加入一个数组中，会判断是否有重复的值，若相同的值在数组中已经存在了，则不加入。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, url : {$addToSet : &quot;$url&quot;}}}]) $first 根据资源文档的排序获取第一个文档数据。 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, first_url : {$first : &quot;$url&quot;}}}]) $last 根据资源文档的排序获取最后一个文档数据 db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, last_url : {$last : &quot;$url&quot;}}}]) 管道 管道在Unix和Linux中一般用于将当前命令的输出结果作为下一个命令的参数。 MongoDB的聚合管道将MongoDB文档在一个管道处理完毕后将结果传递给下一个管道处理。管道操作是可以重复的。 表达式：处理输入文档并输出。表达式是无状态的，只能用于计算当前聚合管道的文档，不能处理其它的文档。 这里我们介绍一下聚合框架中常用的几个操作： $project：修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。 $match：用于过滤数据，只输出符合条件的文档。$match使用MongoDB的标准查询操作。 $limit：用来限制MongoDB聚合管道返回的文档数。 $skip：在聚合管道中跳过指定数量的文档，并返回余下的文档。 $unwind：将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。 $group：将集合中的文档分组，可用于统计结果。 $sort：将输入文档排序后输出。 $geoNear：输出接近某一地理位置的有序文档。 12345678910111213141516171819202122232425262728&gt; db.student.find(){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;age&quot; : 22, &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;age&quot; : 11, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#只显示默认的id与name&gt; db.student.aggregate({$project:{name:1}}){ &quot;_id&quot; : ObjectId(&quot;61e7b65b6e48f4bf534731dd&quot;), &quot;name&quot; : &quot;a1&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6646e48f4bf534731de&quot;), &quot;name&quot; : &quot;a2&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b6f66e48f4bf534731df&quot;), &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;name&quot; : 1 }#去掉默认id，只显示name&gt; db.student.aggregate({$project:{_id:0,name:1}}){ &quot;name&quot; : &quot;a1&quot; }{ &quot;name&quot; : &quot;a2&quot; }{ &quot;name&quot; : &quot;a3&quot; }{ &quot;name&quot; : &quot;a3&quot; }{ &quot;name&quot; : 1 }#显示前三条之后的数据&gt; db.student.aggregate({$skip:3}){ &quot;_id&quot; : ObjectId(&quot;61e7b78a6e48f4bf534731e0&quot;), &quot;age&quot; : 33, &quot;name&quot; : &quot;a3&quot; }{ &quot;_id&quot; : ObjectId(&quot;61e8eedf6e48f4bf534731e1&quot;), &quot;age&quot; : 33, &quot;name&quot; : 1 }#显示大于10 小于等于25的一条数据&gt; db.student.aggregate( [{ $match : { age : { $gt:10,$lte:25}}},{ $group:{ _id: null, count: { $sum: 1 } } } ] );{ &quot;_id&quot; : null, &quot;count&quot; : 3 }&gt; 十、复制 数据冗余备份，硬件故障，服务中断数据恢复。无需停机维护，分布式读取数据。 mongodb的复制至少需要两个节点。其中一个是主节点，负责处理客户端请求，其余的都是从节点，负责复制主节点上的数据。 mongodb各个节点常见的搭配方式为：一主一从、一主多从。 主节点记录在其上的所有操作oplog，从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据与主节点一致。 MongoDB复制结构图如下所示： 以上结构图中，客户端从主节点读取数据，在客户端写入数据到主节点时， 主节点与从节点进行数据交互保障数据的一致性。 副本集特征： N 个节点的集群 任何节点可作为主节点 所有写入操作都在主节点上 自动故障转移 自动恢复 MongoDB副本集设置 在本教程中我们使用同一个MongoDB来做MongoDB主从的实验， 操作步骤如下： 1、关闭正在运行的MongoDB服务器。 现在我们通过指定 --replSet 选项来启动mongoDB。--replSet 基本语法格式如下： 1mongod --port &quot;PORT&quot; --dbpath &quot;YOUR_DB_DATA_PATH&quot; --replSet &quot;REPLICA_SET_INSTANCE_NAME&quot; 示例 1mongod --port 27017 --dbpath &quot;F:\\Softs\\Installed\\MongoDB\\data&quot; --replSet rs0 示例启动一个名为rs0的MongoDB实例，其端口号为27017。启动后打开命令提示框并连接上mongoDB服务。在Mongo客户端使用命令rs.initiate()来启动一个新的副本集。我们可以使用rs.conf()来查看副本集的配置查看副本集状态使用 rs.status() 命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155#关闭服务C:\\Users\\Administrator&gt;net stop mongodb#创建rs0实例C:\\Users\\Administrator&gt;mongod --port 27017 --dbpath &quot;F:\\Softs\\Installed\\MongoDB\\data&quot; --replSet rs0#启动C:\\Users\\Administrator&gt;mongoMongoDB shell version v5.0.6-rc0connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodbImplicit session: session { &quot;id&quot; : UUID(&quot;a7bc72f5-0db4-4de3-b3c1-4f457b526e8b&quot;) }MongoDB server version: 5.0.6-rc0================Warning: the &quot;mongo&quot; shell has been superseded by &quot;mongosh&quot;,which delivers improved usability and compatibility.The &quot;mongo&quot; shell has been deprecated and will be removed inan upcoming release.For installation instructions, seehttps://docs.mongodb.com/mongodb-shell/install/================#启动新的副本集&gt; rs.initiate(){ &quot;info2&quot; : &quot;no configuration specified. Using a default configuration for the set&quot;, &quot;me&quot; : &quot;localhost:27017&quot;, &quot;ok&quot; : 1}查看副本配置rs0:SECONDARY&gt; rs.conf(){ &quot;_id&quot; : &quot;rs0&quot;, &quot;version&quot; : 1, &quot;term&quot; : 1, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;host&quot; : &quot;localhost:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : { }, &quot;secondaryDelaySecs&quot; : NumberLong(0), &quot;votes&quot; : 1 } ], &quot;protocolVersion&quot; : NumberLong(1), &quot;writeConcernMajorityJournalDefault&quot; : true, &quot;settings&quot; : { &quot;chainingAllowed&quot; : true, &quot;heartbeatIntervalMillis&quot; : 2000, &quot;heartbeatTimeoutSecs&quot; : 10, &quot;electionTimeoutMillis&quot; : 10000, &quot;catchUpTimeoutMillis&quot; : -1, &quot;catchUpTakeoverDelayMillis&quot; : 30000, &quot;getLastErrorModes&quot; : { }, &quot;getLastErrorDefaults&quot; : { &quot;w&quot; : 1, &quot;wtimeout&quot; : 0 }, &quot;replicaSetId&quot; : ObjectId(&quot;61ea43d55303298c0a702c41&quot;) }}#查看副本集状态rs0:PRIMARY&gt; rs.status(){ &quot;set&quot; : &quot;rs0&quot;, &quot;date&quot; : ISODate(&quot;2022-01-21T05:26:33.032Z&quot;), &quot;myState&quot; : 1, &quot;term&quot; : NumberLong(1), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;majorityVoteCount&quot; : 1, &quot;writeMajorityCount&quot; : 1, &quot;votingMembersCount&quot; : 1, &quot;writableVotingMembersCount&quot; : 1, &quot;optimes&quot; : { &quot;lastCommittedOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;lastCommittedWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;readConcernMajorityOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;appliedOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;durableOpTime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;lastAppliedWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;) }, &quot;lastStableRecoveryTimestamp&quot; : Timestamp(1642742741, 1), &quot;electionCandidateMetrics&quot; : { &quot;lastElectionReason&quot; : &quot;electionTimeout&quot;, &quot;lastElectionDate&quot; : ISODate(&quot;2022-01-21T05:25:42.687Z&quot;), &quot;electionTerm&quot; : NumberLong(1), &quot;lastCommittedOpTimeAtElection&quot; : { &quot;ts&quot; : Timestamp(1642742741, 1), &quot;t&quot; : NumberLong(-1) }, &quot;lastSeenOpTimeAtElection&quot; : { &quot;ts&quot; : Timestamp(1642742741, 1), &quot;t&quot; : NumberLong(-1) }, &quot;numVotesNeeded&quot; : 1, &quot;priorityAtElection&quot; : 1, &quot;electionTimeoutMillis&quot; : NumberLong(10000), &quot;newTermStartDate&quot; : ISODate(&quot;2022-01-21T05:25:42.963Z&quot;), &quot;wMajorityWriteAvailabilityDate&quot; : ISODate(&quot;2022-01-21T05:25:43.131Z&quot;) }, &quot;members&quot; : [ { &quot;_id&quot; : 0, &quot;name&quot; : &quot;localhost:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 203, &quot;optime&quot; : { &quot;ts&quot; : Timestamp(1642742783, 1), &quot;t&quot; : NumberLong(1) }, &quot;optimeDate&quot; : ISODate(&quot;2022-01-21T05:26:23Z&quot;), &quot;lastAppliedWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;lastDurableWallTime&quot; : ISODate(&quot;2022-01-21T05:26:23.117Z&quot;), &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;Could not find member to sync from&quot;, &quot;electionTime&quot; : Timestamp(1642742742, 1), &quot;electionDate&quot; : ISODate(&quot;2022-01-21T05:25:42Z&quot;), &quot;configVersion&quot; : 1, &quot;configTerm&quot; : 1, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; } ], &quot;ok&quot; : 1, &quot;$clusterTime&quot; : { &quot;clusterTime&quot; : Timestamp(1642742783, 1), &quot;signature&quot; : { &quot;hash&quot; : BinData(0,&quot;AAAAAAAAAAAAAAAAAAAAAAAAAAA=&quot;), &quot;keyId&quot; : NumberLong(0) } }, &quot;operationTime&quot; : Timestamp(1642742783, 1)}rs0:PRIMARY&gt; 副本集添加成员 添加副本集的成员，我们需要使用多台服务器来启动mongo服务。进入Mongo客户端，并使用rs.add()方法来添加副本集的成员。 语法 rs.add() 命令基本语法格式如下： 1&gt;rs.add(HOST_NAME:PORT) 实例 假设你已经启动了一个名为mongod1.net，端口号为27017的Mongo服务。 在客户端命令窗口使用rs.add() 命令将其添加到副本集中，命令如下所示： 12&gt;rs.add(&quot;mongod1.net:27017&quot;)&gt; MongoDB中你只能通过主节点将Mongo服务添加到副本集中， 判断当前运行的Mongo服务是否为主节点可以使用命令db.isMaster() 。 MongoDB的副本集与我们常见的主从有所不同，主从在主机宕机后所有服务将停止，而副本集在主机宕机后，副本会接管主节点成为主节点，不会出现宕机的情况。 十一、分片 分片 在Mongodb里面存在另一种集群，就是分片技术,可以满足MongoDB数据量大量增长的需求。 当MongoDB存储海量的数据时，一台机器可能不足以存储数据，也可能不足以提供可接受的读写吞吐量。这时，我们就可以通过在多台机器上分割数据，使得数据库系统能存储和处理更多的数据。 为什么使用分片 复制所有的写入操作到主节点 延迟的敏感数据会在主节点查询 单个副本集限制在12个节点 当请求量巨大时会出现内存不足。 本地磁盘不足 垂直扩展价格昂贵 MongoDB分片 下图展示了在MongoDB中使用分片集群结构分布： 上图中主要有如下所述三个主要组件： Shard: 用于存储实际的数据块，实际生产环境中一个shard server角色可由几台机器组个一个replica set承担，防止主机单点故障 Config Server: mongod 实例，存储了整个 ClusterMetadata，其中包括 chunk 信息。 Query Routers: 前端路由，客户端由此接入，且让整个集群看上去像单一数据库，前端应用可以透明使用。 分片实例 分片结构端口分布如下： 123456Shard Server 1：27020Shard Server 2：27021Shard Server 3：27022Shard Server 4：27023Config Server ：27100Route Process：40000 步骤一：启动Shard Server 12345678[root@100 /]# mkdir -p /www/mongoDB/shard/s0[root@100 /]# mkdir -p /www/mongoDB/shard/s1[root@100 /]# mkdir -p /www/mongoDB/shard/s2[root@100 /]# mkdir -p /www/mongoDB/shard/s3[root@100 /]# mkdir -p /www/mongoDB/shard/log[root@100 /]# /usr/local/mongoDB/bin/mongod --port 27020 --dbpath=/www/mongoDB/shard/s0 --logpath=/www/mongoDB/shard/log/s0.log --logappend --fork....[root@100 /]# /usr/local/mongoDB/bin/mongod --port 27023 --dbpath=/www/mongoDB/shard/s3 --logpath=/www/mongoDB/shard/log/s3.log --logappend --fork 步骤二： 启动Config Server 12[root@100 /]# mkdir -p /www/mongoDB/shard/config[root@100 /]# /usr/local/mongoDB/bin/mongod --port 27100 --dbpath=/www/mongoDB/shard/config --logpath=/www/mongoDB/shard/log/config.log --logappend --fork **注意：**这里我们完全可以像启动普通mongodb服务一样启动，不需要添加—shardsvr和configsvr参数。因为这两个参数的作用就是改变启动端口的，所以我们自行指定了端口就可以。 步骤三： 启动Route Process 1/usr/local/mongoDB/bin/mongos --port 40000 --configdb localhost:27100 --fork --logpath=/www/mongoDB/shard/log/route.log --chunkSize 500 mongos启动参数中，chunkSize这一项是用来指定chunk的大小的，单位是MB，默认大小为200MB. 步骤四： 配置Sharding 接下来，我们使用MongoDB Shell登录到mongos，添加Shard节点 123456789101112[root@100 shard]# /usr/local/mongoDB/bin/mongo admin --port 40000MongoDB shell version: 2.0.7connecting to: 127.0.0.1:40000/adminmongos&gt; db.runCommand({ addshard:&quot;localhost:27020&quot; }){ &quot;shardAdded&quot; : &quot;shard0000&quot;, &quot;ok&quot; : 1 }......mongos&gt; db.runCommand({ addshard:&quot;localhost:27029&quot; }){ &quot;shardAdded&quot; : &quot;shard0009&quot;, &quot;ok&quot; : 1 }mongos&gt; db.runCommand({ enablesharding:&quot;test&quot; }) #设置分片存储的数据库{ &quot;ok&quot; : 1 }mongos&gt; db.runCommand({ shardcollection: &quot;test.log&quot;, key: { id:1,time:1}}){ &quot;collectionsharded&quot; : &quot;test.log&quot;, &quot;ok&quot; : 1 } 步骤五： 程序代码内无需太大更改，直接按照连接普通的mongo数据库那样，将数据库连接接入接口40000 十二、安装其他工具 下载后将其bin目录下复制到data下bin目录下： https://www.mongodb.com/try/download/database-tools 十三、备份 / 恢复 数据备份 备份(mongodump)与恢复(mongorestore)。在Mongodb中我们使用mongodump命令来备份MongoDB数据。该命令可以导出所有数据到指定目录中。 mongodump命令可以通过参数指定导出的数据量级转存的服务器。 语法 mongodump命令脚本语法如下： 1&gt;mongodump -h dbhost -d dbname -o dbdirectory -h： MongoDB 所在服务器地址，例如：127.0.0.1，当然也可以指定端口号：127.0.0.1:27017 -d： 需要备份的数据库实例，例如：test -o： 备份的数据存放位置，例如：c:\\data\\dump，当然该目录需要提前建立，在备份完成后，系统自动在dump目录下建立一个test目录，这个目录里面存放该数据库实例的备份数据。 实例 在本地使用 27017 启动你的mongod服务。打开命令提示符窗口，进入MongoDB安装目录的bin目录输入命令mongodump: 1&gt;mongodump #即mongod 执行以上命令后，客户端会连接到ip为 127.0.0.1 端口号为 27017 的MongoDB服务上，并备份所有数据到 bin/dump/ 目录中。命令输出结果如下： mongodump 命令可选参数列表如下所示： 语法 描述 实例 mongodump --host HOST_NAME --port PORT_NUMBER 该命令将备份所有MongoDB数据 mongodump --host runoob.com --port 27017 mongodump --dbpath DB_PATH --out BACKUP_DIRECTORY mongodump --dbpath /data/db/ --out /data/backup/ mongodump --collection COLLECTION --db DB_NAME 该命令将备份指定数据库的集合。 mongodump --collection mycol --db test 数据恢复 mongodb使用 mongorestore 命令来恢复备份的数据。 语法 mongorestore命令脚本语法如下： 1&gt;mongorestore -h &lt;hostname&gt;&lt;:port&gt; -d dbname &lt;path&gt; --host &lt;:port&gt;, -h &lt;:port&gt;： MongoDB所在服务器地址，默认为： localhost:27017 --db , -d ： 需要恢复的数据库实例，例如：test，当然这个名称也可以和备份时候的不一样，比如test2 --drop： 恢复的时候，先删除当前数据，然后恢复备份的数据。就是说，恢复后，备份后添加修改的数据都会被删除，慎用哦！ ： mongorestore 最后的一个参数，设置备份数据所在位置，例如：c:\\data\\dump\\test。 你不能同时指定 和 --dir 选项，--dir也可以设置备份目录。 --dir： 指定备份的目录 你不能同时指定 和 --dir 选项。 接下来我们执行以下命令: 1&gt;mongorestore 执行以上命令输出结果如下： 十四、监控 在你已经安装部署并允许MongoDB服务后，你必须要了解MongoDB的运行情况，并查看MongoDB的性能。这样在大流量得情况下可以很好的应对并保证MongoDB正常运作。 MongoDB中提供了mongostat 和 mongotop 两个命令来监控MongoDB的运行情况。 mongostat 命令 mongostat是mongodb自带的状态检测工具，在命令行下使用。它会间隔固定时间获取mongodb的当前运行状态，并输出。如果你发现数据库突然变慢或者有其他问题的话，你第一手的操作就考虑采用mongostat来查看mongo的状态。 启动你的Mongod服务，进入到你安装的MongoDB目录下的bin目录， 然后输入mongostat命令，如下所示： 1D:\\set up\\mongodb\\bin&gt;mongostat 以上命令输出结果如下： mongotop 命令 mongotop也是mongodb下的一个内置工具，mongotop提供了一个方法，用来跟踪一个MongoDB的实例，查看哪些大量的时间花费在读取和写入数据。 mongotop提供每个集合的水平的统计数据。默认情况下，mongotop返回值的每一秒。 启动你的Mongod服务，进入到你安装的MongoDB目录下的bin目录， 然后输入mongotop命令，如下所示： 1D:\\set up\\mongodb\\bin&gt;mongotop 以上命令执行输出结果如下： 带参数实例 1E:\\mongodb-win32-x86_64-2.2.1\\bin&gt;mongotop 10 后面的10是**参数 ，可以不使用，等待的时间长度，以秒为单位，mongotop等待调用之间。通过的默认mongotop返回数据的每一秒。 1E:\\mongodb-win32-x86_64-2.2.1\\bin&gt;mongotop --locks 报告每个数据库的锁的使用中，使用mongotop - 锁，这将产生以下输出： 输出结果字段说明： ns： 包含数据库命名空间，后者结合了数据库名称和集合。 db： 包含数据库的名称。名为 . 的数据库针对全局锁定，而非特定数据库。 total： mongod花费的时间工作在这个命名空间提供总额。 read： 提供了大量的时间，这mongod花费在执行读操作，在此命名空间。 write： 提供这个命名空间进行写操作，这mongod花了大量的时间。 十五、MongoDBCompass使用 MongoDB可视化工具 mongo命令连接服务器后打开工具 点击连接默认的服务器、接口和None认证的服务 十六、Java使用 文档 引入依赖 123456&lt;!--MongoDB--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongodb-driver-sync&lt;/artifactId&gt; &lt;version&gt;4.4.1&lt;/version&gt;&lt;/dependency&gt; 连接 1234567891011121314151617181920212223242526272829303132333435363738394041424344package pers.lxl.mylearnproject.programbase.nosql.mongolearn;import static com.mongodb.client.model.Filters.eq;import org.bson.Document;import com.mongodb.client.MongoClient;import com.mongodb.client.MongoClients;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoDatabase;public class HelloMongoDB {//1.添加依赖// &lt;dependency&gt;// &lt;groupId&gt;org.mongodb&lt;/groupId&gt;// &lt;artifactId&gt;mongodb-driver-sync&lt;/artifactId&gt;// &lt;version&gt;4.4.1&lt;/version&gt;// &lt;/dependency&gt;// 2.连接到数据库public static void main( String args[] ){// try{// // 连接到 mongodb 服务// MongoClient mongoClient = new MongoClient( &quot;localhost&quot; , 27017 );//// // 连接到数据库// MongoDatabase mongoDatabase = mongoClient.getDatabase(&quot;mycol&quot;);// System.out.println(&quot;Connect to database successfully&quot;);//// }catch(Exception e){// System.err.println( e.getClass().getName() + &quot;: &quot; + e.getMessage() );// }// Replace the uri string with your MongoDB deployment's connection string// IllegalArgumentException: Unsupported compressor 'disabled'// String uri = &quot;mongodb://127.0.0.1:27017/?compressors=disabled&amp;gssapiServiceName=mongodb&quot;;// Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: com/mongodb/internal/connection/InternalConnectionPoolSettings 升级springboot版本 String uri = &quot;mongodb://127.0.0.1:27017/?gssapiServiceName=mongodb&quot;; try (MongoClient mongoClient = MongoClients.create(uri)) {// show dbs MongoDatabase database = mongoClient.getDatabase(&quot;student&quot;);// show collections 或 show tables MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;student&quot;); Document doc = collection.find(eq(&quot;age&quot;, 11)).first(); System.out.println(doc.toJson()+&quot;==========&quot;); }}} 十七、关系 MongoDB 的关系表示多个文档之间在逻辑上的相互联系。 文档间可以通过嵌入和引用来建立联系。 MongoDB 中的关系可以是： 1:1 (1对1) 1: N (1对多) N: 1 (多对1) N: N (多对多) 接下来我们来考虑下用户与用户地址的关系。 一个用户可以有多个地址，所以是一对多的关系。 以下是 user 文档的简单结构： 123456{ &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;), &quot;name&quot;: &quot;Tom Hanks&quot;, &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;} 以下是 address 文档的简单结构： 1234567{ &quot;_id&quot;:ObjectId(&quot;52ffc4a5d85242602e000000&quot;), &quot;building&quot;: &quot;22 A, Indiana Apt&quot;, &quot;pincode&quot;: 123456, &quot;city&quot;: &quot;Los Angeles&quot;, &quot;state&quot;: &quot;California&quot;} 嵌入式关系 使用嵌入式方法，我们可以把用户地址嵌入到用户的文档中： 12345678910111213141516171819{ &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;), &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;, &quot;address&quot;: [ { &quot;building&quot;: &quot;22 A, Indiana Apt&quot;, &quot;pincode&quot;: 123456, &quot;city&quot;: &quot;Los Angeles&quot;, &quot;state&quot;: &quot;California&quot; }, { &quot;building&quot;: &quot;170 A, Acropolis Apt&quot;, &quot;pincode&quot;: 456789, &quot;city&quot;: &quot;Chicago&quot;, &quot;state&quot;: &quot;Illinois&quot; }]} 以上数据保存在单一的文档中，可以比较容易的获取和维护数据。 你可以这样查询用户的地址： 1&gt;db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;},{&quot;address&quot;:1}) 注意：以上查询中 db 和 users 表示数据库和集合。 这种数据结构的缺点是，如果用户和用户地址在不断增加，数据量不断变大，会影响读写性能。 引用式关系 引用式关系是设计数据库时经常用到的方法，这种方法把用户数据文档和用户地址数据文档分开，通过引用文档的 id 字段来建立关系。 12345678910{ &quot;_id&quot;:ObjectId(&quot;52ffc33cd85242f436000001&quot;), &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;, &quot;address_ids&quot;: [ ObjectId(&quot;52ffc4a5d85242602e000000&quot;), ObjectId(&quot;52ffc4a5d85242602e000001&quot;) ]} 以上实例中，用户文档的 address_ids 字段包含用户地址的对象id（ObjectId）数组。 我们可以读取这些用户地址的对象id（ObjectId）来获取用户的详细地址信息。 这种方法需要两次查询，第一次查询用户地址的对象id（ObjectId），第二次通过查询的id获取用户的详细地址信息。 12&gt;var result = db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;},{&quot;address_ids&quot;:1})&gt;var addresses = db.address.find({&quot;_id&quot;:{&quot;$in&quot;:result[&quot;address_ids&quot;]}}) 十八、数据库引用 在上一章节MongoDB关系中我们提到了MongoDB的引用来规范数据结构文档。 MongoDB 引用有两种： 手动引用（Manual References） DBRefs DBRefs vs 手动引用 考虑这样的一个场景，我们在不同的集合中 (address_home, address_office, address_mailing, 等)存储不同的地址（住址，办公室地址，邮件地址等）。 这样，我们在调用不同地址时，也需要指定集合，一个文档从多个集合引用文档，我们应该使用 DBRefs。 使用 DBRefs DBRef的形式： 1{ $ref : , $id : , $db : } 三个字段表示的意义为： $ref：集合名称 $id：引用的id $db:数据库名称，可选参数 以下实例中用户数据文档使用了 DBRef, 字段 address： 12345678910{ &quot;_id&quot;:ObjectId(&quot;53402597d852426020000002&quot;), &quot;address&quot;: { &quot;$ref&quot;: &quot;address_home&quot;, &quot;$id&quot;: ObjectId(&quot;534009e4d852427820000002&quot;), &quot;$db&quot;: &quot;runoob&quot;}, &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;} address DBRef 字段指定了引用的地址文档是在 runoob 数据库下的 address_home 集合，id 为 534009e4d852427820000002。 以下代码中，我们通过指定 $ref 参数（address_home 集合）来查找集合中指定id的用户地址信息： 123&gt;var user = db.users.findOne({&quot;name&quot;:&quot;Tom Benzamin&quot;})&gt;var dbRef = user.address&gt;db[dbRef.$ref].findOne({&quot;_id&quot;:(dbRef.$id)}) 以上实例返回了 address_home 集合中的地址数据： 1234567{ &quot;_id&quot; : ObjectId(&quot;534009e4d852427820000002&quot;), &quot;building&quot; : &quot;22 A, Indiana Apt&quot;, &quot;pincode&quot; : 123456, &quot;city&quot; : &quot;Los Angeles&quot;, &quot;state&quot; : &quot;California&quot;} 十九、覆盖索引查询 官方的MongoDB的文档中说明，覆盖查询是以下的查询： 所有的查询字段是索引的一部分 所有的查询返回字段在同一个索引中 由于所有出现在查询中的字段是索引的一部分， MongoDB 无需在整个数据文档中检索匹配查询条件和返回使用相同索引的查询结果。 因为索引存在于RAM中，从索引中获取数据比通过扫描文档读取数据要快得多。 使用覆盖索引查询 为了测试覆盖索引查询，使用以下 users 集合: 12345678{ &quot;_id&quot;: ObjectId(&quot;53402597d852426020000002&quot;), &quot;contact&quot;: &quot;987654321&quot;, &quot;dob&quot;: &quot;01-01-1991&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;name&quot;: &quot;Tom Benzamin&quot;, &quot;user_name&quot;: &quot;tombenzamin&quot;} 我们在 users 集合中创建联合索引，字段为 gender 和 user_name : 1&gt;db.users.ensureIndex({gender:1,user_name:1}) 现在，该索引会覆盖以下查询： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}) 也就是说，对于上述查询，MongoDB的不会去数据库文件中查找。相反，它会从索引中提取数据，这是非常快速的数据查询。 由于我们的索引中不包括 _id 字段，_id在查询中会默认返回，我们可以在MongoDB的查询结果集中排除它。 下面的实例没有排除_id，查询就不会被覆盖： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1}) 最后，如果是以下的查询，不能使用覆盖索引查询： 所有索引字段是一个数组 所有索引字段是一个子文档 二十、查询分析 MongoDB 查询分析可以确保我们所建立的索引是否有效，是查询语句性能分析的重要工具。 MongoDB 查询分析常用函数有：explain() 和 hint()。 使用 explain() explain 操作提供了查询信息，使用索引及查询统计等。有利于我们对索引的优化。 接下来我们在 users 集合中创建 gender 和 user_name 的索引： 1&gt;db.users.ensureIndex({gender:1,user_name:1}) 现在在查询语句中使用 explain ： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}).explain() 以上的 explain() 查询返回如下结果： 1234567891011121314151617181920212223242526272829303132{ &quot;cursor&quot; : &quot;BtreeCursor gender_1_user_name_1&quot;, &quot;isMultiKey&quot; : false, &quot;n&quot; : 1, &quot;nscannedObjects&quot; : 0, &quot;nscanned&quot; : 1, &quot;nscannedObjectsAllPlans&quot; : 0, &quot;nscannedAllPlans&quot; : 1, &quot;scanAndOrder&quot; : false, &quot;indexOnly&quot; : true,//**indexOnly**: 字段为 true ，表示我们使用了索引。 &quot;nYields&quot; : 0, &quot;nChunkSkips&quot; : 0, &quot;millis&quot; : 0, &quot;indexBounds&quot; : { &quot;gender&quot; : [ [ &quot;M&quot;, &quot;M&quot; ] ], &quot;user_name&quot; : [ [ { &quot;$minElement&quot; : 1 }, { &quot;$maxElement&quot; : 1 } ] ] }} 现在，我们看看这个结果集的字段： indexOnly: 字段为 true ，表示我们使用了索引。 cursor：因为这个查询使用了索引，MongoDB 中索引存储在B树结构中，所以这是也使用了 BtreeCursor 类型的游标。如果没有使用索引，游标的类型是 BasicCursor。这个键还会给出你所使用的索引的名称，你通过这个名称可以查看当前数据库下的system.indexes集合（系统自动创建，由于存储索引信息，这个稍微会提到）来得到索引的详细信息。 n：当前查询返回的文档数量。 nscanned/nscannedObjects：表明当前这次查询一共扫描了集合中多少个文档，我们的目的是，让这个数值和返回文档的数量越接近越好。 millis：当前查询所需时间，毫秒数。 indexBounds：当前查询具体使用的索引。 使用 hint() 虽然MongoDB查询优化器一般工作的很不错，但是也可以使用 hint 来强制 MongoDB 使用一个指定的索引。 这种方法某些情形下会提升性能。 一个有索引的 collection 并且执行一个多字段的查询(一些字段已经索引了)。 如下查询实例指定了使用 gender 和 user_name 索引字段来查询： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}).hint({gender:1,user_name:1}) 可以使用 explain() 函数来分析以上查询： 1&gt;db.users.find({gender:&quot;M&quot;},{user_name:1,_id:0}).hint({gender:1,user_name:1}).explain() 二一、MongoDB 原子操作 mongodb不支持事务，所以，在你的项目中应用时，要注意这点。无论什么设计，都不要要求mongodb保证数据的完整性。 但是mongodb提供了许多原子操作，比如文档的保存，修改，删除等，都是原子操作。 所谓原子操作就是要么这个文档保存到Mongodb，要么没有保存到Mongodb，不会出现查询到的文档没有保存完整的情况。 原子操作数据模型 考虑下面的例子，图书馆的书籍及结账信息。 实例说明了在一个相同的文档中如何确保嵌入字段关联原子操作（update：更新）的字段是同步的。 1234567891011book = { _id: 123456789, title: &quot;MongoDB: The Definitive Guide&quot;, author: [ &quot;Kristina Chodorow&quot;, &quot;Mike Dirolf&quot; ], published_date: ISODate(&quot;2010-09-24&quot;), pages: 216, language: &quot;English&quot;, publisher_id: &quot;oreilly&quot;, available: 3, checkout: [ { by: &quot;joe&quot;, date: ISODate(&quot;2012-10-15&quot;) } ] } 你可以使用 db.collection.findAndModify() 方法来判断书籍是否可结算并更新新的结算信息。 在同一个文档中嵌入的 available 和 checkout 字段来确保这些字段是同步更新的: 12345678910db.books.findAndModify ( { query: { _id: 123456789, available: { $gt: 0 } }, update: { $inc: { available: -1 }, $push: { checkout: { by: &quot;abc&quot;, date: new Date() } } }} ) 原子操作常用命令 $set 用来指定一个键并更新键值，若键不存在并创建。 1{ $set : { field : value } } $unset 用来删除一个键。 1{ $unset : { field : 1} } $inc $inc可以对文档的某个值为数字型（只能为满足要求的数字）的键进行增减的操作。 1{ $inc : { field : value } } $push 用法： 1{ $push : { field : value } } 把value追加到field里面去，field一定要是数组类型才行，如果field不存在，会新增一个数组类型加进去。 $pushAll 同$push,只是一次可以追加多个值到一个数组字段内。 1{ $pushAll : { field : value_array } } $pull 从数组field内删除一个等于value值。 1{ $pull : { field : _value } } $addToSet 增加一个值到数组内，而且只有当这个值不在数组内才增加。 $pop 删除数组的第一个或最后一个元素 1{ $pop : { field : 1 } } $rename 修改字段名称 1{ $rename : { old_field_name : new_field_name } } $bit 位操作，integer类型 1{$bit : { field : {and : 5}}} 偏移操作符 12345&gt; t.find() { &quot;_id&quot; : ObjectId(&quot;4b97e62bf1d8c7152c9ccb74&quot;), &quot;title&quot; : &quot;ABC&quot;, &quot;comments&quot; : [ { &quot;by&quot; : &quot;joe&quot;, &quot;votes&quot; : 3 }, { &quot;by&quot; : &quot;jane&quot;, &quot;votes&quot; : 7 } ] } &gt; t.update( {'comments.by':'joe'}, {$inc:{'comments.$.votes':1}}, false, true ) &gt; t.find() { &quot;_id&quot; : ObjectId(&quot;4b97e62bf1d8c7152c9ccb74&quot;), &quot;title&quot; : &quot;ABC&quot;, &quot;comments&quot; : [ { &quot;by&quot; : &quot;joe&quot;, &quot;votes&quot; : 4 }, { &quot;by&quot; : &quot;jane&quot;, &quot;votes&quot; : 7 } ] } 二二、高级索引 考虑以下文档集合（users ）: 12345678910111213{ &quot;address&quot;: { &quot;city&quot;: &quot;Los Angeles&quot;, &quot;state&quot;: &quot;California&quot;, &quot;pincode&quot;: &quot;123&quot; }, &quot;tags&quot;: [ &quot;music&quot;, &quot;cricket&quot;, &quot;blogs&quot; ], &quot;name&quot;: &quot;Tom Benzamin&quot;} 以上文档包含了 address 子文档和 tags 数组。 索引数组字段 假设我们基于标签来检索用户，为此我们需要对集合中的数组 tags 建立索引。 在数组中创建索引，需要对数组中的每个字段依次建立索引。所以在我们为数组 tags 创建索引时，会为 music、cricket、blogs三个值建立单独的索引。 使用以下命令创建数组索引： 1234567891011121314#3.0弃用&gt;db.users.ensureIndex({&quot;tags&quot;:1})&gt; db.users.ensureIndex({&quot;tags&quot;:1})uncaught exception: TypeError: db.users.ensureIndex is not a function :@(shell):1:1&gt; db.users.createIndex({&quot;tags&quot;:1}){ &quot;numIndexesBefore&quot; : 1, &quot;numIndexesAfter&quot; : 2, &quot;createdCollectionAutomatically&quot; : false, &quot;ok&quot; : 1}&gt; 创建索引后，我们可以这样检索集合的 tags 字段： 1&gt;db.users.find({tags:&quot;cricket&quot;}) 为了验证我们使用使用了索引，可以使用 explain 命令： 1&gt;db.users.find({tags:&quot;cricket&quot;}).explain() 以上命令执行结果中会显示 &quot;cursor&quot; : &quot;BtreeCursor tags_1&quot; ，则表示已经使用了索引。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970db.users.find({tags:&quot;cricket&quot;}).explain(){ &quot;explainVersion&quot; : &quot;1&quot;, &quot;queryPlanner&quot; : { &quot;namespace&quot; : &quot;users.users&quot;, &quot;indexFilterSet&quot; : false, &quot;parsedQuery&quot; : { &quot;tags&quot; : { &quot;$eq&quot; : &quot;cricket&quot; } }, &quot;queryHash&quot; : &quot;9D3B61A7&quot;, &quot;planCacheKey&quot; : &quot;3C3D3201&quot;, &quot;maxIndexedOrSolutionsReached&quot; : false, &quot;maxIndexedAndSolutionsReached&quot; : false, &quot;maxScansToExplodeReached&quot; : false, &quot;winningPlan&quot; : { &quot;stage&quot; : &quot;FETCH&quot;, &quot;inputStage&quot; : { &quot;stage&quot; : &quot;IXSCAN&quot;, &quot;keyPattern&quot; : { &quot;tags&quot; : 1 }, &quot;indexName&quot; : &quot;tags_1&quot;, &quot;isMultiKey&quot; : true, &quot;multiKeyPaths&quot; : { &quot;tags&quot; : [ &quot;tags&quot; ] }, &quot;isUnique&quot; : false, &quot;isSparse&quot; : false, &quot;isPartial&quot; : false, &quot;indexVersion&quot; : 2, &quot;direction&quot; : &quot;forward&quot;, &quot;indexBounds&quot; : { &quot;tags&quot; : [ &quot;[\\&quot;cricket\\&quot;, \\&quot;cricket\\&quot;]&quot; ] } } }, &quot;rejectedPlans&quot; : [ ] }, &quot;command&quot; : { &quot;find&quot; : &quot;users&quot;, &quot;filter&quot; : { &quot;tags&quot; : &quot;cricket&quot; }, &quot;$db&quot; : &quot;users&quot; }, &quot;serverInfo&quot; : { &quot;host&quot; : &quot;BF-202103261718&quot;, &quot;port&quot; : 27017, &quot;version&quot; : &quot;5.0.6-rc0&quot;, &quot;gitVersion&quot; : &quot;60af56dfe1a17c60bbd628163fda0a161105b6c0&quot; }, &quot;serverParameters&quot; : { &quot;internalQueryFacetBufferSizeBytes&quot; : 104857600, &quot;internalQueryFacetMaxOutputDocSizeBytes&quot; : 104857600, &quot;internalLookupStageIntermediateDocumentMaxSizeBytes&quot; : 104857600, &quot;internalDocumentSourceGroupMaxMemoryBytes&quot; : 104857600, &quot;internalQueryMaxBlockingSortMemoryUsageBytes&quot; : 104857600, &quot;internalQueryProhibitBlockingMergeOnMongoS&quot; : 0, &quot;internalQueryMaxAddToSetBytes&quot; : 104857600, &quot;internalDocumentSourceSetWindowFieldsMaxMemoryBytes&quot; : 104857600 }, &quot;ok&quot; : 1}&gt; 索引子文档字段 假设我们需要通过city、state、pincode字段来检索文档，由于这些字段是子文档的字段，所以我们需要对子文档建立索引。 为子文档的三个字段创建索引，命令如下： 1&gt;db.users.ensureIndex({&quot;address.city&quot;:1,&quot;address.state&quot;:1,&quot;address.pincode&quot;:1}) 一旦创建索引，我们可以使用子文档的字段来检索数据： 1&gt;db.users.find({&quot;address.city&quot;:&quot;Los Angeles&quot;}) 查询表达不一定遵循指定的索引的顺序，mongodb 会自动优化。所以上面创建的索引将支持以下查询： 1&gt;db.users.find({&quot;address.state&quot;:&quot;California&quot;,&quot;address.city&quot;:&quot;Los Angeles&quot;}) 同样支持以下查询： 1&gt;db.users.find({&quot;address.city&quot;:&quot;Los Angeles&quot;,&quot;address.state&quot;:&quot;California&quot;,&quot;address.pincode&quot;:&quot;123&quot;}) 二三、索引限制 额外开销 每个索引占据一定的存储空间，在进行插入，更新和删除操作时也需要对索引进行操作。所以，如果你很少对集合进行读取操作，建议不使用索引。 内存(RAM)使用 由于索引是存储在内存(RAM)中,你应该确保该索引的大小不超过内存的限制。 如果索引的大小大于内存的限制，MongoDB会删除一些索引，这将导致性能下降。 查询限制 索引不能被以下的查询使用： 正则表达式及非操作符，如 $nin, $not, 等。 算术运算符，如 $mod, 等。 $where 子句 所以，检测你的语句是否使用索引是一个好的习惯，可以用explain来查看。 索引键限制 从2.6版本开始，如果现有的索引字段的值超过索引键的限制，MongoDB中不会创建索引。 插入文档超过索引键限制 如果文档的索引字段值超过了索引键的限制，MongoDB不会将任何文档转换成索引的集合。与mongorestore和mongoimport工具类似。 最大范围 集合中索引不能超过64个 索引名的长度不能超过128个字符 一个复合索引最多可以有31个字段 二四、MongoDB ObjectId 在前面几个章节中我们已经使用了MongoDB 的对象 Id(ObjectId)。 在本章节中，我们将了解的ObjectId的结构。 ObjectId 是一个12字节 BSON 类型数据，有以下格式： 前4个字节表示时间戳 接下来的3个字节是机器标识码 紧接的两个字节由进程id组成（PID） 最后三个字节是随机数。 MongoDB中存储的文档必须有一个&quot;_id&quot;键。这个键的值可以是任何类型的，默认是个ObjectId对象。 在一个集合里面，每个文档都有唯一的&quot;_id&quot;值，来确保集合里面每个文档都能被唯一标识。 MongoDB采用ObjectId，而不是其他比较常规的做法（比如自动增加的主键）的主要原因，因为在多个 服务器上同步自动增加主键值既费力还费时。 创建新的ObjectId 使用以下代码生成新的ObjectId： 1&gt;newObjectId = ObjectId() 上面的语句返回以下唯一生成的id： 1ObjectId(&quot;5349b4ddd2781d08c09890f3&quot;) 你也可以使用生成的id来取代MongoDB自动生成的ObjectId： 1&gt;myObjectId = ObjectId(&quot;5349b4ddd2781d08c09890f4&quot;) 创建文档的时间戳 由于 ObjectId 中存储了 4 个字节的时间戳，所以你不需要为你的文档保存时间戳字段，你可以通过 getTimestamp 函数来获取文档的创建时间: 1&gt;ObjectId(&quot;5349b4ddd2781d08c09890f4&quot;).getTimestamp() 以上代码将返回 ISO 格式的文档创建时间： 1ISODate(&quot;2014-04-12T21:49:17Z&quot;) ObjectId 转换为字符串 在某些情况下，您可能需要将ObjectId转换为字符串格式。你可以使用下面的代码： 1&gt;new ObjectId().str 以上代码将返回Guid格式的字符串：： 15349b4ddd2781d08c09890f3 二五、 Map Reduce Map-Reduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。 MongoDB提供的Map-Reduce非常灵活，对于大规模数据分析也相当实用。 MapReduce 命令 以下是MapReduce的基本语法： 12345678910&gt;db.collection.mapReduce( function() {emit(key,value);}, //map 函数 function(key,values) {return reduceFunction}, //reduce 函数 { out: collection, query: document, sort: document, limit: number }) 使用 MapReduce 要实现两个函数 Map 函数和 Reduce 函数,Map 函数调用 emit(key, value), 遍历 collection 中所有的记录, 将 key 与 value 传递给 Reduce 函数进行处理。 Map 函数必须调用 emit(key, value) 返回键值对。 参数说明: map ：映射函数 (生成键值对序列,作为 reduce 函数参数)。 reduce 统计函数，reduce函数的任务就是将key-values变成key-value，也就是把values数组变成一个单一的值value。。 out 统计结果存放集合 (不指定则使用临时集合,在客户端断开后自动删除)。 query 一个筛选条件，只有满足条件的文档才会调用map函数。（query。limit，sort可以随意组合） sort 和limit结合的sort排序参数（也是在发往map函数前给文档排序），可以优化分组机制 limit 发往map函数的文档数量的上限（要是没有limit，单独使用sort的用处不大） 以下实例在集合 orders 中查找 status:&quot;A&quot; 的数据，并根据 cust_id 来分组，并计算 amount 的总和。 使用 MapReduce 考虑以下文档结构存储用户的文章，文档存储了用户的 user_name 和文章的 status 字段： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;mark&quot;, &quot;status&quot;:&quot;disabled&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;disabled&quot;})WriteResult({ &quot;nInserted&quot; : 1 })&gt;db.posts.insert({ &quot;post_text&quot;: &quot;菜鸟教程，最全的技术文档。&quot;, &quot;user_name&quot;: &quot;runoob&quot;, &quot;status&quot;:&quot;active&quot;})WriteResult({ &quot;nInserted&quot; : 1 }) 现在，我们将在 posts 集合中使用 mapReduce 函数来选取已发布的文章(status:&quot;active&quot;)，并通过user_name分组，计算每个用户的文章数： 12345678&gt;db.posts.mapReduce( function() { emit(this.user_name,1); }, function(key, values) {return Array.sum(values)}, { query:{status:&quot;active&quot;}, out:&quot;post_total&quot; }) 以上 mapReduce 输出结果为： 1234567891011{ &quot;result&quot; : &quot;post_total&quot;, &quot;timeMillis&quot; : 23, &quot;counts&quot; : { &quot;input&quot; : 5, &quot;emit&quot; : 5, &quot;reduce&quot; : 1, &quot;output&quot; : 2 }, &quot;ok&quot; : 1} 结果表明，共有 5 个符合查询条件（status:&quot;active&quot;）的文档， 在map函数中生成了 5 个键值对文档，最后使用reduce函数将相同的键值分为 2 组。 具体参数说明： result：储存结果的collection的名字,这是个临时集合，MapReduce的连接关闭后自动就被删除了。 timeMillis：执行花费的时间，毫秒为单位 input：满足条件被发送到map函数的文档个数 emit：在map函数中emit被调用的次数，也就是所有集合中的数据总量 ouput：结果集合中的文档个数**（count对调试非常有帮助）** ok：是否成功，成功为1 err：如果失败，这里可以有失败原因，不过从经验上来看，原因比较模糊，作用不大 使用 find 操作符来查看 mapReduce 的查询结果： 12345678&gt;db.posts.mapReduce( function() { emit(this.user_name,1); }, function(key, values) {return Array.sum(values)}, { query:{status:&quot;active&quot;}, out:&quot;post_total&quot; }).find() 以上查询显示如下结果: 12{ &quot;_id&quot; : &quot;mark&quot;, &quot;value&quot; : 4 }{ &quot;_id&quot; : &quot;runoob&quot;, &quot;value&quot; : 1 } 用类似的方式，MapReduce可以被用来构建大型复杂的聚合查询。 Map函数和Reduce函数可以使用 JavaScript 来实现，使得MapReduce的使用非常灵活和强大。 二六、全文检索 全文检索对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。 这个过程类似于通过字典中的检索字表查字的过程。 MongoDB 从 2.4 版本开始支持全文检索，目前支持15种语言的全文索引。 danish dutch english finnish french german hungarian italian norwegian portuguese romanian russian spanish swedish turkish 启用全文检索 MongoDB 在 2.6 版本以后是默认开启全文检索的，如果你使用之前的版本，你需要使用以下代码来启用全文检索: 1&gt;db.adminCommand({setParameter:true,textSearchEnabled:true}) 或者使用命令： 1mongod --setParameter textSearchEnabled=true 创建全文索引 考虑以下 posts 集合的文档数据，包含了文章内容（post_text）及标签(tags)： 1234567{ &quot;post_text&quot;: &quot;enjoy the mongodb articles on Runoob&quot;, &quot;tags&quot;: [ &quot;mongodb&quot;, &quot;runoob&quot; ]} 我们可以对 post_text 字段建立全文索引，这样我们可以搜索文章内的内容： 1&gt;db.posts.ensureIndex({post_text:&quot;text&quot;}) 使用全文索引 现在我们已经对 post_text 建立了全文索引，我们可以搜索文章中的关键词 runoob： 1&gt;db.posts.find({$text:{$search:&quot;runoob&quot;}}) 以下命令返回了如下包含 runoob 关键词的文档数据： 12345{ &quot;_id&quot; : ObjectId(&quot;53493d14d852429c10000002&quot;), &quot;post_text&quot; : &quot;enjoy the mongodb articles on Runoob&quot;, &quot;tags&quot; : [ &quot;mongodb&quot;, &quot;runoob&quot; ]} 如果你使用的是旧版本的 MongoDB，你可以使用以下命令： 1&gt;db.posts.runCommand(&quot;text&quot;,{search:&quot;runoob&quot;}) 使用全文索引可以提高搜索效率。 删除全文索引 删除已存在的全文索引，可以使用 find 命令查找索引名： 1&gt;db.posts.getIndexes() 通过以上命令获取索引名，本例的索引名为post_text_text，执行以下命令来删除索引： 1&gt;db.posts.dropIndex(&quot;post_text_text&quot;) 二七、正则表达式 正则表达式是使用单个字符串来描述、匹配一系列符合某个句法规则的字符串。 许多程序设计语言都支持利用正则表达式进行字符串操作。 MongoDB 使用 $regex 操作符来设置匹配字符串的正则表达式。 MongoDB使用PCRE (Perl Compatible Regular Expression) 作为正则表达式语言。 不同于全文检索，我们使用正则表达式不需要做任何配置。 考虑以下 posts 集合的文档结构，该文档包含了文章内容和标签： 1234567{ &quot;post_text&quot;: &quot;enjoy the mongodb articles on runoob&quot;, &quot;tags&quot;: [ &quot;mongodb&quot;, &quot;runoob&quot; ]} 使用正则表达式 以下命令使用正则表达式查找包含 runoob 字符串的文章： 1&gt;db.posts.find({post_text:{$regex:&quot;runoob&quot;}}) 以上查询也可以写为： 1&gt;db.posts.find({post_text:/runoob/}) 不区分大小写的正则表达式 如果检索需要不区分大小写，我们可以设置 $options 为 $i。 以下命令将查找不区分大小写的字符串 runoob： 1&gt;db.posts.find({post_text:{$regex:&quot;runoob&quot;,$options:&quot;$i&quot;}}) 集合中会返回所有包含字符串 runoob 的数据，且不区分大小写： 12345{ &quot;_id&quot; : ObjectId(&quot;53493d37d852429c10000004&quot;), &quot;post_text&quot; : &quot;hey! this is my post on runoob&quot;, &quot;tags&quot; : [ &quot;runoob&quot; ]} 数组元素使用正则表达式 我们还可以在数组字段中使用正则表达式来查找内容。 这在标签的实现上非常有用，如果你需要查找包含以 run 开头的标签数据(ru 或 run 或 runoob)， 你可以使用以下代码： 1&gt;db.posts.find({tags:{$regex:&quot;run&quot;}}) 优化正则表达式查询 如果你的文档中字段设置了索引，那么使用索引相比于正则表达式匹配查找所有的数据查询速度更快。 如果正则表达式是前缀表达式，所有匹配的数据将以指定的前缀字符串为开始。例如： 如果正则表达式为 ^tut ，查询语句将查找以 tut 为开头的字符串。 这里面使用正则表达式有两点需要注意： 正则表达式中使用变量。一定要使用eval将组合的字符串进行转换，不能直接将字符串拼接后传入给表达式。否则没有报错信息，只是结果为空！实例如下： 1var name=eval(&quot;/&quot; + 变量值key +&quot;/i&quot;); 以下是模糊查询包含title关键词, 且不区分大小写: 1title:eval(&quot;/&quot;+title+&quot;/i&quot;) // 等同于 title:{$regex:title,$Option:&quot;$i&quot;} 二八、管理工具: Rockmongo RockMongo是PHP5写的一个MongoDB管理工具。 通过 Rockmongo 你可以管理 MongoDB服务，数据库，集合，文档，索引等等。 它提供了非常人性化的操作。类似 phpMyAdmin（PHP开发的MySql管理工具）。 Rockmongo 下载地址：https://github.com/iwind/rockmongo 简介 主要特征： 使用宽松的New BSD License协议 速度快，安装简单 支持多语言（目前提供中文、英文、日文、巴西葡萄牙语、法语、德语、俄语、意大利语） 系统 可以配置多个主机，每个主机可以有多个管理员 需要管理员密码才能登入操作，确保数据库的安全性 服务器 服务器信息 (WEB服务器, PHP, PHP.ini相关指令 ...) 状态 数据库信息 数据库 查询，创建和删除 执行命令和Javascript代码 统计信息 集合（相当于表） 强大的查询工具 读数据，写数据，更改数据，复制数据，删除数据 查询、创建和删除索引 清空数据 批量删除和更改数据 统计信息 GridFS 查看分块 下载文件 安装 需求 一个能运行PHP的Web服务器，比如Apache Httpd, Nginx ... PHP - 需要PHP v5.1.6或更高版本，需要支持SESSION 为了能连接MongoDB，你需要安装php_mongo扩展 快速安装 下载安装包 解压到你的网站目录下 用编辑器打开config.php，修改host, port, admins等参数 在浏览器中访问index.php，比如说：http://localhost/rockmongo/index.php 使用用户名和密码登录，默认为&quot;admin&quot;和&quot;admin&quot; 开始玩转MongoDB! 二九、GridFS GridFS 用于存储和恢复那些超过16M（BSON文件限制）的文件(如：图片、音频、视频等)。 GridFS 也是文件存储的一种方式，但是它是存储在MonoDB的集合中。 GridFS 可以更好的存储大于16M的文件。 GridFS 会将大文件对象分割成多个小的chunk(文件片段),一般为256k/个,每个chunk将作为MongoDB的一个文档(document)被存储在chunks集合中。 GridFS 用两个集合来存储一个文件：fs.files与fs.chunks。 每个文件的实际内容被存在chunks(二进制数据)中,和文件有关的meta数据(filename,content_type,还有用户自定义的属性)将会被存在files集合中。 以下是简单的 fs.files 集合文档： 1234567{ &quot;filename&quot;: &quot;test.txt&quot;, &quot;chunkSize&quot;: NumberInt(261120), &quot;uploadDate&quot;: ISODate(&quot;2014-04-13T11:32:33.557Z&quot;), &quot;md5&quot;: &quot;7b762939321e146569b07f72c62cca4f&quot;, &quot;length&quot;: NumberInt(646)} 以下是简单的 fs.chunks 集合文档： 12345{ &quot;files_id&quot;: ObjectId(&quot;534a75d19f54bfec8a2fe44b&quot;), &quot;n&quot;: NumberInt(0), &quot;data&quot;: &quot;Mongo Binary Data&quot;} GridFS 添加文件 现在我们使用 GridFS 的 put 命令来存储 mp3 文件。 调用 MongoDB 安装目录下bin的 mongofiles.exe工具。 打开命令提示符，进入到MongoDB的安装目录的bin目录中，找到mongofiles.exe，并输入下面的代码： 1&gt;mongofiles.exe -d gridfs put song.mp3 -d gridfs 指定存储文件的数据库名称，如果不存在该数据库，MongoDB会自动创建。如果不存在该数据库，MongoDB会自动创建。Song.mp3 是音频文件名。 使用以下命令来查看数据库中文件的文档： 1&gt;db.fs.files.find() 以上命令执行后返回以下文档数据： 1234567{ _id: ObjectId('534a811bf8b4aa4d33fdf94d'), filename: &quot;song.mp3&quot;, chunkSize: 261120, uploadDate: new Date(1397391643474), md5: &quot;e4f53379c909f7bed2e9d631e15c1c41&quot;, length: 10401959 } 我们可以看到 fs.chunks 集合中所有的区块，以下我们得到了文件的 _id 值，我们可以根据这个 _id 获取区块(chunk)的数据： 1&gt;db.fs.chunks.find({files_id:ObjectId('534a811bf8b4aa4d33fdf94d')}) 以上实例中，查询返回了 40 个文档的数据，意味着mp3文件被存储在40个区块中。 三十、固定集合（Capped Collections） MongoDB 固定集合（Capped Collections）是性能出色且有着固定大小的集合，对于大小固定，我们可以想象其就像一个环形队列，当集合空间用完后，再插入的元素就会覆盖最初始的头部的元素！ 创建固定集合 我们通过createCollection来创建一个固定集合，且capped选项设置为true： 1&gt;db.createCollection(&quot;cappedLogCollection&quot;,{capped:true,size:10000}) 还可以指定文档个数,加上max:1000属性： 1&gt;db.createCollection(&quot;cappedLogCollection&quot;,{capped:true,size:10000,max:1000}) 判断集合是否为固定集合: 1&gt;db.cappedLogCollection.isCapped() 如果需要将已存在的集合转换为固定集合可以使用以下命令： 1&gt;db.runCommand({&quot;convertToCapped&quot;:&quot;posts&quot;,size:10000}) 以上代码将我们已存在的 posts 集合转换为固定集合。 固定集合查询 固定集合文档按照插入顺序储存的,默认情况下查询就是按照插入顺序返回的,也可以使用$natural调整返回顺序。 1&gt;db.cappedLogCollection.find().sort({$natural:-1}) 固定集合的功能特点 可以插入及更新,但更新不能超出collection的大小,否则更新失败,不允许删除,但是可以调用drop()删除集合中的所有行,但是drop后需要显式地重建集合。 在32位机子上一个cappped collection的最大值约为482.5M,64位上只受系统文件大小的限制。 固定集合属性及用法 属性 属性1:对固定集合进行插入速度极快 属性2:按照插入顺序的查询输出速度极快 属性3:能够在插入最新数据时,淘汰最早的数据 用法 用法1:储存日志信息 用法2:缓存一些少量的文档 三一、自动增长 MongoDB 没有像 SQL 一样有自动增长的功能， MongoDB 的 _id 是系统自动生成的12字节唯一标识。 但在某些情况下，我们可能需要实现 ObjectId 自动增长功能。 由于 MongoDB 没有实现这个功能，我们可以通过编程的方式来实现，以下我们将在 counters 集合中实现_id字段自动增长。 使用 counters 集合 考虑以下 products 文档。我们希望 _id 字段实现 从 1,2,3,4 到 n 的自动增长功能。 12345{ &quot;_id&quot;:1, &quot;product_name&quot;: &quot;Apple iPhone&quot;, &quot;category&quot;: &quot;mobiles&quot;} 为此，创建 counters 集合，序列字段值可以实现自动长： 1&gt;db.createCollection(&quot;counters&quot;) 现在我们向 counters 集合中插入以下文档，使用 productid 作为 key: 1234{ &quot;_id&quot;:&quot;productid&quot;, &quot;sequence_value&quot;: 0} sequence_value 字段是序列通过自动增长后的一个值。 使用以下命令插入 counters 集合的序列文档中： 1&gt;db.counters.insert({_id:&quot;productid&quot;,sequence_value:0}) 创建 Javascript 函数 现在，我们创建函数 getNextSequenceValue 来作为序列名的输入， 指定的序列会自动增长 1 并返回最新序列值。在本文的实例中序列名为 productid 。 123456789&gt;function getNextSequenceValue(sequenceName){ var sequenceDocument = db.counters.findAndModify( { query:{_id: sequenceName }, update: {$inc:{sequence_value:1}}, &quot;new&quot;:true }); return sequenceDocument.sequence_value;} 使用 Javascript 函数 接下来我们将使用 getNextSequenceValue 函数创建一个新的文档， 并设置文档 _id 自动为返回的序列值： 123456789&gt;db.products.insert({ &quot;_id&quot;:getNextSequenceValue(&quot;productid&quot;), &quot;product_name&quot;:&quot;Apple iPhone&quot;, &quot;category&quot;:&quot;mobiles&quot;})&gt;db.products.insert({ &quot;_id&quot;:getNextSequenceValue(&quot;productid&quot;), &quot;product_name&quot;:&quot;Samsung S3&quot;, &quot;category&quot;:&quot;mobiles&quot;}) 就如你所看到的，我们使用 getNextSequenceValue 函数来设置 _id 字段。 为了验证函数是否有效，我们可以使用以下命令读取文档： 1&gt;db.products.find() 以上命令将返回以下结果，我们发现 _id 字段是自增长的： 123{ &quot;_id&quot; : 1, &quot;product_name&quot; : &quot;Apple iPhone&quot;, &quot;category&quot; : &quot;mobiles&quot;}{ &quot;_id&quot; : 2, &quot;product_name&quot; : &quot;Samsung S3&quot;, &quot;category&quot; : &quot;mobiles&quot; }","link":"/2022/03/05/Draft/2021/MongoDB/"},{"title":"MYSQL优化","text":"MySql知识及优化 相关资源：bili视频、官方文档 MySQL基础 为什么要使用数据库 内存 优： 存取速度快；缺： 数据不能永久保存 文件 优： 数据永久保存；缺：1）速度比内存操作慢，频繁的IO操作 2）查询不方便 数据库 1）数据永久保存 2）使用SQL语句，查询方便效率高。3）管理数据方便 什么是SQL？ 结构化查询语言(Structured Query Language)，用于存取数据、查询、更新和管理关系数据库系统的数据库查询语言 什么是MySQL? 关系型数据库管理系统 数据库三大范式是什么 第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 mysql有关权限的表都有哪几个 MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容： user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。 db权限表：记录各个帐号在各个数据库上的操作权限。 table_priv权限表：记录数据表级的操作权限。 columns_priv权限表：记录数据列级的操作权限。 host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。 MySQL的binlog有有几种录入格式？分别有什么区别？ 有三种格式，statement，row和mixed。 statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。 row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。 mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。 执行顺序 ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png) CREATE（drop，use） DATABASE 数据库名; CREATE TABLE IF NOT EXISTS runoob_tbl( runoob_id INT UNSIGNED AUTO_INCREMENT, runoob_title VARCHAR(100) NOT NULL, runoob_author VARCHAR(40) NOT NULL, submission_date DATE, PRIMARY KEY ( runoob_id ) )ENGINE=InnoDB DEFAULT CHARSET=utf8; 增删改查 • INSERT INTO table_name ( field1, field2,...fieldN ) VALUES ( value1, value2,...valueN )， ( value11, value22,...valueNN ); • SELECT column_name,column_name FROM table_name [WHERE condition1 [AND [OR]] condition2 field1 LIKE condition1 [AND [OR]] filed2 = 'somevalue'][LIMIT N][ OFFSET M] 查询语句中你可以使用一个或者多个表，表之间使用逗号(,)分割，并使用WHERE语句来设定查询条件。 SELECT 命令可以读取一条或者多条记录。 你可以使用星号（*）来代替其他字段，SELECT语句会返回表的所有字段数据 你可以使用 WHERE 语句来包含任何条件，使用 AND 或者 OR 指定一个或多个条件。 你可以使用LIKE子句代替等号 =。- 代表单个，% 表示任意 0 个或多个字符。可匹配任意类型和长度的字符，有些情况下若是中文，请使用两个百分号（%%）表示。[]：表示括号内所列字符中的一个（类似正则表达式）。指定一个字符、字符串或范围，要求所匹配对象为它们中的任一个。[^] ：表示不在括号所列之内的单个字符。其取值和 [] 相同，但它要求所匹配对象为指定字符以外的任一个字符。 查询内容包含通配符时,由于通配符的缘故，导致我们查询特殊字符 “%”、“_”、“[” 的语句无法正常实现，而把特殊字符用 “[ ]” 括起便可正常查询。 你可以使用 LIMIT 属性来设定返回的记录数。 你可以通过OFFSET指定SELECT语句开始查询的数据偏移量。默认情况下偏移量为0。 • SELECT expression1, expression2, ... expression_n FROM tables [WHERE conditions] UNION [ALL | DISTINCT] SELECT expression1, expression2, ... expression_n FROM tables [WHERE conditions]; UNION 操作符用于连接两个以上的 SELECT 语句的结果组合到一个结果集合中。多个 SELECT 语句会删除重复的数据。 expression1, expression2, ... expression_n: 要检索的列。 tables: 要检索的数据表。 WHERE conditions: 可选， 检索条件。 DISTINCT: 可选，删除结果集中重复的数据。默认情况下 UNION 操作符已经删除了重复数据，所以 DISTINCT 修饰符对结果没啥影响。 ALL: 可选，返回所有结果集，包含重复数据。 • SELECT field1, field2,...fieldN FROM table_name1, table_name2...ORDER BY field1 [ASC [DESC][默认 ASC]], [field2...] [ASC [DESC][默认 ASC]] • SELECT column_name, function(column_name) FROM table_name WHERE column_name operator value GROUP BY column_name; • 根据一个或多个列对结果集进行分组。 在分组的列上我们可以使用 COUNT, SUM, AVG,等函数。 WITH ROLLUP 可以实现在分组统计数据基础上再进行相同的统计（SUM,AVG,COUNT…） • 使用聚合函数后需要group by分组，having只能用于group by 后面 • INNER JOIN（内连接,或等值连接）【交集】：获取两个表中字段匹配关系的记录。 LEFT JOIN（左连接）【左交】：获取左表所有记录，即使右表没有对应匹配的记录。 RIGHT JOIN（右连接）【左交】： 与 LEFT JOIN 相反，用于获取右表所有记录，即使左表没有对应匹配的记录。 • select p.id,p.name,t.content from person p left join task t on p.id=t.person_id order by p.id • UPDATE table_name SET field1=new-value1, field2=new-value2 [WHERE Clause] 你可以同时更新一个或多个字段。 你可以在 WHERE 子句中指定任何条件。 你可以在一个单独表中同时更新数据。 • update titles_test set emp_no=replace(emp_no,'10001','10005') where id=5 • DELETE FROM table_name [WHERE Clause] 如果没有指定 WHERE 子句，MySQL 表中的所有记录将被删除。 你可以在 WHERE 子句中指定任何条件 您可以在单个表中一次性删除记录。 DROP TABLE table_name ; ALTER TABLE table_name DROP i; ADD i INT ; 删除，添加或修改表字段 MODIFY c CHAR(10) ; CHANGE i j BIGINT; 修改字段类型及名称 ALTER i SET DEFAULT 1000; ALTER i DROP DEFAULT; 修改,删除字段的默认值 ALTER TABLE testalter_tbl RENAME TO alter_tbl;改表名 · 表中只剩余一个字段则无法使用DROP来删除字段 SQL语句主要分为哪几类 **数据定义语言 DDL（Data Ddefinition Language）**CREATE，DROP，ALTER 对逻辑结构等有操作的，其中包括表结构，视图和索引。 数据查询语言 DQL（Data Query Language）SELECT 查询操作，以 select 关键字。简单查询，连接查询等 都属于 DQL。 **数据操纵语言 DML（Data Manipulation Language）**INSERT，UPDATE，DELETE 数据操作，对应上面所说的查询操作 DQL 与 DML 共同构建了多数初级程序员常用的增删改查操作。查询较为特殊划分到 DQL 中。 **数据控制功能 DCL（Data Control Language）**GRANT，REVOKE，COMMIT，ROLLBACK 对数据库安全性完整性等有操作的，权限控制等 DQMC 超键、候选键、主键、外键分别是什么？ 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 SQL 约束有哪几种？ NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK: 用于控制字段的值范围。 六种关联查询 交叉连接（CROSS JOIN） 内连接（INNER JOIN） 外连接（LEFT JOIN/RIGHT JOIN） 联合查询（UNION 与 UNION ALL） 全连接（FULL JOIN） · 交叉连接（CROSS JOIN） SELECT * FROM A,B(,C) 或者 SELECT * FROM A CROSS JOIN B (CROSS JOIN C) #没有任何关联条件，结果是笛卡尔积，结果集会很大，没有意义，很少使用内连接（INNER JOIN）SELECT * FROM A,B WHERE A.id=B.id或者SELECT * FROM A INNER JOIN B ON A.id=B.id多表中同时符合某种条件的数据记录的集合，INNER JOIN可以缩写为JOIN · 内连接分为三类 等值连接：ON A.id=B.id 不等值连接：ON A.id &gt; B.id 自连接：SELECT * FROM A T1 INNER JOIN A T2 ON T1.id=T2.pid · 外连接（LEFT JOIN/RIGHT JOIN） 左外连接：LEFT OUTER JOIN, 以左表为主，先查询出左表，按照 ON 后的关联条件匹配右表，没有匹配到的用 NULL 填充，可以简写成 LEFT JOIN 右外连接：RIGHT OUTER JOIN, 以右表为主，先查询出右表，按照 ON 后的关联条件匹配左表，没有匹配到的用 NULL 填充，可以简写成 RIGHT JOIN · 联合查询（UNION 与 UNION ALL） SELECT * FROM A UNION SELECT * FROM B UNION ... 就是把多个结果集集中在一起，UNION 前的结果为基准，需要注意的是联合查询的列数要相等，相同的记录行会合并 如果使用 UNION ALL，不会合并重复的记录行 效率 UNION 高于 UNION ALL · 全连接（FULL JOIN） MySQL 不支持全连接 可以使用 LEFT JOIN 和 UNION 和 RIGHT JOIN 联合使用 SELECT * FROM A LEFT JOIN B ON A.id=B.id UNIONSELECT * FROM A RIGHT JOIN B ON A.id=B.id 什么是子查询 条件：一条 SQL 语句的查询结果做为另一条查询语句的条件或查询结果 嵌套：多条 SQL 语句嵌套使用，内部的 SQL 查询语句称为子查询。 子查询的三种情况 子查询是单行单列的情况：结果集是一个值，父查询使用：=、 &lt;、&gt; 等运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行单列的情况：结果集类似于一个数组，父查询使用：in 运算符 -- 查询工资最高的员工是谁？ select * from employee where salary=(select max(salary) from employee); 子查询是多行多列的情况：结果集类似于一张虚拟表，不能用于 where 条件，用于 select 子句中做为子表 -- 1) 查询出2011年以后入职的员工信息 -- 2) 查询所有的部门信息，与上面的虚拟表中的信息比对，找出所有部门ID相等的员工。 select * from dept d, (select * from employee where join_date &gt; '2011-1-1') e where e.dept_id = d.id; -- 使用表连接： select d., e. from dept d inner join employee e on d.id = e.dept_id where e.join_date &gt; '2011-1-1' mysql中 in 和 exists 区别 mysql 中的 in 语句是把外表和内表作 hash 连接，而 exists 语句是对外表作 loop 循环，每次 loop 循环再对内表进行查询。一直大家都认为 exists 比 in 语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用 in 和 exists 差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用 exists，子查询表小的用 in。 not in 和 not exists：如果查询语句使用了 not in，那么内外表都进行全表扫描，没有用到索引；而 not extsts 的子查询依然能用到表上的索引。所以无论那个表大，用 not exists 都比 not in 要快。 varchar与char的区别 char 的特点 char 表示定长字符串，长度是固定的； 如果插入数据的长度小于 char 的固定长度时，则用空格填充； 因为长度固定，所以存取速度要比 varchar 快很多，甚至能快 50%，但正因为其长度固定，所以会占据多余的空间，是空间换时间的做法； 对于 char 来说，最多能存放的字符个数为 255，和编码无关 varchar 的特点 varchar 表示可变长字符串，长度是可变的； 插入的数据是多长，就按照多长来存储； varchar 在存取方面与 char 相反，它存取慢，因为长度不固定，但正因如此，不占据多余的空间，是时间换空间的做法； 对于 varchar 来说，最多能存放的字符个数为 65532 总之，结合性能角度（char 更快）和节省磁盘空间角度（varchar 更小），具体情况还需具体来设计数据库才是妥当的做法。 varchar(50)中50的涵义 最多存放 50 个字符，varchar(50)和 (200) 存储 hello 所占空间一样，但后者在排序时会消耗更多内存，因为 order by col 采用 fixed_length 计算 col 长度(memory 引擎也一样)。在早期 MySQL 版本中， 50 代表字节数，现在代表字符数。 int(20)中20的涵义 是指显示字符的长度。20 表示最大显示宽度为 20，但仍占 4 字节存储，存储范围不变； 不影响内部存储，只是影响带 zerofill 定义的 int 时，前面补多少个 0，易于报表展示 mysql为什么这么设计 对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1) 和 int(20) 存储和计算均一样； mysql中int(10)和char(10)以及varchar(10)的区别 int(10) 的 10 表示显示的数据的长度，不是存储数据的大小；chart(10) 和 varchar(10) 的 10 表示存储数据的大小，即表示存储多少个字符。 int(10) 10 位的数据长度 9999999999，占 32 个字节，int 型 4 位 char(10) 10 位固定字符串，不足补空格 最多 10 个字符 varchar(10) 10 位可变字符串，不足补空格 最多 10 个字符 char(10) 表示存储定长的 10 个字符，不足 10 个就用空格补齐，占用更多的存储空间 varchar(10) 表示存储 10 个变长的字符，存储多少个就是多少个，空格也按一个字符存储，这一点是和 char(10) 的空格不同的，char(10) 的空格表示占位不算一个字符 FLOAT和DOUBLE的区别是什么？ FLOAT 类型数据可以存储至多 8 位十进制数，并在内存中占 4 字节。 DOUBLE 类型数据可以存储至多 18 位十进制数，并在内存中占 8 字节。 drop、delete与truncate的区别 ​ Delete Truncate Drop 类型 属于 DML 属于 DDL 属于 DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据行 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行，索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 因此，在不再需要一张表的时候，用 drop；在想删除部分数据行时候，用 delete；在保留表而删除所有数据的时候用 truncate。 UNION与UNION ALL的区别？ 如果使用 UNION ALL，不会合并重复的记录行 效率 UNION 高于 UNION ALL 数据类型 1bit 位 1字节=8bit 1k=1024字节 1兆=1024k 1G=1023M 1T=1024G 引擎 MySQL存储引擎MyISAM与InnoDB区别 存储引擎 Storage engine：MySQL 中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。 MyISAM索引与InnoDB索引的区别？ Innodb 引擎：Innodb 引擎提供了对数据库 ACID 事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。 MyIASM 引擎 (原本 Mysql 的默认引擎)：不提供事务的支持，也不支持行级锁和外键。 MEMORY 引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。 InnoDB 索引是聚簇索引，MyISAM 索引是非聚簇索引。 InnoDB 的主键索引的叶子节点存储着行数据，因此主键索引非常高效。 MyISAM 索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。 InnoDB 非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。 InnoDB引擎的4大特性 插入缓冲（insert buffer) 二次写 (double write) 自适应哈希索引 (ahi) 预读 (read ahead) 存储引擎选择 如果没有特别的需求，使用默认的Innodb即可。 MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。 Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如 OA 自动化办公系统。 索引 什么是索引？ 索引是一种特殊的文件 (InnoDB 数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用 B 树及其变种 B + 树。 更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。 索引有哪些优缺点？ 优 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 缺 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增 / 改 / 删的执行效率； 空间方面：索引需要占物理空间 索引使用场景（重点） where 可以尝试在一个字段未建立索引时，根据该字段查询的效率，然后对该字段建立索引（alter table 表名 add index(字段名)），同样的 SQL 执行的效率，你会发现查询效率会有明显的提升（数据量越大越明显）。 order by 使用order by将查询结果按照某个字段排序时，如果该字段没有建立索引，那么执行计划会将查询出的所有数据使用外部排序（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），这操作很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。 但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可。而且如果分页的，那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的） join 对join语句匹配关系（on）涉及的字段建立索引能够提高效率 索引覆盖 如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。 这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。 索引有哪几种类型？ 1）从存储结构上来划分 · Btree 索引（B+tree，B-tree) · 哈希索引 · full-index 全文索引 · RTree 2）从应用层次上来划分 · 普通索引: 基本的索引类型，没有唯一性的限制，允许为 NULL 值。 CREATE INDEX indexName ON table_name (column_name)或 ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引 • ALTER TABLE testalter_tbl ADD/DROP INDEX (c); · 唯一索引: 数据列不允许重复，允许为 NULL 值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 · 主键索引: 数据列不允许重复，不允许为 NULL，一个表只能有一个主键。 ALTER TABLE table_name ADD PRIMARY KEY ( column ) 3）从表记录的排列顺序和索引的排列顺序是否一致来划分 · **聚集索引：**表记录的排列顺序和索引的排列顺序一致。 • 就是以主键创建的索引。 · **非聚集索引：**表记录的排列顺序和索引的排列顺序不一致。 • 以非主键创建的索引（也叫做二级索引）。 索引的数据结构（b树，hash） 我们经常使用的 InnoDB 存储引擎的默认索引实现为：B + 树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择 BTree 索引。 B 树索引 · mysql 通过存储引擎取数据，基本上 90% 的人用的就是 InnoDB 了，按照实现方式分，InnoDB 的索引类型目前只有两种：BTREE（B 树）索引和 HASH 索引。B 树索引是 Mysql 数据库中使用最频繁的索引类型，基本所有存储引擎都支持 BTree 索引。通常我们说的索引不出意外指的就是（B 树）索引（实际是用 B + 树实现的，因为在查看表索引时，mysql 一律打印 BTREE，所以简称为 B 树索引） • 查询方式： 主键索引区: PI(关联保存的时数据的地址) 按主键查询, 普通索引区: si(关联的 id 的地址, 然后再到达上面的地址)。所以按主键查询, 速度最快 B+tree 性质： 1.）n 棵子 tree 的节点包含 n 个关键字，不用来保存数据而是保存数据的索引。 2.）所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。 3.）所有的非终端结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。 4.）B+ 树中，数据对象的插入和删除仅在叶节点上进行。 5.）B + 树有 2 个头指针，一个是树的根节点，一个是最小关键码的叶节点。 哈希索引 ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png) 简要说下，类似于数据结构中简单实现的 HASH 表（散列表）一样，当我们在 mysql 中用哈希索引时，主要就是通过 Hash 算法（常见的 Hash 算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的 Hash 值，与这条数据的行指针一并存入 Hash 表的对应位置；如果发生 Hash 碰撞（两个不同关键字的 Hash 值相同），则在对应 Hash 键下以链表形式存储。当然这只是简略模拟图。 索引的基本原理 索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理很简单，就是把无序的数据变成有序的查询 把创建了索引的列的内容进行排序 对排序结果生成倒排表 在倒排表内容上拼上数据地址链 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据 索引算法有哪些？ BTree 算法 BTree 是最常用的 mysql 数据库索引算法，也是 mysql 默认的算法。因为它不仅可以被用在 =,&gt;,&gt;=,&lt;,&lt;= 和 between 这些比较操作符上，而且还可以用于 like 操作符，只要它的查询条件是一个不以通配符开头的常量， 例如： -- 只要它的查询条件是一个不以通配符开头的常量 select * from user where name like 'jack%'; -- 如果一通配符开头，或者没有使用常量，则不会使用索引，例如： select * from user where name like '%jack'; Hash 算法 Hash Hash 索引只能用于对等比较，例如 =,&lt;=&gt;（相当于 =）操作符。由于是一次定位数据，不像 BTree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次 IO 访问，所以检索效率远高于 BTree 索引。 索引设计的原则？ 适合索引的列是出现在 where 子句中的列，或者连接子句中指定的列 基数较小的类，索引效果较差，没有必要在此列建立索引 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。 创建索引的原则（重中之重） 索引虽好，但也不是无限制的使用，最好符合一下几个原则 1） 最左前缀匹配原则，组合索引非常重要的原则，mysql 会一直向右匹配直到遇到范围查询 (&gt;、&lt;、between、like) 就停止匹配，比如 a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立 (a,b,c,d) 顺序的索引，d 是用不到索引的，如果建立 (a,b,d,c) 的索引则都可以用到，a,b,d 的顺序可以任意调整。 2）较频繁作为查询条件的字段才去创建索引 3）更新频繁字段不适合创建索引 4）若是不能有效区分数据的列不适合做索引列 (如性别，男女未知，最多也就三种，区分度实在太低) 5）尽量的扩展索引，不要新建索引。比如表中已经有 a 的索引，现在要加 (a,b) 的索引，那么只需要修改原来的索引即可。 6）定义有外键的数据列一定要建立索引。 7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 8）对于定义为 text、image 和 bit 的数据类型的列不要建立索引。 创建索引的三种方式，删除索引 第一种方式：在执行 CREATE TABLE 时创建索引 CREATE TABLE user_index2 ( id INT auto_increment PRIMARY KEY, first_name VARCHAR (16), last_name VARCHAR (16), id_card VARCHAR (18), information text, KEY name (first_name, last_name), FULLTEXT KEY (information), UNIQUE KEY (id_card) ); 第二种方式：使用 ALTER TABLE 命令去增加索引 ALTER TABLE table_name ADD INDEX index_name (column_list); ALTER TABLE 用来创建普通索引、UNIQUE 索引或 PRIMARY KEY 索引。 其中 table_name 是要增加索引的表名，column_list 指出对哪些列进行索引，多列时各列之间用逗号分隔。 索引名 index_name 可自己命名，缺省时，MySQL 将根据第一个索引列赋一个名称。另外，ALTER TABLE 允许在单个语句中更改多个表，因此可以在同时创建多个索引。 第三种方式：使用 CREATE INDEX 命令创建 CREATE INDEX index_name ON table_name (column_list); CREATE INDEX 可对表增加普通索引或 UNIQUE 索引。（但是，不能创建 PRIMARY KEY 索引） 删除索引 根据索引名删除普通索引、唯一索引、全文索引：alter table 表名 drop KEY 索引名 alter table user_index drop KEY name; alter table user_index drop KEY id_card; alter table user_index drop KEY information; 删除主键索引：alter table 表名 drop primary key（因为主键只有一个）。这里值得注意的是，如果主键自增长，那么不能直接执行此操作（自增长依赖于主键索引）： 需要取消自增长再行删除： alter table user_index -- 重新定义字段 MODIFY id int, drop PRIMARY KEY 但通常不会删除主键，因为设计主键一定与业务逻辑无关。 ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png) 创建索引时需要注意什么？ 非空字段：应该指定列为 NOT NULL，除非你想存储 NULL。在 mysql 中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用 0、一个特殊的值或者一个空串代替空值； 取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过 count() 函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高； 索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次 IO 操作获取的数据越大效率越高。 使用索引查询一定能提高查询的性能吗？为什么 通常，通过索引查询数据比全表扫描要快。但是我们也必须注意到它的代价。 索引需要空间来存储，也需要定期维护， 每当有记录在表中增减或索引列被修改时，索引本身也会被修改。 这意味着每条记录的 INSERT，DELETE，UPDATE 将为此多付出 4，5 次的磁盘 I/O。 因为索引需要额外的存储空间和处理，那些不必要的索引反而会使查询反应时间变慢。使用索引查询不一定能提高查询性能，索引范围查询 (INDEX RANGE SCAN) 适用于两种情况: 基于一个范围的检索，一般查询返回结果集小于表中记录数的 30% 基于非唯一性索引的检索 百万级别或以上的数据如何删除 关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件, 所以当我们对数据的增加, 修改, 删除, 都会产生额外的对索引文件的操作, 这些操作需要消耗额外的 IO, 会降低增 / 改 / 删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询 MySQL 官方手册得知删除数据的速度和创建的索引数量是成正比的。 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟） 然后删除其中无用数据（此过程需要不到两分钟） 删除完成后重新创建索引 (此时数据较少了) 创建索引也非常快，约十分钟左右。 与之前的直接删除绝对是要快速很多，更别说万一删除中断, 一切删除会回滚。那更是坑了。 前缀索引 语法：index(field(10))，使用字段值的前 10 个字符建立索引，默认是使用字段的全部内容建立索引。 前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。 实操的难度：在于前缀截取的长度。 我们可以利用select count(*)/count(distinct left(password,prefixLen));，通过从调整prefixLen的值（从 1 自增）查看不同前缀长度的一个平均匹配度，接近 1 时就可以了（表示一个密码的前prefixLen个字符几乎能确定唯一一条记录） 什么是最左前缀原则？什么是最左匹配原则 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where 子句中使用最频繁的一列放在最左边。 最左前缀匹配原则，非常重要的原则，mysql 会一直向右匹配直到遇到范围查询 (&gt;、&lt;、between、like) 就停止匹配，比如 a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立 (a,b,c,d) 顺序的索引，d 是用不到索引的，如果建立 (a,b,d,c) 的索引则都可以用到，a,b,d 的顺序可以任意调整。 = 和 in 可以乱序，比如 a = 1 and b = 2 and c = 3 建立 (a,b,c) 索引可以任意顺序，mysql 的查询优化器会帮你优化成索引可以识别的形式 B树和B+树的区别 在 B 树中，你可以将键和值存放在内部节点和叶子节点；但在 B + 树中，内部节点都是键，没有值，叶子节点同时存放键和值。 B + 树的叶子节点有一条链相连，而 B 树的叶子节点各自独立。 · ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png) 使用B树的好处 B 树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得 B 树在特定数据重复多次查询的场景中更加高效。 使用B+树的好处 由于 B + 树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。 B + 树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B + 树只需要使用 O(logN) 时间找到最小的一个节点，然后通过链进行 O(N) 的顺序遍历即可。而 B 树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间 Hash索引和B+树所有有什么区别或者说优劣呢? 首先要知道 Hash 索引和 B + 树索引的底层实现原理： hash 索引底层就是 hash 表，进行查找时，调用一次 hash 函数就可以获取到相应的键值，之后进行回表查询获得实际数据。B + 树底层实现是多路平衡查找树。对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。 那么可以看出他们有以下的不同： hash 索引进行等值查询更快 (一般情况下)，但是却无法进行范围查询。 因为在 hash 索引中经过 hash 函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。而 B + 树的的所有节点皆遵循 (左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。 hash 索引不支持使用索引进行排序，原理同上。 hash 索引不支持模糊查询以及多列索引的最左前缀匹配。原理也是因为 hash 函数的不可预测。AAAA 和 AAAAB 的索引没有相关性。 hash 索引任何时候都避免不了回表查询数据，而 B + 树在符合某些条件 (聚簇索引，覆盖索引等) 的时候可以只通过索引完成查询。 hash 索引虽然在等值查询上较快，但是不稳定。性能不可预测，当某个键值存在大量重复的时候，发生 hash 碰撞，此时效率可能极差。而 B + 树的查询效率比较稳定，对于所有的查询都是从根节点到叶子节点，且树的高度较低。 因此，在大多数情况下，直接选择 B + 树索引可以获得稳定且较好的查询速度。而不需要使用 hash 索引。 数据库为什么使用B+树而不是B树 B 树只适合随机检索，而 B + 树同时支持随机检索和顺序检索； B + 树空间利用率更高，可减少 I/O 次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘 I/O 消耗。B + 树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比 B 树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO 读写次数也就降低了。而 IO 读写次数是影响索引检索效率的最大因素； B + 树的查询效率更加稳定。B 树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在 B + 树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 B - 树在提高了磁盘 IO 性能的同时并没有解决元素遍历的效率低下的问题。B + 树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而 B 树不支持这样的操作。 增删文件（节点）时，效率更高。因为 B + 树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。 B+树在满足聚簇索引和覆盖索引的时候不需要回表查询数据， 在 B + 树的索引中，叶子节点可能存储了当前的 key 值，也可能存储了当前的 key 值以及整行的数据，这就是聚簇索引和非聚簇索引。 在 InnoDB 中，只有主键索引是聚簇索引，如果没有主键，则挑选一个唯一键建立聚簇索引。如果没有唯一键，则隐式的生成一个键来建立聚簇索引。 当查询使用聚簇索引时，在对应的叶子节点，可以获取到整行数据，因此不用再次进行回表查询。 什么是聚簇索引？何时使用聚簇索引与非聚簇索引 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam 通过 key_buffer 把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在 key buffer 命中时，速度慢的原因 澄清一个概念：innodb 中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值 何时使用聚簇索引与非聚簇索引 · ![desc](file:///C:/Users/ADMINI~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png) 非聚簇索引一定会回表查询吗？ 不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。 举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了 age 信息，不会再次进行回表查询。 联合索引是什么？为什么需要注意联合索引中的顺序？ MySQL 可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 具体原因为: MySQL 使用索引时需要索引有序，假设现在建立了 &quot;name，age，school&quot; 的联合索引，那么索引的排序为: 先按照 name 排序，如果 name 相同，则按照 age 排序，如果 age 的值也相等，则按照 school 进行排序。 当进行查询时，此时索引仅仅按照 name 严格有序，因此必须首先使用 name 字段进行等值查询，之后对于匹配到的列而言，其按照 age 字段严格有序，此时可以使用 age 字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 事务 什么是数据库事务？ 1、用 BEGIN, ROLLBACK, COMMIT来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 2、直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 事务处理可以用来维护数据库的完整性，保证成批的 SQL 语句要么全部执行，要么全部不执行。 事务用来管理 insert,update,delete 语句 事物的四大特性(ACID)介绍一下? **原子性：**一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 **一致性：**在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 **隔离性：**数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 **持久性：**事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 什么是脏读？幻读？不可重复读？ 什么是事务的隔离级别？MySQL的默认隔离级别是什么？ 锁 对MySQL的锁了解吗 隔离级别与锁的关系 按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法 从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了 MySQL中InnoDB引擎的行锁是怎么实现的？ InnoDB存储引擎的锁的算法有三种 什么是死锁？怎么解决？ 数据库的乐观锁和悲观锁是什么？怎么实现的？ 视图 1234567#创建视图create view 视图名称 as select语句;#--查看视图结构：desc user_view; show create view user_view;#--查看视图内容：select * from user_view; 为什么要使用视图？什么是视图？ 由基表通过SQL动态生成的虚拟表，本身不包含数据，基表和视图数据会互相影响。只有基表和视图字段一一对应才可以增删改查。 在Mysql中视图的类型分为： 1、MERGE 将视图的sql语句和引用视图的sql语句合并在一起，最后一起执行。 2、TEMPTABLE 将视图的结果集存放在临时表中，每次执行时从临时表中操作。 3、UNDEFINED 默认的视图类型，DBMS倾向于选择而不是必定选择MERGE，因为MERGE的效率更高，更重要的是临时表视图不能更新。 所以，这里推荐使用MERGE算法类型视图。 视图有哪些特点？ 1）简单：使用视图的用户完全不需要关心后面对应的表的结构、关联条件和筛选条件，对用户来说已经是过滤好的复合条件的结果集。 2）安全：使用视图的用户只能访问他们被允许查询的结果集，对表的权限管理并不能限制到某个行某个列，但是通过视图就可以简单的实现。 3）数据独立：一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响。 总而言之，使用视图的大部分情况是为了保障数据安全性，提高查询效率。 视图的使用场景有哪些？ 分表 启用计算列 如有订单数量和单价，一般数据库设计不会有总价字段，视图可有 导入导出表 使用视图导入导出数据 权限控制，提高安全性 创建视图控制显示字段，用户从视图查询只会显示控制的字段，可以把权限限定到行列级别 解耦合 创建视图后基表结构改变不影响视图，重构数据库不影响程序，向后兼容。 简化数据操作 将复杂的联合等查询创建为视图，可只查询视图来简化操作 视图的优点 视图的缺点 查询性能降低，修改视图会直接修改原数据，基表视图相互依赖。 什么是游标？ 游标的特性 不敏感：数据库可以选择不复制结果集 只读 不滚动：游标只能向一方向前进，并且不可以跳过任何一行数据 游标的优点 游标是针对行操作的，对从数据库中 select 查询得到的结果集的 每一行可以 进行分开的独立的相同或者不相同的操作，是一种分离的思想。 游标的缺点 性能不高 只能一行一行操作 使用游标会产生死锁，造成内存开销大 游标的适用场景 存储过程 函数 触发器 事件 游标的操作 1、游标的定义 DECLARE 光标名称 CURSOR FOR 查询语法 declare cursor_name cursor for select_statement 2、打开游标 OPEN 光标名称 open cursor_name 3、取游标中的数据 FETCH 光标名称 INFO var_name [，var_name ]..... fetch cursor_name info var_name 4、关闭游标 CLOSE curso_name; close 光标名称 5、释放游标 DEALLOCATE 光标名称 deallocate cursor_name; 存储过程与函数 什么是存储过程？有哪些优缺点？ 触发器 什么是触发器？触发器的使用场景有哪些？ 触发器是针对每一行的；对增删改非常频繁的表上切记不要使用触发器，因为它会非常消耗资源。 触发器是针对每一行的；对增删改非常频繁的表上切记不要使用触发器，因为它会非常消耗资源。 1234567891011121314151617181920212223242526CREATE [DEFINER = { user | CURRENT_USER }]TRIGGER trigger_nametrigger_time trigger_eventON tbl_name FOR EACH ROW [trigger_order]trigger_bodytrigger_time: { BEFORE | AFTER }trigger_event: { INSERT | UPDATE | DELETE }trigger_order: { FOLLOWS | PRECEDES } other_trigger_name&gt; BEFORE和AFTER参数指定了触发执行的时间，在事件之前或是之后。&gt; FOR EACH ROW表示任何一条记录上的操作满足触发事件都会触发该触发器，也就是说触发器的触发频率是针对每一行数据触发一次。&gt; tigger_event详解： ①INSERT型触发器：插入某一行时激活触发器，可能通过INSERT、LOAD DATA、REPLACE 语句触发(LOAD DAT语句用于将一个文件装入到一个数据表中，相当与一系列的INSERT操作)； ②UPDATE型触发器：更改某一行时激活触发器，可能通过UPDATE语句触发； ③DELETE型触发器：删除某一行时激活触发器，可能通过DELETE、REPLACE语句触发。&gt; trigger_order是MySQL5.7之后的一个功能，用于定义多个触发器，使用follows(尾随)或precedes(在…之先)来选择触发器执行的先后顺序。 NEW与OLD详解 MySQL 中定义了 NEW 和 OLD，用来表示触发器的所在表中，触发了触发器的那一行数据，来引用触发器中发生变化的记录内容，具体地： ①在INSERT型触发器中，NEW用来表示将要（BEFORE）或已经（AFTER）插入的新数据； ②在UPDATE型触发器中，OLD用来表示将要或已经被修改的原数据，NEW用来表示将要或已经修改为的新数据； ③在DELETE型触发器中，OLD用来表示将要或已经被删除的原数据； 查看触发器 1select * from information_schema.triggers MySQL中都有哪些触发器？ SQL优化 如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？ SQL的生命周期？ 大表数据查询，怎么优化 超大分页怎么处理？ mysql 分页 慢查询日志 关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？ 为什么要尽量设定一个主键？ 主键使用自增ID还是UUID？ 字段为什么要求定义为not null？ 如果要存储用户的密码散列，应该使用什么字段进行存储？ 优化查询过程中的数据访问 优化长难的查询语句 优化特定类型的查询语句 优化关联查询 优化子查询 优化LIMIT分页 优化UNION查询 优化WHERE子句 数据库优化 为什么要优化 数据库结构优化 MySQL数据库cpu飙升到500%的话怎么处理？ 大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？ 垂直分表 适用场景 缺点 水平分表： 适用场景 水平切分的缺点 MySQL的复制原理以及流程 读写分离有哪些解决方案？ 备份计划，mysqldump以及xtranbackup的实现原理 数据表损坏的修复方式有哪些？ Linux下RPM版MYSQL安装、启停 MySQL启动问题、配置文件、编码问题 MYSQL分层、存储引擎 连接层（提供与客户端连接的服务） 服务层（提供各种用户使用的接口，提供SQL优化器【MySQL Query Optimizer】）） 引擎层（提供各种存储数据的方式【InnoDB：事物优先，适合高并发，行锁；MyISAM：性能优先，表锁】） show engines; 显示支持引擎 +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ show variables like ‘%storage_engine%’; 查看当前使用引擎 12345678-- 指定数据库引擎create table tb(​ id int(4) auto_increment,​ name varchar(5),​ dept varchar(5),​ primary key(id))ENGINE=MyISAM AUTO_INCREMENT=1DEFAULT CHARSET=utf-8 存储层（存储数据） SQL解析过程、索引、B树 sql优化： 原因：性能低，执行或等待时间太长，SQL语句欠佳（连接查询）、索引失效、服务器参数设置不对 SQL编写过程与解析过程： 编写:select dinstinct ..from …join ..on ..where ..group by ..having ..order by ..limit 解析:form ..on ..join ..where ..group vy ..having ..select dinstinct ..order by limit.. 索引 简介 sql优化主要是优化索引，索引（index）相当于书的目录。索引是数据结构（默认B树【小左大右】） 优点 提高查询效率（降低IO、CPU使用率） 缺点 索引本身很大（可以存放在内存/硬盘） 索引不是所有情况都适用：少量数据，频繁更新字段，很少使用的字段 索引会降低增删改查效率 分类 单值索引：单字段 唯一索引：不能重复 主键索引：不能重复，不能为null，主键默认是主键索引 复合索引：相当于二级目录（name，age） 创建索引 方式一 create 索引类型 索引名 on 表（字段） 单值索引：create index dept_index on tb(dept); 唯一索引：create unique index name-index on tb(name) 复合索引：create index dept_name_index on tb(dept,name); 方式二 alter table 表名 add 索引类型 索引名（字段） 单值索引：alter table tb add index dept_index(dept); 唯一索引：alter table tb add unique index name-index(name) 复合索引：alter table tb add index dept_name_index(dept,name); 删除索引 drop index 索引名 on 表名 查询索引 show index from 表名\\G SQL优化准备 分析SQL的执行计划：explain+sql语句，模拟sql优化器执行sql语句，从而让开发人员知道自己SQL语句情况 id select_type table partitions type possible_keys key key_len ref rows filtered Extra 1 SIMPLE user const PRIMARY PRIMARY 152 const 1 100.00 id 编号 子查询id不同，关联查询相同 **select_type ** 查询类型** **table ** 查询表 id相同数据量越少越优先查询，id不通id越大越优先 partitions **type ** 类型 **possible_keys ** 预测用到的索引 **key ** 实际用到的索引 **key_len ** 实际使用索引的长度 **ref ** 表之间的引用 **rows ** 通过索引查询到的数据量 filtered **Extra ** 额外的信息 id、table 先查内层，再查外层。子查询id不同，关联查询id相同。 id相同数据量越少越优先查询，id不通id越大越优先 explain SELECT * from teacher where tid=(select tid from course where cid='2') or tcid=(select tcid from teachercard where tcid='3') explain select t.* from teacher t,course c,teacherCard tc where t.tid=c.tid and t.tcid=tc.tcid and (c.cid='2' or tc.tcid='3') select_type（查询类型） PRIMARY：包含子查询SQL中的 主查询（最外层） SUBQUERY：包含子查询SQL中的子查询（非最外层） SIMPLE：简单查询（不包含子查询、union） union： derived：衍生查询（使用到了临时表） 在form子查询中只有一张表 explain select cr.cname from (select * from course where tid in (1,2)) cr; form子查询中，如果有table1 union table2，则table1 就是的riverd explain select cr.cname from (select * from course where tid =1 union select * from course where tid =2) cr; type（索引类型） system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all 对TYPE优化前提：有索引 system，const知识理想情况，实际能达到 ref &gt;range system（忽略）：只有一条数据的系统表；或衍生表只有一条数据的主查询 const：仅仅能查到一条数据的SQL，用于Primayr key 或unique索引（类型与索引类型有关） eq_ref：唯一性索引：对于每个索引建的查询，返回匹配唯一行数据（有且只有1个，不能多、不能0） select ·· from ··where name=···常见唯一索引和主键索引 ref：非唯一性索引，对于每个索引建的查询，返回匹配的所有行（0，多） range：检索指定范围的行，where后面是一个范围查询（between，in，&gt; &lt; &gt;=，in有时候会失效，从而转为all） index：查询全部索引中的数据，只需要扫描索引表，不需要所有表中的所有数据 all：查询全部表中的数据，需要全表所有，即需要所有表中的所有数据 possible_keys 可能用到的索引，是一种预测，不准 key 实际用到的索引 key_len： 索引长度，用于判断复合索引是否完全使用，如果索引字段可为null，则会使用1个字节用于标识，索引字段为可变长度，则会使用2个字节 ref 注意与type值中的ref值区分 作用：指明当前表所参照的字段 select 。。。 where a.c=b.x （） rows 被索引优化查询的数据个数（实际通过索引而查询到的数据个数） extra using filesort：性能消耗大,需要额外的一次排序（查询） 单索引：如果排序和查找是同一个字段，则不会出现using filesort ；如果排序和查找不是同一个字段，则会出现。 避免：where哪些字段就order by哪些字段 复合索引：不能跨列（最佳左前缀） 避免：where和order by 按照复合索引的顺序适应，不要跨列或无序使用 using temporary：性能损耗较大，用到了临时表。一般出现在 group by 语句中； explain select a1 from test02 where a1 in(‘1’,’2’,’3’) group by a1; explain select a1 from test02 where a1 in(‘1’,’2’,’3’) group by a2; —–using tempporary 避免：查询哪些列，就根据哪些列group by using temporary：性能提升，索引覆盖，原因：不读取源文件，只从索引中获取数据，使用到的列全部都在索引中 using where：需要回原表查询 impossible where：where子句永远为false 优化示例 create table test03 (a1 int(4) not null, a2 int(4) not null, a3 int(4) not null, a4 int(4) not null ) alter table test03 add index idx_a1_a2_a3_a4(a1,a2,a3,a4); explain select a1,a2,a3,a4 from test03 where a1=1 and a2=2 and a3=3 and a4=4;—-推荐写法 explain select a1,a2,a3,a4 from test03 where a4=1 and a2=2 and a1=3 and a4=4;—-虽然编写后的顺序和索引顺序不一致，但是sql执行前经过了sql优化器的调整，结果与上条sql是一致的。 —–以上两个sql使用了全部的索引 explain select a1,a2,a3,a4 from test03 where a4=1 and a2=2 and a4=4 order by and a1; —-以上SQL用到了a1 a2两个索引，该两个字段 不需要回表查询using index ；a4跨列使用造成了该索引失效，所以需要回表查询，因此是using where ，以上可以通过key_len验证 explain select a1,a2,a3,a4 from test03 where a4=1 and a4=4 order by and a3; —-以上SQL出现了using filesort（文件内排序，”多了一次额外的查找/排序“）不要跨列使用（where 和order by拼接起来使用） explain select a1,a2,a3,a4 from test03 where a4=1 and a4=4 order by and a2,a3; ===这样便不会出现 using filesort，如果复合索引和使用顺序全部一致，则复合索引全部使用，部分一致则使用部分索引。 单表优化及总结 （1）单表优化 create table book ( bid int(4) primary key, name varchar(20) not null, authorid int(4) not null, publicid int(4) not null, typeid int(4) not null ); insert into book values(1,'tjava',1,1,2) ; insert into book values(2,'tc',2,1,2) ; insert into book values(3,'wx',3,2,1) ; insert into book values(4,'math',4,2,3) ; commit; 查询authorid=1且 typeid为2或3的 bid explain select bid from book where typeid in(2,3) and authorid=1 order by typeid desc ; (a,b,c) (a,b) 优化：加索引 alter table book add index idx_bta (bid,typeid,authorid); 索引一旦进行 升级优化，需要将之前废弃的索引删掉，防止干扰。 drop index idx_bta on book; 根据SQL实际解析的顺序，调整索引的顺序： alter table book add index idx_tab (typeid,authorid,bid); --虽然可以回表查询bid，但是将bid放到索引中 可以提升使用using index ; 再次优化（之前是index级别）：思路。因为范围查询in有时会实现，因此交换 索引的顺序，将typeid in(2,3) 放到最后。 drop index idx_tab on book; alter table book add index idx_atb (authorid,typeid,bid); explain select bid from book where authorid=1 and typeid in(2,3) order by typeid desc ; ​ ​ --小结： a.最佳做前缀，保持索引的定义和使用的顺序一致性 b.索引需要逐步优化 c.将含In的范围查询 放到where条件的最后，防止失效。 ​ ​ 本例中同时出现了Using where（需要回原表）; Using index（不需要回原表）：原因，where authorid=1 and typeid in(2,3)中authorid在索引(authorid,typeid,bid)中，因此不需要回原表（直接在索引表中能查到）；而typeid虽然也在索引(authorid,typeid,bid)中，但是含in的范围查询已经使该typeid索引失效，因此相当于没有typeid这个索引，所以需要回原表（using where）； ​ 例如以下没有了In，则不会出现using where ​ explain select bid from book where authorid=1 and typeid =3 order by typeid desc ; ​ ​ 还可以通过key_len证明In可以使索引失效。 ​ （2）两表优化 create table teacher2 ( tid int(4) primary key, cid int(4) not null ); insert into teacher2 values(1,2); insert into teacher2 values(2,1); insert into teacher2 values(3,3); create table course2 ( cid int(4) , cname varchar(20) ); insert into course2 values(1,'java'); insert into course2 values(2,'python'); insert into course2 values(3,'kotlin'); commit; 左连接： explain select *from teacher2 t left outer join course2 c on t.cid=c.cid where c.cname='java'; 索引往哪张表加？ -小表驱动大表 -索引建立经常使用的字段上 （本题 t.cid=c.cid可知，t.cid字段使用频繁，因此给该字段加索引） [一般情况对于左外连接，给左表加索引；右外连接，给右表加索引] 小表：10 大表：300 where 小表.x 10 = 大表.y 300; --循环了几次？10 大表.y 300=小表.x 10 --循环了300次 小表:10 大表:300 select ...where 小表.x10=大表.x300 ; for(int i=0;i&lt;小表.length10;i++) { for(int j=0;j&lt;大表.length300;j++) { ... } } select ...where 大表.x300=小表.x10 ; for(int i=0;i&lt;大表.length300;i++) { for(int j=0;j&lt;小表.length10;j++) { ... } } --以上2个FOR循环，最终都会循环3000次；但是 对于双层循环来说：一般建议 将数据小的循环 放外层；数据大的循环放内存。 --当编写 ..on t.cid=c.cid 时，将数据量小的表 放左边（假设此时t表数据量小） alter table teacher2 add index index_teacher2_cid(cid) ; alter table course2 add index index_course2_cname(cname); Using join buffer:extra中的一个选项，作用：Mysql引擎使用了 连接缓存。 （3）三张表优化 ​ a.小表驱动大表 b.索引建立在经常查询的字段上 示例： create table test03 ( a1 int(4) not null, a2 int(4) not null, a3 int(4) not null, a4 int(4) not null ); alter table test03 add index idx_a1_a2_a3_4(a1,a2,a3,a4) ; 12345678910explain select a1,a2,a3,a4 from test03 where a1=1 and a2=2 and a3=3 and a4 =4 ; --推荐写法，因为 索引的使用顺序（where后面的顺序） 和 复合索引的顺序一致explain select a1,a2,a3,a4 from test03 where a4=1 and a3=2 and a2=3 and a1 =4 ; --虽然编写的顺序 和索引顺序不一致，但是 sql在真正执行前 经过了SQL优化器的调整，结果与上条SQL是一致的。--以上 2个SQL，使用了 全部的复合索引explain select a1,a2,a3,a4 from test03 where a1=1 and a2=2 and a4=4 order by a3; --以上SQL用到了a1 a2两个索引，该两个字段 不需要回表查询using index ;而a4因为跨列使用，造成了该索引失效，需要回表查询 因此是using where；以上可以通过 key_len进行验证explain select a1,a2,a3,a4 from test03 where a1=1 and a4=4 order by a3; --以上SQL出现了 using filesort(文件内排序，“多了一次额外的查找/排序”) ：不要跨列使用( where和order by 拼起来，不要跨列使用) 1explain select a1,a2,a3,a4 from test03 where a1=1 and a4=4 order by a2 , a3; --不会using filesort 123--总结：i.如果 (a,b,c,d)复合索引 和使用的顺序全部一致(且不跨列使用)，则复合索引全部使用。如果部分一致(且不跨列使用)，则使用部分索引。select a,c where a = and b= and d= ii.where和order by 拼起来，不要跨列使用 1using temporary:需要额外再多使用一张表. 一般出现在group by语句中；已经有表了，但不适用，必须再来一张表。 解析过程： from .. on.. join ..where ..group by ....having ...select dinstinct ..order by limit ... a. explain select * from test03 where a2=2 and a4=4 group by a2,a4 ;--没有using temporary b. explain select * from test03 where a2=2 and a4=4 group by a3 ; 避免索引失效的一些原则 12345678910111213141516171819202122232425（1）复合索引 a.复合索引，不要跨列或无序使用（最佳左前缀） (a,b,c) b.复合索引，尽量使用全索引匹配 (a,b,c) （2）不要在索引上进行任何操作（计算、函数、类型转换），否则索引失效 select ..where A.x = .. ; --假设A.x是索引 不要：select ..where A.x*3 = .. ; explain select * from book where authorid = 1 and typeid = 2 ;--用到了at2个索引 explain select * from book where authorid = 1 and typeid*2 = 2 ;--用到了a1个索引 explain select * from book where authorid*2 = 1 and typeid*2 = 2 ;----用到了0个索引 explain select * from book where authorid*2 = 1 and typeid = 2 ;----用到了0个索引,原因：对于复合索引，如果左边失效，右侧全部失效。(a,b,c)，例如如果 b失效，则b c同时失效。 drop index idx_atb on book ; alter table book add index idx_authroid (authorid) ; alter table book add index idx_typeid (typeid) ; explain select * from book where authorid*2 = 1 and typeid = 2 ;（3）复合索引不能使用不等于（!= &lt;&gt;）或is null (is not null)，否则自身以及右侧所有全部失效。 复合索引中如果有&gt;，则自身和右侧索引全部失效。explain select * from book where authorid = 1 and typeid =2 ;-- SQL优化，是一种概率层面的优化。至于是否实际使用了我们的优化，需要通过explain进行推测。explain select * from book where authorid != 1 and typeid =2 ;explain select * from book where authorid != 1 and typeid !=2 ; 1234567891011121314151617181920212223242526体验概率情况(&lt; &gt; =)：原因是服务层中有SQL优化器，可能会影响我们的优化。drop index idx_typeid on book;drop index idx_authroid on book;alter table book add index idx_book_at (authorid,typeid);explain select * from book where authorid = 1 and typeid =2 ;--复合索引at全部使用explain select * from book where authorid &gt; 1 and typeid =2 ; --复合索引中如果有&gt;，则自身和右侧索引全部失效。explain select * from book where authorid = 1 and typeid &gt;2 ;--复合索引at全部使用----明显的概率问题---explain select * from book where authorid &lt; 1 and typeid =2 ;--复合索引at只用到了1个索引explain select * from book where authorid &lt; 4 and typeid =2 ;--复合索引全部失效--我们学习索引优化 ，是一个大部分情况适用的结论，但由于SQL优化器等原因 该结论不是100%正确。--一般而言， 范围查询（&gt; &lt; in），之后的索引失效。（4）补救。尽量使用索引覆盖（using index） （a,b,c）select a,b,c from xx..where a= .. and b =.. ;(5) like尽量以“常量”开头，不要以'%'开头，否则索引失效select * from xx where name like '%x%' ; --name索引失效explain select * from teacher where tname like '%x%'; --tname索引失效explain select * from teacher where tname like 'x%'; explain select tname from teacher where tname like '%x%'; --如果必须使用like '%x%'进行模糊查询，可以使用索引覆盖 挽救一部分。 123456（6）尽量不要使用类型转换（显示、隐式），否则索引失效explain select * from teacher where tname = 'abc' ;explain select * from teacher where tname = 123 ;//程序底层将 123 -&gt; '123'，即进行了类型转换，因此索引失效（7）尽量不要使用or，否则索引失效explain select * from teacher where tname ='' or tcid &gt;1 ; --将or左侧的tname 失效。 常见优化方法及慢查询SQL排查 exist和in select 。。from table where exist/in(子查询)； 如果主查询的数据集大，则使用In效率更高 如果子查询的数据集大，则使用exist效率更高 exist语法：将主查询的结果，放到子查询结果中进行条件校验（看子查询是否有数据，如果有数据，则校验成功，保留数据） order by using filesort 有两种算法：双路排序、单路排序 （根据IO的次数） MySQL4.1之前 默认使用 双路排序；双路：扫描2次磁盘（1：从磁盘读取排序字段 ,对排序字段进行排序（在buffer中进行的排序） 2：扫描其他字段 ） --IO较消耗性能 MySQL4.1之后 默认使用 单路排序 ： 只读取一次（全部字段），在buffer中进行排序。但种单路排序 会有一定的隐患 （不一定真的是“单路|1次IO”，有可能多次IO）。原因：如果数据量特别大，则无法 将所有字段的数据 一次性读取完毕，因此 会进行“分片读取、多次读取”。 注意：单路排序 比双路排序 会占用更多的buffer。 单路排序在使用时，如果数据大，可以考虑调大buffer的容量大小： set max_length_for_sort_data = 1024 单位byte 如果max_length_for_sort_data值太低，则mysql会自动从 单路-&gt;双路 （太低：需要排序的列的总大小超过了max_length_for_sort_data定义的字节数） 提高order by查询的策略： a.选择使用单路、双路 ；调整buffer的容量大小； b.避免select * ... （*会多花一些时间，且无法索引覆盖） c.复合索引 不要跨列使用 ，避免using filesort d.保证全部的排序字段 排序的一致性（都是升序 或 降序） 慢查询阀值和mysqldumpslow工具 :MySQL提供的一种日志记录，用于记录MySQL种响应时间超过阀值的SQL语句 （long_query_time，默认10秒） 慢查询日志默认是关闭的；建议：开发调优是 打开，而 最终部署时关闭。 检查是否开启了 慢查询日志 ： show variables like '%slow_query_log%' ; 临时开启： set global slow_query_log = 1 ; --在内存种开启 exit service mysql restart 永久开启： /etc/my.cnf 中追加配置： vi /etc/my.cnf [mysqld] slow_query_log=1 slow_query_log_file=/var/lib/mysql/localhost-slow.log ​ ​ 慢查询阀值： ​ show variables like '%long_query_time%' ; ​ ​ 临时设置阀值： ​ set global long_query_time = 5 ; --设置完毕后，重新登陆后起效 （不需要重启服务） ​ ​ 永久设置阀值： ​ ​ /etc/my.cnf 中追加配置： ​ vi /etc/my.cnf ​ [mysqld] ​ long_query_time=3 select sleep(4); select sleep(5); select sleep(3); select sleep(3); --查询超过阀值的SQL数量： show global status like '%slow_queries%' ; (1)慢查询的sql被记录在了日志中，因此可以通过日志 查看具体的慢SQL。 cat /var/lib/mysql/localhost-slow.log (2)通过mysqldumpslow工具查看慢SQL,可以通过一些过滤条件 快速查找出需要定位的慢SQL mysqldumpslow --help s：排序方式 r:逆序 l:锁定时间 g:正则匹配模式 --获取返回记录最多的3个SQL mysqldumpslow -s r -t 3 /var/lib/mysql/localhost-slow.log --获取访问次数最多的3个SQL mysqldumpslow -s c -t 3 /var/lib/mysql/localhost-slow.log --按照时间排序，前10条包含left join查询语句的SQL mysqldumpslow -s t -t 10 -g &quot;left join&quot; /var/lib/mysql/localhost-slow.log 语法： mysqldumpslow 各种参数 慢查询日志的文件 模拟并通过profiles分析海量数据 a.模拟海量数据 a.模拟海量数据 存储过程（无return）/存储函数（有return） create database testdata ; use testdata 1234567891011121314151617create table dept(dno int(5) primary key default 0,dname varchar(20) not null default '',loc varchar(30) default '')engine=innodb default charset=utf8;create table emp(eid int(5) primary key,ename varchar(20) not null default '',job varchar(20) not null default '',deptno int(5) not null default 0)engine=innodb default charset=utf8; 通过存储函数 插入海量数据： 创建存储函数： randstring(6) -&gt;aXiayx 用于模拟员工名称 1234567891011121314delimiter $ #声明分号不是结束符create function randstring(n int) returns varchar(255) begin declare all_str varchar(100) default 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' ; declare return_str varchar(255) default '' ; declare i int default 0 ; while i&lt;n do set return_str = concat( return_str, substring(all_str, FLOOR(1+rand()*52) ,1) ); set i=i+1 ; end while ; return return_str; end $ --如果报错：You have an error in your SQL syntax，说明SQL语句语法有错，需要修改SQL语句； 如果报错This function has none of DETERMINISTIC, NO SQL, or READS SQL DATA in its declaration and binary logging is enabled (you might want to use the less safe log_bin_trust_function_creators variable) 是因为 存储过程/存储函数在创建时 与之前的 开启慢查询日志冲突了 解决冲突： 临时解决( 开启log_bin_trust_function_creators ) show variables like '%log_bin_trust_function_creators%'; set global log_bin_trust_function_creators = 1; 永久解决： /etc/my.cnf [mysqld] log_bin_trust_function_creators = 1 12345678--产生随机整数create function ran_num() returns int(5)begin declare i int default 0; set i =floor( rand()*100 ) ; return i ;end $ 1234567891011121314--通过存储过程插入海量数据：emp表中 ， 10000, 100000create procedure insert_emp( in eid_start int(10),in data_times int(10))begin declare i int default 0; set autocommit = 0 ; repeat insert into emp values(eid_start + i, randstring(5) ,'other' ,ran_num()) ; set i=i+1 ; until i=data_times end repeat ; commit ;end $ 12345678910111213--通过存储过程插入海量数据：dept表中 create procedure insert_dept(in dno_start int(10) ,in data_times int(10)) begin declare i int default 0; set autocommit = 0 ; repeat insert into dept values(dno_start+i ,randstring(6),randstring(8)) ; set i=i+1 ; until i=data_times end repeat ; commit ; end$ 1234--插入数据 delimiter ; call insert_emp(1000,800000) ; call insert_dept(10,30) ; ​ b.分析海量数据: ​ （1）profiles ​ show profiles ; --默认关闭 ​ show variables like '%profiling%'; ​ set profiling = on ; ​ show profiles ：会记录所有profiling打开之后的 全部SQL查询语句所花费的时间。缺点：不够精确，只能看到 总共消费的时间，不能看到各个硬件消费的时间（cpu io ） ​ ​ (2)精确分析:sql诊断 ​ show profile all for query 上一步查询的的Query_Id ​ show profile cpu,block io for query 上一步查询的的Query_Id ​ ​ (3)全局查询日志 ：记录开启之后的 全部SQL语句。 （这次全局的记录操作 仅仅在调优、开发过程中打开即可，在最终的部署实施时 一定关闭） ​ show variables like '%general_log%'; ​ 12345678--执行的所有SQL记录在表中set global general_log = 1 ;--开启全局日志set global log_output='table' ; --设置 将全部的SQL 记录在表中--执行的所有SQL记录在文件中set global log_output='file' ;set global general_log = on ;set global general_log_file='/tmp/general.log' ; 12开启后，会记录所有SQL ： 会被记录 mysql.general_log表中。 select * from mysql.general_log ; 锁机制详解 锁机制 ：解决因资源共享 而造成的并发问题。 示例：买最后一件衣服X A: X 买 ： X加锁 -&gt;试衣服...下单..付款..打包 -&gt;X解锁 B: X 买：发现X已被加锁，等待X解锁， X已售空 分类： 操作类型： a.读锁（共享锁）： 对同一个数据（衣服），多个读操作可以同时进行，互不干扰。 b.写锁（互斥锁）： 如果当前写操作没有完毕（买衣服的一系列操作），则无法进行其他的读操作、写操作 操作范围： a.表锁 ：一次性对一张表整体加锁。如MyISAM存储引擎使用表锁，开销小、加锁快；无死锁；但锁的范围大，容易发生锁冲突、并发度低。 b.行锁 ：一次性对一条数据加锁。如InnoDB存储引擎使用行锁，开销大，加锁慢；容易出现死锁；锁的范围较小，不易发生锁冲突，并发度高（很小概率 发生高并发问题：脏读、幻读、不可重复度、丢失更新等问题）。 c.页锁 示例： （1）表锁 ： --自增操作 MYSQL/SQLSERVER 支持；oracle需要借助于序列来实现自增 create table tablelock ( id int primary key auto_increment , name varchar(20) )engine myisam; insert into tablelock(name) values('a1'); insert into tablelock(name) values('a2'); insert into tablelock(name) values('a3'); insert into tablelock(name) values('a4'); insert into tablelock(name) values('a5'); commit; 1234567891011121314151617181920212223增加锁：locak table 表1 read/write ,表2 read/write ,...查看加锁的表：show open tables ;会话：session :每一个访问数据的dos命令行、数据库客户端工具 都是一个会话===加读锁： 会话0： lock table tablelock read ; select * from tablelock; --读（查），可以 delete from tablelock where id =1 ; --写（增删改），不可以 select * from emp ; --读，不可以 delete from emp where eid = 1; --写，不可以 结论1： --如果某一个会话 对A表加了read锁，则 该会话 可以对A表进行读操作、不能进行写操作； 且 该会话不能对其他表进行读、写操作。 --即如果给A表加了读锁，则当前会话只能对A表进行读操作。 会话1（其他会话）： select * from tablelock; --读（查），可以 delete from tablelock where id =1 ; --写，会“等待”会话0将锁释放 会话1（其他会话）： select * from emp ; --读（查），可以 delete from emp where eno = 1; --写，可以 结论2： --总结： 会话0给A表加了锁；其他会话的操作：a.可以对其他表（A表以外的表）进行读、写操作 b.对A表：读-可以； 写-需要等待释放锁。 释放锁: unlock tables ; 写锁示例与MyISAM模式特征 ===加写锁： 会话0： lock table tablelock write ; 当前会话（会话0） 可以对加了写锁的表 进行任何操作（增删改查）；但是不能 操作（增删改查）其他表 其他会话： 对会话0中加写锁的表 可以进行增删改查的前提是：等待会话0释放写锁 MySQL表级锁的锁模式 MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁， 在执行更新操作（DML）前，会自动给涉及的表加写锁。 所以对MyISAM表进行操作，会有以下情况： a、对MyISAM表的读操作（加读锁），不会阻塞其他进程（会话）对同一表的读请求， 但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。 b、对MyISAM表的写操作（加写锁），会阻塞其他进程（会话）对同一表的读和写操作， 只有当写锁释放后，才会执行其它进程的读写操作。 表锁情况分析及行锁解析 分析表锁定： 查看哪些表加了锁： show open tables ; 1代表被加了锁 分析表锁定的严重程度： show status like 'table%' ; Table_locks_immediate :立刻能获取到的锁数 Table_locks_waited：需要等待的表锁数(如果该值越大，说明存在越大的锁竞争) 一般建议： Table_locks_immediate/Table_locks_waited &gt; 5000， 建议采用InnoDB引擎，否则MyISAM引擎 ​ （2）行表（InnoDB） create table linelock( id int(5) primary key auto_increment, name varchar(20) )engine=innodb ; insert into linelock(name) values('1') ; insert into linelock(name) values('2') ; insert into linelock(name) values('3') ; insert into linelock(name) values('4') ; insert into linelock(name) values('5') ; --mysql默认自动commit; oracle默认不会自动commit ; 为了研究行锁，暂时将自动commit关闭; set autocommit =0 ; 以后需要通过commit 会话0： 写操作 insert into linelock values( 'a6') ; 会话1： 写操作 同样的数据 update linelock set name='ax' where id = 6; 对行锁情况： 1.如果会话x对某条数据a进行 DML操作（研究时：关闭了自动commit的情况下），则其他会话必须等待会话x结束事务(commit/rollback)后 才能对数据a进行操作。 2.表锁 是通过unlock tables，也可以通过事务解锁 ; 行锁 是通过事务解锁。 ​ 行锁，操作不同数据： 会话0： 写操作 insert into linelock values(8,'a8') ; 会话1： 写操作， 不同的数据 update linelock set name='ax' where id = 5; 行锁，一次锁一行数据；因此 如果操作的是不同数据，则不干扰。 行锁的注意事项及使用情况分析 ​ 行锁的注意事项： ​ a.如果没有索引，则行锁会转为表锁 ​ show index from linelock ; ​ alter table linelock add index idx_linelock_name(name); ​ 会话0： 写操作 ​ update linelock set name = 'ai' where name = '3' ; ​ ​ 会话1： 写操作， 不同的数据 ​ update linelock set name = 'aiX' where name = '4' ; ​ 会话0： 写操作 ​ update linelock set name = 'ai' where name = 3 ; ​ ​ 会话1： 写操作， 不同的数据 ​ update linelock set name = 'aiX' where name = 4 ; ​ ​ --可以发现，数据被阻塞了（加锁） ​ -- 原因：如果索引类 发生了类型转换，则索引失效。 因此 此次操作，会从行锁 转为表锁。 ​ ​ b.行锁的一种特殊情况：间隙锁：值在范围内，但却不存在 ​ --此时linelock表中 没有id=7的数据 ​ update linelock set name ='x' where id &gt;1 and id&lt;9 ; --即在此where范围中，没有id=7的数据，则id=7的数据成为间隙。 ​ 间隙：Mysql会自动给 间隙 加索 -&gt;间隙锁。即 本题 会自动给id=7的数据加 间隙锁（行锁）。 ​ 行锁：如果有where，则实际加索的范围 就是where后面的范围（不是实际的值） 查询行锁 ​ 如何仅仅是查询数据，能否加锁？ 可以 for update ​ 研究学习时，将自动提交关闭： ​ set autocommit =0 ; ​ start transaction ; ​ begin ; ​ select * from linelock where id =2 for update ; ​ ​ 通过for update对query语句进行加锁。 ​ ​ 行锁： ​ InnoDB默认采用行锁； ​ 缺点： 比表锁性能损耗大。 ​ 优点：并发能力强，效率高。 ​ 因此建议，高并发用InnoDB，否则用MyISAM。 ​ ​ 行锁分析： ​ show status like '%innodb_row_lock%' ; ​ Innodb_row_lock_current_waits :当前正在等待锁的数量 ​ Innodb_row_lock_time：等待总时长。从系统启到现在 一共等待的时间 ​ Innodb_row_lock_time_avg ：平均等待时长。从系统启到现在平均等待的时间 ​ Innodb_row_lock_time_max ：最大等待时长。从系统启到现在最大一次等待的时间 ​ Innodb_row_lock_waits ： 等待次数。从系统启到现在一共等待的次数 主从复制 ​ windows:mysql 主 ​ linux:mysql从 安装windows版mysql: 如果之前计算机中安装过Mysql，要重新再安装 则需要：先卸载 再安装 先卸载： 通过电脑自带卸载工具卸载Mysql (电脑管家也可以) 删除一个mysql缓存文件C:\\ProgramData\\MySQL 删除注册表regedit中所有mysql相关配置 --重启计算机 安装MYSQL： 安装时，如果出现未响应： 则重新打开D:\\MySQL\\MySQL Server 5.5\\bin\\MySQLInstanceConfig.exe 图形化客户端： SQLyog, Navicat 如果要远程连接数据库，则需要授权远程访问。 授权远程访问 :(A-&gt;B,则再B计算机的Mysql中执行以下命令) GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; FLUSH PRIVILEGES; 如果仍然报错：可能是防火墙没关闭 ： 在B关闭防火墙 service iptables stop 实现主从同步（主从复制）：图 1.master将改变的数 记录在本地的 二进制日志中（binary log） ；该过程 称之为：二进制日志件事 2.slave将master的binary log拷贝到自己的 relay log（中继日志文件）中 3.中继日志事件，将数据读取到自己的数据库之中 MYSQL主从复制 是异步的，串行化的， 有延迟 master:slave = 1:n 配置： windows(mysql: my.ini) linux(mysql: my.cnf) 配置前，为了无误，先将权限(远程访问)、防火墙等处理： 关闭windows/linux防火墙： windows：右键“网络” ,linux: service iptables stop Mysql允许远程连接(windowos/linux)： GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION; FLUSH PRIVILEGES; 主机（以下代码和操作 全部在主机windows中操作）： my.ini [mysqld] #id server-id=1 #二进制日志文件（注意是/ 不是\\） log-bin=&quot;D:/MySQL/MySQL Server 5.5/data/mysql-bin&quot; #错误记录文件 log-error=&quot;D:/MySQL/MySQL Server 5.5/data/mysql-error&quot; #主从同步时 忽略的数据库 binlog-ignore-db=mysql #(可选)指定主从同步时，同步哪些数据库 binlog-do-db=test windows中的数据库 授权哪台计算机中的数据库 是自己的从数据库： GRANT REPLICATION slave,reload,super ON . TO 'root'@'192.168.2.%' IDENTIFIED BY 'root'; flush privileges ; 查看主数据库的状态（每次在左主从同步前，需要观察 主机状态的最新值） show master status; （mysql-bin.000001、 107） 从机（以下代码和操作 全部在从机linux中操作）： my.cnf [mysqld] server-id=2 log-bin=mysql-bin replicate-do-db=test linux中的数据 授权哪台计算机中的数控 是自己的主计算机 CHANGE MASTER TO MASTER_HOST = '192.168.2.2', MASTER_USER = 'root', MASTER_PASSWORD = 'root', MASTER_PORT = 3306, master_log_file='mysql-bin.000001', master_log_pos=107; 如果报错：This operation cannot be performed with a running slave; run STOP SLAVE first 解决：STOP SLAVE ;再次执行上条授权语句 开启主从同步： 从机linux: start slave ; 检验 show slave status \\G 主要观察： Slave_IO_Running和 Slave_SQL_Running，确保二者都是yes；如果不都是yes，则看下方的 Last_IO_Error。 本次 通过 Last_IO_Error发现错误的原因是 主从使用了相同的server-id， 检查:在主从中分别查看serverid: show variables like 'server_id' ; 可以发现，在Linux中的my.cnf中设置了server-id=2，但实际执行时 确实server-id=1，原因：可能是 linux版Mysql的一个bug，也可能是 windows和Linux版本不一致造成的兼容性问题。 解决改bug： set global server_id =2 ; stop slave ; set global server_id =2 ; start slave ; show slave status \\G 演示： 主windows =&gt;从 windows: 将表，插入数据 观察从数据库中该表的数据 数据库+后端 spring boot（企业级框架,目前使用较多） 命令集合 mysql -u root -p 登录 show engines; 显示支持引擎 show variables like ‘%storage_engine%’; 查看当前使用引擎 12345678-- 指定数据库引擎create table tb(​ id int(4) auto_increment,​ name varchar(5),​ dept varchar(5),​ primary key(id))ENGINE=MyISAM AUTO_INCREMENT=1DEFAULT CHARSET=utf-8 常用语句 数据库 1234567891011121314151617181920212223#1.查看已有库 show databases;#2.创建库(指定字符集) create database 库名 character set utf8 ；或 charset=utf8; create database 库名 default charset utf8 collate utf8_general_ci;#案例：创建stu数据库，编码为utf8 create database stu character set utf8; create database stu charset=utf8;#3.查看创建库的语句(字符集) show create database 库名;#案例：查看stu创建方法 show create database stu;#4.查看当前所在库 select database();#5.切换库 use 库名;#案例：使用stu数据库 use stu;#6.删除库 drop database 库名;#案例：删除test数据库 drop database test; #7.库名的命名规则##数字、字母、下划线,但不能使用纯数字##库名区分字母大小写##不能使用特殊字符和mysql关键字 表 12345678910111213141516#创建CREATE TABLE IF NOT EXISTS `表名`( `字段名` INT UNSIGNED AUTO_INCREMENT, `字段名` VARCHAR(100) NOT NULL, `字段名` VARCHAR(40) NOT NULL, `字段名` DATE, PRIMARY KEY ( `主键字段名` ))ENGINE=InnoDB DEFAULT CHARSET=utf8;#对创建后的表添加列alter table 表名 add column 新增列名 类型 not null DEFAULT 默认值 after 前一个列名#对创建表后添加主外键alter table 表名 add constraint primary key 表名（主键字段名）foreign key (外建名) references 外建字段所在表名 (外建字段名) on delete cascade; #on delete cascade 级联删除 数据 1234#增#删#改#查 连接 1234567#内连接 inner join on 交集#外连接##左连接 left join on / left outer join on 左表+右表符合条件数据，记录不足的地方为null##右连接 right join on / right outer join on 右表+左表符合条件数据，记录不足的地方为null#全连接 union /union all 索引 123456#创建索引create 索引类型 索引名 on 表名(字段名);alter table 表名 add 索引类型 索引名(字段名);#使用强制索引select * from 表名 FORCE INDEX (索引名) where 条件; 视图 1234567#创建视图create view 视图名称 as select语句;#--查看视图结构：desc user_view; show create view user_view;#--查看视图内容：select * from user_view; 游标 12345678910111213141516171819#定义DECLARE 光标名称 CURSOR FOR 查询语法declare cursor_name cursor for select_statement#打开OPEN 光标名称open cursor_name#取游标中的数据FETCH 光标名称 INFO var_name [，var_name ].....fetch cursor_name info var_name#关闭游标CLOSE curso_name;close 光标名称#释放游标DEALLOCATE 光标名称deallocate cursor_name; 触发器 1234567891011121314#创建##只有一个执行语句CREATE TRIGGER 触发器名 BEFORE|AFTER 触发事件 ON 表名 FOR EACH ROW 执行语句;##多个个执行语句CREATE TRIGGER 触发器名 BEFORE|AFTER 触发事件ON 表名 FOR EACH ROWBEGIN 执行语句列表END;#示例create TRIGGER audit_log after INSERT on employees_test for each row begin INSERT INTO audit VALUES (NEW.ID, NEW.NAME);end 其他 1234#查看表字段信息desc 表名#查看创建表sql（表所有信息）show create table 表名; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#输出格式date_format(date,&quot;%Y-%m&quot;) #插入存在忽略insert ignore存在忽略#去除重复distinct字段去重#分组group by 。。having##group by 在where 最后一个限制条件后面，不能在限制条件中间，HAVING是在分组后找到特定的分组。select user_id from order_info where date &gt; '2025-10-15' and product_name in ('C++','Java','Python') and status='completed' group by user_id HAVING count(user_id)&gt;1 order by user_id # in exists##in先内再外not in##EXISTS先外再内SELECT 字段 from 表名 where not EXISTS (select emp_no from dept_emp where employees.emp_no = dept_emp.emp_no)#连接,concat(last_name,'\\'',first_name)SELECT dept_no,GROUP_CONCAT(emp_no SEPARATOR ',') as employees from dept_emp group by dept_no#语法: group_concat([distinct] 字段 [order by 排序字段 asc/desc] separator '分隔符')#函数默认的分隔符是 &quot;,&quot;，默认可以省略#替换replaceupdate titles_test set emp_no=replace(emp_no,'10001','10005') where id=5#更换表名alter table titles_test rename to titles_2017round(avg(score),3)#正则匹配，替换，计算长度select id,length( regexp_replace(STRING, '[A-Z0-9]', '')) from strings#截取select 字段 from 表名 order by right/left(字段,2)substring(str,index)substring_index(str,delim,count)subdate(date,day)subtime(expr1,expr2)#分页select * from employees LIMIT 5 offset 5select * from employees LIMIT 5,5#窗口函数# 排序函数#mysql—排序函数rank() over()、dense_rank() over()、row_num() over()#条件相同排名相同， 间断不连续 rank() over() 跳过重复，比如 113446#条件相同排名相同， 间断连续 dense_rank() over() 不跳过重复，比如 112334#条件相同排名不相同，不间断连续 row_num() over() 依次相加，比如 123456select id,number,dense_rank() over(order by number desc) as 'rank' from passing_number #以grade排序，number按顺序以及grade分组累加#grade number#A 2#D 1#C 2#B 2select grade,sum(number) over(order by grade) t_rank from class_grade#时间##当前时间current_date/curdate() now()current_timestamp, current_timestamp()#格式转换 字符串转时间str_to_date('08/09/2008', '%m/%d/%Y'); -- 2008-08-09str_to_date('08/09/08' , '%m/%d/%y'); -- 2008-08-09str_to_date('08.09.2008', '%m.%d.%Y'); -- 2008-08-09str_to_date('08:09:30', '%h:%i:%s'); -- 08:09:30str_to_date('08.09.2008 08:09:30', '%m.%d.%Y %h:%i:%s'); -- 2008-08-09 08:09:30#时间增减#条件分支(case when 条件 then 语句 when 条件 then 语句 else 语句 end) as bonusselect e.emp_no,e.first_name,e.last_name,eb.btype,s.salary,(case when eb.btype = 1 then s.salary*0.1 when eb.btype= 2 then s.salary*0.2 else s.salary*0.3 end) as bonus from employees e INNER join emp_bonus eb on e.emp_no=eb.emp_no INNER join salaries s on e.emp_no=s.emp_no where s.to_date='9999-01-01'; 窗口函数 1.窗口函数语法 12&lt;窗口函数&gt; over (partition by &lt;用于分组的列名&gt; order by &lt;用于排序的列名&gt;) &lt;窗口函数&gt;的位置，可以放以下两种函数： 1） 专用窗口函数，比如 序号函数 rank, dense_rank, row_number、 分布函数：PERCENT_RANK()、CUME_DIST()、PERCENT_RANK() 前后函数：LAG(expr,n)、LEAD(expr,n) 前后函数：LAG(expr,n)、LEAD(expr,n) 其它函数：NTH_VALUE(expr, n)、NTILE(n)、NTH_VALUE(expr,n) 2） 聚合函数，如sum. avg, count, max, min等 2.窗口函数有以下功能： 1）同时具有分组（partition by）和排序（order by）的功能 2）不减少原表的行数，所以经常用来在每组内排名 3.注意事项 窗口函数原则上只能写在select子句中 4.窗口函数使用场景 1）业务需求“在每组内排名” mysql截取字符串函数 1、left(str,length) 从左边截取length 2、right(str,length)从右边截取length 3、substring(str,index)当index&gt;0从左边开始截取直到结束 当index&lt;0从右边开始截取直到结束 当index=0返回空 4、substring(str,index,len) 截取str,从index开始，截取len长度 5、substring_index(str,delim,count)，str是要截取的字符串，delim是截取的字段 count是从哪里开始截取(为0则是左边第0个开始，1位左边开始第一个选取左边的，-1从右边第一个开始选取右边的 6、subdate(date,day)截取时间，时间减去后面的day 7、subtime(expr1,expr2) 时分秒expr1-expr2 8、mysql索引从1开始 全文索引： 是目前搜索引擎使用的一种关键技术。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 查询数据库中表信息 123456SELECT *FROM information_schema.`TABLES`WHERE TABLE_SCHEMA = 'hz_hbzy'; Postgres 查询字段备注 12select a.attnum,a.attname,concat_ws('',t.typname,SUBSTRING(format_type(a.atttypid,a.atttypmod) from '\\(.*\\)')) as type,d.description from pg_class c,pg_attribute a,pg_type t,pg_description dwhere c.relname='com_capture' and a.attnum&gt;0 and a.attrelid=c.oid and a.atttypid=t.oid and d.objoid=a.attrelid and d.objsubid=a.attnum; 取geom数据 12select st_astext(geom) from ttt # test形式SELECT st_asgeojson(geom) FROM &quot;ygjcgd&quot; # geojson形式 存geom 12点INSERT into ttt values ('1',ST_GeomFromText('POINT(119.104393005 29.3091735840001)',4490))面INSERT into ttt values ('2',ST_GeomFromText('POLYGON((120.489420304 30.2855081450001,120.48927819 30.2855092200001,120.489034534 30.2855205130001,120.488415416 30.2855540510001,120.488238647 30.285563627,120.487775051 30.2856122420001,120.487543209 30.2856365540001,120.48696129 30.28571928,120.486939727 30.285722842,120.486661766 30.2857687420001,120.48676176 30.2859599880001,120.486786439 30.2860551310001,120.486803289 30.286632664,120.487353662 30.2866268860001,120.487351682 30.2864715210001,120.487351632 30.286467605,120.487350721 30.2863964210001,120.487346937 30.2860998030001,120.487342737 30.285770433,120.487400427 30.2857633680001,120.487405079 30.285760184,120.48740623 30.285961001,120.487409165 30.2864727900001,120.487409564 30.286542108,120.487409728 30.2865708910001,120.487410051 30.2866269530001,120.487415064 30.2866269360001,120.487781689 30.2866291820001,120.488148313 30.286631426,120.488229104 30.286631782,120.488646675 30.2866336280001,120.488672652 30.2866335310001,120.489108909 30.286635501,120.489545164 30.286637469,120.489554908 30.2866377670001,120.489587466 30.2866073650001,120.489631665 30.286574779,120.489633218 30.2865468840001,120.489635952 30.286497766,120.489630193 30.2864750590001,120.489620367 30.286436291,120.489576415 30.2862629480001,120.489538252 30.286111927,120.489518579 30.286034075,120.489490667 30.2859208350001,120.489445946 30.285699532,120.489442796 30.285683949,120.489435137 30.2856060400001,120.489420304 30.2855081450001))',4490)) 添加geom扩展 12345CREATE EXTENSION postgis;CREATE EXTENSION fuzzystrmatch;CREATE EXTENSION postgis_tiger_geocoder;CREATE EXTENSION address_standardizer;CREATE EXTENSION postgis_topology; 上好例题 时间格式，截取匹配，分组，排序 1select job,date_format(date,&quot;%Y-%m&quot;) as mon,sum(num) as cnt from resume_info where left(date,4)= '2025' group by job,mon order by mon desc,cnt desc 单输入，多字段查询 123&lt;if test=&quot;text!=null and text!=''&quot;&gt; and CONCAT(t.city,t.bh,t.cun,t.jsztmc,t.xmxmmc) like concat ('%',#{text},'%')&lt;/if&gt; Java字段映射 全字段详表 数值 参考 类型 大小 范围（有符号） 范围（无符号UNSIGNED） 用途 TINYINT 1 Bytes (-128，127) (0，255) 小整数值 SMALLINT 2 Bytes (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 Bytes (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 Bytes (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 Bytes (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 Bytes (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 DOUBLE 8 Bytes (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 时间 类型 大小 ( bytes) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 '-838:59:59'/'838:59:59' HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串 类型 大小 用途 CHAR 0-255 bytes 定长字符串 VARCHAR 0-65535 bytes 变长字符串 TINYBLOB 0-255 bytes 不超过 255 个字符的二进制字符串 TINYTEXT 0-255 bytes 短文本字符串 BLOB 0-65 535 bytes 二进制形式的长文本数据 TEXT 0-65 535 bytes 长文本数据 MEDIUMBLOB 0-16 777 215 bytes 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215 bytes 中等长度文本数据 LONGBLOB 0-4 294 967 295 bytes 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295 bytes 极大文本数据 1bit 位 1字节=8bit 1k=1024字节 1兆=1024k 1G=1023M 1T=1024G 常用Java映射 Mysql Java 包 字段名 备注 datetime Date 数据库设计 名称 **表是否：**is_xxx 类型：任何字段非负数必须是 unsigned tinyint (1是 0否) POJO中需要从is_xxx 去掉is前缀映射到 xxx **表名、字段名：**全小写【windows不区分大小写，linux区分】、非数字开头、两个下划线中不能只是数字、表名不使用复数。不使用保留字。表名称建议：业务名称_表的作用。 **索引名：**主键索引：pk_xxx、唯一索引名：uk_xxx、普通索引：idx_xxx **小数：**decimal （超过其范围建议拆成整数和小数分开存储），禁用float、double 【精度问题】 **字符串：**数据长度几乎相等用char定长，vachar为可变长字符串，长度超过5000 时 使用text类型独立表存储 **表单必备三字段：**id（bigint unsigned）、gmt_create / gmt_modified(datetime,前者现在时表示主动式创建，后者过去分词表示被动式更新) **库名：**建议同应用名 **备注：**修改字段含义与状态添加时及时更新备注。 **其他：**字段适当长度冗余，单标数据超过500万行或容量超过2GB才建议分库分表。三年的时间估计。 索引 具有唯一特性的字段，即使是组合字段，也必须建成唯一索引 超过三个表禁止join。需要join的字段，数据类型保持绝对一致；多表关联查询时，保证被关联的字段需要有索引。 在varchar字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度 页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。 如果有order by的场景，请注意利用索引的有序性。order by 最后的字段是组合索引的一部分，并且放在索引组合顺序的最后，避免出现file_sort的情况，影响查询性能。 利用覆盖索引来进行查询操作，避免回表。 利用延迟关联或者子查询优化超多分页场景。 SQL性能优化的目标：至少要达到 range 级别，要求是ref级别，如果可以是consts最好。 建组合索引的时候，区分度最高的在最左边。 防止因字段类型不同造成的隐式转换，导致索引失效。 语句 不要使用count(列名)或count(常量)来替代count()，count()是SQL92定义的标准统计行数的语法，跟数据库无关，跟NULL和非NULL无关。 说明：count(*)会统计值为NULL的行，而count(列名)不会统计此列为NULL值的行。 】count(distinct col) 计算该列除NULL之外的不重复行数，注意 count(distinct col1, col2) 如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。 当某一列的值全是NULL时，count(col)的返回结果为0，但sum(col)的返回结果为NULL，因此使用sum()时需注意NPE问题。 正例：可以使用如下方式来避免sum的NPE问题：SELECT IFNULL(SUM(column), 0) FROM table; 使用ISNULL()来判断是否为NULL值。 说明：NULL与任何值的直接比较都为NULL。 1） NULL&lt;&gt;NULL的返回结果是NULL，而不是false。 2） NULL=NULL的返回结果是NULL，而不是true。 3） NULL&lt;&gt;1的返回结果是NULL，而不是true。 代码中写分页查询逻辑时，若count为0应直接返回，避免执行后面的分页语句。 不得使用外键与级联，一切外键概念必须在应用层解决。 禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。 数据订正（特别是删除或修改记录操作）时，要先select，避免出现误删除，确认无误才能执行更新语句。 对于数据库中表记录的查询和变更，只要涉及多个表，都需要在列名前加表的别名（或表名）进行限定，否则同名字段存在多表中会报错。别名前加as使别名更容易识别。 n操作能避免则避免，若实在避免不了，需要仔细评估in后边的集合元素数量，控制在1000个之内。 ORM映射 一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。因为会 1）增加查询分析器解析成本。2）增减字段容易与resultMap配置不一致。3）无用字段增加网络消耗，尤其是text类型的字段。 POJO类的布尔属性不能加is，而数据库字段必须加is_，要求在resultMap中进行字段与属性之间的映射。 不要用resultClass当返回参数，即使所有类属性名与数据库字段一一对应，也需要定义；反过来，每一个表也必然有一个与之对应。配置映射关系，使字段与DO类解耦，方便维护。 sql.xml配置参数使用：#{}，#param# 不要使用${} 此种方式容易出现SQL注入 iBATIS自带的queryForList(String statementName,int start,int size)不推荐使用。 不允许直接拿HashMap与Hashtable作为查询结果集的输出。 更新数据表记录时，必须同时更新记录对应的gmt_modified字段值为当前时间。 执行SQL时，不要更新无改动的字段，一是易出错；二是效率低；三是增加binlog存储 @Transactional事务不要滥用。事务会影响数据库的QPS，另外使用事务的地方需要考虑各方面的回滚方案，包括缓存回滚、搜索引擎回滚、消息补偿、统计修正等。 中的compareValue是与属性值对比的常量，一般是数字，表示相等时带上此条件；表示不为空且不为null时执行；表示不为null值时执行。 阿里规范摘要 数据库字段设计及对应关系 数据库三原则 数据库设计工具 如何设计数据库","link":"/2021/02/24/Draft/2021/MYSQL%E4%BC%98%E5%8C%96/"},{"title":"魑魅先生 | Redis","text":"简介 Redis（Remote Dictionary Server）远程字典服务 免费开源的，C语言编写，提供多种语言使用，支持网络，可持久化，遵守 BSD 协议，是一个高性能的 key-value 数据库。 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 学习工具 命令查询 狂神视频 Redis 最全面试题及答案_岁月安然-CSDN博客_redis题库 数据类型概览 数据类型 String Hash List Set ZSet Geospatial Hyperloglog Bitmap 特点 字符串，整数或浮点数 包含键（string ）值对的无序散列表 链表上的节点字符串元素 不同无序字符串元素,哈希表 不同字符串，还有带double类型的可重复分数的有序集合 地理位置 基数统计不占内存记录网站UV Bitmap 位图，数据结构！二进制位记录，0 和 1 两个状态 增 set hset lpush，rpush sadd zadd geoadd pfadd setbit 取 get hget （取单个） lrange取区间值lpop左取rpop 右取 smembers 取所有 zscore单取 zrange 范围取 zrevrange 升序取zrangebyscore 所有降序取zrevrangebyscore 所有升序取 geoposgeodist（两点距离）georadius（点附近点，可数量限制） 删 del hdel srem zrem 取旧改新 getset 自增 incr （可指定 HINCRBY增加指定步长 自减 decr （可指定 追加 append 长度 strlen hlen llen zcard pfcount（不重复数量） bitcount(计算状态数量) 批量同时设置一个或多个 key-value 对 mset hmset 返回所有一个或多个给定 key 的值 mget hmget 获取对象中所有的键值对 hgetall （取所有） 判断元素是否存在 hexists sismember 获取字段名 hkeys 获取字段值 hvals 集合运算 sunion 并集，sinter 交集，sdiff 交差 pfmerge(合并) 应用场景 与MongoDB对比 MongoDB Redis Nosql由来 单机mysql mysql+缓存+垂直拆分 主从读写分离 分表分库+水平拆分+mysql集群 现在用户社交网络数据成倍增长，sql难以支撑 NoSQL（Not Only SQL）非关系型数据库，不需要固定模式，无需多于操作就可以横向操作 简介 Redis（Remote Dictionary Server）远程字典服务 免费开源，C语言编写，提供多种语言使用，支持网络，可持久化，遵守 BSD 协议，是一个高性能的 key-value 数据库。 类型多样，不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis 优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 数据类型多样 – Redis支持二进制的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 数据的备份，即master-slave模式的数据备份。 Redis与其他key-value存储有什么不同？ Redis有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。Redis运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样Redis可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 redis的应用场景 ​ 1、会话高速缓存（最常用） ​ 2、消息队列，比如支付 ​ 3、活动排行榜或计数 ​ 4、发布、订阅消息（消息通知） ​ 5、商品列表、评论列表、计时器、计数器 ​ 6、地图信息分析 ​ 7、内存存储、持久化（rdb，aof） 3V+3高 大数据时代3V（描述问题） 海量Volume 多样Variety 实时Velocity 互联网需求3高（对程序要求） 高并发 高可扩 高性能 经典应用 阿里巴巴 数据库四大分类 KV键值对 文档型数据库 列存储数据库 图关系数据库（图结构） CAP C：Consistency（强一致性） A：Availability（可用性） P：Partition tolerance（分区容错性） 三进二（C必实现） CA 单点集群，满足一致性 CP AP BASE 基本可用（Basically Available） 软状态（Soft state） 最终一致性（Eventually consistent） 分布式与集群 ACID关系型数据库 Redis 安装 windows 企业里面几乎不用windows开发Redis https://github.com/tporadowski/redis/releases redis-server.exe redis.windows.conf 另启一个 cmd 窗口，原来的不要关闭，不然就无法访问服务端了（可直接设置为服务自启动如下） 设置redis环境变量之后在解压文件夹中执行命令 redis-server.exe --service-install redis.windows.conf --loglevel verbose 修改服务为自动启动 redis-cli.exe -h 127.0.0.1 -p 6379 set myKey abc get myKey Linux ​ 下载 == 解压（opt） == 基本环境安装【yum install gcc-c++】== make == make install == redis默认安装路径/usr/local/bin == 拷贝redis的conf文件使用拷贝配置文件启动 == daemonize改为yes改为后台启动 == 启动服务redis-server redisconfig/redis.conf == 客户端连接 redis-cli -p 6379 == ping 测试连接 === 查看redis进程是否开启 ps -ef|grep redis == shutdown关闭服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187####新建文件夹[root@iZpwlt9baenyc8Z ~]# mkdir /usr/local/redis[root@iZpwlt9baenyc8Z ~]# cd /usr/local/redis[root@iZpwlt9baenyc8Z redis]# ls####下载源码[root@iZpwlt9baenyc8Z redis]# wget http://download.redis.io/releases/redis-6.0.5.tar.gz--2022-02-10 14:00:45-- http://download.redis.io/releases/redis-6.0.5.tar.gzredis-6.0.5.tar.gz 100%[==================================================================================&gt;] 2.11M 42.0KB/s in 57s 。。。####解压缩[root@iZpwlt9baenyc8Z redis]# tar -zxvf redis-6.0.5.tar.gz####安装[root@iZpwlt9baenyc8Z redis]# lsredis-6.0.5 redis-6.0.5.tar.gz[root@iZpwlt9baenyc8Z redis]# cd redis-6.0.5[root@iZpwlt9baenyc8Z redis-6.0.5]# ls00-RELEASENOTES CONTRIBUTING deps Makefile README.md runtest runtest-moduleapi sentinel.conf tests utilsBUGS COPYING INSTALL MANIFESTO redis.conf runtest-cluster runtest-sentinel src TLS.md[root@iZpwlt9baenyc8Z redis-6.0.5]# makecd src &amp;&amp; make allmake[1]: Entering directory '/usr/local/redis/redis-6.0.5/src' 。。。Hint: It's a good idea to run 'make test' ;)make[1]: Leaving directory '/usr/local/redis/redis-6.0.5/src'[root@iZpwlt9baenyc8Z redis-6.0.5]# ls00-RELEASENOTES CONTRIBUTING deps Makefile README.md runtest runtest-moduleapi sentinel.conf tests utilsBUGS COPYING INSTALL MANIFESTO redis.conf runtest-cluster runtest-sentinel src TLS.md[root@iZpwlt9baenyc8Z redis-6.0.5]# cd src####迁出可执行程序，与源码分离[root@iZpwlt9baenyc8Z src]# make install PREFIX=/opt/redis6 CC Makefile.depHint: It's a good idea to run 'make test' ;)####创建redis配置文件目录并复制配置文件[root@iZpwlt9baenyc8Z src]# mkdir /etc/redis[root@iZpwlt9baenyc8Z src]# cd ..[root@iZpwlt9baenyc8Z redis-6.0.5]# cd ..[root@iZpwlt9baenyc8Z redis]# lsredis-6.0.5 redis-6.0.5.tar.gz[root@iZpwlt9baenyc8Z redis]# cd ..[root@iZpwlt9baenyc8Z local]# lsaegis bin etc games include lib lib64 libexec redis sbin share src[root@iZpwlt9baenyc8Z local]# cd etc[root@iZpwlt9baenyc8Z etc]# ls[root@iZpwlt9baenyc8Z etc]# cd /etc[root@iZpwlt9baenyc8Z etc]# cd redis[root@iZpwlt9baenyc8Z redis]# ls[root@iZpwlt9baenyc8Z redis]# cp /usr/local/redis/redis-6.0.5/redis.conf /etc/redis/6379.conf[root@iZpwlt9baenyc8Z redis]# cd ..[root@iZpwlt9baenyc8Z etc]# cd ..[root@iZpwlt9baenyc8Z /]# lsbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var[root@iZpwlt9baenyc8Z ~]# cd /etc/redis/[root@iZpwlt9baenyc8Z redis]# ls6379.conf[root@iZpwlt9baenyc8Z redis]# cd /opt/redis6[root@iZpwlt9baenyc8Z redis6]# lsbin###创建redis服务，复制redis示例服务并修改[root@iZpwlt9baenyc8Z bin]# cp /usr/local/redis/redis-6.0.5/utils/systemd-redis_server.service /lib/systemd/system/redis_6379.service[root@iZpwlt9baenyc8Z bin]# cd /lib/systemd/system/[root@iZpwlt9baenyc8Z system]# vim /lib/systemd/system/redis_6379.service########################### example systemd service unit file for redis-server## In order to use this as a template for providing a redis service in your# environment, _at the very least_ make sure to adapt the redis configuration# file you intend to use as needed (make sure to set &quot;supervised systemd&quot;), and# to set sane TimeoutStartSec and TimeoutStopSec property values in the unit's# &quot;[Service]&quot; section to fit your needs.## Some properties, such as User= and Group=, are highly desirable for virtually# all deployments of redis, but cannot be provided in a manner that fits all# expectable environments. Some of these properties have been commented out in# this example service unit file, but you are highly encouraged to set them to# fit your needs.## Please refer to systemd.unit(5), systemd.service(5), and systemd.exec(5) for# more information.[Unit]Description=Redis_6379After=network.target#Documentation=https://redis.io/documentation#Before=your_application.service another_example_application.service#AssertPathExists=/var/lib/redis[Service]Type=forkingPIDFile=/var/run/redis_6379.pidExecStart=/opt/redis6/bin/redis-server /etc/redis/6379.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDPrivateTmp=true## Alternatively, have redis-server load a configuration file:#ExecStart=/usr/local/bin/redis-server /path/to/your/redis.conf#LimitNOFILE=10032#NoNewPrivileges=yes#OOMScoreAdjust=-900#PrivateTmp=yes#TimeoutStartSec=infinity#TimeoutStopSec=infinity#UMask=0077#User=redis#Group=redis#WorkingDirectory=/var/lib/redis[Install]WantedBy=multi-user.target:wq!#######[root@iZpwlt9baenyc8Z system]# cd /###使用systemctl 开启关闭服务#如果报错：Job for redis-server.service failed because a timeout was exceeded.[root@iZpwlt9baenyc8Z ~]# systemctl status redis_6379● redis_6379.service - Redis_6379 └─151178 /opt/redis6/bin/redis-server 127.0.0.1:6379Feb 10 14:28:06 iZpwlt9baenyc8Z redis-server[151178]: `-.__.-'。。。Feb 10 14:28:06 iZpwlt9baenyc8Z redis-server[151178]: 151178:M 10 Feb 2022 14:28:06.493 * Ready to accept connectionslines 1-19/19 (END)[root@iZpwlt9baenyc8Z /]# cd /opt/redis6[root@iZpwlt9baenyc8Z redis6]# lsbin[root@iZpwlt9baenyc8Z redis6]# cd bin[root@iZpwlt9baenyc8Z bin]# lsredis-benchmark redis-check-aof redis-check-rdb redis-cli redis-sentinel redis-server###cd /opt/redis6/bin 跳转到命令目录后使用redis-server开启服务##全局使用redis命令解决方法#修改profile文件： #vi /etc/profile #在最后行添加: #export PATH=$PATH:/opt/redis6/bin#注意：/opt/software/redis/src 表示的是redis-cli 命令存在的目录路径#重新加载/etc/profile #source /etc/profile [root@iZpwlt9baenyc8Z bin]# redis-server151212:C 10 Feb 2022 14:38:14.346 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo151212:C 10 Feb 2022 14:38:14.346 # Redis version=6.0.5, bits=64, commit=00000000, modified=0, pid=151212, just started151212:C 10 Feb 2022 14:38:14.346 # Warning: no config file specified, using the default config. In order to specify a config file use ./redis-server /path/to/redis.conf _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 6.0.5 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 151212 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 151212:M 10 Feb 2022 14:38:14.347 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.151212:M 10 Feb 2022 14:38:14.347 # Server initialized151212:M 10 Feb 2022 14:38:14.347 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.151212:M 10 Feb 2022 14:38:14.347 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.151212:M 10 Feb 2022 14:38:14.348 * Ready to accept connections[root@iZpwlt9baenyc8Z ~]# cd /opt/redis6/bin#使用 ./redis-cli 打开终端[root@iZpwlt9baenyc8Z bin]# ./redis-cli ###检测服务是否启动127.0.0.1:6379&gt; pingPONG redis-benchmark性能测试 可选参数： 测试：100个并发连接 100000请求 ./redis-benchmark -h localhost -p 6379 -c 100 -n 100000 基础知识 redis默认有16个数据库，默认使用的是第0个 可以使用 select 进行切换数据库！select 3 DBSIZE # 查看DB大小！ keys * # 查看数据库所有的key 清除当前数据库 flushdb 清除全部数据库的内容 FLUSHALL Redis 为什么单线程还这么快？ 1、误区1：高性能的服务器一定是多线程的？ 2、误区2：多线程（CPU上下文会切换！）一定比单线程效率高！ 先去CPU&gt;内存&gt;硬盘的速度要有所了解！ 核心：redis 是将所有的数据全部放在内存中的，所以说使用单线程去操作效率就是最高的，多线程 （CPU上下文会切换：耗时的操作！！！），对于内存系统来说，如果没有上下文切换效率就是最高 的！多次读写都是在一个CPU上的，在内存情况下，这个就是最佳的方案！ IO多路复用（Epoll）原理 简单描述： 执行 epoll_create 函数会在内核的高速缓存区中建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着应用程序执行 epoll_ctl 函数添加文件描述符会在红黑树上增加相应的结点。 执行 epoll_ctl 的 add 操作时，不仅将文件描述符放到红黑树上，而且也注册了 callBack 函数。内核在检测到某文件描述符可读/可写时会调用回调函数，该回调函数将文件描述符放在就绪链表中。 执行 epoll_wait 函数只用观察就绪链表中有无数据即可，最后将链表的数据及就绪的数量返回给应用程序，应用程序只需要遍历依次处理即可。这里返回的文件描述符是通过内存映射函数 mmap 让内核和用户空间共享同一块内存实现传递的，减少了不必要的拷贝。 mmap：将用户空间的一段内存区域映射到内核空间，映射成功后，用户对这段内存区域的修改可以直接反映到内核空间。同样，内核空间对这段区域的修改也直接反映用户空间。那么对于内核空间、用户空间两者之间需要大量数据传输等操作的话效率是非常高的。 具体原理可以查看下面这篇博客： https://blog.csdn.net/armlinuxww/article/details/92803381 Redis 配置 Redis 的配置文件位于 Redis 安装目录下，文件名为 redis.conf(Windows 名为 redis.windows.conf)。 你可以通过 CONFIG 命令查看或设置配置项。 CONFIG GET CONFIG_SETTING_NAME CONFIG GET loglevel CONFIG GET * 编辑配置 可以通过修改 redis.conf 文件或使用 CONFIG set 命令来修改配置。 CONFIG SET CONFIG_SETTING_NAME NEW_CONFIG_VALUE CONFIG SET loglevel &quot;notice&quot; 参数说明 redis.conf 配置项说明如下： https://www.runoob.com/redis/redis-conf.html Redis 数据类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748127.0.0.1:6379&gt; keys * # 查看所有的key(empty list or set)127.0.0.1:6379&gt; set name kuangshen # set keyOK127.0.0.1:6379&gt; keys *1) &quot;name&quot;127.0.0.1:6379&gt; set age 1OK127.0.0.1:6379&gt; keys *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:6379&gt; EXISTS name # 判断当前的key是否存在(integer) 1127.0.0.1:6379&gt; EXISTS name1(integer) 0127.0.0.1:6379&gt; move name 1 # 移除当前的key(integer) 1127.0.0.1:6379&gt; keys *1) &quot;age&quot;127.0.0.1:6379&gt; set name qinjiangOK127.0.0.1:6379&gt; keys *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:6379&gt; clear127.0.0.1:6379&gt; keys *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:6379&gt; get name&quot;qinjiang&quot;127.0.0.1:6379&gt; EXPIRE name 10 # 设置key的过期时间，单位是秒(integer) 1127.0.0.1:6379&gt; ttl name # 查看当前key的剩余时间(integer) 4127.0.0.1:6379&gt; ttl name(integer) 3127.0.0.1:6379&gt; ttl name(integer) 2127.0.0.1:6379&gt; ttl name(integer) 1127.0.0.1:6379&gt; ttl name(integer) -2127.0.0.1:6379&gt; get name(nil)127.0.0.1:6379&gt; type name # 查看当前key的一个类型！string127.0.0.1:6379&gt; type agestring String（字符串） string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。 string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据。比如jpg图片或者序列化的对象。 string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。 SET runoob &quot;菜鸟教程&quot; GET runoob 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131##########################################################################127.0.0.1:6379&gt; set key1 v1 # 设置值OK127.0.0.1:6379&gt; get key1 # 获得值&quot;v1&quot;127.0.0.1:6379&gt; keys * # 获得所有的key1) &quot;key1&quot;127.0.0.1:6379&gt; EXISTS key1 # 判断某一个key是否存在(integer) 1127.0.0.1:6379&gt; APPEND key1 &quot;hello&quot; # 追加字符串，如果当前key不存在，就相当于set key(integer) 7127.0.0.1:6379&gt; get key1&quot;v1hello&quot;127.0.0.1:6379&gt; STRLEN key1 # 获取字符串的长度！(integer) 7127.0.0.1:6379&gt; APPEND key1 &quot;,kaungshen&quot;(integer) 17127.0.0.1:6379&gt; STRLEN key1(integer) 17127.0.0.1:6379&gt; get key1&quot;v1hello,kaungshen&quot;########################################################################### i++# 步长 i+=127.0.0.1:6379&gt; set views 0 # 初始浏览量为0OK127.0.0.1:6379&gt; get views&quot;0&quot;127.0.0.1:6379&gt; incr views # 自增1 浏览量变为1(integer) 1127.0.0.1:6379&gt; incr views(integer) 2127.0.0.1:6379&gt; get views&quot;2&quot;127.0.0.1:6379&gt; decr views # 自减1 浏览量-1(integer) 1127.0.0.1:6379&gt; decr views(integer) 0127.0.0.1:6379&gt; decr views(integer) -1127.0.0.1:6379&gt; get views&quot;-1&quot;127.0.0.1:6379&gt; INCRBY views 10 # 可以设置步长，指定增量！(integer) 9127.0.0.1:6379&gt; INCRBY views 10(integer) 19127.0.0.1:6379&gt; DECRBY views 5(integer) 14########################################################################### 字符串范围 range127.0.0.1:6379&gt; set key1 &quot;hello,kuangshen&quot; # 设置 key1 的值OK127.0.0.1:6379&gt; get key1&quot;hello,kuangshen&quot;127.0.0.1:6379&gt; GETRANGE key1 0 3 # 截取字符串 [0,3]&quot;hell&quot;127.0.0.1:6379&gt; GETRANGE key1 0 -1 # 获取全部的字符串 和 get key是一样的&quot;hello,kuangshen&quot;# 替换！127.0.0.1:6379&gt; set key2 abcdefgOK127.0.0.1:6379&gt; get key2&quot;abcdefg&quot;127.0.0.1:6379&gt; SETRANGE key2 1 xx # 替换指定位置开始的字符串！(integer) 7127.0.0.1:6379&gt; get key2&quot;axxdefg&quot;########################################################################### setex (set with expire) # 设置过期时间# setnx (set if not exist) # 不存在在设置 （在分布式锁中会常常使用！）127.0.0.1:6379&gt; setex key3 30 &quot;hello&quot; # 设置key3 的值为 hello,30秒后过期OK127.0.0.1:6379&gt; ttl key3(integer) 26127.0.0.1:6379&gt; get key3&quot;hello&quot;127.0.0.1:6379&gt; setnx mykey &quot;redis&quot; # 如果mykey 不存在，创建mykey(integer) 1127.0.0.1:6379&gt; keys *1) &quot;key2&quot;2) &quot;mykey&quot;3) &quot;key1&quot;127.0.0.1:6379&gt; ttl key3(integer) -2127.0.0.1:6379&gt; setnx mykey &quot;MongoDB&quot; # 如果mykey存在，创建失败！(integer) 0127.0.0.1:6379&gt; get mykey&quot;redis&quot;##########################################################################msetmget127.0.0.1:6379&gt; mset k1 v1 k2 v2 k3 v3 # 同时设置多个值OK127.0.0.1:6379&gt; keys *1) &quot;k1&quot;2) &quot;k2&quot;3) &quot;k3&quot;127.0.0.1:6379&gt; mget k1 k2 k3 # 同时获取多个值1) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; msetnx k1 v1 k4 v4 # msetnx 是一个原子性的操作，要么一起成功，要么一起失败！(integer) 0127.0.0.1:6379&gt; get k4(nil)# 对象set user:1 {name:zhangsan,age:3} # 设置一个user:1 对象 值为 json字符来保存一个对象！# 这里的key是一个巧妙的设计： user:{id}:{filed} , 如此设计在Redis中是完全OK了！127.0.0.1:6379&gt; mset user:1:name zhangsan user:1:age 2OK127.0.0.1:6379&gt; mget user:1:name user:1:age1) &quot;zhangsan&quot;2) &quot;2&quot;##########################################################################getset # 先get然后在set127.0.0.1:6379&gt; getset db redis # 如果不存在值，则返回 nil(nil)127.0.0.1:6379&gt; get db&quot;redis127.0.0.1:6379&gt; getset db mongodb # 如果存在值，获取原来的值，并设置新的值&quot;redis&quot;127.0.0.1:6379&gt; get db&quot;mongodb&quot; 数据结构是相同的！ String类似的使用场景：value除了是我们的字符串还可以是我们的数字！ 计数器 统计多单位的数量 粉丝数 对象缓存存储！ Hash（哈希） Redis hash 是一个键值(key=&gt;value)对集合。 Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。 - DEL runoob 用于删除前面测试用过的 key，不然会报错：(error) WRONGTYPE Operation against a key holding the wrong kind of value - HMSET 设置了两个 field=&gt;value 对, HGET 获取对应 field 对应的 value。 每个 hash 可以存储 232 -1 键值对（40多亿）。 HMSET runoob field1 &quot;Hello&quot; field2 &quot;World&quot; HGET runoob field1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748Map集合，key-map! 时候这个值是一个map集合！ 本质和String类型没有太大区别，还是一个简单的key-vlaue！set myhash field kuangshen5) &quot;d&quot;##########################################################################127.0.0.1:6379&gt; hset myhash field1 kuangshen # set一个具体 key-vlaue(integer) 1127.0.0.1:6379&gt; hget myhash field1 # 获取一个字段值&quot;kuangshen&quot;127.0.0.1:6379&gt; hmset myhash field1 hello field2 world # set多个 key-vlaueOK127.0.0.1:6379&gt; hmget myhash field1 field2 # 获取多个字段值1) &quot;hello&quot;2) &quot;world&quot;127.0.0.1:6379&gt; hgetall myhash # 获取全部的数据，1) &quot;field1&quot;2) &quot;hello&quot;3) &quot;field2&quot;4) &quot;world&quot;127.0.0.1:6379&gt; hdel myhash field1 # 删除hash指定key字段！对应的value值也就消失了！(integer) 1127.0.0.1:6379&gt; hgetall myhash1) &quot;field2&quot;2) &quot;world&quot;##########################################################################hlen127.0.0.1:6379&gt; hmset myhash field1 hello field2 worldOK127.0.0.1:6379&gt; HGETALL myhash1) &quot;field2&quot;2) &quot;world&quot;3) &quot;field1&quot;4) &quot;hello&quot;127.0.0.1:6379&gt; hlen myhash # 获取hash表的字段数量！(integer) 2##########################################################################127.0.0.1:6379&gt; HEXISTS myhash field1 # 判断hash中指定字段是否存在！(integer) 1127.0.0.1:6379&gt; HEXISTS myhash field3(integer) 0########################################################################### 只获得所有field# 只获得所有value127.0.0.1:6379&gt; hkeys myhash # 只获得所有field1) &quot;field2&quot;2) &quot;field1&quot;hash变更的数据 user name age,尤其是是用户信息之类的，经常变动的信息！ hash 更适合于对象的存储，String更加适合字符串存储！ List（列表） Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 lpush runoob redis lpush runoob mongodb lrange runoob 0 10 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180基本的数据类型，列表127.0.0.1:6379&gt; msetnx k1 v1 k4 v4 # msetnx 是一个原子性的操作，要么一起成功，要么一起失败！(integer) 0127.0.0.1:6379&gt; get k4(nil)# 对象set user:1 {name:zhangsan,age:3} # 设置一个user:1 对象 值为 json字符来保存一个对象！# 这里的key是一个巧妙的设计： user:{id}:{filed} , 如此设计在Redis中是完全OK了！127.0.0.1:6379&gt; mset user:1:name zhangsan user:1:age 2OK127.0.0.1:6379&gt; mget user:1:name user:1:age1) &quot;zhangsan&quot;2) &quot;2&quot;##########################################################################getset # 先get然后在set127.0.0.1:6379&gt; getset db redis # 如果不存在值，则返回 nil(nil)127.0.0.1:6379&gt; get db&quot;redis127.0.0.1:6379&gt; getset db mongodb # 如果存在值，获取原来的值，并设置新的值&quot;redis&quot;127.0.0.1:6379&gt; get db&quot;mongodb&quot;在redis里面，我们可以把list玩成 ，栈、队列、阻塞队列！所有的list命令都是用l开头的，Redis不区分大小命令##########################################################################127.0.0.1:6379&gt; LPUSH list one # 将一个值或者多个值，插入到列表头部 （左）(integer) 1127.0.0.1:6379&gt; LPUSH list two(integer) 2127.0.0.1:6379&gt; LPUSH list three(integer) 3127.0.0.1:6379&gt; LRANGE list 0 -1 # 获取list中值！1) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;127.0.0.1:6379&gt; LRANGE list 0 1 # 通过区间获取具体的值！1) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; Rpush list righr # 将一个值或者多个值，插入到列表位部 （右）(integer) 4127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;4) &quot;righr&quot;##########################################################################LPOPRPOP127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;two&quot;3) &quot;one&quot;4) &quot;righr&quot;127.0.0.1:6379&gt; Lpop list # 移除list的第一个元素&quot;three&quot;127.0.0.1:6379&gt; Rpop list # 移除list的最后一个元素&quot;righr&quot;127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;two&quot;2) &quot;one&quot;##########################################################################Lindex127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;two&quot;2) &quot;one&quot;127.0.0.1:6379&gt; lindex list 1 # 通过下标获得 list 中的某一个值！&quot;one&quot;127.0.0.1:6379&gt; lindex list 0&quot;two&quot;##########################################################################Llen127.0.0.1:6379&gt; Lpush list one(integer) 1127.0.0.1:6379&gt; Lpush list two(integer) 2127.0.0.1:6379&gt; Lpush list three(integer) 3127.0.0.1:6379&gt; Llen list # 返回列表的长度(integer) 3##########################################################################移除指定的值！取关 uidLrem127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;three&quot;3) &quot;two&quot;4) &quot;one&quot;127.0.0.1:6379&gt; lrem list 1 one # 移除list集合中指定个数的value，精确匹配(integer) 1127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;three&quot;3) &quot;two&quot;127.0.0.1:6379&gt; lrem list 1 three(integer) 1127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;three&quot;2) &quot;two&quot;127.0.0.1:6379&gt; Lpush list three(integer) 3127.0.0.1:6379&gt; lrem list 2 three(integer) 2127.0.0.1:6379&gt; LRANGE list 0 -11) &quot;two&quot;##########################################################################trim 修剪。； list 截断!127.0.0.1:6379&gt; keys *(empty list or set)127.0.0.1:6379&gt; Rpush mylist &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; Rpush mylist &quot;hello1&quot;(integer) 2127.0.0.1:6379&gt; Rpush mylist &quot;hello2&quot;(integer) 3127.0.0.1:6379&gt; Rpush mylist &quot;hello3&quot;(integer) 4127.0.0.1:6379&gt; ltrim mylist 1 2 # 通过下标截取指定的长度，这个list已经被改变了，截断了只剩下截取的元素！OK127.0.0.1:6379&gt; LRANGE mylist 0 -11) &quot;hello1&quot;2) &quot;hello2&quot;##########################################################################rpoplpush # 移除列表的最后一个元素，将他移动到新的列表中！127.0.0.1:6379&gt; rpush mylist &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; rpush mylist &quot;hello1&quot;(integer) 2127.0.0.1:6379&gt; rpush mylist &quot;hello2&quot;(integer) 3127.0.0.1:6379&gt; rpoplpush mylist myotherlist # 移除列表的最后一个元素，将他移动到新的列表中！&quot;hello2&quot;127.0.0.1:6379&gt; lrange mylist 0 -1 # 查看原来的列表1) &quot;hello&quot;2) &quot;hello1&quot;127.0.0.1:6379&gt; lrange myotherlist 0 -1 # 查看目标列表中，确实存在改值！1) &quot;hello2&quot;##########################################################################lset 将列表中指定下标的值替换为另外一个值，更新操作127.0.0.1:6379&gt; EXISTS list # 判断这个列表是否存在(integer) 0127.0.0.1:6379&gt; lset list 0 item # 如果不存在列表我们去更新就会报错(error) ERR no such key127.0.0.1:6379&gt; lpush list value1(integer) 1127.0.0.1:6379&gt; LRANGE list 0 01) &quot;value1&quot;127.0.0.1:6379&gt; lset list 0 item # 如果存在，更新当前下标的值OK127.0.0.1:6379&gt; LRANGE list 0 01) &quot;item&quot;127.0.0.1:6379&gt; lset list 1 other # 如果不存在，则会报错！(error) ERR index out of range##########################################################################linsert # 将某个具体的value插入到列把你中某个元素的前面或者后面！127.0.0.1:6379&gt; Rpush mylist &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; Rpush mylist &quot;world&quot;(integer) 2127.0.0.1:6379&gt; LINSERT mylist before &quot;world&quot; &quot;other&quot;(integer) 3127.0.0.1:6379&gt; LRANGE mylist 0 -11) &quot;hello&quot;2) &quot;other&quot;3) &quot;world&quot;127.0.0.1:6379&gt; LINSERT mylist after world new(integer) 4127.0.0.1:6379&gt; LRANGE mylist 0 -11) &quot;hello&quot;2) &quot;other&quot;3) &quot;world&quot;4) &quot;new&quot;小结他实际上是一个链表，before Node after ， left，right 都可以插入值如果key 不存在，创建新的链表如果key存在，新增内容 Set（集合） Redis 的 Set 是 string 类型的无序集合。 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 sadd 命令 添加一个 string 元素到 key 对应的 set 集合中，成功返回 1，如果元素已经在集合中返回 0。 sadd key member 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 sadd runoob redis smembers runoob 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100set中的值是不能重读的！##########################################################################127.0.0.1:6379&gt; sadd myset &quot;hello&quot; # set集合中添加匀速(integer) 1127.0.0.1:6379&gt; sadd myset &quot;kuangshen&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;lovekuangshen&quot;(integer) 1127.0.0.1:6379&gt; SMEMBERS myset # 查看指定set的所有值1) &quot;hello&quot;2) &quot;lovekuangshen&quot;3) &quot;kuangshen&quot;127.0.0.1:6379&gt; SISMEMBER myset hello # 判断某一个值是不是在set集合中！(integer) 1127.0.0.1:6379&gt; SISMEMBER myset world(integer) 0##########################################################################127.0.0.1:6379&gt; scard myset # 获取set集合中的内容元素个数！(integer) 4##########################################################################rem127.0.0.1:6379&gt; srem myset hello # 移除set集合中的指定元素(integer) 1127.0.0.1:6379&gt; scard myset(integer) 3127.0.0.1:6379&gt; SMEMBERS myset1) &quot;lovekuangshen2&quot;2) &quot;lovekuangshen&quot;3) &quot;kuangshen&quot;##########################################################################set 无序不重复集合。抽随机！127.0.0.1:6379&gt; SMEMBERS myset1) &quot;lovekuangshen2&quot;2) &quot;lovekuangshen&quot;3) &quot;kuangshen&quot;127.0.0.1:6379&gt; SRANDMEMBER myset # 随机抽选出一个元素&quot;kuangshen&quot;127.0.0.1:6379&gt; SRANDMEMBER myset&quot;kuangshen&quot;127.0.0.1:6379&gt; SRANDMEMBER myset&quot;kuangshen&quot;127.0.0.1:6379&gt; SRANDMEMBER myset&quot;kuangshen&quot;127.0.0.1:6379&gt; SRANDMEMBER myset 2 # 随机抽选出指定个数的元素1) &quot;lovekuangshen&quot;2) &quot;lovekuangshen2&quot;127.0.0.1:6379&gt; SRANDMEMBER myset 21) &quot;lovekuangshen&quot;2) &quot;lovekuangshen2&quot;127.0.0.1:6379&gt; SRANDMEMBER myset # 随机抽选出一个元素&quot;lovekuangshen2&quot;##########################################################################删除定的key，随机删除key！127.0.0.1:6379&gt; SMEMBERS myset1) &quot;lovekuangshen2&quot;2) &quot;lovekuangshen&quot;3) &quot;kuangshen&quot;127.0.0.1:6379&gt; spop myset # 随机删除一些set集合中的元素！&quot;lovekuangshen2&quot;127.0.0.1:6379&gt; spop myset&quot;lovekuangshen&quot;127.0.0.1:6379&gt; SMEMBERS myset1) &quot;kuangshen&quot;##########################################################################将一个指定的值，移动到另外一个set集合！127.0.0.1:6379&gt; sadd myset &quot;hello&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;world&quot;(integer) 1127.0.0.1:6379&gt; sadd myset &quot;kuangshen&quot;(integer) 1127.0.0.1:6379&gt; sadd myset2 &quot;set2&quot;(integer) 1127.0.0.1:6379&gt; smove myset myset2 &quot;kuangshen&quot; # 将一个指定的值，移动到另外一个set集合！(integer) 1127.0.0.1:6379&gt; SMEMBERS myset1) &quot;world&quot;2) &quot;hello&quot;127.0.0.1:6379&gt; SMEMBERS myset21) &quot;kuangshen&quot;2) &quot;set2&quot;##########################################################################数字集合类：- 差集 SDIFF- 交集- 并集127.0.0.1:6379&gt; SDIFF key1 key2 # 差集1) &quot;b&quot;2) &quot;a&quot;127.0.0.1:6379&gt; SINTER key1 key2 # 交集 共同好友就可以这样实现1) &quot;c&quot;127.0.0.1:6379&gt; SUNION key1 key2 # 并集1) &quot;b&quot;2) &quot;c&quot;3) &quot;e&quot;4) &quot;a&quot;微博，A用户将所有关注的人放在一个set集合中！将它的粉丝也放在一个集合中！共同关注，共同爱好，二度好友，推荐好友！（六度分割理论） ZSet(有序集合) Redis zset (sorted set：有序集合)和 set 一样也是string类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd 命令 添加元素到集合，元素在集合中存在则更新对应score zadd key score member zadd runoob 0 redis ZRANGEBYSCORE runoob 0 1000 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879在set的基础上，增加了一个值，set k1 v1 zset k1 score1 v1127.0.0.1:6379&gt; hvals myhash # 只获得所有value1) &quot;world&quot;2) &quot;hello&quot;##########################################################################incr decr127.0.0.1:6379&gt; hset myhash field3 5 #指定增量！(integer) 1127.0.0.1:6379&gt; HINCRBY myhash field3 1(integer) 6127.0.0.1:6379&gt; HINCRBY myhash field3 -1(integer) 5127.0.0.1:6379&gt; hsetnx myhash field4 hello # 如果不存在则可以设置(integer) 1127.0.0.1:6379&gt; hsetnx myhash field4 world # 如果存在则不能设置(integer) 0127.0.0.1:6379&gt; zadd myset 1 one # 添加一个值(integer) 1127.0.0.1:6379&gt; zadd myset 2 two 3 three # 添加多个值(integer) 2127.0.0.1:6379&gt; ZRANGE myset 0 -11) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;##########################################################################排序如何实现127.0.0.1:6379&gt; zadd salary 2500 xiaohong # 添加三个用户(integer) 1127.0.0.1:6379&gt; zadd salary 5000 zhangsan(integer) 1127.0.0.1:6379&gt; zadd salary 500 kaungshen(integer) 1# ZRANGEBYSCORE key min max127.0.0.1:6379&gt; ZRANGEBYSCORE salary -inf +inf # 显示全部的用户 从小到大！1) &quot;kaungshen&quot;2) &quot;xiaohong&quot;3) &quot;zhangsan&quot;127.0.0.1:6379&gt; ZREVRANGE salary 0 -1 # 从大到进行排序！1) &quot;zhangsan&quot;2) &quot;kaungshen&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE salary -inf +inf withscores # 显示全部的用户并且附带成绩1) &quot;kaungshen&quot;2) &quot;500&quot;3) &quot;xiaohong&quot;4) &quot;2500&quot;5) &quot;zhangsan&quot;6) &quot;5000&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE salary -inf 2500 withscores # 显示工资小于2500员工的升序排序！1) &quot;kaungshen&quot;2) &quot;500&quot;3) &quot;xiaohong&quot;4) &quot;2500&quot;########################################################################### 移除rem中的元素127.0.0.1:6379&gt; zrange salary 0 -11) &quot;kaungshen&quot;2) &quot;xiaohong&quot;3) &quot;zhangsan&quot;127.0.0.1:6379&gt; zrem salary xiaohong # 移除有序集合中的指定元素(integer) 1127.0.0.1:6379&gt; zrange salary 0 -11) &quot;kaungshen&quot;2) &quot;zhangsan&quot;127.0.0.1:6379&gt; zcard salary # 获取有序集合中的个数(integer) 2##########################################################################127.0.0.1:6379&gt; zadd myset 1 hello(integer) 1127.0.0.1:6379&gt; zadd myset 2 world 3 kuangshen(integer) 2127.0.0.1:6379&gt; zcount myset 1 3 # 获取指定区间的成员数量！(integer) 3127.0.0.1:6379&gt; zcount myset 1 2(integer) 2案例思路：set 排序 存储班级成绩表，工资表排序！普通消息，1， 重要消息 2，带权重进行判断！排行榜应用实现，取Top N 测试！ Geospatial 地理位置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118朋友的定位，附近的人，打车距离计算？Redis 的 Geo 在Redis3.2 版本就推出了！ 这个功能可以推算地理位置的信息，两地之间的距离，方圆几里的人！可以查询一些测试数据：http://www.jsons.cn/lngcodeinfo/0706D99C19A781A3/只有 六个命令：4) &quot;2500&quot;5) &quot;zhangsan&quot;6) &quot;5000&quot;127.0.0.1:6379&gt; ZRANGEBYSCORE salary -inf 2500 withscores # 显示工资小于2500员工的升序排序！1) &quot;kaungshen&quot;2) &quot;500&quot;3) &quot;xiaohong&quot;4) &quot;2500&quot;########################################################################### 移除rem中的元素127.0.0.1:6379&gt; zrange salary 0 -11) &quot;kaungshen&quot;2) &quot;xiaohong&quot;3) &quot;zhangsan&quot;127.0.0.1:6379&gt; zrem salary xiaohong # 移除有序集合中的指定元素(integer) 1127.0.0.1:6379&gt; zrange salary 0 -11) &quot;kaungshen&quot;2) &quot;zhangsan&quot;127.0.0.1:6379&gt; zcard salary # 获取有序集合中的个数(integer) 2##########################################################################127.0.0.1:6379&gt; zadd myset 1 hello(integer) 1127.0.0.1:6379&gt; zadd myset 2 world 3 kuangshen(integer) 2127.0.0.1:6379&gt; zcount myset 1 3 # 获取指定区间的成员数量！(integer) 3127.0.0.1:6379&gt; zcount myset 1 2(integer) 2官方文档：https://www.redis.net.cn/order/3685.htmlgetaddgetpos获得当前定位：一定是一个坐标值！GEODIST# getadd 添加地理位置# 规则：两级无法直接添加，我们一般会下载城市数据，直接通过java程序一次性导入！# 有效的经度从-180度到180度。# 有效的纬度从-85.05112878度到85.05112878度。# 当坐标位置超出上述指定范围时，该命令将会返回一个错误。# 127.0.0.1:6379&gt; geoadd china:city 39.90 116.40 beijin(error) ERR invalid longitude,latitude pair 39.900000,116.400000# 参数 key 值（）127.0.0.1:6379&gt; geoadd china:city 116.40 39.90 beijing(integer) 1127.0.0.1:6379&gt; geoadd china:city 121.47 31.23 shanghai(integer) 1127.0.0.1:6379&gt; geoadd china:city 106.50 29.53 chongqi 114.05 22.52 shengzhen(integer) 2127.0.0.1:6379&gt; geoadd china:city 120.16 30.24 hangzhou 108.96 34.26 xian(integer) 2127.0.0.1:6379&gt; GEOPOS china:city beijing # 获取指定的城市的经度和纬度！1) 1) &quot;116.39999896287918091&quot;2) &quot;39.90000009167092543&quot;127.0.0.1:6379&gt; GEOPOS china:city beijing chongqi1) 1) &quot;116.39999896287918091&quot;2) &quot;39.90000009167092543&quot;2) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;两人之间的距离！单位：m 表示单位为米。km 表示单位为千米。mi 表示单位为英里。ft 表示单位为英尺。georadius 以给定的经纬度为中心， 找出某一半径内的元素我附近的人？ （获得所有附近的人的地址，定位！）通过半径来查询！获得指定数量的人，200所有数据应该都录入：china:city ，才会让结果更加请求！127.0.0.1:6379&gt; GEODIST china:city beijing shanghai km # 查看上海到北京的直线距离&quot;1067.3788&quot;127.0.0.1:6379&gt; GEODIST china:city beijing chongqi km # 查看重庆到北京的直线距离&quot;1464.0708&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 1000 km # 以110，30 这个经纬度为中心，寻找方圆1000km内的城市1) &quot;chongqi&quot;2) &quot;xian&quot;3) &quot;shengzhen&quot;4) &quot;hangzhou&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km1) &quot;chongqi&quot;2) &quot;xian&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withdist # 显示到中间距离的位置1) 1) &quot;chongqi&quot;2) &quot;341.9374&quot;2) 1) &quot;xian&quot;2) &quot;483.8340&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withcoord # 显示他人的定位信息1) 1) &quot;chongqi&quot;2) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;2) 1) &quot;xian&quot;2) 1) &quot;108.96000176668167114&quot;2) &quot;34.25999964418929977&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withdist withcoord count 1 #筛选出指定的结果！1) 1) &quot;chongqi&quot;2) &quot;341.9374&quot;3) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;127.0.0.1:6379&gt; GEORADIUS china:city 110 30 500 km withdist withcoord count 21) 1) &quot;chongqi&quot;2) &quot;341.9374&quot;3) 1) &quot;106.49999767541885376&quot;2) &quot;29.52999957900659211&quot;2) 1) &quot;xian&quot;2) &quot;483.8340&quot;3) 1) &quot;108.96000176668167114&quot;2) &quot;34.25999964418929977&quot;GEORADIUSBYMEMBERGEOHASH 命令 - 返回一个或多个位置元素的 Geohash 表示该命令将返回11个字符的Geohash字符串!GEO 底层的实现原理其实就是 Zset！我们可以使用Zset命令来操作geo！ Hyperloglog 什么是基数？ A B 基数（不重复的元素） = 5，可以接受误差！ 简介 Redis 2.8.9 版本就更新了 Hyperloglog 数据结构！ Redis Hyperloglog 基数统计的算法！ 优点：占用的内存是固定，2^64 不同的元素的技术，只需要废 12KB内存！如果要从内存角度来比较的话 Hyperloglog 首选！ 网页的 UV （一个人访问一个网站多次，但是还是算作一个人！） 传统的方式， set 保存用户的id，然后就可以统计 set 中的元素数量作为标准判断 ! 这个方式如果保存大量的用户id，就会比较麻烦！我们的目的是为了计数，而不是保存用户id； 0.81% 错误率！ 统计UV任务，可以忽略不计的！ 测试使用 1234567891011121314127.0.0.1:6379&gt; PFadd mykey a b c d e f g h i j # 创建第一组元素 mykey(integer) 1127.0.0.1:6379&gt; PFCOUNT mykey # 统计 mykey 元素的基数数量(integer) 10127.0.0.1:6379&gt; PFadd mykey2 i j z x c v b n m # 创建第二组元素 mykey2(integer) 1127.0.0.1:6379&gt; PFCOUNT mykey2(integer) 9127.0.0.1:6379&gt; PFMERGE mykey3 mykey mykey2 # 合并两组 mykey mykey2 =&gt; mykey3 并集OK127.0.0.1:6379&gt; PFCOUNT mykey3 # 看并集的数量！(integer) 15如果允许容错，那么一定可以使用 Hyperloglog ！如果不允许容错，就使用 set 或者自己的数据类型即可！ Bitmap 位存储 统计用户信息，活跃，不活跃！ 登录 、 未登录！ 打卡，365打卡！ 两个状态的，都可以使用 Bitmaps！ Bitmap 位图，数据结构！ 都是操作二进制位来进行记录，就只有0 和 1 两个状态！ 365 天 = 365 bit 1字节 = 8bit 46 个字节左右！ Redis 命令 Redis 命令 语法 Redis 客户端的基本语法为： $ redis-cli - 以下实例讲解了如何启动 redis 客户端： 启动 redis 服务器，打开终端并输入命令 redis-cli，该命令会连接本地的 redis 服务。 $ redis-cli redis 127.0.0.1:6379&gt; redis 127.0.0.1:6379&gt; PING PONG 在以上实例中我们连接到本地的 redis 服务并执行 PING 命令，该命令用于检测 redis 服务是否启动。 在远程服务上执行命令 $ redis-cli -h host -p port -a password Redis keys 命令大全 https://redis.io/commands Redis HyperLogLog Redis 在 2.8.9 版本添加了 HyperLogLog 结构。 Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 什么是基数? 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 PFADD runoobkey &quot;redis&quot; PFCOUNT runoobkey Redis 发布订阅 Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 订阅/发布消息图： 第一个：消息发送者， 第二个：频道 第三个：消息订阅者！ 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的 关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户 端： 命令 这些命令被广泛用于构建即时通信应用，比如网络聊天室(chatroom)和实时广播、实时提醒等 测试 123456789101112131415161718192021订阅端：127.0.0.1:6379&gt; SUBSCRIBE kuangshenshuo # 订阅一个频道 kuangshenshuoReading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;kuangshenshuo&quot;3) (integer) 1# 等待读取推送的信息1) &quot;message&quot; # 消息2) &quot;kuangshenshuo&quot; # 那个频道的消息3) &quot;hello,kuangshen&quot; # 消息的具体内容1) &quot;message&quot;2) &quot;kuangshenshuo&quot;3) &quot;hello,redis&quot;发送端：127.0.0.1:6379&gt; PUBLISH kuangshenshuo &quot;hello,kuangshen&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; PUBLISH kuangshenshuo &quot;hello,redis&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; 原理 Redis是使用C实现的，通过分析 Redis 源码里的 pubsub.c 文件，了解发布和订阅机制的底层实现，借此加深对 Redis 的理解。 Redis 通过 PUBLISH 、SUBSCRIBE 和 PSUBSCRIBE 等命令实现发布和订阅功能。 微信： 通过 SUBSCRIBE 命令订阅某频道后，redis-server 里维护了一个字典，字典的键就是一个个 频道！， 而字典的值则是一个链表，链表中保存了所有订阅这个 channel 的客户端。SUBSCRIBE 命令的关键， 就是将客户端添加到给定 channel 的订阅链表中。 通过 PUBLISH 命令向订阅者发送消息，redis-server 会使用给定的频道作为键，在它所维护的 channel 字典中查找记录了订阅这个频道的所有客户端的链表，遍历这个链表，将消息发布给所有订阅者。Pub/Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个 key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应 的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 使用场景： 1、实时消息系统！ 2、事实聊天！（频道当做聊天室，将信息回显给所有人即可！） 3、订阅，关注系统都是可以的！ 稍微复杂的场景我们就会使用 消息中间件 MQ （） SUBSCRIBE runoobChat PUBLISH runoobChat &quot;Redis PUBLISH test&quot; 重新开启个 redis 客户端在同一个频道 runoobChat 发布两次消息，订阅者就能接收到消息。 Redis 事务(不回滚，批量执行作用) Redis 事务可以一次执行多个命令， 并且带有以下三个重要的保证： 批量操作在发送 EXEC 命令前被放入队列缓存。 收到 EXEC 命令后进入事务执行，事务中任意命令执行失败，其余的命令依然被执行,代码错误，所有命令都不会执行。 在事务执行过程，其他客户端提交的命令请求不会插入到事务执行命令序列中。 DISCARD放弃事物，所有不执行一个事务从开始到执行会经历以下三个阶段： 开始事务。 命令入队。 执行事务。 MULTI 开始一个事务 将多个命令入队到事务中 SET book-name &quot;Mastering C++ in 21 days&quot; GET book-name SADD tag &quot;C++&quot; &quot;Programming&quot; &quot;Mastering Series&quot; SMEMBERS tag EXEC/DISCARD 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 事务可以理解为一个打包的批量执行脚本，但批量指令并非原子化的操作，中间某条指令的失败不会导致前面已做指令的回滚，也不会造成后续的指令不做。 悲观锁： 很悲观，认为什么时候都会出问题，无论做什么都会加锁！ 乐观锁： 很乐观，认为什么时候都不会出问题，所以不会上锁！ 更新数据的时候去判断一下，在此期间是否有人修改过这个数据， 获取version 更新的时候比较 version Redis测监视测试 正常执行成功！ 测试多线程修改值 , 使用watch 可以当做redis的乐观锁操作！ 123456789101112131415127.0.0.1:6379&gt; set money 100OK127.0.0.1:6379&gt; set out 0OK127.0.0.1:6379&gt; watch money # 监视 money 对象OK127.0.0.1:6379&gt; multi # 事务正常结束，数据期间没有发生变动，这个时候就正常执行成功！OK127.0.0.1:6379&gt; DECRBY money 20QUEUED127.0.0.1:6379&gt; INCRBY out 20QUEUED127.0.0.1:6379&gt; exec1) (integer) 802) (integer) 20 测试多线程修改值 , 使用watch 可以当做redis的乐观锁操作！ 1234567891011127.0.0.1:6379&gt; watch money # 监视 moneyOK127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; DECRBY money 10QUEUED127.0.0.1:6379&gt; INCRBY out 10QUEUED127.0.0.1:6379&gt; exec # 执行之前，另外一个线程，修改了我们的值，这个时候，就会导致事务执行失败！(nil) Redis 脚本 Redis 脚本使用 Lua 解释器来执行脚本。 Redis 2.6 版本通过内嵌支持 Lua 环境。执行脚本的常用命令为 EVAL。 EVAL script numkeys key [key ...] arg [arg ...] EVAL &quot;return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}&quot; 2 key1 key2 first second Redis 连接 Redis 连接命令主要是用于连接 redis 服务。 客户端如何通过密码验证连接到 redis 服务，并检测服务是否在运行： AUTH &quot;password&quot; PING 验证密码是否正确 1 AUTH password 打印字符串 2 ECHO message 查看服务是否运行 3 PING 关闭当前连接 4 QUIT 切换到指定的数据库 5 SELECT index Redis服务器 Redis服务器命令主要是用于管理redis服务 Redis GEO Redis GEO 主要用于存储地理位置信息，并对存储的信息进行操作，该功能在 Redis 3.2 版本新增。 Redis GEO 操作方法有： geoadd：添加地理位置的坐标。 geopos：获取地理位置的坐标。 geodist：计算两个位置之间的距离。 georadius：根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。 georadiusbymember：根据储存在位置集合里面的某个地点获取指定范围内的地理位置集合。 geohash：返回一个或多个位置对象的 geohash 值。 geoadd geoadd 用于存储指定的地理空间位置，可以将一个或多个经度(longitude)、纬度(latitude)、位置名称(member)添加到指定的 key 中。 geoadd 语法格式如下： GEOADD key longitude latitude member [longitude latitude member ...] - GEOADD Sicily 13.361389 38.115556 &quot;Palermo&quot; 15.087269 37.502669 &quot;Catania&quot; - GEODIST Sicily Palermo Catania - geodist 用于返回两个给定位置之间的距离。 geodist 语法格式如下： GEODIST key member1 member2 [m|km|ft|mi] member1 member2 为两个地理位置。 最后一个距离单位参数说明： m ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 - GEORADIUS Sicily 15 37 100 km - georadius 以给定的经纬度为中心， 返回键包含的位置元素当中， 与中心的距离不超过给定最大距离的所有位置元素。 georadiusbymember 和 GEORADIUS 命令一样， 都可以找出位于指定范围内的元素， 但是 georadiusbymember 的中心点是由给定的位置元素决定的， 而不是使用经度和纬度来决定中心点。 georadius 与 georadiusbymember 语法格式如下： GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key] 参数说明： m ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 WITHDIST: 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 WITHCOORD: 将位置元素的经度和维度也一并返回。 WITHHASH: 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 COUNT 限定返回的记录数。 ASC: 查找结果根据距离从近到远排序。 DESC: 查找结果根据从远到近排序。 - GEOPOS Sicily Palermo Catania NonExisting - geopos 用于从给定的 key 里返回所有指定名称(member)的位置（经度和纬度），不存在的返回 nil。 geopos 语法格式如下： GEOPOS key member [member ...] - GEOHASH Sicily Palermo Catania - geohash Redis GEO 使用 geohash 来保存地理位置的坐标。 geohash 用于获取一个或多个位置元素的 geohash 值。 geohash 语法格式如下： GEOHASH key member [member ...] Redis Stream Redis Stream 是 Redis 5.0 版本新增加的数据结构。 Redis Stream 主要用于消息队列（MQ，Message Queue），Redis 本身是有一个 Redis 发布订阅 (pub/sub) 来实现消息队列的功能，但它有个缺点就是消息无法持久化，如果出现网络断开、Redis 宕机等，消息就会被丢弃。 简单来说发布订阅 (pub/sub) 可以分发消息，但无法记录历史消息。 而 Redis Stream 提供了消息的持久化和主备复制功能，可以让任何客户端访问任何时刻的数据，并且能记住每一个客户端的访问位置，还能保证消息不丢失。 Redis Stream 的结构如下所示，它有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容： 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。 上图解析： Consumer Group ：消费组，使用 XGROUP CREATE 命令创建，一个消费组有多个消费者(Consumer)。 last_delivered_id ：游标，每个消费组会有个游标 last_delivered_id，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动。 pending_ids ：消费者(Consumer)的状态变量，作用是维护消费者的未确认的 id。 pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack (Acknowledge character：确认字符）。 - 消息队列相关命令： XADD - 添加消息到末尾 XTRIM - 对流进行修剪，限制长度 XDEL - 删除消息 XLEN - 获取流包含的元素数量，即消息长度 XRANGE - 获取消息列表，会自动过滤已经删除的消息 XREVRANGE - 反向获取消息列表，ID 从大到小 XREAD - 以阻塞或非阻塞方式获取消息列表 消费者组相关命令： XGROUP CREATE - 创建消费者组 XREADGROUP GROUP - 读取消费者组中的消息 XACK - 将消息标记为&quot;已处理&quot; XGROUP SETID - 为消费者组设置新的最后递送消息ID XGROUP DELCONSUMER - 删除消费者 XGROUP DESTROY - 删除消费者组 XPENDING - 显示待处理消息的相关信息 XCLAIM - 转移消息的归属权 XINFO - 查看流和消费者组的相关信息； XINFO GROUPS - 打印消费者组的信息； XINFO STREAM - 打印流信息 XADD 使用 XADD 向队列添加消息，如果指定的队列不存在，则创建一个队列，XADD 语法格式： XADD key ID field value [field value ...] key ：队列名称，如果不存在就创建 ID ：消息 id，我们使用 * 表示由 redis 生成，可以自定义，但是要自己保证递增性。 field value ： 记录。 - XADD mystream * name Sara surname OConnor - XADD mystream * field1 value1 field2 value2 field3 value3 - XLEN mystream - XRANGE mystream - + XTRIM 使用 XTRIM 对流进行修剪，限制长度， 语法格式： XTRIM key MAXLEN [~] count key ：队列名称 MAXLEN ：长度 count ：数量 - XADD mystream * field1 A field2 B field3 C field4 D - XTRIM mystream MAXLEN 2 - XRANGE mystream - + XDEL 使用 XDEL 删除消息，语法格式： XDEL key ID [ID ...] key：队列名称 ID ：消息 ID 使用 XDEL 删除消息，语法格式： XLEN 使用 XLEN 获取流包含的元素数量，即消息长度，语法格式： XLEN key key：队列名称 - XADD mystream * item 1 - XLEN mystream XRANGE 使用 XRANGE 获取消息列表，会自动过滤已经删除的消息 ，语法格式： XRANGE key start end [COUNT count] key ：队列名 start ：开始值， - 表示最小值 end ：结束值， + 表示最大值 count ：数量 - XADD writers * name Ngozi surname Adichie - XLEN writers - XRANGE writers - + COUNT 2 XREVRANGE 使用 XREVRANGE 获取消息列表，会自动过滤已经删除的消息 ，语法格式： XREVRANGE key end start [COUNT count] key ：队列名 end ：结束值， + 表示最大值 start ：开始值， - 表示最小值 count ：数量 - XADD writers * name Virginia surname Woolf - XLEN writers - XREVRANGE writers + - COUNT 1 XREAD 使用 XREAD 以阻塞或非阻塞方式获取消息列表 ，语法格式： XREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key ...] id [id ...] count ：数量 milliseconds ：可选，阻塞毫秒数，没有设置就是非阻塞模式 key ：队列名 id ：消息 ID - # 从 Stream 头部读取两条消息 XREAD COUNT 2 STREAMS mystream writers 0-0 0-0 XGROUP CREATE 使用 XGROUP CREATE 创建消费者组，语法格式： XGROUP [CREATE key groupname id-or-$] [SETID key groupname id-or-$] [DESTROY key groupname] [DELCONSUMER key groupname consumername] key ：队列名称，如果不存在就创建 groupname ：组名。 $ ： 表示从尾部开始消费，只接受新消息，当前 Stream 消息会全部忽略。 从头开始消费: XGROUP CREATE mystream consumer-group-name 0-0 从尾部开始消费: XGROUP CREATE mystream consumer-group-name $ XREADGROUP GROUP 使用 XREADGROUP GROUP 读取消费组中的消息，语法格式： XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...] group ：消费组名 consumer ：消费者名。 count ： 读取数量。 milliseconds ： 阻塞毫秒数。 key ： 队列名。 ID ： 消息 ID。 XREADGROUP GROUP consumer-group-name consumer-name COUNT 1 STREAMS mystream &gt; Redis 高级教程 Redis主从复制 概念 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点 (master/leader)，后者称为从节点(slave/follower)；数据的复制是单向的，只能由主节点到从节点。 Master以写为主，Slave 以读为主。 默认情况下，每台Redis服务器都是主节点； 且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。 主从复制的作用： 1、数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 2、故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务 的冗余。 3、负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务 （即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写 少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 4、高可用（集群）基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复 制是Redis高可用的基础。 一般来说，要将Redis运用于工程项目中，只使用一台Redis是万万不能的（宕机），原因如下： 1、从结构上，单个Redis服务器会发生单点故障，并且一台服务器需要处理所有的请求负载，压力较 大； 2、从容量上，单个Redis服务器内存容量有限，就算一台Redis服务器内存容量为256G，也不能将所有 内存用作Redis存储内存，一般来说，单台Redis最大使用内存不应该超过20G。 电商网站上的商品，一般都是一次上传，无数次浏览的，说专业点也就是&quot;多读少写&quot;。 对于这种场景，我们可以使如下这种架构： 主从复制，读写分离！ 80% 的情况下都是在进行读操作！减缓服务器的压力！架构中经常使用！ 一主二从！ 只要在公司中，主从复制就是必须要使用的，因为在真实的项目中不可能单机使用Redis！ 环境配置 只配置从库，不用配置主库！ 123456789101112127.0.0.1:6379&gt; info replication # 查看当前库的信息# Replicationrole:master # 角色 masterconnected_slaves:0 # 没有从机master_replid:b63c90e6c501143759cb0e7f450bd1eb0c70882amaster_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0 复制3个配置文件，然后修改对应的信息 1、端口 2、pid 名字 3、log文件名字 4、dump.rdb 名字 修改完毕之后，启动我们的3个redis服务器，可以通过进程信息查看！ 一主二从 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152默认情况下，每台Redis服务器都是主节点； 我们一般情况下只用配置从机就好了！认老大！ 一主 （79）二从（80，81）127.0.0.1:6379&gt; info replication # 查看当前库的信息# Replicationrole:master # 角色 masterconnected_slaves:0 # 没有从机master_replid:b63c90e6c501143759cb0e7f450bd1eb0c70882amaster_replid2:0000000000000000000000000000000000000000master_repl_offset:0second_repl_offset:-1repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0127.0.0.1:6380&gt; SLAVEOF 127.0.0.1 6379 # SLAVEOF host 6379 找谁当自己的老大！OK127.0.0.1:6380&gt; info replication# Replicationrole:slave # 当前角色是从机master_host:127.0.0.1 # 可以的看到主机的信息master_port:6379master_link_status:upmaster_last_io_seconds_ago:3master_sync_in_progress:0slave_repl_offset:14slave_priority:100slave_read_only:1connected_slaves:0master_replid:a81be8dd257636b2d3e7a9f595e69d73ff03774emaster_replid2:0000000000000000000000000000000000000000master_repl_offset:14second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:14# 在主机中查看！127.0.0.1:6379&gt; info replication# Replicationrole:masterconnected_slaves:1 # 多了从机的配置slave0:ip=127.0.0.1,port=6380,state=online,offset=42,lag=1 # 多了从机的配置master_replid:a81be8dd257636b2d3e7a9f595e69d73ff03774emaster_replid2:0000000000000000000000000000000000000000master_repl_offset:42second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:42如果两个都配置完了，就是有两个从机的真实的从主配置应该在配置文件中配置，这样的话是永久的，我们这里使用的是命令，暂时的！ 细节 主机可以写，从机不能写只能读！主机中的所有信息和数据，都会自动被从机保存！ 测试：主机断开连接，从机依旧连接到主机的，但是没有写操作，这个时候，主机如果回来了，从机依旧可以直接获取到主机写的信息！ 如果是使用命令行，来配置的主从，这个时候如果重启了，就会变回主机！只要变为从机，立马就会从主机中获取值！ 复制原理 Slave 启动成功连接到 master 后会发送一个sync同步命令 Master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行 完毕之后，master将传送整个数据文件到slave，并完成一次完全同步。 全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master 继续将新的所有收集到的修改命令依次传给slave，完成同步 但是只要是重新连接master，一次完全同步（全量复制）将被自动执行！ 我们的数据一定可以在从机中 看到！ 层层链路 上一个M链接下一个 S！这时候也可以完成我们的主从复制！如果没有老大了，这个时候能不能选择一个老大出来呢？ 手动！谋朝篡位 如果主机断开了连接，我们可以使用 SLAVEOF no one 让自己变成主机！其他的节点就可以手动连 接到最新的这个主节点（手动）！如果这个时候老大修复了，那就重新连接！ 哨兵模式 概述 （自动选举老大的模式） 主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工 干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑 哨兵模式。Redis从2.8开始正式提供了Sentinel（哨兵） 架构来解决这个问题。 谋朝篡位的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库。 哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独 立运行。其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。 这里的哨兵有两个作用 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。 当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服 务器，修改配置文件，让它们切换主机。 然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。 各个哨兵之间还会进行监控，这样就形成了多哨兵模式。 假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认 为主服务器不可用，这个现象成为主观下线。当后面的哨兵也检测到主服务器不可用，并且数量达到一 定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover[故障转移]操作。 切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为 客观下线。 测试 我们目前的状态是 一主二从！ 1、配置哨兵配置文件 sentinel.conf 12# sentinel monitor 被监控的名称 host port 1sentinel monitor myredis 127.0.0.1 6379 1 后面的这个数字1，代表主机挂了，slave投票看让谁接替成为主机，票数最多的，就会成为主机！ 2、启动哨兵！ 123456789101112131415161718192021222324252627282930313233[root@kuangshen bin]# redis-sentinel kconfig/sentinel.conf26607:X 31 Mar 2020 21:13:10.027 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo26607:X 31 Mar 2020 21:13:10.027 # Redis version=5.0.8, bits=64,commit=00000000, modified=0, pid=26607, just started26607:X 31 Mar 2020 21:13:10.027 # Configuration loaded_.__.-``__ ''-.__.-`` `. `_. ''-._ Redis 5.0.8 (00000000/0) 64 bit.-`` .-```. ```\\/ _.,_ ''-._( ' , .-` | `, ) Running in sentinel mode|`-._`-...-` __...-.``-._|'` _.-'| Port: 26379| `-._ `._ / _.-' | PID: 26607`-._ `-._ `-./ _.-' _.-'|`-._`-._ `-.__.-' _.-'_.-'|| `-._`-._ _.-'_.-' | http://redis.io`-._ `-._`-.__.-'_.-' _.-'|`-._`-._ `-.__.-' _.-'_.-'|| `-._`-._ _.-'_.-' |`-._ `-._`-.__.-'_.-' _.-'`-._ `-.__.-' _.-'`-._ _.-'`-.__.-'26607:X 31 Mar 2020 21:13:10.029 # WARNING: The TCP backlog setting of 511cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower valueof 128.26607:X 31 Mar 2020 21:13:10.031 # Sentinel ID is4c780da7e22d2aebe3bc20c333746f202ce7299626607:X 31 Mar 2020 21:13:10.031 # +monitor master myredis 127.0.0.1 6379 quorum126607:X 31 Mar 2020 21:13:10.031 * +slave slave 127.0.0.1:6380 127.0.0.1 6380 @myredis 127.0.0.1 637926607:X 31 Mar 2020 21:13:10.033 * +slave slave 127.0.0.1:6381 127.0.0.1 6381 @myredis 127.0.0.1 6379 如果Master 节点断开了，这个时候就会从从机中随机选择一个服务器！ （这里面有一个投票算法！） 哨兵模式 优点： 1、哨兵集群，基于主从复制模式，所有的主从配置优点，它全有 2、 主从可以切换，故障可以转移，系统的可用性就会更好 3、哨兵模式就是主从模式的升级，手动到自动，更加健壮！ 缺点： 1、Redis 不好啊在线扩容的，集群容量一旦到达上限，在线扩容就十分麻烦！ 2、实现哨兵模式的配置其实是很麻烦的，里面有很多选择！ 哨兵模式的全部配置！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Example sentinel.conf# 哨兵sentinel实例运行的端口 默认26379port 26379# 哨兵sentinel的工作目录dir /tmp# 哨兵sentinel监控的redis主节点的 ip port# master-name 可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符&quot;.-_&quot;组成。# quorum 配置多少个sentinel哨兵统一认为master主节点失联 那么这时客观上认为主节点失联了# sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt;sentinel monitor mymaster 127.0.0.1 6379 2# 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass mymaster MySUPER--secret-0123passw0rd# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒# sentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt;sentinel down-after-milliseconds mymaster 30000# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。# sentinel parallel-syncs &lt;master-name&gt; &lt;numslaves&gt;sentinel parallel-syncs mymaster 1# 故障转移的超时时间 failover-timeout 可以用在以下这些方面：#1. 同一个sentinel对同一个master两次failover之间的间隔时间。#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。#3.当想要取消一个正在进行的failover所需要的时间。#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了# 默认三分钟# sentinel failover-timeout &lt;master-name&gt; &lt;milliseconds&gt;sentinel failover-timeout mymaster 180000# SCRIPTS EXECUTION#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。#对于脚本的运行结果有以下规则：#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，一个是事件的类型，一个是事件的描述。如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。#通知脚本# shell编程# sentinel notification-script &lt;master-name&gt; &lt;script-path&gt;sentinel notification-script mymaster /var/redis/notify.sh# 客户端重新配置主节点参数脚本# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。# 以下参数将会在调用脚本时传给脚本:# &lt;master-name&gt; &lt;role&gt; &lt;state&gt; &lt;from-ip&gt; &lt;from-port&gt; &lt;to-ip&gt; &lt;to-port&gt;# 目前&lt;state&gt;总是“failover”,# &lt;role&gt;是“leader”或者“observer”中的一个。# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的# 这个脚本应该是通用的，能被多次调用，不是针对性的。# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt;sentinel client-reconfig-script mymaster /var/redis/reconfig.sh # 一般都是由运维来配置！ Redis缓存穿透和雪崩 服务的高可用问题！ 在这里我们不会详细的区分析解决方案的底层！ Redis缓存的使用，极大的提升了应用程序的性能和效率，特别是数据查询方面。但同时，它也带来了一 些问题。其中，最要害的问题，就是数据的一致性问题，从严格意义上讲，这个问题无解。如果对数据 的一致性要求很高，那么就不能使用缓存。 另外的一些典型问题就是，缓存穿透、缓存雪崩和缓存击穿。目前，业界也都有比较流行的解决方案。 缓存穿透（查不到） 概念 **【查不到的数据去查数据库】**缓存穿透的概念很简单，用户想要查询一个数据，发现redis内存数据库没有，也就是缓存没有命中，于 是向持久层数据库查询。发现也没有，于是本次查询失败。当用户很多的时候，缓存都没有命中（秒杀），于是都去请求了持久层数据库。这会给持久层数据库造成很大的压力，这时候就相当于出现了 缓存穿透。 解决方案 布隆过滤器 布隆过滤器是一种数据结构，对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则 丢弃，从而避免了对底层存储系统的查询压力； 缓存空对象 当存储层不命中后，即使返回的空对象也将其缓存起来，同时会设置一个过期时间，之后再访问这个数 据将会从缓存中获取，保护了后端数据源； 但是这种方法会存在两个问题： 1、如果空值能够被缓存起来，这就意味着缓存需要更多的空间存储更多的键，因为这当中可能会有很多 的空值的键； 2、即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于 需要保持一致性的业务会有影响。 解决： 查询前先做规则校验，不合法的数据直接拦截返回 查询数据库没有数据也写一个 NULL 值到缓存里面并设置一个过期时间(不要超过1分钟，避免正常情况下也不能使用)，下一次查询在缓存失效前就能命中缓存直接返回 使用布隆过滤器，利用高效的数据结构和算法快速判断出这个 Key 是否在数据库中存在（需要提前将数据库所有数据放入到布隆过滤器的集合上，且布隆过滤器最大缺点就是无法删除数据。替换方案：布谷鸟过滤器） 缓存击穿 概述 **【过频繁访问热点数据】**这里需要注意和缓存击穿的区别，缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中 对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一 个屏障上凿开了一个洞。 当某个key在过期的瞬间，有大量的请求并发访问，这类数据一般是热点数据，由于缓存过期，会同时访 问数据库来查询最新数据，并且回写缓存，会导使数据库瞬间压力过大。 解决方案 设置热点数据永不过期 从缓存层面来看，没有设置过期时间，所以不会出现热点 key 过期后产生的问题。 加互斥锁 分布式锁：使用分布式锁，保证对于每个key同时只有一个线程去查询后端服务，其他线程没有获得分布 式锁的权限，因此只需要等待即可。这种方式将高并发的压力转移到了分布式锁，因此对分布式锁的考 验很大。 解决： 热点数据可以设置永不过期 增加分布式锁，等待抢到锁的请求构建完缓存后再释放锁。其他未抢到锁的请求进行阻塞等待，被唤醒后重新请求缓存获取数据 缓存雪崩 概念 **【大量失效】**缓存雪崩，是指在某一个时间段，缓存集中过期失效。Redis 宕机！ 产生雪崩的原因之一，比如在写本文的时候，马上就要到双十二零点，很快就会迎来一波抢购，这波商 品时间比较集中的放入了缓存，假设缓存一个小时。那么到了凌晨一点钟的时候，这批商品的缓存就都 过期了。而对这批商品的访问查询，都落到了数据库上，对于数据库而言，就会产生周期性的压力波峰。于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会挂掉的情况。 其实集中过期，倒不是非常致命，比较致命的缓存雪崩，是缓存服务器某个节点宕机或断网。因为自然 形成的缓存雪崩，一定是在某个时间段集中创建缓存，这个时候，数据库也是可以顶住压力的。无非就 是对数据库产生周期性的压力而已。而缓存服务节点的宕机，对数据库服务器造成的压力是不可预知 的，很有可能瞬间就把数据库压垮。 解决方案 redis高可用 这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续 工作，其实就是搭建的集群。（异地多活！） 限流降级（在SpringCloud讲解过！） 这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对 某个key只允许一个线程查询数据和写缓存，其他线程等待。 数据预热 数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数 据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让 缓存失效的时间点尽量均匀。 解决： 设置随机过期时间，避免同一时间大面积失效 如果是集群部署，将热点数据均匀分布在不同的 Redis 库上避免全部失效 设置热点数据永不过期，有更新缓存 对源服务访问进行限流、资源隔离（熔断）、降级等 如果是时点数据，可以进行预加载。等时点达到后，进行切换 Redis持久化 面试和工作，持久化都是重点！ Redis 是内存数据库，如果不将内存中的数据库状态保存到磁盘，那么一旦服务器进程退出，服务器中 的数据库状态也会消失。所以 Redis 提供了持久化功能！ RDB （Redis DataBase） 什么是RDB 在主从复制中，rdb就是备用了！从机上面！ 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。 Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程 都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的。 这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那 RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。我们默认的就是 RDB，一般情况下不需要修改这个配置！ 有时候在生产环境我们会将这个文件进行备份！ rdb保存的文件是dump.rdb 都是在我们的配置文件中快照中进行配置的！ 触发机制 1、save的规则满足的情况下，会自动触发rdb规则 2、执行 flushall 命令，也会触发我们的rdb规则！ 3、退出redis，也会产生 rdb 文件！ 备份就自动生成一个 dump.rdb 如何恢复rdb文件 1、只需要将rdb文件放在我们redis启动目录就可以，redis启动的时候会自动检查dump.rdb 恢复其中 的数据！ 2、查看需要存在的位置 几乎就他自己默认的配置就够用了，但是我们还是需要去学习！ 优点： 1、适合大规模的数据恢复！ 2、对数据的完整性要不高！ 缺点： 1、需要一定的时间间隔进程操作！如果redis意外宕机了，这个最后一次修改数据就没有的了！ 2、fork进程的时候，会占用一定的内容空间！！ AOF（Append Only File） 将我们的所有命令都记录下来，history，恢复的时候就把这个文件全部在执行一遍！ 是什么 127.0.0.1:6379&gt; config get dir 1) &quot;dir&quot; 2) &quot;/usr/local/bin&quot; # 如果在这个目录下存在 dump.rdb 文件，启动就会自动恢复其中的数据 以日志的形式来记录每个写操作，将Redis执行过的所有指令记录下来（读操作不记录），只许追加文件 但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis重启的话就根据日志文件 的内容将写指令从前到后执行一次以完成数据的恢复工作 Aof保存的是 appendonly.aof 文件 默认是不开启的，我们需要手动进行配置！我们只需要将 appendonly 改为yes就开启了 aof！ 重启，redis 就可以生效了！ 如果这个 aof 文件有错位，这时候 redis 是启动不起来的吗，我们需要修复这个aof文件 redis 给我们提供了一个工具 redis-check-aof --fix aof 默认就是文件的无限追加，文件会越来越大！ 如果 aof 文件大于 64m，太大了！ fork一个新的进程来将我们的文件进行重写！ 优点和缺点！ 1234567appendonly no # 默认是不开启aof模式的，默认是使用rdb方式持久化的，在大部分所有的情况下，rdb完全够用！appendfilename &quot;appendonly.aof&quot; # 持久化的文件的名字# appendfsync always # 每次修改都会 sync。消耗性能appendfsync everysec # 每秒执行一次 sync，可能会丢失这1s的数据！# appendfsync no # 不执行 sync，这个时候操作系统自己同步数据，速度最快！# rewrite 重写， 优点： 1、每一次修改都同步，文件的完整会更加好！ 2、每秒同步一次，可能会丢失一秒的数据 3、从不同步，效率最高的！ 缺点： 1、相对于数据文件来说，aof远远大于 rdb，修复的速度也比 rdb慢！ 2、Aof 运行效率也要比 rdb 慢，所以我们redis默认的配置就是rdb持久化！ 扩展： 1、RDB 持久化方式能够在指定的时间间隔内对你的数据进行快照存储 2、AOF 持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始 的数据，AOF命令以Redis 协议追加保存每次写的操作到文件末尾，Redis还能对AOF文件进行后台重 写，使得AOF文件的体积不至于过大。 3、只做缓存，如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化 4、同时开启两种持久化方式 在这种情况下，当redis重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF 文件保存的数据集要比RDB文件保存的数据集要完整。 RDB 的数据不实时，同时使用两者时服务器重启也只会找AOF文件，那要不要只使用AOF呢？作者 建议不要，因为RDB更适合用于备份数据库（AOF在不断变化不好备份），快速重启，而且不会有 AOF可能潜在的Bug，留着作为一个万一的手段。 5、性能建议 因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够 了，只保留 save 900 1 这条规则。 如果Enable AOF ，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自 己的AOF文件就可以了，代价一是带来了持续的IO，二是AOF rewrite 的最后将 rewrite 过程中产 生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite 的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上，默认超过原大小100%大小重 写可以改到适当的数值。 如果不Enable AOF ，仅靠 Master-Slave Repllcation 实现高可用性也可以，能省掉一大笔IO，也 减少了rewrite时带来的系统波动。代价是如果Master/Slave 同时倒掉，会丢失十几分钟的数据， 启动脚本也要比较两个 Master/Slave 中的 RDB文件，载入较新的那个，微博就是这种架构。 命令 1234567891011121314151617181920测试订阅端：127.0.0.1:6379&gt; SUBSCRIBE kuangshenshuo # 订阅一个频道 kuangshenshuoReading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;kuangshenshuo&quot;3) (integer) 1# 等待读取推送的信息1) &quot;message&quot; # 消息2) &quot;kuangshenshuo&quot; # 那个频道的消息3) &quot;hello,kuangshen&quot; # 消息的具体内容1) &quot;message&quot;2) &quot;kuangshenshuo&quot;3) &quot;hello,redis&quot;发送端：127.0.0.1:6379&gt; PUBLISH kuangshenshuo &quot;hello,kuangshen&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; PUBLISH kuangshenshuo &quot;hello,redis&quot; # 发布者发布消息到频道！(integer) 1127.0.0.1:6379&gt; 原理 Redis是使用C实现的，通过分析 Redis 源码里的 pubsub.c 文件，了解发布和订阅机制的底层实现，借此加深对 Redis 的理解。 Redis 通过 PUBLISH 、SUBSCRIBE 和 PSUBSCRIBE 等命令实现发布和订阅功能。 微信： 通过 SUBSCRIBE 命令订阅某频道后，redis-server 里维护了一个字典，字典的键就是一个个 频道！， 而字典的值则是一个链表，链表中保存了所有订阅这个 channel 的客户端。SUBSCRIBE 命令的关键， 就是将客户端添加到给定 channel 的订阅链表中。 Pub/Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个 key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应 的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 使用场景： 1、实时消息系统！ 2、事实聊天！（频道当做聊天室，将信息回显给所有人即可！） 3、订阅，关注系统都是可以的！ 稍微复杂的场景我们就会使用 消息中间件 MQ （） Redis 数据备份与恢复 SAVE 该命令将在 redis 安装目录中创建dump.rdb文件。 如果需要恢复数据，只需将备份文件 (dump.rdb) 移动到 redis 安装目录并启动服务即可。获取 redis 目录可以使用 CONFIG 命令 CONFIG GET dir 以上命令 CONFIG GET dir 输出的 redis 安装目录为 /usr/local/redis/bin。 Bgsave 创建 redis 备份文件也可以使用命令 BGSAVE，该命令在后台执行。 实例 127.0.0.1:6379&gt; BGSAVE Background saving started Redis 安全 通过 redis 的配置文件设置密码参数，这样客户端连接到 redis 服务就需要密码验证，这样可以让你的 redis 服务更安全。 查看是否设置了密码验证： CONFIG get requirepass默认情况下 requirepass 参数是空的，这就意味着你无需通过密码验证就可以连接到 redis 服务。 CONFIG set requirepass &quot;runoob&quot; AUTH &quot;runoob&quot; SET mykey &quot;Test value&quot; GET mykey Redis 性能测试 Redis 性能测试是通过同时执行多个命令实现的。 redis-benchmark [option] [option value] 注意：该命令是在 redis 的目录下执行的，而不是 redis 客户端的内部指令 以下实例同时执行 10000 个请求来检测性能： redis-benchmark -n 10000 -q - redis 性能测试工具可选参数如下所示： 序号 选项 描述 默认值 1 -h 指定服务器主机名 127.0.0.1 2 -p 指定服务器端口 6379 3 -s 指定服务器 socket 4 -c 指定并发连接数 50 5 -n 指定请求数 10000 6 -d 以字节的形式指定 SET/GET 值的数据大小 2 7 -k 1=keep alive 0=reconnect 1 8 -r SET/GET/INCR 使用随机 key, SADD 使用随机值 9 -P 通过管道传输 请求 1 10 -q 强制退出 redis。仅显示 query/sec 值 11 --csv 以 CSV 格式输出 12 -l 生成循环，永久执行测试 13 -t 仅运行以逗号分隔的测试命令列表。 14 -I Idle 模式。仅打开 N 个 idle 连接并等待。 - redis-benchmark -h 127.0.0.1 -p 6379 -t set,lpush -n 10000 -q Redis 客户端连接 Redis 通过监听一个 TCP 端口或者 Unix socket 的方式来接收来自客户端的连接，当一个连接建立后，Redis 内部会进行以下一些操作： 首先，客户端 socket 会被设置为非阻塞模式，因为 Redis 在网络事件处理上采用的是非阻塞多路复用模型。 然后为这个 socket 设置 TCP_NODELAY 属性，禁用 Nagle 算法 然后创建一个可读的文件事件用于监听这个客户端 socket 的数据发送 最大连接数 在 Redis2.4 中，最大连接数是被直接硬编码在代码里面的，而在2.6版本中这个值变成可配置的。 maxclients 的默认值是 10000，你也可以在 redis.conf 中对这个值进行修改。 config get maxclients - redis-server --maxclients 100000 - S.N. 命令 描述 1 CLIENT LIST 返回连接到 redis 服务的客户端列表 2 CLIENT SETNAME 设置当前连接的名称 3 CLIENT GETNAME 获取通过 CLIENT SETNAME 命令设置的服务名称 4 CLIENT PAUSE 挂起客户端连接，指定挂起的时间以毫秒计 5 CLIENT KILL 关闭客户端连接 Redis管道技术 Redis是一种基于客户端-服务端模型以及请求/响应协议的TCP服务。这意味着通常情况下一个请求会遵循以下步骤： 客户端向服务端发送一个查询请求，并监听套接字返回，通常以一对模式，等待服务端响应。 服务端处理命令，可以结果返回给客户端。 Redis管道技术可以在服务端未响应时，客户端可以继续向服务端发送请求，并最终一次性读取所有服务端的响应。 查看redis管道，只需要启动redis实例并输入以下命令： $（echo -en“ PING \\ r \\ n SET runoobkey redis \\ r \\ nGET runoobkey \\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ n”；睡眠10）| 数控本地主机6379（echo - zh - cn “ PING \\ r \\ n SET redoobkey redis \\ r \\ nGET runoobkey \\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ nINCR访问者\\ r \\ n” ；睡眠10 ）| 数控本地主机6379 - 以上实例中我们通过使用PING命令查看redis服务是否可用，之后我们设置了runoobkey的变量redis，然后我们获取runoobkey的值并使其访客自增3次。 在返回的结果中我们可以看到这些命令一次性向redis服务提交，并最终一次性读取所有服务端的响应 管道技术的优势 管道技术最显着的优势是提高了redis服务的性能。 Redis 分区 分区是分割数据到多个Redis实例的处理过程，因此每个实例只保存key的一个子集。 分区的优势 通过利用多台计算机内存的和值，允许我们构造更大的数据库。 通过多核和多台计算机，允许我们扩展计算能力；通过多台计算机和网络适配器，允许我们扩展网络带宽。 分区的不足 redis的一些特性在分区方面表现的不是很好： 涉及多个key的操作通常是不被支持的。举例来说，当两个set映射到不同的redis实例上时，你就不能对这两个set执行交集操作。 涉及多个key的redis事务不能使用。 当使用分区时，数据处理较为复杂，比如你需要处理多个rdb/aof文件，并且从多个实例和主机备份持久化文件。 增加或删除容量也比较复杂。redis集群大多数支持在运行时增加、删除节点的透明数据平衡的能力，但是类似于客户端分区、代理等其他系统则不支持这项特性。然而，一种叫做presharding的技术对此是有帮助的。 分区类型 Redis 有两种类型分区。 假设有4个Redis实例 R0，R1，R2，R3，和类似user:1，user:2这样的表示用户的多个key，对既定的key有多种不同方式来选择这个key存放在哪个实例中。也就是说，有不同的系统来映射某个key到某个Redis服务。 范围分区 最简单的分区方式是按范围分区，就是映射一定范围的对象到特定的Redis实例。 比如，ID从0到10000的用户会保存到实例R0，ID从10001到 20000的用户会保存到R1，以此类推。 这种方式是可行的，并且在实际中使用，不足就是要有一个区间范围到实例的映射表。这个表要被管理，同时还需要各 种对象的映射表，通常对Redis来说并非是好的方法。 哈希分区 另外一种分区方法是hash分区。这对任何key都适用，也无需是object_name:这种形式，像下面描述的一样简单： 用一个hash函数将key转换为一个数字，比如使用crc32 hash函数。对key foobar执行crc32(foobar)会输出类似93024922的整数。 对这个整数取模，将其转化为0-3之间的数字，就可以将这个整数映射到4个Redis实例中的一个了。93024922 % 4 = 2，就是说key foobar应该被存到R2实例中。注意：取模操作是取除的余数，通常在多种编程语言中用%操作符实现。 Java 使用 Redis jedis 安装 开始在 Java 中使用 Redis 前， 我们需要确保已经安装了 redis 服务及 Java redis 驱动，且你的机器上能正常使用 Java。 Java的安装配置可以参考我们的 Java 开发环境配置 接下来让我们安装 Java redis 驱动： 首先你需要下载驱动包 下载 jedis.jar，确保下载最新驱动包。 在你的 classpath 中包含该驱动包。 maven jedis 连接到 redis 服务 12345678910111213- import redis.clients.jedis.Jedis;public class RedisJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); // 如果 Redis 服务设置来密码，需要下面这行，没有就不需要 // jedis.auth(&quot;123456&quot;); System.out.println(&quot;连接成功&quot;); //查看服务是否运行 System.out.println(&quot;服务正在运行: &quot;+jedis.ping()); }} Redis Java String(字符串) 实例 12345678910111213- import redis.clients.jedis.Jedis;public class RedisStringJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); System.out.println(&quot;连接成功&quot;); //设置 redis 字符串数据 jedis.set(&quot;runoobkey&quot;, &quot;www.runoob.com&quot;); // 获取存储的数据并输出 System.out.println(&quot;redis 存储的字符串为: &quot;+ jedis.get(&quot;runoobkey&quot;)); }} Redis Java List(列表) 实例 12345678910111213141516171819- import java.util.List; import redis.clients.jedis.Jedis;public class RedisListJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); System.out.println(&quot;连接成功&quot;); //存储数据到列表中 jedis.lpush(&quot;site-list&quot;, &quot;Runoob&quot;); jedis.lpush(&quot;site-list&quot;, &quot;Google&quot;); jedis.lpush(&quot;site-list&quot;, &quot;Taobao&quot;); // 获取存储的数据并输出 List&lt;String&gt; list = jedis.lrange(&quot;site-list&quot;, 0 ,2); for(int i=0; i&lt;list.size(); i++) { System.out.println(&quot;列表项为: &quot;+list.get(i)); } }} Redis Java Keys 实例 1234567891011121314151617 - import java.util.Iterator; import java.util.Set; import redis.clients.jedis.Jedis;public class RedisKeyJava { public static void main(String[] args) { //连接本地的 Redis 服务 Jedis jedis = new Jedis(&quot;localhost&quot;); System.out.println(&quot;连接成功&quot;); // 获取数据并输出 Set&lt;String&gt; keys = jedis.keys(&quot;*&quot;); Iterator&lt;String&gt; it=keys.iterator() ; while(it.hasNext()){ String key = it.next(); System.out.println(key); }}} Jedis事务 12345678910111213141516171819202122232425public static void main(String[] args) {Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379);jedis.flushDB();JSONObject jsonObject = new JSONObject();jsonObject.put(&quot;hello&quot;,&quot;world&quot;);jsonObject.put(&quot;name&quot;,&quot;kuangshen&quot;);// 开启事务Transaction multi = jedis.multi();String result = jsonObject.toJSONString();// jedis.watch(result)try {multi.set(&quot;user1&quot;,result);multi.set(&quot;user2&quot;,result);int i = 1/0 ; // 代码抛出异常事务，执行失败！multi.exec(); // 执行事务！} catch (Exception e) {multi.discard(); // 放弃事务e.printStackTrace();} finally {System.out.println(jedis.get(&quot;user1&quot;));System.out.println(jedis.get(&quot;user2&quot;));jedis.close(); // 关闭连接}} SpringBoot整合Redis SpringBoot 操作数据：spring-data jpa jdbc mongodb redis！ SpringData 也是和 SpringBoot 齐名的项目！ 说明： 在 SpringBoot2.x 之后，原来使用的jedis 被替换为了 lettuce? jedis : 采用的直连，多个线程操作的话，是不安全的，如果想要避免不安全的，使用 jedis pool 连接池！ 更像 BIO 模式 lettuce : 采用netty，实例可以再多个线程中进行共享，不存在线程不安全的情况！可以减少线程数据了，更像 NIO 模式 源码分析： 12345678910111213141516171819202122@Bean@ConditionalOnMissingBean(name = &quot;redisTemplate&quot;) // 我们可以自己定义一个redisTemplate来替换这个默认的！public RedisTemplate&lt;Object, Object&gt; redisTemplate(RedisConnectionFactoryredisConnectionFactory)throws UnknownHostException {// 默认的 RedisTemplate 没有过多的设置，redis 对象都是需要序列化！// 两个泛型都是 Object, Object 的类型，我们后使用需要强制转换 &lt;String, Object&gt;RedisTemplate&lt;Object, Object&gt; template = new RedisTemplate&lt;&gt;();template.setConnectionFactory(redisConnectionFactory);return template;}@Bean@ConditionalOnMissingBean // 由于 String 是redis中最常使用的类型，所以说单独提出来了一个bean！public StringRedisTemplate stringRedisTemplate(RedisConnectionFactoryredisConnectionFactory)throws UnknownHostException {StringRedisTemplate template = new StringRedisTemplate();template.setConnectionFactory(redisConnectionFactory);return template;} 1234567891011121314151617181920212223242526272829303132333435361、导入依赖&lt;!-- 操作redis --&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;2、配置连接# 配置redisspring.redis.host=127.0.0.1spring.redis.port=63793、测试！@SpringBootTestclass Redis02SpringbootApplicationTests {@Autowiredprivate RedisTemplate redisTemplate;@Testvoid contextLoads() {// redisTemplate 操作不同的数据类型，api和我们的指令是一样的// opsForValue 操作字符串 类似String// opsForList 操作List 类似List// opsForSet// opsForHash// opsForZSet// opsForGeo// opsForHyperLogLog// 除了进本的操作，我们常用的方法都可以直接通过redisTemplate操作，比如事务，和基本的CRUD// 获取redis的连接对象// RedisConnection connection =redisTemplate.getConnectionFactory().getConnection();// connection.flushDb();// connection.flushAll();redisTemplate.opsForValue().set(&quot;mykey&quot;,&quot;关注狂神说公众号&quot;);System.out.println(redisTemplate.opsForValue().get(&quot;mykey&quot;));}} 应用场景 计算器、限速器、好友关系 缓存——提升热点数据的访问速度 共享数据——数据的存储和共享的问题 全局 ID —— 分布式全局 ID 的生成方案（分库分表） 分布式锁——进程间共享数据的原子操作保证 在线用户统计和计数 —— 使用位图进行位运算 队列、栈——跨进程的队列/栈 消息队列——异步解耦的消息机制 服务注册与发现 —— RPC 通信机制的服务协调中心（Dubbo 支持 Redis） 共享用户 Session —— 用户Session的更新和获取都可以快速完成 排行榜—— 通过分配元素 score 值进行排序 综合问题 😧 Redis多线程还是单线程？为什么这样？ ​ 单线程，Redis 6.0版本之后开始引入了多线程处理网络请求，将读取网络数据到输入缓冲区、协议解析和将执行结果写入输出缓冲区的过程变为了多线程，执行命令仍然是单线程。 ​ 为什么不使用多线程？因为Redis数据在内存中，多线程会增加上下文切换的代价，没有才是效率最高的。单线程没有数据一致性问题，不需要竞争锁和CAS操作。 😧所有数据类型 常用的五种数据类型： String：可以存储字符串、整数、浮点数，允许设置过期时间自动删除。 Hash：包含键值对的无序散列表。通过 “数组 + 链表” 的链地址法来解决部分哈希冲突，value 只能是字符串。 List：双向链表，可以充当队列和栈的角色。 Set：相当于Java 中的 HashSet ，内部的键值对是无序、唯一的。key 就是元素的值，value 为 null。 Zset：相当于 Java 中的 SortedSet 和 HashMap 的结合体，内部的键值对是有序、唯一的。可以为每个元素赋予一个 score 值，用来代表排序的权重。 不常用的4种数据类型： BitMap：即位图，其实也就是 byte 数组，用二进制表示，只有 0 和 1 两个数字。实际上来说是归属于 String 类型下面。 Hyperloglogs：用来做基数统计的算法。 Geospatial：将用户给定的地理位置信息储存起来， 并对这些信息进行操作。 Pub/Sub：支持多播的可持久化的消息队列，用于实现发布订阅功能。 缓存血崩 / 穿透是什么，如何解决？ 缓存血崩是什么，如何解决？","link":"/2021/03/01/Draft/2021/Redis/"},{"title":"魑魅先生 | 设计模式","text":"学习目的：增加系统的健壮性，易修改性和可扩展性，阅读源码的通用工具，通用问题的工具箱，程序员的通用语言 学习方法:视频引入，博客弥补，书籍细化，代码实践 学习资源：大话设计模式书籍，bili视频，博客1,博客2 学习目标程度：了解即可 实际运用 JDK SSM Springboot 设计模式原则 开闭原则 ：对修改关闭，对扩展开放 由Bertrand Meyer提出的开闭原则（Open Closed Principle）是指，软件应该对扩展开放， 而对修改关闭。这里的意思是在增加新功能的时候，能不改代码就尽量不要改， 如果只增加代码就完成了新功能，那是最好的。一个类从头到尾都是自己写的可以更改，别人的要符合开闭原则 依赖倒转原则 ：针对接口和抽象编程 上层（调用别的方法的）不能依赖于下层（方法被调用的），他们都应该依赖于抽象 合成复用原则 ：多用组合少用继承 继承（实线空箭头） 依赖 关联（实线箭头）：组合（实心菱形加箭头）（鸟和翅膀）；聚合（空心菱形加箭头）（雁和雁群） 迪米特原则（最少知道原则） ：实体之间尽量少的相互作用 个对象应对其它对象尽可能少的了解，和朋友（类中字段，方法参数，方法返回值，方法实例出来的对象）通信 里式代换原则 ：基类可以出现的地方子类也可以出现。 里氏替换原则是Barbara Liskov提出的，这是一种面向对象的设计原则， 即如果我们调用一个父类的方法可以成功，那么替换成子类调用也应该完全可以运行。 重写时子类限制等级不能更高，错误不能抛出更多 单一职责原则 ：每个方法，类，框架只负责一件事 接口隔离原则 ：使用多个专门的接口比一个总接口好 ---分--- 学习设计模式，关键是学习设计思想，不能简单地生搬硬套， 也不能为了使用设计模式而过度设计，要合理平衡设计的复杂度和灵活性， 并意识到设计模式也并不是万能的。 算法更像是菜谱： 提供达成目标的明确步骤。 而模式更像是蓝图： 你可以看到最终的结果和模式的功能， 但需要自己确定实现步骤。 意图 部分简单描述问题和解决方案。 动机 部分将进一步解释问题并说明模式会如何提供解决方案。 结构 部分展示模式的每个部分和它们之间的关系。 在不同语言中的实现 提供流行编程语言的代码， 让读者更好地理解模式背后的思想。 设计原则 一句话归纳 目的 开闭原则 对扩展开放，对修改关闭 降低维护带来的新风险 依赖倒置原则 高层不应该依赖低层，要面向接口编程 更利于代码结构的升级扩展 单一职责原则 一个类只干一件事，实现类要单一 便于理解，提高代码的可读性 接口隔离原则 一个接口只干一件事，接口要精简单一 功能解耦，高聚合、低耦合 迪米特法则 不该知道的不要知道，一个类应该保持对其它对象最少的了解，降低耦合度 只和朋友交流，不和陌生人说话，减少代码臃肿 里氏替换原则 不要破坏继承体系，子类重写方法功能发生改变，不应该影响父类方法的含义 防止继承泛滥 合成复用原则 尽量使用组合或者聚合关系实现代码复用，少使用继承 降低代码耦合 学习进度 设计原则 开闭原则 里氏替换原则 依赖倒置原则 单一职责原则 接口隔离原则 迪米特法则 合成复用原则 行为模式 职责链模式（Chain of Responsibility）😎😎😎😎😎 命令模式（Command） 迭代器模式（Iterator） 调停者（中介者）模式（Mediator） 备忘录模式（Memento） 观察者模式（Observer） 状态模式（State） 策略模式（Strategy） 模板方法模式（Template Method） 访问者模式（Visitor） 解释器模式（Interpreter） 结构型模式 适配器模式（Adapter） 桥接模式（Bridge） 组合模式（Composite） 装饰模式（Decorator） 外观模式（Facade） 享元模式（Flyweight） 代理模式（Proxy） 创建型模式 简单工厂模式（Simple Factory） 工厂方法模式（Factory Method） 抽象工厂模式（Abstract Factory） 创建者（生成器）模式（Builder） 原型模式（Prototype） 单例模式（Singleton） 进阶学习 总归纳复习 设计模式简述 模式之间的关系 Refactoringguru.cn 创建型模式 这类模式提供创建对象的机制， 能够提升已有代码的灵活性和可复用性。 工厂方法Factory Method抽象工厂Abstract Factory生成器Builder原型Prototype单例Singleton 结构型模式 这类模式介绍如何将对象和类组装成较大的结构， 并同时保持结构的灵活和高效。 适配器Adapter桥接Bridge组合Composite装饰Decorator外观Facade享元Flyweight代理Proxy 行为模式 这类模式负责对象间的高效沟通和职责委派。 责任链Chain of Responsibility命令Command迭代器Iterator中介者Mediator备忘录Memento观察者Observer状态State策略Strategy模板方法Template Method访问者Visitor 创建型模式 创建型模式的主要关注点是“怎样创建对象？”，它的主要特点是“将对象的创建与使用分离”。这样可以降低系统的耦合度，使用者不需要关注对象的创建细节，对象的创建由相关的工厂来完成。就像我们去商场购买商品时，不需要知道商品是怎么生产出来一样，因为它们由专门的厂商生产。 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（FactoryMethod）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 结构型模式 结构型模式描述如何将类或对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者釆用组合或聚合来组合对象。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象结构型模式比类结构型模式具有更大的灵活性。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现的，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态地给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 行为型模式 行为型模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。 模板方法（Template Method）模式：定义一个操作中的算法骨架，将算法的一些步骤延迟到子类中，使得子类在可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 UML 工具：processon 方法和属性的访问权限 - private # protected + public ~ package private 关系 概览 继承 | 泛化【继承并特殊】 12【泛化关系】：是一种继承关系，表示一般与特殊的关系，它指定了子类如何特化父类的所有特征和行为。例如：老虎是动物的一种，即有老虎的特性也有动物的共性。【箭头指向】：带三角箭头的实线，箭头指向父类 实现【接口全实现】 123【实现关系】：是一种类与接口的关系，表示类是接口所有特征和行为的实现【箭头指向】：带三角箭头的虚线，箭头指向接口 关联【我知你些属】 1234【关联关系】：是一种拥有的关系,它使一个类知道另一个类的属性和方法；如：老师与学生，丈夫与妻子关联可以是双向的，也可以是单向的。双向的关联可以有两个箭头或者没有箭头，单向的关联有一个箭头。【代码体现】：成员变量【箭头及指向】：带普通箭头的实心线，指向被拥有者 聚合【一团分几个】 1234【聚合关系】：是整体与部分的关系，且部分可以离开整体而单独存在。如车和轮胎是整体和部分的关系.聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。【代码体现】：成员变量【箭头及指向】：带空心菱形的实心线，菱形指向整体 组合【一个切几个】 123【组合关系】：是整体与部分的关系，但部分不能离开整体而单独存在。没有公司就不存在部门 组合关系是关联关系的一种，是比聚合关系还要强的关系，它要求普通的聚合关系中代表整体的对象负责代表部分的对象的生命周期【代码体现】：成员变量【箭头及指向】：带实心菱形的实线，菱形指向整体 依赖【我要用你的】 12345【依赖关系】：是一种使用的关系,所以要尽量不使用双向的互相依赖。【代码表现】：局部变量、方法的参数或者对静态方法的调用【箭头及指向】：带箭头的虚线，指向被使用者 各种关系的强弱顺序： 泛化= 实现&gt; 组合&gt; 聚合&gt; 关联&gt; 依赖 设计模式重点详解 Mybatis（ 1、Builder模式5、组合模式9、迭代器模式2、工厂模式3、单例模式4、代理6、模板方法模式7、适配器模式8、装饰者模式） Spring（1.简单工厂2.工厂方法3.单例模式4.适配器模式5.装饰器模式6.代理模式7.观察者模式8.策略模式9.模版方法模式） 创建型模式 《单例模式Singleton》 特点： 单例类只有一个实例对象； 该单例对象必须由单例类自行创建； 单例类对外提供一个访问该单例的全局访问点。 优点： 单例模式可以保证内存里只有一个实例，减少了内存的开销。 可以避免对资源的多重占用。 单例模式设置全局访问点，可以优化和共享资源的访问。 缺点： 单例模式一般没有接口，扩展困难。如果要扩展，则除了修改原来的代码，没有第二种途径，违背开闭原则。 在并发测试中，单例模式不利于代码调试。在调试过程中，如果单例中的代码没有执行完，也不能模拟生成一个新的对象。 单例模式的功能代码通常写在一个类中，如果功能设计不合理，则很容易违背单一职责原则。 单例模式的应用场景： 需要频繁创建的一些类，使用单例可以降低系统的内存压力，减少 GC。 某类只要求生成一个对象的时候，如一个班中的班长、每个人的身份证号等。 某些类创建实例时占用资源较多，或实例化耗时较长，且经常使用。 某类需要频繁实例化，而创建的对象又频繁被销毁的时候，如多线程的线程池、网络连接池等。 频繁访问数据库或文件的对象。 对于一些控制硬件级别的操作，或者从系统上来讲应当是单一控制逻辑的操作，如果有多个实例，则系统会完全乱套。 当对象需要被共享的场合。由于单例模式只允许创建一个对象，共享该对象可以节省内存，并加快对象访问速度。如 Web 中的配置对象、数据库的连接池等。 分类JAVA实现： 懒汉式：类加载时没有生成单例，只有当第一次调用 getlnstance 方法时才去创建这个单例 ​ 如果编写的是多线程程序，则不要删除上例代码中的关键字 volatile 和 synchronized，否则将存在线程非安全的问题。如果不删除这两个关键字就能保证线程安全，但是每次访问时都要同步，会影响性能，且消耗更多的资源，这是懒汉式单例的缺点。 123456789101112public class LazySingleton { private static volatile LazySingleton instance = null; //保证 instance 在所有线程中同步 private LazySingleton() { } //private 避免类在外部被实例化 public static synchronized LazySingleton getInstance() { //getInstance 方法前加同步 if (instance == null) { instance = new LazySingleton(); } return instance; }} 饿汉式：类一旦加载就创建一个单例，保证在调用 getInstance 方法之前单例已经存在了。 12345678public class HungrySingleton { private static final HungrySingleton instance = new HungrySingleton(); private HungrySingleton() { } public static HungrySingleton getInstance() { return instance; }} ​ 饿汉式单例在类创建的同时就已经创建好一个静态的对象供系统使用，以后不再改变，所以是线程安全的，可以直接用于多线程而不会出现问题。 《原型模式Prototype》 特点： 用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型相同或相似的新对象。 原型实例指定了要创建的对象的种类。用这种方式创建对象非常高效，根本无须知道对象创建的细节。 优点： Java自带的原型模式基于内存二进制流的复制，在性能上比直接 new 一个对象更加优良。 深克隆方式保存对象的状态，使用原型模式将对象复制一份，并将其状态保存起来，简化创建对象过程，以便在需要的时候使用（例如恢复到历史某一状态），可辅助实现撤销操作。 缺点： 需要为每一个类都配置一个 clone 方法 clone 方法位于类的内部，当对已有类进行改造的时候，需要修改代码，违背了开闭原则。 当实现深克隆时，需要编写较为复杂的代码，而且当对象之间存在多重嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来会比较麻烦。因此，深克隆、浅克隆需要运用得当。 角色： 抽象原型类：规定了具体原型对象必须实现的接口。 具体原型类：实现抽象原型类的 clone() 方法，它是可被复制的对象。 访问类：使用具体原型类中的 clone() 方法来复制新的对象。 浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 原型模式的应用场景： 对象之间相同或相似，即只是个别的几个属性不同的时候。 创建对象成本较大，例如初始化时间长，占用CPU太多，或者占用网络资源太多等，需要优化资源。 创建一个对象需要繁琐的数据准备或访问权限等，需要提高性能或者提高安全性。 系统中大量使用该类对象，且各个调用者都需要给它的属性重新赋值。 JAVA实现： 浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址。 深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.util.*;interface Shape extends Cloneable { public Object clone(); //拷贝 public void countArea(); //计算面积}class Circle implements Shape { public Object clone() { Circle w = null; try { w = (Circle) super.clone(); } catch (CloneNotSupportedException e) { System.out.println(&quot;拷贝圆失败!&quot;); } return w; } public void countArea() { int r = 0; System.out.print(&quot;这是一个圆，请输入圆的半径：&quot;); Scanner input = new Scanner(System.in); r = input.nextInt(); System.out.println(&quot;该圆的面积=&quot; + 3.1415 * r * r + &quot;\\n&quot;); }}class Square implements Shape { public Object clone() { Square b = null; try { b = (Square) super.clone(); } catch (CloneNotSupportedException e) { System.out.println(&quot;拷贝正方形失败!&quot;); } return b; } public void countArea() { int a = 0; System.out.print(&quot;这是一个正方形，请输入它的边长：&quot;); Scanner input = new Scanner(System.in); a = input.nextInt(); System.out.println(&quot;该正方形的面积=&quot; + a * a + &quot;\\n&quot;); }}class ProtoTypeManager { private HashMap&lt;String, Shape&gt; ht = new HashMap&lt;String, Shape&gt;(); public ProtoTypeManager() { ht.put(&quot;Circle&quot;, new Circle()); ht.put(&quot;Square&quot;, new Square()); } public void addshape(String key, Shape obj) { ht.put(key, obj); } public Shape getShape(String key) { Shape temp = ht.get(key); return (Shape) temp.clone(); }}public class ProtoTypeShape { public static void main(String[] args) { ProtoTypeManager pm = new ProtoTypeManager(); Shape obj1 = (Circle) pm.getShape(&quot;Circle&quot;); obj1.countArea(); Shape obj2 = (Shape) pm.getShape(&quot;Square&quot;); obj2.countArea(); }} 《简单工厂模式Simple Factory Pattern》 特点： 定义一个创建产品对象的工厂接口，将产品对象的实际创建工作推迟到具体子工厂类当中。这满足创建型模式中所要求的“创建与使用相分离”的特点 需要生成复杂对象的地方，都可以尝试考虑使用工厂模式。复杂对象指的是类的构造函数参数过多等对类的构造有影响的情况，因为类的构造过于复杂，如果直接在其他业务类内使用，则两者的耦合过重，后续业务更改，就需要在任何引用该类的源代码内进行更改，光是查找所有依赖就很消耗时间了，更别说要一个一个修改了。 优点： 工厂类包含必要的逻辑判断，可以决定在什么时候创建哪一个产品的实例。客户端可以免除直接创建产品对象的职责，很方便的创建出相应的产品。工厂和产品的职责区分明确。 客户端无需知道所创建具体产品的类名，只需知道参数即可。 也可以引入配置文件，在不修改客户端代码的情况下更换和添加新的具体产品类。 缺点： 简单工厂模式的工厂类单一，负责所有产品的创建，职责过重，一旦异常，整个系统将受影响。且工厂类代码会非常臃肿，违背高聚合原则。 使用简单工厂模式会增加系统中类的个数（引入新的工厂类），增加系统的复杂度和理解难度 系统扩展困难，一旦增加新产品不得不修改工厂逻辑，在产品类型较多时，可能造成逻辑过于复杂 简单工厂模式使用了 static 工厂方法，造成工厂角色无法形成基于继承的等级结构。 角色： 简单工厂（SimpleFactory）：是简单工厂模式的核心，负责实现创建所有实例的内部逻辑。工厂类的创建产品类的方法可以被外界直接调用，创建所需的产品对象。 抽象产品（Product）：是简单工厂创建的所有对象的父类，负责描述所有实例共有的公共接口。 具体产品（ConcreteProduct）：是简单工厂模式的创建目标。 简单工厂模式的应用场景： 生成复杂对象 JAVA实现 123456789101112131415161718192021222324252627282930313233343536public class Client { public static void main(String[] args) { } //抽象产品 public interface Product { void show(); } //具体产品：ProductA static class ConcreteProduct1 implements Product { public void show() { System.out.println(&quot;具体产品1显示...&quot;); } } //具体产品：ProductB static class ConcreteProduct2 implements Product { public void show() { System.out.println(&quot;具体产品2显示...&quot;); } } final class Const { static final int PRODUCT_A = 0; static final int PRODUCT_B = 1; static final int PRODUCT_C = 2; } static class SimpleFactory { public static Product makeProduct(int kind) { switch (kind) { case Const.PRODUCT_A: return new ConcreteProduct1(); case Const.PRODUCT_B: return new ConcreteProduct2(); } return null; } }} 《工厂模式Factory Pattern》 特点： 可以使系统在不修改原来代码的情况下引进新的产品，即满足开闭原则。 优点： 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程。 灵活性增强，对于新产品的创建，只需多写一个相应的工厂类。 典型的解耦框架。高层模块只需要知道产品的抽象类，无须关心其他实现类，满足迪米特法则、依赖倒置原则和里氏替换原则。 缺点： 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象产品只能生产一种产品，此弊端可使用抽象工厂模式解决。 角色： 抽象工厂（Abstract Factory）：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法 newProduct() 来创建产品。 具体工厂（ConcreteFactory）：主要是实现抽象工厂中的抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应。 工厂模式的应用场景： 客户只知道创建产品的工厂名，而不知道具体的产品名。如 TCL 电视工厂、海信电视工厂等。 创建对象的任务由多个具体子工厂中的某一个完成，而抽象工厂只提供创建产品的接口。 客户不关心创建产品的细节，只关心产品的品牌 JAVA实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package FactoryMethod;public class AbstractFactoryTest { public static void main(String[] args) { try { Product a; AbstractFactory af; af = (AbstractFactory) ReadXML1.getObject(); a = af.newProduct(); a.show(); } catch (Exception e) { System.out.println(e.getMessage()); } }}//抽象产品：提供了产品的接口interface Product { public void show();}//具体产品1：实现抽象产品中的抽象方法class ConcreteProduct1 implements Product { public void show() { System.out.println(&quot;具体产品1显示...&quot;); }}//具体产品2：实现抽象产品中的抽象方法class ConcreteProduct2 implements Product { public void show() { System.out.println(&quot;具体产品2显示...&quot;); }}//抽象工厂：提供了厂品的生成方法interface AbstractFactory { public Product newProduct();}//具体工厂1：实现了厂品的生成方法class ConcreteFactory1 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂1生成--&gt;具体产品1...&quot;); return new ConcreteProduct1(); }}//具体工厂2：实现了厂品的生成方法class ConcreteFactory2 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂2生成--&gt;具体产品2...&quot;); return new ConcreteProduct2(); }} 12345678910111213141516171819202122232425262728package FactoryMethod;import javax.xml.parsers.*;import org.w3c.dom.*;import java.io.*;class ReadXML1 { //该方法用于从XML配置文件中提取具体类类名，并返回一个实例对象 public static Object getObject() { try { //创建文档对象 DocumentBuilderFactory dFactory = DocumentBuilderFactory.newInstance(); DocumentBuilder builder = dFactory.newDocumentBuilder(); Document doc; doc = builder.parse(new File(&quot;src/FactoryMethod/config1.xml&quot;)); //获取包含类名的文本节点 NodeList nl = doc.getElementsByTagName(&quot;className&quot;); Node classNode = nl.item(0).getFirstChild(); String cName = &quot;FactoryMethod.&quot; + classNode.getNodeValue(); //System.out.println(&quot;新类名：&quot;+cName); //通过类名生成实例对象并将其返回 Class&lt;?&gt; c = Class.forName(cName); Object obj = c.newInstance(); return obj; } catch (Exception e) { e.printStackTrace(); return null; } }} 《抽象工厂模式Abstract Factory Pattern》 特点： 一种为访问类提供一个创建一组相关或相互依赖对象的接口，且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。 抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。 使用抽象工厂模式一般要满足以下条件。 系统中有多个产品族，每个具体工厂创建同一族但属于不同等级结构的产品。 系统一次只可能消费其中某一族产品，即同族的产品一起使用。 优点： 用户只需要知道具体工厂的名称就可得到所要的产品，无须知道产品的具体创建过程。 灵活性增强，对于新产品的创建，只需多写一个相应的工厂类。 典型的解耦框架。高层模块只需要知道产品的抽象类，无须关心其他实现类，满足迪米特法则、依赖倒置原则和里氏替换原则。 可以在类的内部对产品族中相关联的多等级产品共同管理，而不必专门引入多个新的类来进行管理。 当需要产品族时，抽象工厂可以保证客户端始终只使用同一个产品的产品组。 抽象工厂增强了程序的可扩展性，当增加一个新的产品族时，不需要修改原代码，满足开闭原则。 缺点： 类的个数容易过多，增加复杂度 增加了系统的抽象性和理解难度 抽象产品只能生产一种产品，此弊端可使用抽象工厂模式解决。 当产品族中需要增加一个新的产品时，所有的工厂类都需要进行修改。增加了系统的抽象性和理解难度。 角色： 抽象工厂中方法个数不同，抽象产品的个数也不同 抽象工厂（Abstract Factory）：提供了创建产品的接口，它包含多个创建产品的方法 newProduct()，可以创建多个不同等级的产品。 具体工厂（Concrete Factory）：主要是实现抽象工厂中的多个抽象方法，完成具体产品的创建。 抽象产品（Product）：定义了产品的规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品（ConcreteProduct）：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系。 抽象工厂模式的应用场景： 抽象工厂模式最早的应用是用于创建属于不同操作系统的视窗构件。如 Java 的 AWT 中的 Button 和 Text 等构件在 Windows 和 UNIX 中的本地实现是不同的。 抽象工厂模式通常适用于以下场景： 当需要创建的对象是一系列相互关联或相互依赖的产品族时，如电器工厂中的电视机、洗衣机、空调等。 系统中有多个产品族，但每次只使用其中的某一族产品。如有人只喜欢穿某一个品牌的衣服和鞋。 系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。 JAVA实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package FactoryMethod;public class AbstractFactoryTest { public static void main(String[] args) { try { Product a; AbstractFactory af; af = (AbstractFactory) ReadXML1.getObject(); a = af.newProduct(); a.show(); } catch (Exception e) { System.out.println(e.getMessage()); } }}//抽象产品：提供了产品的接口interface Product { public void show();}//具体产品1：实现抽象产品中的抽象方法class ConcreteProduct1 implements Product { public void show() { System.out.println(&quot;具体产品1显示...&quot;); }}//具体产品2：实现抽象产品中的抽象方法class ConcreteProduct2 implements Product { public void show() { System.out.println(&quot;具体产品2显示...&quot;); }}//抽象工厂：提供了厂品的生成方法interface AbstractFactory { public Product newProduct();}//具体工厂1：实现了厂品的生成方法class ConcreteFactory1 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂1生成--&gt;具体产品1...&quot;); return new ConcreteProduct1(); }}//具体工厂2：实现了厂品的生成方法class ConcreteFactory2 implements AbstractFactory { public Product newProduct() { System.out.println(&quot;具体工厂2生成--&gt;具体产品2...&quot;); return new ConcreteProduct2(); }} 12345678910111213141516171819202122232425262728package FactoryMethod;import javax.xml.parsers.*;import org.w3c.dom.*;import java.io.*;class ReadXML1 { //该方法用于从XML配置文件中提取具体类类名，并返回一个实例对象 public static Object getObject() { try { //创建文档对象 DocumentBuilderFactory dFactory = DocumentBuilderFactory.newInstance(); DocumentBuilder builder = dFactory.newDocumentBuilder(); Document doc; doc = builder.parse(new File(&quot;src/FactoryMethod/config1.xml&quot;)); //获取包含类名的文本节点 NodeList nl = doc.getElementsByTagName(&quot;className&quot;); Node classNode = nl.item(0).getFirstChild(); String cName = &quot;FactoryMethod.&quot; + classNode.getNodeValue(); //System.out.println(&quot;新类名：&quot;+cName); //通过类名生成实例对象并将其返回 Class&lt;?&gt; c = Class.forName(cName); Object obj = c.newInstance(); return obj; } catch (Exception e) { e.printStackTrace(); return null; } }} 《建造者模式 Bulider》 特点： 将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示 它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。 优点： 封装性好，构建和表示分离。 扩展性好，各个具体的建造者相互独立，有利于系统的解耦。 客户端不必知道产品内部组成的细节，建造者可以对创建过程逐步细化，而不对其它模块产生任何影响，便于控制细节风险。 缺点： 产品的组成部分必须相同，这限制了其使用范围。 如果产品的内部变化复杂，如果产品内部发生变化，则建造者也要同步修改，后期维护成本较大。 角色： 产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。 建造者模式的应用场景： 相同的方法，不同的执行顺序，产生不同的结果。 多个部件或零件，都可以装配到一个对象中，但是产生的结果又不相同。 产品类非常复杂，或者产品类中不同的调用顺序产生不同的作用。 初始化一个对象特别复杂，参数多，而且很多参数都具有默认值。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package pers.lxl.mylearnproject.programbase.designpatterns.creationalpattern.bulider;/**产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。*/class Product { private String partA; private String partB; private String partC; public void setPartA(String partA) { this.partA = partA; } public void setPartB(String partB) { this.partB = partB; } public void setPartC(String partC) { this.partC = partC; } public void show() { System.out.println(partA+' '+partB+' '+partC); }}/**抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。*/abstract class Builder { //创建产品对象 protected Product product = new Product(); public abstract void buildPartA(); public abstract void buildPartB(); public abstract void buildPartC(); //返回产品对象 public Product getResult() { return product; }}/**具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。*/class ConcreteBuilder1 extends Builder { @Override public void buildPartA() { product.setPartA(&quot;建造 PartA&quot;); } @Override public void buildPartB() { product.setPartB(&quot;建造 PartB&quot;); } @Override public void buildPartC() { product.setPartC(&quot;建造 PartC&quot;); }}/**指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。*/class Director { private Builder builder; public Director(Builder builder) { this.builder = builder; } //产品构建与组装方法 public Product construct() { builder.buildPartA(); builder.buildPartB(); builder.buildPartC(); return builder.getResult(); }}public class ConcreteBuilder { public static void main(String[] args) { Builder builder = new ConcreteBuilder1(); Director director = new Director(builder); Product product = director.construct(); product.show(); }} 结构型模式 《代理模式 Proxy 》 特点： 中介 由于某些原因需要给某对象提供一个代理以控制对该对象的访问。这时，访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。 优点： 代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度，增加了程序的可扩展性 缺点：(动态代理方式解决) 代理模式会造成系统设计中类的数量增加 在客户端和目标对象之间增加一个代理对象，会造成请求处理速度变慢； 增加了系统的复杂度； 角色： 抽象主题（Subject）类：通过接口或抽象类声明真实主题和代理对象实现的业务方法。 真实主题（Real Subject）类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理（Proxy）类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能。 根据代理的创建时期，代理模式分为静态代理和动态代理。 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。 动态：在程序运行时，运用反射机制动态创建而成 代理模式的应用场景： 远程代理，这种方式通常是为了隐藏目标对象存在于不同地址空间的事实，方便客户端访问。例如，用户申请某些网盘空间时，会在用户的文件系统中建立一个虚拟的硬盘，用户访问虚拟硬盘时实际访问的是网盘空间。 虚拟代理，这种方式通常用于要创建的目标对象开销很大时。例如，下载一幅很大的图像需要很长时间，因某种计算比较复杂而短时间无法完成，这时可以先用小比例的虚拟代理替换真实的对象，消除用户对服务器慢的感觉。 安全代理，这种方式通常用于控制不同种类客户对真实对象的访问权限。 智能指引，主要用于调用目标对象时，代理附加一些额外的处理功能。例如，增加计算真实对象的引用次数的功能，这样当该对象没有被引用时，就可以自动释放它。 延迟加载，指为了提高系统的性能，延迟对目标的加载。例如，Hibernate 中就存在属性的延迟加载和关联表的延时加载。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435package proxy;public class ProxyTest { public static void main(String[] args) { Proxy proxy = new Proxy(); proxy.Request(); }}//抽象主题interface Subject { void Request();}//真实主题class RealSubject implements Subject { public void Request() { System.out.println(&quot;访问真实主题方法...&quot;); }}//代理class Proxy implements Subject { private RealSubject realSubject; public void Request() { if (realSubject == null) { realSubject = new RealSubject(); } preRequest(); realSubject.Request(); postRequest(); } public void preRequest() { System.out.println(&quot;访问真实主题之前的预处理。&quot;); } public void postRequest() { System.out.println(&quot;访问真实主题之后的后续处理。&quot;); }} 《适配器模式 Adapter 》 特点： 将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 适配器模式分为类结构型模式和对象结构型模式两种，前者类之间的耦合度比后者高，且要求程序员了解现有组件库中的相关组件的内部结构，所以应用相对较少些。 优点： 客户端通过适配器可以透明地调用目标接口。 复用了现存的类，程序员不需要修改原有代码而重用现有的适配者类。 将目标类和适配者类解耦，解决了目标类和适配者类接口不一致的问题。 在很多业务场景中符合开闭原则。 缺点： 适配器编写过程需要结合业务场景全面考虑，可能会增加系统的复杂性。 增加代码阅读难度，降低代码可读性，过多使用适配器会使系统代码变得凌乱。 角色： 目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 适配器模式的应用场景： 以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致。 使用第三方提供的组件，但组件接口定义和自己要求的接口定义不同。 JAVA实现： 对象适配器 1234567891011121314151617181920212223242526package pers.lxl.mylearnproject.programbase.designpatterns.structuralpattern.adapter;//对象适配器类class ObjectAdapter implements Target{ private Adaptee adaptee; public ObjectAdapter(Adaptee adaptee) { this.adaptee=adaptee; } @Override public void request() { adaptee.specificRequest(); }}//客户端代码public class objectAdapterPattern{ public static void main(String[] args) { System.out.println(&quot;对象适配器模式测试：&quot;); Adaptee adaptee = new Adaptee(); Target target = new ObjectAdapter(adaptee); target.request(); }} 类适配器 1234567891011121314151617181920212223242526272829303132333435package pers.lxl.mylearnproject.programbase.designpatterns.structuralpattern.adapter;//目标接口interface Target{ public void request();}//适配者接口class Adaptee{ public void specificRequest() { System.out.println(&quot;适配者中的业务代码被调用！&quot;); }}//类适配器类 extends Adaptee implements Targetclass ClassAdapter extends Adaptee implements Target{ @Override public void request() { specificRequest(); }}//客户端代码public class ClassAdapterPattern{ public static void main(String[] args) { System.out.println(&quot;类适配器模式测试：&quot;); Target target = new ClassAdapter(); target.request(); }} 适配器模式（Adapter）可扩展为双向适配器模式，双向适配器类既可以把适配者接口转换成目标接口，也可以把目标接口转换成适配者接口，其结构图如图所示。 《桥接模式 Bridge》 特点： 将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 优点： 抽象与实现分离，扩展能力强 符合开闭原则 符合合成复用原则 其实现细节对客户透明 缺点： 由于聚合关系建立在抽象层，要求开发者针对抽象化进行设计与编程，能正确地识别出系统中两个独立变化的维度，这增加了系统的理解与设计难度。 角色： 抽象化（Abstraction）角色：定义抽象类，并包含一个对实现化对象的引用。 扩展抽象化（Refined Abstraction）角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化（Implementor）角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化（Concrete Implementor）角色：给出实现化角色接口的具体实现。 桥接模式的应用场景： 当一个类存在两个独立变化的维度，且这两个维度都需要进行扩展时。 当一个系统不希望使用继承或因为多层次继承导致系统类的个数急剧增加时。 当一个系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性时。 JAVA实现： 123456789101112131415161718192021222324252627282930313233343536package bridge;public class BridgeTest { public static void main(String[] args) { Implementor imple = new ConcreteImplementorA(); Abstraction abs = new RefinedAbstraction(imple); abs.Operation(); }}//实现化角色interface Implementor { public void OperationImpl();}//具体实现化角色class ConcreteImplementorA implements Implementor { public void OperationImpl() { System.out.println(&quot;具体实现化(Concrete Implementor)角色被访问&quot;); }}//抽象化角色abstract class Abstraction { protected Implementor imple; protected Abstraction(Implementor imple) { this.imple = imple; } public abstract void Operation();}//扩展抽象化角色class RefinedAbstraction extends Abstraction { protected RefinedAbstraction(Implementor imple) { super(imple); } public void Operation() { System.out.println(&quot;扩展抽象化(Refined Abstraction)角色被访问&quot;); imple.OperationImpl(); }} 《装饰器模式 Decorator Pattern》 特点： 不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。 结构修饰 优点： 装饰器是继承的有力补充，比继承灵活，在不改变原有对象的情况下，动态的给一个对象扩展功能，即插即用 通过使用不用装饰类及这些装饰类的排列组合，可以实现不同效果 装饰器模式完全遵守开闭原则 缺点： 装饰器模式会增加许多子类，过度使用会增加程序得复杂性 角色： 抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。 具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。 抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。 具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 装饰者模式的应用场景： 当需要给一个现有类添加附加职责，而又不能采用生成子类的方法进行扩充时。例如，该类被隐藏或者该类是终极类或者采用继承方式会产生大量的子类。 当需要通过对现有的一组基本功能进行排列组合而产生非常多的功能时，采用继承关系很难实现，而采用装饰器模式却很好实现。 当对象的功能要求可以动态地添加，也可以再动态地撤销时。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package decorator;public class DecoratorPattern { public static void main(String[] args) { Component p = new ConcreteComponent(); p.operation(); System.out.println(&quot;---------------------------------&quot;); Component d = new ConcreteDecorator(p); d.operation(); }}//抽象构件角色interface Component { public void operation();}//具体构件角色class ConcreteComponent implements Component { public ConcreteComponent() { System.out.println(&quot;创建具体构件角色&quot;); } public void operation() { System.out.println(&quot;调用具体构件角色的方法operation()&quot;); }}//抽象装饰角色class Decorator implements Component { private Component component; public Decorator(Component component) { this.component = component; } public void operation() { component.operation(); }}//具体装饰角色class ConcreteDecorator extends Decorator { public ConcreteDecorator(Component component) { super(component); } public void operation() { super.operation(); addedFunction(); } public void addedFunction() { System.out.println(&quot;为具体构件角色增加额外的功能addedFunction()&quot;); }} 《外观模式 Facade Pattern》 特点： 门面模式，是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关心内部子系统的具体细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。 结构修饰 优点： 降低了子系统与客户端之间的耦合度，使得子系统的变化不会影响调用它的客户类。 对客户屏蔽了子系统组件，减少了客户处理的对象数目，并使得子系统使用起来更加容易。 降低了大型软件系统中的编译依赖性，简化了系统在不同平台之间的移植过程，因为编译一个子系统不会影响其他的子系统，也不会影响外观对象。 缺点： 不能很好地限制客户使用子系统类，很容易带来未知风险。 增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则” 角色： 外观（Facade）角色：为多个子系统对外提供一个共同的接口。 子系统（Sub System）角色：实现系统的部分功能，客户可以通过外观角色访问它。 客户（Client）角色：通过一个外观角色访问各个子系统的功能。 外观 模式的应用场景： 对分层结构系统构建时，使用外观模式定义子系统中每层的入口点可以简化子系统之间的依赖关系。 当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。 当客户端与多个子系统之间存在很大的联系时，引入外观模式可将它们分离，从而提高子系统的独立性和可移植性。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435public class FacadePattern { public static void main(String[] args) { Facade f = new Facade(); f.method(); }}//外观角色class Facade { private SubSystem01 obj1 = new SubSystem01(); private SubSystem02 obj2 = new SubSystem02(); private SubSystem03 obj3 = new SubSystem03(); public void method() { obj1.method1(); obj2.method2(); obj3.method3(); }}//子系统角色class SubSystem01 { public void method1() { System.out.println(&quot;子系统01的method1()被调用！&quot;); }}//子系统角色class SubSystem02 { public void method2() { System.out.println(&quot;子系统02的method2()被调用！&quot;); }}//子系统角色class SubSystem03 { public void method3() { System.out.println(&quot;子系统03的method3()被调用！&quot;); }} 《享元模式 Flyweight Pattern》 特点： 运用共享技术来有效地支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量、避免大量相似类的开销，从而提高系统资源的利用率。 优点： 相同对象只要保存一份，这降低了系统中对象的数量，从而降低了系统中细粒度对象给内存带来的压力。 缺点： 为了使对象可以共享，需要将一些不能共享的状态外部化，这将增加程序的复杂性。 读取享元模式的外部状态会使得运行时间稍微变长。 角色： ​ 享元模式的定义提出了两个要求，细粒度和共享对象。因为要求细粒度，所以不可避免地会使对象数量多且性质相近，此时我们就将这些对象的信息分为两个部分：内部状态和外部状态。 ​ 内部状态指对象共享出来的信息，存储在享元信息内部，并且不回随环境的改变而改变； ​ 外部状态指对象得以依赖的一个标记，随环境的改变而改变，不可共享。 比如，连接池中的连接对象，保存在连接对象中的用户名、密码、连接URL等信息，在创建对象的时候就设置好了，不会随环境的改变而改变，这些为内部状态。而当每个连接要被回收利用时，我们需要将它标记为可用状态，这些为外部状态。享元模式的本质是缓存共享对象，降低内存消耗。 抽象享元角色（Flyweight）：是所有的具体享元类的基类，为具体享元规范需要实现的公共接口，非享元的外部状态以参数的形式通过方法传入。 具体享元（Concrete Flyweight）角色：实现抽象享元角色中所规定的接口。 非享元（Unsharable Flyweight)角色：是不可以共享的外部状态，它以参数的形式注入具体享元的相关方法中。 享元工厂（Flyweight Factory）角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检査系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。 享元模式的应用场景： 系统中存在大量相同或相似的对象，这些对象耗费大量的内存资源。 大部分的对象可以按照内部状态进行分组，且可将不同部分外部化，这样每一个组只需保存一个内部状态。 由于享元模式需要额外维护一个保存享元的数据结构，所以应当在有足够多的享元实例时才值得使用享元模式。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class FlyweightPattern { public static void main(String[] args) { FlyweightFactory factory = new FlyweightFactory(); Flyweight f01 = factory.getFlyweight(&quot;a&quot;); Flyweight f02 = factory.getFlyweight(&quot;a&quot;); Flyweight f03 = factory.getFlyweight(&quot;a&quot;); Flyweight f11 = factory.getFlyweight(&quot;b&quot;); Flyweight f12 = factory.getFlyweight(&quot;b&quot;); f01.operation(new UnsharedConcreteFlyweight(&quot;第1次调用a。&quot;)); f02.operation(new UnsharedConcreteFlyweight(&quot;第2次调用a。&quot;)); f03.operation(new UnsharedConcreteFlyweight(&quot;第3次调用a。&quot;)); f11.operation(new UnsharedConcreteFlyweight(&quot;第1次调用b。&quot;)); f12.operation(new UnsharedConcreteFlyweight(&quot;第2次调用b。&quot;)); }}//非享元角色class UnsharedConcreteFlyweight { private String info; UnsharedConcreteFlyweight(String info) { this.info = info; } public String getInfo() { return info; } public void setInfo(String info) { this.info = info; }}//抽象享元角色interface Flyweight { public void operation(UnsharedConcreteFlyweight state);}//具体享元角色class ConcreteFlyweight implements Flyweight { private String key; ConcreteFlyweight(String key) { this.key = key; System.out.println(&quot;具体享元&quot; + key + &quot;被创建！&quot;); } public void operation(UnsharedConcreteFlyweight outState) { System.out.print(&quot;具体享元&quot; + key + &quot;被调用，&quot;); System.out.println(&quot;非享元信息是:&quot; + outState.getInfo()); }}//享元工厂角色class FlyweightFactory { private HashMap&lt;String, Flyweight&gt; flyweights = new HashMap&lt;String, Flyweight&gt;(); public Flyweight getFlyweight(String key) { Flyweight flyweight = (Flyweight) flyweights.get(key); if (flyweight != null) { System.out.println(&quot;具体享元&quot; + key + &quot;已经存在，被成功获取！&quot;); } else { flyweight = new ConcreteFlyweight(key); flyweights.put(key, flyweight); } return flyweight; }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106import javax.swing.*;import java.awt.*;import java.awt.event.MouseAdapter;import java.awt.event.MouseEvent;import java.util.ArrayList;public class WzqGame { public static void main(String[] args) { new Chessboard(); }}//棋盘class Chessboard extends MouseAdapter { WeiqiFactory wf; JFrame f; Graphics g; JRadioButton wz; JRadioButton bz; private final int x = 50; private final int y = 50; private final int w = 40; //小方格宽度和高度 private final int rw = 400; //棋盘宽度和高度 Chessboard() { wf = new WeiqiFactory(); f = new JFrame(&quot;享元模式在五子棋游戏中的应用&quot;); f.setBounds(100, 100, 500, 550); f.setVisible(true); f.setResizable(false); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); JPanel SouthJP = new JPanel(); f.add(&quot;South&quot;, SouthJP); wz = new JRadioButton(&quot;白子&quot;); bz = new JRadioButton(&quot;黑子&quot;, true); ButtonGroup group = new ButtonGroup(); group.add(wz); group.add(bz); SouthJP.add(wz); SouthJP.add(bz); JPanel CenterJP = new JPanel(); CenterJP.setLayout(null); CenterJP.setSize(500, 500); CenterJP.addMouseListener(this); f.add(&quot;Center&quot;, CenterJP); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } g = CenterJP.getGraphics(); g.setColor(Color.BLUE); g.drawRect(x, y, rw, rw); for (int i = 1; i &lt; 10; i++) { //绘制第i条竖直线 g.drawLine(x + (i * w), y, x + (i * w), y + rw); //绘制第i条水平线 g.drawLine(x, y + (i * w), x + rw, y + (i * w)); } } public void mouseClicked(MouseEvent e) { Point pt = new Point(e.getX() - 15, e.getY() - 15); if (wz.isSelected()) { ChessPieces c1 = wf.getChessPieces(&quot;w&quot;); c1.DownPieces(g, pt); } else if (bz.isSelected()) { ChessPieces c2 = wf.getChessPieces(&quot;b&quot;); c2.DownPieces(g, pt); } }}//抽象享元角色：棋子interface ChessPieces { public void DownPieces(Graphics g, Point pt); //下子}//具体享元角色：白子class WhitePieces implements ChessPieces { public void DownPieces(Graphics g, Point pt) { g.setColor(Color.WHITE); g.fillOval(pt.x, pt.y, 30, 30); }}//具体享元角色：黑子class BlackPieces implements ChessPieces { public void DownPieces(Graphics g, Point pt) { g.setColor(Color.BLACK); g.fillOval(pt.x, pt.y, 30, 30); }}//享元工厂角色class WeiqiFactory { private ArrayList&lt;ChessPieces&gt; qz; public WeiqiFactory() { qz = new ArrayList&lt;ChessPieces&gt;(); ChessPieces w = new WhitePieces(); qz.add(w); ChessPieces b = new BlackPieces(); qz.add(b); } public ChessPieces getChessPieces(String type) { if (type.equalsIgnoreCase(&quot;w&quot;)) { return (ChessPieces) qz.get(0); } else if (type.equalsIgnoreCase(&quot;b&quot;)) { return (ChessPieces) qz.get(1); } else { return null; } }} 《组合模式 Composite Pattern》 特点： 将对象组合成树状的层次结构的模式，用来表示“整体-部分”的关系，使用户对单个对象和组合对象具有一致的访问性 部分-整体 优点： 组合模式使得客户端代码可以一致地处理单个对象和组合对象，无须关心自己处理的是单个对象，还是组合对象，这简化了客户端代码； 更容易在组合体内加入新的对象，客户端不会因为加入了新的对象而更改源代码，满足“开闭原则”； 缺点： 设计较复杂，客户端需要花更多时间理清类之间的层次关系； 不容易限制容器中的构件； 不容易用继承的方法来增加构件的新功能； 角色： 分类： 透明方式 安全方式 抽象构件（Component）角色：它的主要作用是为树叶构件和树枝构件声明公共接口，并实现它们的默认行为。在透明式的组合模式中抽象构件还声明访问和管理子类的接口；在安全式的组合模式中不声明访问和管理子类的接口，管理工作由树枝构件完成。（总的抽象类或接口，定义一些通用的方法，比如新增、删除） 树叶构件（Leaf）角色：是组合中的叶节点对象，它没有子节点，用于继承或实现抽象构件。 树枝构件（Composite）角色 / 中间构件：是组合中的分支节点对象，它有子节点，用于继承和实现抽象构件。它的主要作用是存储和管理子部件，通常包含 Add()、Remove()、GetChild() 等方法。 组合模式的应用场景： 在需要表示一个对象整体与部分的层次结构的场合。 要求对用户隐藏组合对象与单个对象的不同，用户可以用统一的接口使用组合结构中的所有对象的场合。 JAVA实现： 安全组合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.util.ArrayList;public class Safe { public static void main(String[] args) { Composite c0 = new Composite(); Composite c1 = new Composite(); Component leaf1 = new Leaf(&quot;1&quot;); Component leaf2 = new Leaf(&quot;2&quot;); Component leaf3 = new Leaf(&quot;3&quot;); c0.add(leaf1); c0.add(c1); c1.add(leaf2); c1.add(leaf3); c0.operation(); }}//抽象构件interface Component {// public void add(Component c);// public void remove(Component c);// public Component getChild(int i); public void operation();}//树叶构件class Leaf implements Component { private String name; public Leaf(String name) { this.name = name; } public void add(Component c) { } public void remove(Component c) { } public Component getChild(int i) { return null; } @Override public void operation() { System.out.println(&quot;树叶&quot; + name + &quot;：被访问！&quot;); }}//树枝构件class Composite implements Component { private ArrayList&lt;Component&gt; children = new ArrayList&lt;Component&gt;(); public void add(Component c) { children.add(c); } public void remove(Component c) { children.remove(c); } public Component getChild(int i) { return children.get(i); } @Override public void operation() { for (Object obj : children) { ((Component) obj).operation(); } }} 透明组合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import java.util.ArrayList;public class transparent { public static void main(String[] args) { Component c0 = new Composite(); Component c1 = new Composite(); Component leaf1 = new Leaf(&quot;1&quot;); Component leaf2 = new Leaf(&quot;2&quot;); Component leaf3 = new Leaf(&quot;3&quot;); c0.add(leaf1); c0.add(c1); c1.add(leaf2); c1.add(leaf3); c0.operation(); }}//抽象构件interface Component { public void add(Component c); public void remove(Component c); public Component getChild(int i); public void operation();}//树叶构件class Leaf implements Component { private String name; public Leaf(String name) { this.name = name; } @Override public void add(Component c) { } @Override public void remove(Component c) { } @Override public Component getChild(int i) { return null; } @Override public void operation() { System.out.println(&quot;树叶&quot; + name + &quot;：被访问！&quot;); }}//树枝构件class Composite implements Component { private ArrayList&lt;Component&gt; children = new ArrayList&lt;Component&gt;(); @Override public void add(Component c) { children.add(c); } @Override public void remove(Component c) { children.remove(c); } @Override public Component getChild(int i) { return children.get(i); } @Override public void operation() { for (Object obj : children) { ((Component) obj).operation(); } }} 行为型模式 《**模板方法模式 **Template method pattern》 特点： 定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 优点： 它封装了不变部分，扩展可变部分。它把认为是不变部分的算法封装到父类中实现，而把可变部分算法由子类继承实现，便于子类继续扩展。 它在父类中提取了公共的部分代码，便于代码复用。 部分方法是由子类实现的，因此子类可以通过扩展方式增加相应的功能，符合开闭原则。 缺点： 对每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象，间接地增加了系统实现的复杂度。 父类中的抽象方法由子类实现，子类执行的结果会影响父类的结果，这导致一种反向的控制结构，它提高了代码阅读的难度。 由于继承关系自身的缺点，如果父类添加新的抽象方法，则所有子类都要改一遍。 角色： 1）抽象类/抽象模板（Abstract Class） 抽象模板类，负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。这些方法的定义如下。 ① 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法。 ② 基本方法：是整个算法中的一个步骤，包含以下几种类型。 抽象方法：在抽象类中声明，由具体子类实现。 具体方法：在抽象类中已经实现，在具体子类中可以继承或重写它。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写的空方法两种。 2）具体子类/具体实现（Concrete Class） 具体实现类，实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的一个组成步骤。 模板方法模式的应用场景： 算法的整体步骤很固定，但其中个别部分易变时，这时候可以使用模板方法模式，将容易变的部分抽象出来，供子类实现。 当多个子类存在公共的行为时，可以将其提取出来并集中到一个公共父类中以避免代码重复。首先，要识别现有代码中的不同之处，并且将不同之处分离为新的操作。最后，用一个调用这些新的操作的模板方法来替换这些不同的代码。 当需要控制子类的扩展时，模板方法只在特定点调用钩子操作，这样就只允许在这些点进行扩展。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334public class TemplateMethodPattern { public static void main(String[] args) { AbstractClass tm = new ConcreteClass(); tm.TemplateMethod(); }}//抽象类abstract class AbstractClass { //模板方法 public void TemplateMethod() { SpecificMethod(); abstractMethod1(); abstractMethod2(); } //具体方法 public void SpecificMethod() { System.out.println(&quot;抽象类中的具体方法被调用...&quot;); } //抽象方法1 public abstract void abstractMethod1(); //抽象方法2 public abstract void abstractMethod2();}//具体子类class ConcreteClass extends AbstractClass { @Override public void abstractMethod1() { System.out.println(&quot;抽象方法1的实现被调用...&quot;); } @Override public void abstractMethod2() { System.out.println(&quot;抽象方法2的实现被调用...&quot;); }} 《**策略模式 **Strategy Pattern》 特点： 该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。 优点： 多重条件语句不易维护，而使用策略模式可以避免使用多重条件语句，如 if...else 语句、switch...case 语句。 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码。 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的。 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法。 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离。 缺点： 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类。 策略模式造成很多的策略类，增加维护难度。 角色： 抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。 环境（Context）类：持有一个策略类的引用，最终给客户端调用。 策略模式的应用场景： 一个系统需要动态地在几种算法中选择一种时，可将每个算法封装到策略类中。 一个类定义了多种行为，并且这些行为在这个类的操作中以多个条件语句的形式出现，可将每个条件分支移入它们各自的策略类中以代替这些条件语句。 系统中各算法彼此完全独立，且要求对客户隐藏具体算法的实现细节时。 系统要求使用算法的客户不应该知道其操作的数据时，可使用策略模式来隐藏与算法相关的数据结构。 多个类只区别在表现行为不同，可以使用策略模式，在运行时动态选择具体要执行的行为。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041public class StrategyPattern { public static void main(String[] args) { Context c = new Context(); Strategy s = new ConcreteStrategyA(); c.setStrategy(s); c.strategyMethod(); System.out.println(&quot;-----------------&quot;); s = new ConcreteStrategyB(); c.setStrategy(s); c.strategyMethod(); }}//抽象策略类interface Strategy { public void strategyMethod(); //策略方法}//具体策略类Aclass ConcreteStrategyA implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略A的策略方法被访问！&quot;); }}//具体策略类Bclass ConcreteStrategyB implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略B的策略方法被访问！&quot;); }}//环境类class Context { private Strategy strategy; public Strategy getStrategy() { return strategy; } public void setStrategy(Strategy strategy) { this.strategy = strategy; } public void strategyMethod() { strategy.strategyMethod(); }} 《**命令模式 **Command Pattern》 特点： 将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行储存、传递、调用、增加与管理。 优点： 通过引入中间件（抽象接口）降低系统的耦合度。 扩展性良好，增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，且满足“开闭原则”。 可以实现宏命令。命令模式可以与组合模式结合，将多个命令装配成一个组合命令，即宏命令。 方便实现 Undo 和 Redo 操作。命令模式可以与后面介绍的备忘录模式结合，实现命令的撤销与恢复。 可以在现有命令的基础上，增加额外功能。比如日志记录，结合装饰器模式会更加灵活。 缺点： 可能产生大量具体的命令类。因为每一个具体操作都需要设计一个具体命令类，这会增加系统的复杂性。 命令模式的结果其实就是接收方的执行结果，但是为了以命令的形式进行架构、解耦请求与实现，引入了额外类型结构（引入了请求方与抽象命令接口），增加了理解上的困难。不过这也是设计模式的通病，抽象必然会额外增加类的数量，代码抽离肯定比代码聚合更加难理解 角色： 抽象命令类（Command）角色：声明执行命令的接口，拥有执行命令的抽象方法 execute()。 具体命令类（Concrete Command）角色：是抽象命令类的具体实现类，它拥有接收者对象，并通过调用接收者的功能来完成命令要执行的操作。 实现者/接收者（Receiver）角色：执行命令功能的相关操作，是具体命令对象业务的真正实现者。 调用者/请求者（Invoker）角色：是请求的发送者，它通常拥有很多的命令对象，并通过访问命令对象来执行相关请求，它不直接访问接收者。 命令模式的应用场景： 请求调用者需要与请求接收者解耦时，命令模式可以使调用者和接收者不直接交互。 系统随机请求命令或经常增加、删除命令时，命令模式可以方便地实现这些功能。 当系统需要执行一组操作时，命令模式可以定义宏命令来实现该功能。 当系统需要支持命令的撤销（Undo）操作和恢复（Redo）操作时，可以将命令对象存储起来，采用备忘录模式来实现。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041public class StrategyPattern { public static void main(String[] args) { Context c = new Context(); Strategy s = new ConcreteStrategyA(); c.setStrategy(s); c.strategyMethod(); System.out.println(&quot;-----------------&quot;); s = new ConcreteStrategyB(); c.setStrategy(s); c.strategyMethod(); }}//抽象策略类interface Strategy { public void strategyMethod(); //策略方法}//具体策略类Aclass ConcreteStrategyA implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略A的策略方法被访问！&quot;); }}//具体策略类Bclass ConcreteStrategyB implements Strategy { public void strategyMethod() { System.out.println(&quot;具体策略B的策略方法被访问！&quot;); }}//环境类class Context { private Strategy strategy; public Strategy getStrategy() { return strategy; } public void setStrategy(Strategy strategy) { this.strategy = strategy; } public void strategyMethod() { strategy.strategyMethod(); }} 《**责任/职责 链模式 **Chain of Responsibility pattern》 特点： 为了避免请求发送者与多个请求处理者耦合在一起，于是将所有请求的处理者通过前一对象记住其下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。 优点： 降低了对象之间的耦合度。该模式使得一个对象无须知道到底是哪一个对象处理其请求以及链的结构，发送者和接收者也无须拥有对方的明确信息。 增强了系统的可扩展性。可以根据需要增加新的请求处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态地改变链内的成员或者调动它们的次序，也可动态地新增或者删除责任。 责任链简化了对象之间的连接。每个对象只需保持一个指向其后继者的引用，不需保持其他所有处理者的引用，这避免了使用众多的 if 或者 if···else 语句。 责任分担。每个类只需要处理自己该处理的工作，不该处理的传递给下一个对象完成，明确各类的责任范围，符合类的单一职责原则。 缺点： 不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。 对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。 角色： 抽象处理者（Handler）角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者（Concrete Handler）角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类（Client）角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程。 责任/职责 链模式的应用场景： 多个对象可以处理一个请求，但具体由哪个对象处理该请求在运行时自动确定。 可动态指定一组对象处理请求，或添加新的处理者。 需要在不明确指定请求处理者的情况下，向多个处理者中的一个提交请求。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ChainOfResponsibilityPattern { public static void main(String[] args) { //组装责任链 Handler handler1 = new ConcreteHandler1(); Handler handler2 = new ConcreteHandler2(); handler1.setNext(handler2); //提交请求 handler1.handleRequest(&quot;two&quot;); }}//抽象处理者角色abstract class Handler { private Handler next; public void setNext(Handler next) { this.next = next; } public Handler getNext() { return next; } //处理请求的方法 public abstract void handleRequest(String request);}//具体处理者角色1class ConcreteHandler1 extends Handler { @Override public void handleRequest(String request) { if (request.equals(&quot;one&quot;)) { System.out.println(&quot;具体处理者1负责处理该请求！&quot;); } else { if (getNext() != null) { getNext().handleRequest(request); } else { System.out.println(&quot;没有人处理该请求！&quot;); } } }}//具体处理者角色2class ConcreteHandler2 extends Handler { @Override public void handleRequest(String request) { if (request.equals(&quot;two&quot;)) { System.out.println(&quot;具体处理者2负责处理该请求！&quot;); } else { if (getNext() != null) { getNext().handleRequest(request); } else { System.out.println(&quot;没有人处理该请求！&quot;); } } }} 《**状态模式 **State pattern》 特点： 对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。 优点： 结构清晰，状态模式将与特定状态相关的行为局部化到一个状态中，并且将不同状态的行为分割开来，满足“单一职责原则”。 将状态转换显示化，减少对象间的相互依赖。将不同的状态引入独立的对象中会使得状态转换变得更加明确，且减少对象间的相互依赖。 状态类职责明确，有利于程序的扩展。通过定义新的子类很容易地增加新的状态和转换。 缺点： 状态模式的使用必然会增加系统的类与对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当会导致程序结构和代码的混乱。 状态模式对开闭原则的支持并不太好，对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源码，否则无法切换到新增状态，而且修改某个状态类的行为也需要修改对应类的源码。 角色： 环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。 状态模式的应用场景： 当一个对象的行为取决于它的状态，并且它必须在运行时根据状态改变它的行为时，就可以考虑使用状态模式。 一个操作中含有庞大的分支结构，并且这些分支决定于对象的状态时。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class StatePatternClient { public static void main(String[] args) { Context context = new Context(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); }}//环境类class Context { private State state; //定义环境类的初始状态 public Context() { this.state = new ConcreteStateA(); } //设置新状态 public void setState(State state) { this.state = state; } //读取状态 public State getState() { return (state); } //对请求做处理 public void Handle() { state.Handle(this); }}//抽象状态类abstract class State { public abstract void Handle(Context context);}//具体状态A类class ConcreteStateA extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 A.&quot;); context.setState(new ConcreteStateB()); }}//具体状态B类class ConcreteStateB extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 B.&quot;); context.setState(new ConcreteStateA()); }} 《观察者模式 Observer pattern》 特点： 指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式。 优点： 降低了目标与观察者之间的耦合关系，两者之间是抽象耦合关系。符合依赖倒置原则。 目标与观察者之间建立了一套触发机制。 缺点： 目标与观察者之间的依赖关系并没有完全解除，而且有可能出现循环引用。 当观察者对象很多时，通知的发布会花费很多时间，影响程序的效率。 角色： 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。 观察者模式的应用场景： 对象间存在一对多关系，一个对象的状态发生改变会影响其他对象。 当一个抽象模型有两个方面，其中一个方面依赖于另一方面时，可将这二者封装在独立的对象中以使它们可以各自独立地改变和复用。 实现类似广播机制的功能，不需要知道具体收听者，只需分发广播，系统中感兴趣的对象会自动接收该广播。 多层级嵌套使用，形成一种链式触发机制，使得事件具备跨域（跨越两种观察者类型）通知。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class StatePatternClient { public static void main(String[] args) { Context context = new Context(); //创建环境 context.Handle(); //处理请求 context.Handle(); context.Handle(); context.Handle(); }}//环境类class Context { private State state; //定义环境类的初始状态 public Context() { this.state = new ConcreteStateA(); } //设置新状态 public void setState(State state) { this.state = state; } //读取状态 public State getState() { return (state); } //对请求做处理 public void Handle() { state.Handle(this); }}//抽象状态类abstract class State { public abstract void Handle(Context context);}//具体状态A类class ConcreteStateA extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 A.&quot;); context.setState(new ConcreteStateB()); }}//具体状态B类class ConcreteStateB extends State { @Override public void Handle(Context context) { System.out.println(&quot;当前状态是 B.&quot;); context.setState(new ConcreteStateA()); }} 《中介者模式 Mediator Pattern》 特点： 定义一个中介对象来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者模式又叫调停模式，它是迪米特法则的典型应用。 优点： 类之间各司其职，符合迪米特法则。 降低了对象之间的耦合性，使得对象易于独立地被复用。 将对象间的一对多关联转变为一对一的关联，提高系统的灵活性，使得系统易于维护和扩展。 缺点： 中介者模式将原本多个对象直接的相互依赖变成了中介者和多个同事类的依赖关系。当同事类越多时，中介者就会越臃肿，变得复杂且难以维护。 角色： 抽象中介者（Mediator）角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者（Concrete Mediator）角色：实现中介者接口，定义一个 List 来管理同事对象，协调各个同事角色之间的交互关系，因此它依赖于同事角色。 抽象同事类（Colleague）角色：定义同事类的接口，保存中介者对象，提供同事对象交互的抽象方法，实现所有相互影响的同事类的公共功能。 具体同事类（Concrete Colleague）角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。 中介者模式的应用场景： 当对象之间存在复杂的网状结构关系而导致依赖关系混乱且难以复用时。 当想创建一个运行于多个类之间的对象，又不想生成新的子类时 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.*;public class MediatorPattern { public static void main(String[] args) { Mediator md = new ConcreteMediator(); Colleague c1, c2; c1 = new ConcreteColleague1(); c2 = new ConcreteColleague2(); md.register(c1); md.register(c2); c1.send(); System.out.println(&quot;-------------&quot;); c2.send(); }}//抽象中介者abstract class Mediator { public abstract void register(Colleague colleague); public abstract void relay(Colleague cl); //转发}//具体中介者class ConcreteMediator extends Mediator { private List&lt;Colleague&gt; colleagues = new ArrayList&lt;Colleague&gt;(); public void register(Colleague colleague) { if (!colleagues.contains(colleague)) { colleagues.add(colleague); colleague.setMedium(this); } } public void relay(Colleague cl) { for (Colleague ob : colleagues) { if (!ob.equals(cl)) { ((Colleague) ob).receive(); } } }}//抽象同事类abstract class Colleague { protected Mediator mediator; public void setMedium(Mediator mediator) { this.mediator = mediator; } public abstract void receive(); public abstract void send();}//具体同事类class ConcreteColleague1 extends Colleague { public void receive() { System.out.println(&quot;具体同事类1收到请求。&quot;); } public void send() { System.out.println(&quot;具体同事类1发出请求。&quot;); mediator.relay(this); //请中介者转发 }}//具体同事类class ConcreteColleague2 extends Colleague { public void receive() { System.out.println(&quot;具体同事类2收到请求。&quot;); } public void send() { System.out.println(&quot;具体同事类2发出请求。&quot;); mediator.relay(this); //请中介者转发 }} 《迭代器模式 Iterator Pattern》 特点： 提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 优点： 访问一个聚合对象的内容而无须暴露它的内部表示。 遍历任务交由迭代器完成，这简化了聚合类。 它支持以不同方式遍历一个聚合，甚至可以自定义迭代器的子类以支持新的遍历。 增加新的聚合类和迭代器类都很方便，无须修改原有代码。 封装性良好，为遍历不同的聚合结构提供一个统一的接口。 缺点： 增加了类的个数，这在一定程度上增加了系统的复杂性。 角色： 抽象聚合（Aggregate）角色：定义存储、添加、删除聚合对象以及创建迭代器对象的接口。 具体聚合（ConcreteAggregate）角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器（Iterator）角色：定义访问和遍历聚合元素的接口，通常包含 hasNext()、first()、next() 等方法。 具体迭代器（Concretelterator）角色：实现抽象迭代器接口中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。 中介者模式的应用场景： 当需要为聚合对象提供多种遍历方式时。 当需要为遍历不同的聚合结构提供一个统一的接口时。 当访问一个聚合对象的内容而无须暴露其内部细节的表示时。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import java.util.*;public class IteratorPattern { public static void main(String[] args) { Aggregate ag = new ConcreteAggregate(); ag.add(&quot;中山大学&quot;); ag.add(&quot;华南理工&quot;); ag.add(&quot;韶关学院&quot;); System.out.print(&quot;聚合的内容有：&quot;); Iterator it = ag.getIterator(); while (it.hasNext()) { Object ob = it.next(); System.out.print(ob.toString() + &quot;\\t&quot;); } Object ob = it.first(); System.out.println(&quot;\\nFirst：&quot; + ob.toString()); }}//抽象聚合 定义存储、添加、删除聚合对象以及创建迭代器对象的接口。interface Aggregate { public void add(Object obj); public void remove(Object obj); public Iterator getIterator();}//具体聚合 实现抽象聚合类，返回一个具体迭代器的实例。class ConcreteAggregate implements Aggregate { private List&lt;Object&gt; list = new ArrayList&lt;Object&gt;(); @Override public void add(Object obj) { list.add(obj); } @Override public void remove(Object obj) { list.remove(obj); } @Override public Iterator getIterator() { return (new ConcreteIterator(list)); }}//抽象迭代器interface Iterator { Object first(); Object next(); boolean hasNext();}//具体迭代器class ConcreteIterator implements Iterator { private List&lt;Object&gt; list = null; private int index = -1; public ConcreteIterator(List&lt;Object&gt; list) { this.list = list; } @Override public boolean hasNext() { if (index &lt; list.size() - 1) { return true; } else { return false; } } @Override public Object first() { index = 0; Object obj = list.get(index); ; return obj; } @Override public Object next() { Object obj = null; if (this.hasNext()) { obj = list.get(++index); } return obj; }} 《访问者模式 Visitor Pattern》 特点： 将作用于某种数据结构中的各元素的操作分离出来封装成独立的类，使其在不改变数据结构的前提下可以添加作用于这些元素的新的操作，为数据结构中的每个元素提供多种访问方式。它将对数据的操作与数据结构进行分离，是行为类模式中最复杂的一种模式。 优点： 扩展性好。能够在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能。 复用性好。可以通过访问者来定义整个对象结构通用的功能，从而提高系统的复用程度。 灵活性好。访问者模式将数据结构与作用于结构上的操作解耦，使得操作集合可相对自由地演化而不影响系统的数据结构。 符合单一职责原则。访问者模式把相关的行为封装在一起，构成一个访问者，使每一个访问者的功能都比较单一。 缺点： 增加新的元素类很困难。在访问者模式中，每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了“开闭原则”。 破坏封装。访问者模式中具体元素对访问者公布细节，这破坏了对象的封装性。 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 角色： 抽象访问者（Visitor）角色：定义一个访问具体元素的接口，为每个具体元素类对应一个访问操作 visit() ，该操作中的参数类型标识了被访问的具体元素。 具体访问者（ConcreteVisitor）角色：实现抽象访问者角色中声明的各个访问操作，确定访问者访问一个元素时该做什么。 抽象元素（Element）角色：声明一个包含接受操作 accept() 的接口，被接受的访问者对象作为 accept() 方法的参数。 具体元素（ConcreteElement）角色：实现抽象元素角色提供的 accept() 操作，其方法体通常都是 visitor.visit(this) ，另外具体元素中可能还包含本身业务逻辑的相关操作。 对象结构（Object Structure）角色：是一个包含元素角色的容器，提供让访问者对象遍历容器中的所有元素的方法，通常由 List、Set、Map 等聚合类实现。 访问者模式的应用场景： 对象结构相对稳定，但其操作算法经常变化的程序。 对象结构中的对象需要提供多种不同且不相关的操作，而且要避免让这些操作的变化影响对象的结构。 对象结构包含很多类型的对象，希望对这些对象实施一些依赖于其具体类型的操作。 JAVA实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.util.*;public class VisitorPattern { public static void main(String[] args) { ObjectStructure os = new ObjectStructure(); os.add(new ConcreteElementA()); os.add(new ConcreteElementB()); Visitor visitor = new ConcreteVisitorA(); os.accept(visitor); System.out.println(&quot;------------------------&quot;); visitor = new ConcreteVisitorB(); os.accept(visitor); }}//抽象访问者interface Visitor { void visit(ConcreteElementA element); void visit(ConcreteElementB element);}//具体访问者A类class ConcreteVisitorA implements Visitor { public void visit(ConcreteElementA element) { System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationA()); } public void visit(ConcreteElementB element) { System.out.println(&quot;具体访问者A访问--&gt;&quot; + element.operationB()); }}//具体访问者B类class ConcreteVisitorB implements Visitor { public void visit(ConcreteElementA element) { System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationA()); } public void visit(ConcreteElementB element) { System.out.println(&quot;具体访问者B访问--&gt;&quot; + element.operationB()); }}//抽象元素类interface Element { void accept(Visitor visitor);}//具体元素A类class ConcreteElementA implements Element { public void accept(Visitor visitor) { visitor.visit(this); } public String operationA() { return &quot;具体元素A的操作。&quot;; }}//具体元素B类class ConcreteElementB implements Element { public void accept(Visitor visitor) { visitor.visit(this); } public String operationB() { return &quot;具体元素B的操作。&quot;; }}//对象结构角色class ObjectStructure { private List&lt;Element&gt; list = new ArrayList&lt;Element&gt;(); public void accept(Visitor visitor) { Iterator&lt;Element&gt; i = list.iterator(); while (i.hasNext()) { ((Element) i.next()).accept(visitor); } } public void add(Element element) { list.add(element); } public void remove(Element element) { list.remove(element); }} 《备忘录/快照 模式 Memento Pattern》 特点： 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便以后当需要时能将该对象恢复到原先保存的状态。该模式又叫快照模式。 优点： 提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史的状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息。 简化了发起人类。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 缺点： 资源消耗大。如果要保存的内部状态信息过多或者特别频繁，将会占用比较大的内存资源。 角色： 发起人（Originator）角色：记录当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录（Memento）角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者（Caretaker）角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。 备忘录/快照 模式的应用场景： JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class MementoPattern { public static void main(String[] args) { Originator or = new Originator(); Caretaker cr = new Caretaker(); or.setState(&quot;S0&quot;); System.out.println(&quot;初始状态:&quot; + or.getState()); cr.setMemento(or.createMemento()); //保存状态 or.setState(&quot;S1&quot;); System.out.println(&quot;新的状态:&quot; + or.getState()); or.restoreMemento(cr.getMemento()); //恢复状态 System.out.println(&quot;恢复状态:&quot; + or.getState()); }}//备忘录class Memento { private String state; public Memento(String state) { this.state = state; } public void setState(String state) { this.state = state; } public String getState() { return state; }}//发起人class Originator { private String state; public void setState(String state) { this.state = state; } public String getState() { return state; } public Memento createMemento() { return new Memento(state); } public void restoreMemento(Memento m) { this.setState(m.getState()); }}//管理者class Caretaker { private Memento memento; public void setMemento(Memento m) { memento = m; } public Memento getMemento() { return memento; }} 《解释器模式 Interpreter Pattern》 特点： 给分析对象定义一个语言，并定义该语言的文法表示，再设计一个解析器来解释语言中的句子。也就是说，用编译语言的方式来分析应用中的实例。这种模式实现了文法表达式处理的接口，该接口解释一个特定的上下文。 优点： 扩展性好。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变或扩展文法。 容易实现。在语法树中的每个表达式节点类都是相似的，所以实现其文法较为容易。 缺点： 执行效率较低。解释器模式中通常使用大量的循环和递归调用，当要解释的句子较复杂时，其运行速度很慢，且代码的调试过程也比较麻烦。 会引起类膨胀。解释器模式中的每条规则至少需要定义一个类，当包含的文法规则很多时，类的个数将急剧增加，导致系统难以管理与维护。 可应用的场景比较少。在软件开发中，需要定义语言文法的应用实例非常少，所以这种模式很少被使用到。 角色： 文法（主谓宾）、句子、语法树 抽象表达式（Abstract Expression）角色：定义解释器的接口，约定解释器的解释操作，主要包含解释方法 interpret()。 终结符表达式（Terminal Expression）角色：是抽象表达式的子类，用来实现文法中与终结符相关的操作，文法中的每一个终结符都有一个具体终结表达式与之相对应。 非终结符表达式（Nonterminal Expression）角色：也是抽象表达式的子类，用来实现文法中与非终结符相关的操作，文法中的每条规则都对应于一个非终结符表达式。 环境（Context）角色：通常包含各个解释器需要的数据或是公共的功能，一般用来传递被所有解释器共享的数据，后面的解释器可以从这里获取这些值。 客户端（Client）：主要任务是将需要分析的句子或表达式转换成使用解释器对象描述的抽象语法树，然后调用解释器的解释方法，当然也可以通过环境角色间接访问解释器的解释方法。 解释器模式的应用场景： 当语言的文法较为简单，且执行效率不是关键问题时。 当问题重复出现，且可以用一种简单的语言来进行表达时。 当一个语言需要解释执行，并且语言中的句子可以表示为一个抽象语法树的时候，如 XML 文档解释。 JAVA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import java.util.*;/*文法规则 &lt;expression&gt; ::= &lt;city&gt;的&lt;person&gt; &lt;city&gt; ::= 韶关|广州 &lt;person&gt; ::= 老人|妇女|儿童*/public class InterpreterPatternDemo { public static void main(String[] args) { Context bus = new Context(); bus.freeRide(&quot;韶关的老人&quot;); bus.freeRide(&quot;韶关的年轻人&quot;); bus.freeRide(&quot;广州的妇女&quot;); bus.freeRide(&quot;广州的儿童&quot;); bus.freeRide(&quot;山东的儿童&quot;); }}//抽象表达式类interface Expression { public boolean interpret(String info);}//终结符表达式类class TerminalExpression implements Expression { private Set&lt;String&gt; set = new HashSet&lt;String&gt;(); public TerminalExpression(String[] data) { for (int i = 0; i &lt; data.length; i++) set.add(data[i]); } @Override public boolean interpret(String info) { if (set.contains(info)) { return true; } return false; }}//非终结符表达式类class AndExpression implements Expression { private Expression city = null; private Expression person = null; public AndExpression(Expression city, Expression person) { this.city = city; this.person = person; } @Override public boolean interpret(String info) { String s[] = info.split(&quot;的&quot;); return city.interpret(s[0]) &amp;&amp; person.interpret(s[1]); }}//环境类class Context { private String[] citys = {&quot;韶关&quot;, &quot;广州&quot;}; private String[] persons = {&quot;老人&quot;, &quot;妇女&quot;, &quot;儿童&quot;}; private Expression cityPerson; public Context() { Expression city = new TerminalExpression(citys); Expression person = new TerminalExpression(persons); cityPerson = new AndExpression(city, person); } public void freeRide(String info) { boolean ok = cityPerson.interpret(info); if (ok) { System.out.println(&quot;您是&quot; + info + &quot;，您本次乘车免费！&quot;); } else { System.out.println(info + &quot;，您不是免费人员，本次乘车扣费2元！&quot;); } }} 全模式总结 一句话全模式总结 分类 设计模式 简述 一句话归纳 目的 生活案例 创建型设计模式 （简单来说就是用来创建对象的） [工厂模式（Factory Pattern）](#《工厂模式Factory Pattern》) 不同条件下创建不同实例 产品标准化，生产更高效 封装创建细节 实体工厂 单例模式（Singleton Pattern） 保证一个类仅有一个实例，并且提供一个全局访问点 世上只有一个我 保证独一无二 CEO 原型模式（Prototype Pattern） 通过拷贝原型创建新的对象 拔一根猴毛，吹出千万个 高效创建对象 克隆 建造者模式（Builder Pattern） 用来创建复杂的复合对象 高配中配和低配，想选哪配就哪配 开放个性配置步骤 选配 结构型设计模式 （关注类和对象的组合） [代理模式（Proxy Pattern）](#《代理模式 Proxy 》) 为其他对象提供一种代理以控制对这个对象的访问 没有资源没时间，得找别人来帮忙 增强职责 媒婆 [外观模式（Facade Pattern）](#《外观模式 Facade Pattern》) 对外提供一个统一的接口用来访问子系统 打开一扇门，通向全世界 统一访问入口 前台 [装饰器模式（Decorator Pattern）](#《装饰器模式 Decorator Pattern》) 为对象添加新功能 他大舅他二舅都是他舅 灵活扩展、同宗同源 煎饼 [享元模式（Flyweight Pattern）](#《享元模式 Flyweight Pattern》) 使用对象池来减少重复对象的创建 优化资源配置，减少重复浪费 共享资源池 全国社保联网 [组合模式（Composite Pattern）](#《组合模式 Composite Pattern》) 将整体与局部（树形结构）进行递归组合，让客户端能够以一种的方式对其进行处理 人在一起叫团伙，心在一起叫团队 统一整体和个体 组织架构树 [适配器模式（Adapter Pattern）](#《适配器模式 Adapter 》) 将原来不兼容的两个类融合在一起 万能充电器 兼容转换 电源适配 [桥接模式（Bridge Pattern）](#《桥接模式 Bridge》) 将两个能够独立变化的部分分离开来 约定优于配置 不允许用继承 桥 行为型设计模式 （关注对象之间的通信） 模板模式（Template Pattern） 定义一套流程模板，根据需要实现模板中的操作 流程全部标准化，需要微调请覆盖 逻辑复用 把大象装进冰箱 策略模式（Strategy Pattern） 封装不同的算法，算法之间能互相替换 条条大道通罗马，具体哪条你来定 把选择权交给用户 选择支付方式 责任链模式（Chain of Responsibility Pattern） 拦截的类都实现统一接口，每个接收者都包含对下一个接收者的引用。将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。 各人自扫门前雪，莫管他们瓦上霜 解耦处理逻辑 踢皮球 迭代器模式（Iterator Pattern） 提供一种方法顺序访问一个聚合对象中的各个元素 流水线上坐一天，每个包裹扫一遍 统一对集合的访问方式 逐个检票进站 命令模式（Command Pattern） 将请求封装成命令，并记录下来，能够撤销与重做 运筹帷幄之中，决胜千里之外 解耦请求和处理 遥控器 状态模式（State Pattern） 根据不同的状态做出不同的行为 状态驱动行为，行为决定状态 绑定状态和行为 订单状态跟踪 备忘录模式（Memento Pattern） 保存对象的状态，在需要时进行恢复 失足不成千古恨，想重来时就重来 备份、后悔机制 草稿箱 中介者模式（Mediator Pattern） 将对象之间的通信关联关系封装到一个中介类中单独处理，从而使其耦合松散 联系方式我给你，怎么搞定我不管 统一管理网状资源 朋友圈 解释器模式（Interpreter Pattern） 给定一个语言，定义它的语法表示，并定义一个解释器，这个解释器使用该标识来解释语言中的句子 我想说”方言“，一切解释权都归我 实现特定语法解析 摩斯密码 观察者模式（Observer Pattern） 状态发生改变时通知观察者，一对多的关系 到点就通知我 解耦观察者与被观察者 闹钟 访问者模式（Visitor Pattern） 稳定数据结构，定义新的操作行为 横看成岭侧成峰，远近高低各不同 解耦数据结构和数据操作 KPI考核 委派模式（Delegate Pattern） 允许对象组合实现与继承相同的代码重用，负责任务的调用和分配 这个需求很简单，怎么实现我不管 只对结果负责 授权委托书 资料来源 编程帮 Refactoringguru.cn","link":"/2021/03/01/Draft/2021/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"GIS","text":"MAPBOX MAPBOX流程 准备：熟悉linux 安装、网络、文件等基本操作，JS、HTML等熟悉，Mapbox熟悉，SQL熟悉，，， 注意参数：同一套坐标、经纬度等 Ubuntu环境准备===》其他格式数据===》shp数据===》Windows导入postgresql数据库===》通过ogr2ogr转换为geojson===》通过tippecanoe切片（参数自控）》~~数据打包（传输更快）~~windows共享文件夹=》准备mapbox2000 js css文件====》mapbox前端代码导入数据并显示====》准备图标数据===》filter分类图标显示=====》分层数据显示====》细节优化（颜色，光，字体大小，图片大小）》加入点击显示位置》其他点击功能====》wfts底图切换==》控件加载===》wfs，wms数据加载（不同数据源）==》其他模型加入 详细步骤 环境准备 Ubuntu虚拟机： 网络环境，数据共享，Postgresql，Tippecanoe，PROJ.4、GEOS和GDAL（ogr2ogr），gcc、g++、make windows： Postgresql,VMware,Ubuntu镜像（官方指定），Navicat（方便查看数据和建立空间数据库），VSCode 详细步骤： 安装Postgresql数据库，环境配置，导入shp Windows安装Postgresql 下载 环境变量添加 设置外网访问（pg_hba.conf） 1234567891011# TYPE DATABASE USER ADDRESS METHOD# IPv4 local connections:host all all 127.0.0.1/32 md5host all all 0.0.0.0/0 md5# IPv6 local connections:host all all ::1/128 md5# Allow replication connections from localhost, by a user with the# replication privilege.#host replication postgres 127.0.0.1/32 md5#host replication postgres ::1/128 md5 建立空间数据库,目标数据库执行以下语句 1234create extension postgis;create extension postgis_topology;create extension fuzzystrmatch;create extension postgis_tiger_geocoder; 在SHP文件夹新建txt文件，粘贴以下代码，导入数据到数据库，相关参数如下，改成bat后缀，双击运行后输入密码，显示类似 insert 1成功 123shp2pgsql -s 4544 -c -W “GBK” DJQ5120812018.shp public.DJQ5120812018 | psql -d shp2pgsqldemo -U postgres -W示例 ：shp2pgsql -s 4544 -c -W &quot;UTF-8&quot; ADDRESS.shp public.rrrrr| psql -h 192.168.22.128 -d postgres -U postgres -W 编码格式 SHP数据 public下的rrrrr表 导入数据库地址 数据库名 用户名 也可分为两个步骤，先转换为sql语句，再导入,步骤同上 12shp2pgsql -s 4544 -c -W “GBK” DJQ5120812018.shp&gt;DJQ5120812018.sqlpsql -d shp2pgsqldemo -U postgres -f DJQ5120812018.sql -W 可能出现的问题 解决办法 乱码 更改编码 导入中断，数据库无数据 数据过大 参数 含义 -s 空间参考标识符（SRID） -d 重新建立表，并插入数据 -a 在同一个表中增加数据 -c 建立新表，并插入数据(缺省) -p 只创建表 -g 指定要创建的表的空间字段名称(在追加数据时有用) -D 使用dump方式，比缺省生成sql的速度快 -G 使用类型geography -k 保持标识符（列名，模式，属性）大小写。 -i 将所有整型都转为标准的32-bit整数 -I 在几何列上建立GIST索引 -S 生成简单几何，而非MULTI几何 -t 指定几何的维度 -w 指定输出格式为WKT -W 输入的dbf文件编码方式 -N 指定几何为空时的操作 -n 只导入dbf文件 -T 指定表的表空间 -X 指定索引的表空间 -? 帮助 通过ogr2ogr转换为geojson 12ogr2ogr -f &quot;GeoJSON&quot; ./asstln.json PG:&quot;host=localhost dbname=postgres user=postgres password=111111&quot; -sql &quot;select * from asstln&quot; 目标文件名 导出数据库连接信息 导出表名 通过tippecanoe切片（参数自控） 单数据源 12tippecanoe -e tracenln -pC -Z0 -z20 -f tracenln.json 目标文件夹 切片等级空值 源文件名 单层多数据源合并 123#where确定级别ogr2ogr -f &quot;GeoJSON&quot; ./veg_py.json PG:&quot;host=126.10.9.16 dbname=postgres user=postgres password=724111&quot; -sql &quot;select * from veg_py fscale=10&quot; tippecanoe -e tracenln -pC -Z0 -z20 -f tracenln1.json tracenln2.json tracenln3.json 参数查看 数据打包 复制到编程端（传输更快） windows文件共享文件夹，或Tomcat，或在线服务器以便数据调用，注意解决触跨域资源访问 准备mapbox2000 js css文件 windows 安装 git Node.JS 安装说明 Yarn安装、配置、镜像源修改 12npm install --global yarnyarn --version Node.js安装本地插件构建工具node-gyp GitHub Mapbox源码地址：https://github.com/mapbox/mapbox-gl-js 2000坐标源码， 项目编译 yarn install 安装headless-gl，并将node_modules/headless-gl/deps/windows/dll/x64/*.dll 复制到c:\\windows\\system32 npm install gl yarn run start-debug yarn run build-dev 准备好js和css文件 mapbox 前端代码导入地图数据并显示 3D模型 针对对应的建筑数据，进行建筑物3D显示 123456789101112131415161718paint: { // 'fill-color': 'red', // 'fill-opacity': 1, 'fill-extrusion-color': '#f5f4ee', // use an 'interpolate' expression to add a smooth transition effect to the // buildings as the user zooms in 'fill-extrusion-height': [ &quot;interpolate&quot;, [&quot;linear&quot;], [&quot;zoom&quot;], 15, 0, 15.05, [&quot;get&quot;, &quot;height&quot;] ], 'fill-extrusion-base': [ &quot;interpolate&quot;, [&quot;linear&quot;], [&quot;zoom&quot;], 15, 0, 15.05, [&quot;get&quot;, &quot;min_height&quot;] ], 'fill-extrusion-opacity': 0.85 }, vscode 插件服务器 插件安装 Live Server 准备图标数据，PBF字体数据 https://github.com/mapbox/spritezero 从零生成图标资源工具，可网上下载 生成如图形式文件， filter 分类图标显示 123456789101112131415'filter': [ 'any', [ '==', 'fcode', '4206002500'//2 ] ,[ '==', 'fcode', '4305010500'//2 ] ], 分层数据显示 1&quot;layers&quot;: []//中越靠前的在底层 细节优化（颜色，光，字体大小，图片大小） 控件加载 Supermap查看 加入点击显示相关信息 123456789101112131415//弹出框​ map.on('click', function (e) {​ var features = map.queryRenderedFeatures(e.point, {​ layers: ['13'] // replace this with the id of the layer​ });​ if (!features.length) {​ return;​ }​ var feature = features[0];​ var popup = new mapboxgl.Popup({ offset: [0, -15] })​ .setLngLat(feature.geometry.coordinates)​ .setHTML('&lt;h3&gt;' + feature.properties.shortname + '&lt;/h3&gt;&lt;p&gt;' + feature.properties.name + '&lt;/p&gt;')​ .addTo(map);​ }); wfts 底图切换 wfs，wms 数据加载（不同数据源） 定位 数据查询 数据可视化 Supermap查看 优化 窗口样式优化，弹出框样式优化，字体等调节 一些小技巧 待更新 Mapbox源码编译 环境准备 GIT环境搭建： 详细点击：（一）windows 安装 git Node.JS环境搭建： 详细点击：（一）Node.JS 安装说明 Yarn环境搭建： 详细点击：（一）Yarn安装、配置、镜像源修改 12npm install --global yarnyarn --version Npm and node-gyp依赖安装 详细点击：（二）Node.js安装本地插件构建工具node-gyp 其他地址： GitHub Mapbox源码地址：https://github.com/mapbox/mapbox-gl-js 项目编译 yarn install 安装headless-gl，并将node_modules/headless-gl/deps/windows/dll/x64/*.dll 复制到c:\\windows\\system32 npm install gl yarn run start-debug yarn run build-dev debug/index.html中代码最上方增加token mapboxgl.accessToken='pk.eyJ1IjoibGltbiIsImEiOiJja2t1bG1na2IxZGU0MnZvNmlzY3FhZXM4In0.oQx4VguycOR4TK80Pyusmw'; var map = window.map = new mapboxgl.Map({ MAPBOX专业术语 矢量瓦片： 栅格瓦片： MAPBOX学习 中文文档：http://www.mapbox.cn/mapbox-gl-js/api/ Styles (8) 为地图添加生成的图标 为地图添加动画图标 为地图生成及添加缺失的图标 为地图添加图标 使用自定义样式展示地图 显示卫星地图 改变一个地图的样式 显示一个地图 Layers (30) 用3D形式呈现建筑物 拉伸多边形以绘制3D室内地图 添加3D模型 调整图层不透明度 为线添加动画效果 为一系列图像添加动画效果 为点添加动画效果 按照缩放级别改变建筑颜色 更改标注的大小写 显示具有自定义属性的HTML聚类 创建样式聚类 使用按钮更改图层颜色 添加自定义样式图层 给线添加数据驱动属性的样式。 给圆添加数据驱动属性的样式 显示多种文本格式并设置其样式 为多边形添加图案 在标签下添加新图层 添加 GeoJSON 线 绘制 GeoJSON 点 添加 GeoJSON 多边形 创建热力图图层 添加晕暄 使用表达式创建渐变色线条 设置海洋深度数据样式 显示和隐藏图层 改变行政边界世界观 根据缩放级别更新等值线图层 变量标签位置 可视化人口密度 Sources (9) 将本地JSON数据与矢量切片图形连接 添加影像 添加实时数据 更新实时要素 添加栅格切片数据源 添加一个第三方矢量切片来源 添加一个矢量图片数据源 添加一个视频 添加一个 WMS 源 User interaction (17) 基于周边声音给3D建筑添加动画效果 禁用地图旋转 创建可拖动的点 创建可拖动的标记（Marker） 通过文本输入筛选符号 在 map view 中筛选要素 通过切换列表筛选符号 创建悬停效果 显示非交互式地图 更改地图的语言 高亮包含相似数据的部分 从点击点周围选择特征 限制地图平移在某一区域 获取鼠标下点的特征 切换交互 创建时间滑动条 高亮一个选择框范围内的特征 Camera (11) 使地图相机环绕一点运动 为路线中的点添加动画效果 将地图居中于被单击的符号上 缓慢飞至某个位置 将地图缩放至边界框内 飞至某一位置 使用游戏式控件浏览地图 跳至一系列地点 以幻灯片形式播放地图位置。 根据滚动位置飞到某处 设置 pitch 和 bearing Controls and overlays (16) 为标记(marker)添加动画效果 改变注释的默认位置 使用 Markers 添加自定义图标 禁用滚轮缩放 全屏查看地图 定位用户 在不同地图之间滑动 显示驾驶方向 显示已绘制的多边形区域 添加地理编码器 利用地点名称添加标记 点击时显示多边形信息 悬浮时显示弹出窗 点击时显示一个弹出窗 显示一个弹出窗 将弹出窗口附加到 marker 实例 Geocoder (8) 从其他数据源中补充进一步的地理编码查询结果 接收输入坐标至地理编码器 使用地理编码器时采用自定义渲染功能 将地理编码的结果限制在指定地区范围内 在使用地理编码器的过程中结合使用自定义相机动画 在地图上添加位置搜索框 将地理编码器进行指定语言的本地化 在Geocoder产生结果后设置一个点 Browser support (1) 检查浏览器支持 Internationalization support (2) 使用本地生成的表意文字 为从右至左书写的脚本提供支持 SUPERMAP 一个mapbox华丽外衣与装备 Cusium 3D、2.5D、2D地图展示 源码解析准备 官网 中文文档 注册 源码：下载 环境：node、npm 使用：VUE 运行：方法一： 解压 打开命令行，进入当前目录 安装依赖，输入:cnpm install,等待安装完成，然后输入: node server.js 1234$ cd ./Cesium-1.40$ cnpm install ... Installed 37 packages Linked 569 latest versions$ npm start 或者 node server.js(新版本是 node server.cjs)Cesium development server running locally. Connect to http://localhost:8080/复制 方法二： 123这里如果使用 express发布遇到问题，可以使用 http-server$ npm install http-server -g$ http-server -c-1 (如果只输入http-server，更新代码后，页面不会同步更新) 方法三：VSCode 插件运行 获取Token 使用 12Cesium.Ion.defaultAccessToken = 'token';var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;, {}) 主页说明 Cesium ion Cesium ion is your hub for discovering 3D content and tiling your own data for streaming. CesiumJS and ion work together to enable you to build world class 3D mapping applications. Sign up for a free account to get your access token required for using ion's Bing Maps global imagery and Cesium World Terrain assets. Local links Documentation The complete API documentation and reference. Hello World The simplest possible Cesium application. Cesium Viewer A sample Cesium reference application which allows you to browse the globe and select from a variety of imagery and terrain layers as well as load CZML, GeoJSON, and other formats supported by Cesium. Sandcastle Cesium's live code editor and example gallery. Browse examples highlighting features of the Cesium API and edit and run them all in your web browser. Cesium applications created in Sandcastle can be saved and downloaded. Run tests Run Cesium's full suite of unit tests External links Cesium Learn about the Cesium team and the Cesium ion platform Cesium Blog Follow project news, developer tips, and posts about Cesium's innovative technology Tutorials Step-by-step guides for learning to use CesiumJS Community Forum Questions and feedback welcome from all skill levels GitHub The official CesiumJS GitHub repository User Stories Demos and information about projects using CesiumJS WebGL Report Provides technical information about WebGL capabilities supported by your browser 界面介绍 自项目编写 环境：VUE2 ，cesium 1.89 C:\\Users\\Administrator&gt;node -v v14.16.0 C:\\Users\\Administrator&gt;npm -v 6.14.11 项目创建 1vue init webpack gislearn 安装cesium 1.89 1npm i cesium 修改webpack.prod.conf.js 修改 --Cesium--标记处 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163'use strict'const path = require('path')const utils = require('./utils')const webpack = require('webpack')const config = require('../config')const merge = require('webpack-merge')const baseWebpackConfig = require('./webpack.base.conf')const CopyWebpackPlugin = require('copy-webpack-plugin')const HtmlWebpackPlugin = require('html-webpack-plugin')const ExtractTextPlugin = require('extract-text-webpack-plugin')const OptimizeCSSPlugin = require('optimize-css-assets-webpack-plugin')const UglifyJsPlugin = require('uglifyjs-webpack-plugin')// 定义路径--Cesium--const cesiumSource = 'node_modules/cesium/Source'const cesiumWorkers = '../Build/Cesium/Workers'const env = process.env.NODE_ENV === 'testing' ? require('../config/test.env') : require('../config/prod.env')const webpackConfig = merge(baseWebpackConfig, { module: { rules: utils.styleLoaders({ sourceMap: config.build.productionSourceMap, extract: true, usePostCSS: true }) }, devtool: config.build.productionSourceMap ? config.build.devtool : false, output: { path: config.build.assetsRoot, filename: utils.assetsPath('js/[name].[chunkhash].js'), chunkFilename: utils.assetsPath('js/[id].[chunkhash].js') }, plugins: [ // 添加如下插件--Cesium-- new CopyWebpackPlugin([{ from: path.join(cesiumSource, cesiumWorkers), to: 'Workers' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Assets'), to: 'Assets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Widgets'), to: 'Widgets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'ThirdParty/Workers'), to: 'ThirdParty/Workers' }]), new webpack.DefinePlugin({ // 注意这里和dev的配置不同 // 定义Cesium从哪里加载资源，如果使用默认的''，却变成了绝对路径了，所以这里使用'./'，使用相对路径 CESIUM_BASE_URL: JSON.stringify('./') }), // http://vuejs.github.io/vue-loader/en/workflow/production.html new webpack.DefinePlugin({ 'process.env': env }), new UglifyJsPlugin({ uglifyOptions: { compress: { warnings: false } }, sourceMap: config.build.productionSourceMap, parallel: true }), // extract css into its own file new ExtractTextPlugin({ filename: utils.assetsPath('css/[name].[contenthash].css'), // Setting the following option to `false` will not extract CSS from codesplit chunks. // Their CSS will instead be inserted dynamically with style-loader when the codesplit chunk has been loaded by webpack. // It's currently set to `true` because we are seeing that sourcemaps are included in the codesplit bundle as well when it's `false`, // increasing file size: https://github.com/vuejs-templates/webpack/issues/1110 allChunks: true, }), // Compress extracted CSS. We are using this plugin so that possible // duplicated CSS from different components can be deduped. new OptimizeCSSPlugin({ cssProcessorOptions: config.build.productionSourceMap ? { safe: true, map: { inline: false } } : { safe: true } }), // generate dist index.html with correct asset hash for caching. // you can customize output by editing /index.html // see https://github.com/ampedandwired/html-webpack-plugin new HtmlWebpackPlugin({ filename: process.env.NODE_ENV === 'testing' ? 'index.html' : config.build.index, template: 'index.html', inject: true, minify: { removeComments: true, collapseWhitespace: true, removeAttributeQuotes: true // more options: // https://github.com/kangax/html-minifier#options-quick-reference }, // necessary to consistently work with multiple chunks via CommonsChunkPlugin chunksSortMode: 'dependency' }), // keep module.id stable when vendor modules does not change new webpack.HashedModuleIdsPlugin(), // enable scope hoisting new webpack.optimize.ModuleConcatenationPlugin(), // split vendor js into its own file new webpack.optimize.CommonsChunkPlugin({ name: 'vendor', minChunks(module) { // any required modules inside node_modules are extracted to vendor return ( module.resource &amp;&amp; /\\.js$/.test(module.resource) &amp;&amp; module.resource.indexOf( path.join(__dirname, '../node_modules') ) === 0 ) } }), // extract webpack runtime and module manifest to its own file in order to // prevent vendor hash from being updated whenever app bundle is updated new webpack.optimize.CommonsChunkPlugin({ name: 'manifest', minChunks: Infinity }), // This instance extracts shared chunks from code splitted chunks and bundles them // in a separate chunk, similar to the vendor chunk // see: https://webpack.js.org/plugins/commons-chunk-plugin/#extra-async-commons-chunk new webpack.optimize.CommonsChunkPlugin({ name: 'app', async: 'vendor-async', children: true, minChunks: 3 }), // copy custom static assets new CopyWebpackPlugin([ { from: path.resolve(__dirname, '../static'), to: config.build.assetsSubDirectory, ignore: ['.*'] } ]) ]})if (config.build.productionGzip) { const CompressionWebpackPlugin = require('compression-webpack-plugin') webpackConfig.plugins.push( new CompressionWebpackPlugin({ asset: '[path].gz[query]', algorithm: 'gzip', test: new RegExp( '\\\\.(' + config.build.productionGzipExtensions.join('|') + ')$' ), threshold: 10240, minRatio: 0.8 }) )}if (config.build.bundleAnalyzerReport) { const BundleAnalyzerPlugin = require('webpack-bundle-analyzer').BundleAnalyzerPlugin webpackConfig.plugins.push(new BundleAnalyzerPlugin())}module.exports = webpackConfig webpack.dev.conf.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110'use strict'const utils = require('./utils')const webpack = require('webpack')const config = require('../config')const merge = require('webpack-merge')const path = require('path')const baseWebpackConfig = require('./webpack.base.conf')const CopyWebpackPlugin = require('copy-webpack-plugin')const HtmlWebpackPlugin = require('html-webpack-plugin')const FriendlyErrorsPlugin = require('friendly-errors-webpack-plugin')const portfinder = require('portfinder')const HOST = process.env.HOSTconst PORT = process.env.PORT &amp;&amp; Number(process.env.PORT)// 定义路径：--Cesium--const cesiumSource = 'node_modules/cesium/Source'const cesiumWorkers = '../Build/Cesium/Workers'const devWebpackConfig = merge(baseWebpackConfig, { module: { rules: utils.styleLoaders({ sourceMap: config.dev.cssSourceMap, usePostCSS: true }) }, // cheap-module-eval-source-map is faster for development devtool: config.dev.devtool, // these devServer options should be customized in /config/index.js devServer: { clientLogLevel: 'warning', historyApiFallback: { rewrites: [ { from: /.*/, to: path.posix.join(config.dev.assetsPublicPath, 'index.html') }, ], }, hot: true, contentBase: false, // since we use CopyWebpackPlugin. compress: true, host: HOST || config.dev.host, port: PORT || config.dev.port, open: config.dev.autoOpenBrowser, overlay: config.dev.errorOverlay ? { warnings: false, errors: true } : false, publicPath: config.dev.assetsPublicPath, proxy: config.dev.proxyTable, quiet: true, // necessary for FriendlyErrorsPlugin watchOptions: { poll: config.dev.poll, } }, plugins: [ // 添加如下插件--Cesium-- new CopyWebpackPlugin([{ from: path.join(cesiumSource, cesiumWorkers), to: 'Workers' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Assets'), to: 'Assets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'Widgets'), to: 'Widgets' }]), new CopyWebpackPlugin([{ from: path.join(cesiumSource, 'ThirdParty/Workers'), to: 'ThirdParty/Workers' }]), new webpack.DefinePlugin({ // Define relative base path in cesium for loading assets CESIUM_BASE_URL: JSON.stringify('') }), new webpack.DefinePlugin({ 'process.env': require('../config/dev.env') }), new webpack.HotModuleReplacementPlugin(), new webpack.NamedModulesPlugin(), // HMR shows correct file names in console on update. new webpack.NoEmitOnErrorsPlugin(), // https://github.com/ampedandwired/html-webpack-plugin new HtmlWebpackPlugin({ filename: 'index.html', template: 'index.html', inject: true }), // copy custom static assets new CopyWebpackPlugin([ { from: path.resolve(__dirname, '../static'), to: config.dev.assetsSubDirectory, ignore: ['.*'] } ]) ]})module.exports = new Promise((resolve, reject) =&gt; { portfinder.basePort = process.env.PORT || config.dev.port portfinder.getPort((err, port) =&gt; { if (err) { reject(err) } else { // publish the new Port, necessary for e2e tests process.env.PORT = port // add port to devServer config devWebpackConfig.devServer.port = port // Add FriendlyErrorsPlugin devWebpackConfig.plugins.push(new FriendlyErrorsPlugin({ compilationSuccessInfo: { messages: [`Your application is running here: http://${devWebpackConfig.devServer.host}:${port}`], }, onErrors: config.dev.notifyOnErrors ? utils.createNotifierCallback() : undefined })) resolve(devWebpackConfig) } })}) webpack.base.conf.js 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130'use strict'const path = require('path')const utils = require('./utils')const config = require('../config')const vueLoaderConfig = require('./vue-loader.conf')//定义 Cesium 源码路径 --Cesium--const cesiumSource = '../node_modules/cesium/Source'const cesiumWorkers = path.join(cesiumSource, 'Workers');function resolve(dir) { return path.join(__dirname, '..', dir)}const createLintingRule = () =&gt; ({ test: /\\.(js|vue)$/, loader: 'eslint-loader', enforce: 'pre', include: [resolve('src'), resolve('test')], options: { formatter: require('eslint-friendly-formatter'), emitWarning: !config.dev.showEslintErrorsInOverlay }})module.exports = { context: path.resolve(__dirname, '../'), entry: { app: './src/main.js' }, // output: { // path: config.build.assetsRoot, // filename: '[name].js', // publicPath: process.env.NODE_ENV === 'production' // ? config.build.assetsPublicPath // : config.dev.assetsPublicPath // }, // 让webpack正确处理多行字符串，在output中添加sourcePrefix:' '--Cesium-- output: { path: config.build.assetsRoot, filename: '[name].js', publicPath: process.env.NODE_ENV === 'production' ? config.build.assetsPublicPath : config.dev.assetsPublicPath, sourcePrefix: ' ' //让webpack正确处理多行字符串 }, // resolve: { // extensions: ['.js', '.vue', '.json'], // alias: { // 'vue$': 'vue/dist/vue.esm.js', // '@': resolve('src'), // } // }, // 需要在resolve中设置cesium别名，这样在引入的时候就可以根据别名找到Cesium的包。--Cesium-- // （注：也可以不设置别名，导包是直接导入'cesium/Source/Cesium.js'就行。其实设置别名的目的就是让“别名”指向/ node_modules / cesium / Source目录） resolve: { extensions: ['.js', '.vue', '.json'], alias: { 'vue$': 'vue/dist/vue.esm.js', '@': resolve('src'), cesium: path.resolve(__dirname, cesiumSource) //设置cesium别名 } }, module: { // 解决报错 1.87版本无效，只能暂时退版本--Cesium-- // npm install @open-wc/webpack-import-meta-loader --save-dev // error in ./node_modules/cesium/Source/ThirdParty/zip.js // Module parse failed: Unexpected token(6400:57) // You may need an appropriate loader to handle this file type. rules: [ { test: /\\.js$/, use: { loader: '@open-wc/webpack-import-meta-loader', }, }, // 创建工程师会选择 ESLint 代码校验 // Use ESLint to lint your code? Yes // ...(config.dev.useEslint ? [createLintingRule()] : []), { test: /\\.vue$/, loader: 'vue-loader', options: vueLoaderConfig }, { test: /\\.js$/, loader: 'babel-loader', include: [resolve('src'), resolve('test'), resolve('node_modules/webpack-dev-server/client')] }, { test: /\\.(png|jpe?g|gif|svg)(\\?.*)?$/, loader: 'url-loader', options: { limit: 10000, name: utils.assetsPath('img/[name].[hash:7].[ext]') } }, { test: /\\.(mp4|webm|ogg|mp3|wav|flac|aac)(\\?.*)?$/, loader: 'url-loader', options: { limit: 10000, name: utils.assetsPath('media/[name].[hash:7].[ext]') } }, { test: /\\.(woff2?|eot|ttf|otf)(\\?.*)?$/, loader: 'url-loader', options: { limit: 10000, name: utils.assetsPath('fonts/[name].[hash:7].[ext]') } } ], unknownContextCritical: false,//阻止依赖警告--Cesium-- }, node: { // prevent webpack from injecting useless setImmediate polyfill because Vue // source contains it (although only uses it if it's native). setImmediate: false, // prevent webpack from injecting mocks to Node native modules // that does not make sense for the client dgram: 'empty', fs: 'empty', net: 'empty', tls: 'empty', child_process: 'empty' }} 创建地图组件 src\\view\\earth.vue 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135&lt;template&gt; &lt;div class=&quot;container&quot;&gt; &lt;div id=&quot;cesiumContainer&quot;&gt;&lt;/div&gt; &lt;div id=&quot;credit&quot;&gt;&lt;/div&gt; &lt;/div&gt;&lt;/template&gt; &lt;script&gt;// 这里不能使用 import Cesium from 'cesium/Cesium' 导入模块，因为Cesium 1.63 版本以后使用的是ES6。应该使用一下方式// import { Viewer } from &quot;cesium/Cesium&quot;;import * as Cesium from &quot;cesium/Cesium&quot;; //正确import &quot;cesium/Widgets/widgets.css&quot;;export default { name: &quot;earth&quot;, data() { return {}; }, mounted() { // let viewer = new Viewer(&quot;cesiumContainer&quot;); // 1. Geocoder : 查找位置工具，查找到之后会将镜头对准找到的地址，默认使用bing地图 // 2. Home Button :视角返回初始位置. // 3. Scene Mode Picker : 选择视角的模式，有三种：3D，2D，哥伦布视图(CV) // 4. Base Layer Picker : 图层选择器，选择要显示的地图服务和地形服务. // 5. Navigation Help Button :导航帮助按钮，显示默认的地图控制帮助. // 6. Animation : 动画器件，控制视图动画的播放速度. // 7. Timeline :时间线,指示当前时间，并允许用户跳到特定的时间. // 8. Credits Display :版权显示，显示数据归属，必选 // 9. Fullscreen Button :全屏按钮. Cesium.Ion.defaultAccessToken =&quot;自己的token&quot;; var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;, { // geocoder: false, // homeButton: false, // sceneModePicker: false, // baseLayerPicker: false, // navigationHelpButton: false, // animation: false, creditContainer: &quot;credit&quot;, // timeline: false, // fullscreenButton: false, // vrButton: false, // skyBox : new Cesium.SkyBox({ // sources : { // positiveX : 'stars/TychoSkymapII.t3_08192x04096_80_px.jpg', // negativeX : 'stars/TychoSkymapII.t3_08192x04096_80_mx.jpg', // positiveY : 'stars/TychoSkymapII.t3_08192x04096_80_py.jpg', // negativeY : 'stars/TychoSkymapII.t3_08192x04096_80_my.jpg', // positiveZ : 'stars/TychoSkymapII.t3_08192x04096_80_pz.jpg', // negativeZ : 'stars/TychoSkymapII.t3_08192x04096_80_mz.jpg' // } // }) }); // 显示帧速(FPS) viewer.scene.debugShowFramesPerSecond = true; // 绘制形状 // 方式一： var redBox = viewer.entities.add({ name: &quot;Red box with black outline&quot;, position: Cesium.Cartesian3.fromDegrees(-107.0, 40.0, 300000.0), box: { dimensions: new Cesium.Cartesian3(400000.0, 300000.0, 500000.0), material: Cesium.Color.BLUE.withAlpha(0.5), outline: true, outlineColor: Cesium.Color.BLACK, }, }); viewer.zoomTo(viewer.entities); // 方式二： var czml = [ { id: &quot;document&quot;, name: &quot;box&quot;, version: &quot;1.0&quot;, }, { id: &quot;shape2&quot;, name: &quot;Red box with black outline&quot;, position: { cartographicDegrees: [-107.0, 50.0, 300000.0], }, box: { dimensions: { cartesian: [400000.0, 300000.0, 500000.0], }, material: { solidColor: { color: { rgba: [255, 0, 0, 128], }, }, }, outline: true, outlineColor: { rgba: [0, 0, 0, 255], }, }, }, ]; // 重复创建Viewer会添加多个窗口 // var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;); var dataSourcePromise = Cesium.CzmlDataSource.load(czml); viewer.dataSources.add(dataSourcePromise); viewer.zoomTo(dataSourcePromise); },};&lt;/script&gt; &lt;style scoped&gt;.container { width: 100%; height: 100%;}#cesiumContainer { width: 100%; height: 100%;}/* 不占据空间，无法点击，右上角按钮组，左下角动画控件 ，时间线，logo信息*//* .cesium-viewer-toolbar,.cesium-viewer-animationContainer,.cesium-viewer-timelineContainer,.cesium-viewer-bottom { display: none;} *//* 全屏按钮 *//* .cesium-viewer-fullscreenContainer { position: absolute; top: -999em;} *//* 注：全屏按钮不能通过display:none的方式来达到隐藏的目的，这是因为生成的按钮控件的行内样式设置了display属性，会覆盖引入的css属性&lt;div class=&quot;cesium-viewer-fullscreenContainer&quot; style=&quot;display: block;&quot;&gt;...&lt;/div&gt; */&lt;/style&gt; 添加路由 src\\router\\index.js 12345678910111213141516171819202122import Vue from 'vue'import Router from 'vue-router'import HelloWorld from '@/components/HelloWorld'import earth from '@/view/earth'Vue.use(Router)export default new Router({ routes: [ { path: '/', name: 'HelloWorld', component: HelloWorld }, { path: '/earth', name: 'earth', component: earth } ]}) 运行报错1 error in ./node_modules/cesium/Source/ThirdParty/zip.js 解决 1npm i @open-wc/webpack-import-meta-loader -S vue.config.js 或 webpack.base.conf.js配置 123456789101112131415module.exports = { configureWebpack: { module: { rules: [ { test: /\\.js$/, use: { loader: '@open-wc/webpack-import-meta-loader', }, }, ], }, },} 运行报错2 error in (webpack)/hot/dev-server.js Module build failed: TypeError: Cannot read property 'length' of undefined at Object.module.exports (F:\\demo\\cesiumtest-master\\node_modules@open-wc\\webpack-import-meta-loader\\webpack-import-meta-loader.js:20:63) 解决 点击报错跳转到对应文件行，将其注释即可 1234567module.exports = function (source) { const path = require('path'); const relativePath = this.context.substring( // this.context.indexOf(this.rootContext) + this.rootContext.length + 1, this.resource.lastIndexOf(path.sep) + 1, ); 运行 npm run dev http://localhost:8081/#/earth npm run build 打包成功 部署dist http://xx.xx.xx.11:8084/#/earth 成功 功能开发 控件显示控制 Geocoder : 查找位置工具，查找到之后会将镜头对准找到的地址，默认使用bing地图 Home Button :视角返回初始位置. Scene Mode Picker : 选择视角的模式，有三种：3D，2D，哥伦布视图(CV) Base Layer Picker : 图层选择器，选择要显示的地图服务和地形服务. Navigation Help Button :导航帮助按钮，显示默认的地图控制帮助. Animation : 动画器件，控制视图动画的播放速度. Timeline :时间线,指示当前时间，并允许用户跳到特定的时间. Credits Display :版权显示，显示数据归属，必选 Fullscreen Button :全屏按钮. js方式 官方文档 123456789101112131415161718192021222324var viewer = new Viewer(&quot;cesiumContainer&quot;, { geocoder: false, homeButton: false, sceneModePicker: false, baseLayerPicker: false, navigationHelpButton: false, animation: false, // creditContainer: &quot;credit&quot;, timeline: false, fullscreenButton: false, vrButton: false, // skyBox : new Cesium.SkyBox({ // sources : { // positiveX : 'stars/TychoSkymapII.t3_08192x04096_80_px.jpg', // negativeX : 'stars/TychoSkymapII.t3_08192x04096_80_mx.jpg', // positiveY : 'stars/TychoSkymapII.t3_08192x04096_80_py.jpg', // negativeY : 'stars/TychoSkymapII.t3_08192x04096_80_my.jpg', // positiveZ : 'stars/TychoSkymapII.t3_08192x04096_80_pz.jpg', // negativeZ : 'stars/TychoSkymapII.t3_08192x04096_80_mz.jpg' // } // })});// 显示帧速(FPS)viewer.scene.debugShowFramesPerSecond = true; css方式 12345678910/* 不占据空间，无法点击 */.cesium-viewer-toolbar, /* 右上角按钮组 */.cesium-viewer-animationContainer, /* 左下角动画控件 */.cesium-viewer-timelineContainer, /* 时间线 */.cesium-viewer-bottom /* logo信息 */{ display: none;}.cesium-viewer-fullscreenContainer /* 全屏按钮 */{ position: absolute; top: -999em; } 创建形状 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 绘制形状 正方体 // 方式一：Entity//其他形状：http://cesium.xin/cesium/Documentation/Entity.html?classFilter=entity var redBox = viewer.entities.add({ name: &quot;Red box with black outline&quot;, position: Cesium.Cartesian3.fromDegrees(-107.0, 40.0, 300000.0), box: { dimensions: new Cesium.Cartesian3(400000.0, 300000.0, 500000.0), material: Cesium.Color.BLUE.withAlpha(0.5), outline: true, outlineColor: Cesium.Color.BLACK, }, }); viewer.zoomTo(viewer.entities); // 方式二：CZML(可创建几何形状也可创建动画) var czml = [ { id: &quot;document&quot;, name: &quot;box&quot;, version: &quot;1.0&quot;, }, { id: &quot;shape2&quot;, name: &quot;Red box with black outline&quot;, position: { cartographicDegrees: [-107.0, 50.0, 300000.0], }, box: { dimensions: { cartesian: [400000.0, 300000.0, 500000.0], }, material: { solidColor: { color: { rgba: [255, 0, 0, 128], }, }, }, outline: true, outlineColor: { rgba: [0, 0, 0, 255], }, }, }, ]; // 重复创建Viewer会添加多个窗口 // var viewer = new Cesium.Viewer(&quot;cesiumContainer&quot;); var dataSourcePromise = Cesium.CzmlDataSource.load(czml); viewer.dataSources.add(dataSourcePromise); viewer.zoomTo(dataSourcePromise); 添加不同类型数据 概览 Web Map Service (WMS) - 一种OGC标准，从分布式地理数据库中通过地图的地理范围来请求切片。 Cesium使用 WebMapServiceImageryProvider去支持这种。 Tile Map Service (TMS) - 一种访问地图切片的REST接口。 可以用CesiumLab， MapTiler 或者 GDAL2Tiles . Cesium中使用TileMapServiceImageryProvider. OpenGIS Web Map Tile Service (WMTS) - 一种OGC标准，主要是为预渲染的地图切片形式. Cesium中使用 WebMapTileServiceImageryProvider. OpenStreetMap - 访问 OpenStreetMap 切片 或者 任意 Slippy map tiles.有很多方法 发布这种服务 .Cesium中使用createOpenStreetMapImageryProvider. Bing 地图 - 使用Bing 地图 REST 服务访问切片. 在这里 https://www.bingmapsportal.com/创建Bing地图的key. Cesium中使用 BingMapsImageryProvider. Esri ArcGIS MapServer - 使用 ArcGIS Server REST API 访问存储在ArcGIS Server上的切片。Cesium中使用ArcGisMapServerImageryProvider. Google Earth Enterprise - 访问Google Earth 企业版服务器发布的影像切片。Cesium中 GoogleEarthImageryProvider. Mapbox - 使用 Mapbox API访问切片. 在这里新建用户，并且创建一个 access token. Cesium中使用 MapboxImageryProvider. 普通图片文件 - 使用一张普通图片创建影像图层. Cesium中使用 SingleTileImageryProvider. 自定义切片机制 - 使用UrlTemplateImageryProvider, 可以通过 URL 模板连接各种影像资源 。比如TMS服务的URL模板是： //cesiumjs.org/tilesets/imagery/naturalearthii/{z}/{x}/{reverseY}.jpg. 切片坐标 - 用来显示全球是如何被切片的，支持多种切片规则，画出每个切片的地理边界，并且用文字标注每个切片的level，x，y坐标。 百度地图 - 用来加载百度默认地图或者自定义样式地图，请联系我们。 跨域问题解决 12345678910111213141516171819module.exports = { dev: { // Paths assetsSubDirectory: 'static', assetsPublicPath: '/', //接口地址原本是/satellite/z={z}&amp;x={x}&amp;y={y} 但是为了匹配代理地址 调用时需在前面加一个 /satellite, 因此接口地址需要写成这样的即可生效 /satellite/satellite/z={z}&amp;x={x}&amp;y={y} proxyTable: { '/ArcGIS': {//代理的目的：只要是/satellite开头的路径都往localhost:3000进行转发 target: 'https://sampleserver1.arcgisonline.com', //后端接口地址 设置代理服务器地址 转发地址 ws: true,//WebSocket协议 changeOrigin: true, //表示是否改变原域名；这个一定要选择为true; 是否允许跨域[ 如果接口跨域 则要配置这个参数] secure: false, // 如果是https接口 需要配置这个参数 pathRewrite: {// 把程序中的地址转换成“真实地址”+‘/satellite’后面的部分如‘/satellite/satellite/z={z}&amp;x={x}&amp;y={y}'，被转换成'http://localhost:3000/satellite/z={z}&amp;x={x}&amp;y={y}' '^/ArcGIS': ''//修改pathRewrite地址 将前缀'^satellite'转为'/satellite' } }, }, 加载不同类型数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 加载不同类型数据 从上到下依次覆盖============================================== // 数据类型1： -----------------------------------Web Map Service (WMS) ------------------------------------- // var provider = new Cesium.WebMapServiceImageryProvider({ // // url: &quot;https://sampleserver1.arcgisonline.com/ArcGIS/services/Specialty/ESRI_StatesCitiesRivers_USA/MapServer/WMSServer&quot;, // url: &quot;ArcGIS/ArcGIS/services/Specialty/ESRI_StatesCitiesRivers_USA/MapServer/WMSServer&quot;, // layers: &quot;0&quot;, // // proxy: new Cesium.DefaultProxy(&quot;/proxy/&quot;), // }); // viewer.imageryLayers.addImageryProvider(provider); // 数据类型2：-----------------------------------OpenGIS Web Map Tile Service (WMTS)----------------------------------- // var shadedRelief2 = new Cesium.WebMapTileServiceImageryProvider({ // url: &quot;http://basemap.nationalmap.gov/arcgis/rest/services/USGSShadedReliefOnly/MapServer/WMTS/tile/1.0.0/USGSShadedReliefOnly/{Style}/{TileMatrixSet}/{TileMatrix}/{TileRow}/{TileCol}.jpg&quot;, // layer: &quot;USGSShadedReliefOnly&quot;, // style: &quot;default&quot;, // format: &quot;image/jpeg&quot;, // tileMatrixSetID: &quot;default028mm&quot;, // maximumLevel: 19, // credit: new Cesium.Credit(&quot;U. S. Geological Survey&quot;), // }); // viewer.imageryLayers.addImageryProvider(shadedRelief2); // url:&quot;http://61.175.211.102/arcgis/rest/services/wzmap/map/MapServer/WMTS?service=WMTS&amp;request=GetTile&amp;layer=wzmap&amp;style=default&amp;tilematriX={TileMatrix}&amp;tilerow={TileRow}&amp;tilecoL={TileCol}&quot; // 数据类型3： -----------------------------------Tile Map Service (TMS)----------------------------------- // var tms = new Cesium.TileMapServiceImageryProvider({ // url: &quot;../images/cesium_maptiler/Cesium_Logo_Color&quot;, // fileExtension: &quot;png&quot;, // maximumLevel: 4, // rectangle: new Cesium.Rectangle( // Cesium.Math.toRadians(-120.0), // Cesium.Math.toRadians(20.0), // Cesium.Math.toRadians(-60.0), // Cesium.Math.toRadians(40.0) // ), // }); // viewer.imageryLayers.addImageryProvider(tms); // 数据类型4： -----------------------------------ArcGis----------------------------------- // var esri = new Cesium.ArcGisMapServerImageryProvider({ // url: &quot;https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer&quot;, // }); // viewer.imageryLayers.addImageryProvider(esri); // 数据类型5：-----------------------------------Mapbox----------------------------------- var mapbox = new Cesium.MapboxImageryProvider({ mapId: &quot;mapbox.mapbox-streets-v8&quot;, accessToken: &quot;pk.eyJ1IjoibGltbiIsImEiOiJja2t1bG1na2IxZGU0MnZvNmlzY3FhZXM4In0.oQx4VguycOR4TK80Pyusmw&quot;, }); viewer.imageryLayers.addImageryProvider(mapbox); Leaflet an open-source JavaScript library for mobile-friendly interactive maps Leaflet 是一个为建设移动设备友好的互动地图，而开发的现代的、开源的 JavaScript 库。虽然代码仅有 38 KB，但它具有开发人员开发在线地图的大部分功能。Leaflet设计坚持简便、高性能和可用性好的思想，在所有主要桌面和移动平台能高效运作，在现代浏览器上会利用HTML5和CSS3的优势，同时也支持旧的浏览器访问。支持插件扩展，有一个友好、易于使用的API文档和一个简单的、可读的源代码。详细见官方网站https://leafletjs.com/ 3.3.1地图 3.3.1.1矢量电子地图 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;!--添加leaflet样式--&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;!--添加leafletjs包--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;!--添加坐标库--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], //切图原点 } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30.25168, 120.16179], 4); //设置比例尺和中心点级别 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.1.2影像电子地图 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加影像地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.1.3手动加载/移除地图 3.3.2图层 3.3.2.1弹出信息框 点击地图弹出信息，示例代码bindPopup 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;!--添加leaflet样式--&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;!--添加leafletjs包--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;!--添加坐标库--&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], //切图原点 } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30.25168, 120.16179], 4); //设置比例尺和中心点级别 L.marker([30.25168, 120.16179]).addTo(map).bindPopup(&quot;&lt;b&gt;Hello world!&lt;/b&gt;&lt;br /&gt;I am a popup.&quot;).openPopup(); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.2.2矢量图层（wms） 加载WMS地图服务，支持filter筛选 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;调用WMS&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet.wms.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100% } body { height: 100%; margin: 0; padding: 0; } .map { height: 100% } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06 ]; var crs = new L.Proj.CRS('EPSG:4490', '+proj=longlat +ellps=GRS80 +no_defs', { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map('mapid', { crs: crs }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 // L.WMS.source var wmsurl = L.WMS.overlay(&quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wms&quot;, { &quot;transparent&quot;: true, &quot;srs&quot;: &quot;EPSG:4490&quot;, &quot;layers&quot;: &quot;lsyt_dmdz_csbj0619&quot;, &quot;format&quot;: &quot;image/png&quot;, &quot;CQL_FILTER&quot;: &quot;quxian='余杭区'&quot; }); map.addLayer(wmsurl); map.on('click', function (e) { alert(&quot;You clicked the map at &quot; + e.latlng); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.2.3矢量图层（wfs） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;加载WFS电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 map.on(&quot;click&quot;, function (e) { alert(&quot;You clicked the map at &quot; + e.latlng); }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3查询 3.3.3.1点查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;点查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 map.on(&quot;click&quot;, function (e) { // alert(&quot;You clicked the map at &quot; + e.latlng); var point = e.latlng.lng + &quot;,&quot; + e.latlng.lat + &quot;,&quot; + e.latlng.lng + &quot;,&quot; + e.latlng.lat;// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;You clicked the map at &quot; + text.features[1].properties['quxian']); } }); }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3.2面查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;面查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 //var draw=new L.Control.Draw(map,drawControl.options.rectangle).enable(); var options = { position: &quot;topleft&quot;, draw: { polyline: false, polygon: false, circle: false, marker: false, circlemarker: false, rectangle: { shapeOptions: { clickable: true, }, }, }, }; var drawControl = new L.Control.Draw(options); map.addControl(drawControl); map.on(L.Draw.Event.CREATED, function (e) { var type = e.layerType; if (type == &quot;rectangle&quot;) { var bound = e.layer.getBounds(); var point = bound._southWest.lng + &quot;,&quot; + bound._southWest.lat + &quot;,&quot; + bound._northEast.lng + &quot;,&quot; + bound._northEast.lat; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert( &quot;你选择的是 &quot; + text.features[1].properties[&quot;quxian&quot;] ); }, }); } }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3.3属性查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;点查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } .h1 { z-index: 1009; position: absolute; left: 100px; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt; &lt;h1&gt;点击地图根据属性查询&lt;/h1&gt; &lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 map.on(&quot;click&quot;, function (e) { var point = e.latlng.lng + &quot;,&quot; + e.latlng.lat + &quot;,&quot; + e.latlng.lng + &quot;,&quot; + e.latlng.lat; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=&quot;; qurl += &quot; quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;根据属性选择的是 &quot; + text.features[0].properties[&quot;quxian&quot;]); }, }); }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 3.3.3.4组合查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;面查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-wfs.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4-compressed.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/pro/proj4leaflet.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/leaflet/V1.3/leaflet/Plugins/leaflet-draw/leaflet.draw.js&quot;&gt;&lt;/script&gt; &lt;style&gt; html { height: 100%; } body { height: 100%; margin: 0; padding: 0; } .map { height: 100%; } &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;mapid&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var res = [ 0.00549933137239034, // Level 0 0.00274966568619517, // Level 1 0.00137483284309758, // Level 2 0.000687416421548792, // Level 3 0.000343708210774396, // Level 4 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var crs = new L.Proj.CRS( &quot;EPSG:4490&quot;, &quot;+proj=longlat +ellps=GRS80 +no_defs&quot;, { resolutions: res, origin: [118.122911693886, 31.2869311022836], } ); var map = L.map(&quot;mapid&quot;, { crs: crs, /*fullscreenControl: { pseudoFullscreen: false },*/ }); var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;; var attrib = &quot;&amp;copy 杭州市规划资源局&quot;; var basemap = new L.TileLayer(url + &quot;/tile/{z}/{y}/{x}&quot;, { tileSize: 256, attribution: attrib, }); map.addLayer(basemap); map.setView([30, 120], 4); //设置比例尺和中心点级别 //var draw=new L.Control.Draw(map,drawControl.options.rectangle).enable(); var options = { position: &quot;topleft&quot;, draw: { polyline: false, polygon: false, circle: false, marker: false, circlemarker: false, rectangle: { shapeOptions: { clickable: true, }, }, }, }; var drawControl = new L.Control.Draw(options); map.addControl(drawControl); map.on(L.Draw.Event.CREATED, function (e) { var type = e.layerType; if (type == &quot;rectangle&quot;) { var bound = e.layer.getBounds(); var point = bound._southWest.lng + &quot;,&quot; + bound._southWest.lat + &quot;,&quot; + bound._northEast.lng + &quot;,&quot; + bound._northEast.lat; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); if (!text.features.length) { alert(&quot;你选择的是非上城区，无查询结果！&quot;); } else { alert(&quot;你选择的是 &quot; + text.features[0].properties[&quot;quxian&quot;]); } }, }); } }); var myStyle = { color: &quot;#ff0000&quot;, weight: 2, opacity: 1, fillOpacity: 0.01, }; var mblayer; $.ajax({ url: &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;, success: function (data) { var geojson = JSON.parse(data); mblayer = L.geoJSON(geojson, { style: myStyle, }).addTo(map); }, }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; ArcGIS ArcGIS API for JavaScript就是ESRI公司用JavaScipt语言编写的一套程序接口。用户可以通过调用API获取ArcGIS server、geoserver等提供的服务，例如浏览、编辑、渲染地图，以及一些常用的空间分析功能。API中包含了ArcGIS API for JavaScript中每个类的详细描述。使用API查找每个类的构造函数选项以及属性、方法和事件。 详情见官方网站https://developers.arcgis.com/javascript/3/jsapi/ 3.1.1地图 3.1.1.1矢量电子地图 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with Vertor&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.1.2影像电子地图 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with Raster&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver&quot;, { id: &quot;raster&quot;, } ); map.addLayer(layer); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.1.3手动加载/移除地图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with Control&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } .details span { ​ cursor: pointer; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var vector = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); // map.addLayer(vector); var raster = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver&quot;, { id: &quot;raster&quot;, } ); // map.addLayer(raster); $(&quot;#details span&quot;).click(function () { var name = $(this).attr(&quot;name&quot;); switch (name) { case &quot;vector&quot;: if (!map.getLayer(&quot;vector&quot;)) map.addLayer(vector); break; case &quot;raster&quot;: if (!map.getLayer(&quot;raster&quot;)) map.addLayer(raster); break; case &quot;rvector&quot;: if (map.getLayer(&quot;vector&quot;)) map.removeLayer(vector); break; case &quot;rraster&quot;: if (map.getLayer(&quot;raster&quot;)) map.removeLayer(raster); break; } }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt; ​ &lt;span name=&quot;vector&quot;&gt;添加矢量图层&lt;/span&gt; ​ &lt;span name=&quot;raster&quot;&gt;添加影像图层&lt;/span&gt; ​ &lt;span name=&quot;rvector&quot;&gt;移除矢量图层&lt;/span&gt; ​ &lt;span name=&quot;rraster&quot;&gt;移除影像图层&lt;/span&gt; &lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.2图层 3.1.2.1弹出信息框 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with infoWindow&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WFSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WFSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); map.on('click', function (e) { map.infoWindow.setTitle(&quot;详细信息&quot;); map.infoWindow.setContent(&quot;当前坐标：&quot; + e.mapPoint.x + &quot;,&quot; + e.mapPoint.y); map.infoWindow.show(e.mapPoint); }) }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.2.2矢量图层（wms） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with wmslayer&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.2.3矢量图层（wfs） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Map with wfslayer&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } .esriControlsBR .logo-med { ​ display: none; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map, symbol; require([ &quot;esri/map&quot;, &quot;esri/layers/WFSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/symbols/SimpleLineSymbol&quot;, &quot;esri/symbols/SimpleFillSymbol&quot;, &quot;esri/graphic&quot;, &quot;esri/geometry/Extent&quot;, &quot;esri/geometry/Polygon&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WFSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, SimpleLineSymbol, SimpleFillSymbol, Graphic, Extent, Polygon, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer = new ArcGISTiledMapServiceLayer( &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver&quot;, { id: &quot;vector&quot;, } ); map.addLayer(layer); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; $.ajax({ url: url, success: function (data) { console.log(data); var text = JSON.parse(data); for (var i = 0; i &lt; text.features.length; i++) { symbol = new SimpleFillSymbol(); if (text.features[i].geometry.type == &quot;Polygon&quot;) { var polygonjson = { &quot;rings&quot;: text.features[i].geometry.coordinates[0], &quot;spatialReference&quot;: { &quot;wkid&quot;: 4490 } } var poly = new Polygon(polygonjson) var graphic = new Graphic(poly, symbol); //var gra = new Graphic(text.features[i].geometry, symbol); map.graphics.add(graphic); } else if (text.features[i].geometry.type == &quot;MultiPolygon&quot;) { var len = text.features[i].geometry.coordinates.length; for (var k = 0; k &lt; len; k++) { var polygonjson = { &quot;rings&quot;: text.features[i].geometry.coordinates[k], &quot;spatialReference&quot;: { &quot;wkid&quot;: 4490 } } var poly = new Polygon(polygonjson) var graphic = new Graphic(poly, symbol); map.graphics.add(graphic); } } else { alert(); } } } }); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3查询 3.1.3.1点查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query By Point&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { // basemap: &quot;streets&quot;, // center: [120.1594, 30.2571], center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { // id: layerInfo.LayerName, id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); }); map.on(&quot;click&quot;, function (e) { console.log(&quot;获取到的点&quot; + e); var point = e.mapPoint.x + &quot;,&quot; + e.mapPoint.y + &quot;,&quot; + e.mapPoint.x + &quot;,&quot; + e.mapPoint.y;// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); $('#details').html(text.features[1].properties['quxian']); } }); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3.2面查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query By Rectangle&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map, toolbar; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/toolbars/draw&quot;, &quot;esri/symbols/SimpleLineSymbol&quot;, &quot;esri/symbols/SimpleFillSymbol&quot;, &quot;esri/graphic&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, Draw, SimpleLineSymbol, SimpleFillSymbol, Graphic, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { // basemap: &quot;streets&quot;, // center: [120.1594, 30.2571], center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { // id: layerInfo.LayerName, id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); toolbar = new Draw(map); toolbar.on(&quot;draw-end&quot;, showinfo); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }); function showinfo(evt) { var symbol; toolbar.deactivate(); map.graphics.clear(); symbol = new SimpleFillSymbol(); var graphic = new Graphic(evt.geometry, symbol); map.graphics.add(graphic); getAttr(evt.geometry); } function getAttr(geo) { var point = geo.cache._extent.xmin + &quot;,&quot; + geo.cache._extent.ymin + &quot;,&quot; + geo.cache._extent.xmax + &quot;,&quot; + geo.cache._extent.ymax; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); var attr = &quot;&quot;; for (var i = 0; i &lt; text.features.length; i++) { attr += text.features[i].properties[&quot;quxian&quot;] + &quot;&lt;/br&gt;&quot;; } $(&quot;#details&quot;).html(attr); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }, }); } }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3.3属性查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query by Attr&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); }); map.on(&quot;click&quot;, function (e) { console.log(&quot;获取到的点&quot; + e); var point = &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); var attr = &quot;&quot;; for (var i = 0; i &lt; text.features.length; i++) { attr += text.features[i].properties[&quot;quxian&quot;] + &quot;&lt;/br&gt;&quot;; } $(&quot;#details&quot;).html(attr); }, }); }); }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;点击地图实现按属性查询（区县=上城区）&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 3.1.3.4组合查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;initial-scale=1, maximum-scale=1,user-scalable=no&quot; /&gt; &lt;title&gt;Query&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/dijit/themes/claro/claro.css&quot; /&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/esri/css/esri.css&quot; /&gt; &lt;style&gt; html, body, \\#map { ​ height: 100%; ​ width: 100%; ​ margin: 0; ​ padding: 0; } &lt;/style&gt; &lt;script src=&quot;http://172.18.109.232:8082/arcgis_js_api/3.24/init.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt; &lt;script&gt; var map, toolbar; require([ &quot;esri/map&quot;, &quot;esri/layers/WMSLayer&quot;, &quot;esri/layers/WMSLayerInfo&quot;, &quot;esri/toolbars/draw&quot;, &quot;esri/symbols/SimpleLineSymbol&quot;, &quot;esri/symbols/SimpleFillSymbol&quot;, &quot;esri/graphic&quot;, &quot;esri/layers/ArcGISTiledMapServiceLayer&quot;, &quot;esri/geometry/Extent&quot;, &quot;dojo/_base/array&quot;, &quot;dojo/dom&quot;, &quot;dojo/dom-construct&quot;, &quot;dojo/parser&quot;, &quot;dijit/layout/BorderContainer&quot;, &quot;dijit/layout/ContentPane&quot;, &quot;dojo/domReady!&quot;, ], function ( Map, WMSLayer, WMSLayerInfo, Draw, SimpleLineSymbol, SimpleFillSymbol, Graphic, ArcGISTiledMapServiceLayer, Extent, array, dom, domConst, parser ) { parser.parse(); map = new Map(&quot;map&quot;, { center: new esri.geometry.Point( 120.1594, 30.2571, new esri.SpatialReference({ wkid: 4490 }) ), zoom: 3, }); var layer2 = new ArcGISTiledMapServiceLayer( &quot;http://21.15.116.31/e8b4611714092ac7bc35cb5e8d476e4824f85f3a/Tile/ArcGISREST/hzsyvector.gis&quot;, { id: &quot;底图&quot;, } ); map.addLayer(layer2); map.on(&quot;load&quot;, function () { var extent = map.extent; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms&quot;; var resourceInfo = { extent: extent, layerInfos: [], version: &quot;1.1.1&quot;, visibleLayers: [&quot;1&quot;], }; var wmslayer = new esri.layers.WMSLayer(url, { resourceInfo: resourceInfo, }); wmslayer.setImageFormat(&quot;png&quot;); wmslayer.setVisibleLayers(&quot;gpserver&quot;); map.addLayer(wmslayer); toolbar = new Draw(map); toolbar.on(&quot;draw-end&quot;, showinfo); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }); function showinfo(evt) { var symbol; toolbar.deactivate(); map.graphics.clear(); symbol = new SimpleFillSymbol(); var graphic = new Graphic(evt.geometry, symbol); map.graphics.add(graphic); getAttr(evt.geometry); } function getAttr(geo) { var point = geo.cache._extent.xmin + &quot;,&quot; + geo.cache._extent.ymin + &quot;,&quot; + geo.cache._extent.xmax + &quot;,&quot; + geo.cache._extent.ymax; // &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); var attr = &quot;&quot;; for (var i = 0; i &lt; text.features.length; i++) { attr += text.features[i].properties[&quot;quxian&quot;] + &quot;&lt;/br&gt;&quot;; } $(&quot;#details&quot;).html(attr); toolbar.activate(Draw[&quot;RECTANGLE&quot;]); }, }); } }); &lt;/script&gt;&lt;/head&gt;&lt;body class=&quot;claro&quot;&gt; &lt;div id=&quot;content&quot; data-dojo-type=&quot;dijit.layout.BorderContainer&quot; data-dojo-props=&quot;design:'headline', gutters:true&quot; style=&quot;width: 100%; height: 100%; margin: 0;&quot; \\&gt; &lt;div ​ id=&quot;details&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'left', splitter:true&quot; ​ style=&quot;overflow: auto; width: 200px;&quot; \\&gt;在框选范围内查询属性（区县=上城区）的数据&lt;/div&gt; &lt;div ​ id=&quot;map&quot; ​ data-dojo-type=&quot;dijit.layout.ContentPane&quot; ​ data-dojo-props=&quot;region:'center'&quot; ​ style=&quot;overflow: hidden;&quot; \\&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; Openlayers OpenLayers 是一个专为Web GIS 客户端开发提供的JavaScript 类库包，用于实现标准格式发布的地图数据访问。OpenLayers实现访问地理空间数据的方法都符合行业标准。OpenLayers 支持Open GIS 协会制定的WMS（Web Mapping Service）和WFS（Web Feature Service）等网络服务规范，可以通过远程服务的方式，将以OGC 服务形式发布的地图数据加载到基于浏览器的OpenLayers 客户端中进行显示。详细见官方网站https://openlayers.org/ 地图 矢量电子地图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot;&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = 'image/png'; var bounds = [119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439 ]; var projection = new ol.proj.Projection({ code: 'EPSG:4490', units: 'degrees', axisOrientation: 'neu', global: true }); var fullExtent = [118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = (-tileCoord[2] - 1); var url = 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/' + z + '/' + y + '/' + x; console.log(url); return url; }, projection: 'EPSG:4490' }) }); var map = new ol.Map({ target: 'map', layers: [ tileLayer ], view: new ol.View({ projection: projection }) }); map.getView().fit(bounds, map.getSize()); //非常重要 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 影像电子地图 添加电子地图 var format = 'image/png'; var bounds = [119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439 ]; var projection = new ol.proj.Projection({ code: 'EPSG:4490', units: 'degrees', axisOrientation: 'neu', global: true }); var fullExtent = [118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = (-tileCoord[2] - 1); var url = 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyraster/Mapserver/tile/' + z + '/' + y + '/' + x; console.log(url); return url; }, projection: 'EPSG:4490' }) }); var map = new ol.Map({ target: 'map', layers: [ tileLayer ], view: new ol.View({ projection: projection }) }); map.getView().fit(bounds, map.getSize()); //非常重要 图层 弹出框信息框 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;弹出信息框&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;style&gt; #map { height: 100%; width: 100%; } #mypopup { background: #fff; }&lt;/style&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;div id=&quot;mypopup&quot;&gt; &lt;div id=&quot;mypopup-content&quot;&gt;内容自定义&lt;/div&gt; &lt;button id=&quot;closeOverlay&quot;&gt;关闭&lt;/button&gt; &lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; // console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var info = $(&quot;#mypopup&quot;); var content = $(&quot;#mypopup-content&quot;); var overlay = new ol.Overlay({ element: info[0], autopan: true, autoPanMargin: 20, positioning: &quot;center-center&quot;, }); //map.addOverlay(overlay); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer], overlays: [overlay], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 $(&quot;#closeOverlay&quot;).click(function () { info.hide(); return false; }); map.on(&quot;click&quot;, function (e) { var location = e.coordinate; content.html(&quot;这是信息框&quot;); info.show(); overlay.setPosition(location); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 矢量图层（wms） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;添加电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot;&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = 'image/png'; var bounds = [119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439 ]; var projection = new ol.proj.Projection({ code: 'EPSG:4490', units: 'degrees', axisOrientation: 'neu', global: true }); var fullExtent = [118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599E-05, 4.29635263467995E-05, 2.14817631733998E-05, 1.07408815866999E-05, 5.37044079334994E-06, 2.68522039667497E-06, 1.34261019833748E-06, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = (-tileCoord[2] - 1); var url = 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/' + z + '/' + y + '/' + x; console.log(url); return url; }, projection: 'EPSG:4490' }) }); var untiled = new ol.layer.Image({ source: new ol.source.ImageWMS({ ratio: 1, //url: 'http://126.10.2.27:8080/geoserver/gpserver/wms', url: 'http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wms?', params: { 'FORMAT': format, 'VERSION': '1.1.1', &quot;LAYERS&quot;: 'lsyt_dmdz_qxbj0619', &quot;exceptions&quot;: 'application/vnd.ogc.se_inimage', } }) }); var map = new ol.Map({ target: 'map', layers: [ tileLayer, untiled ], view: new ol.View({ projection: projection }) }); map.getView().fit(bounds, map.getSize()); //非常重要 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 矢量图层（wfs） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;添加WFS电子地图&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 查询 点查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;点查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 map.on(&quot;click&quot;, function (e) { var location = e.coordinate; var point = location[0] + &quot;,&quot; + location[1] + &quot;,&quot; + location[0] + &quot;,&quot; + location[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;You clicked the map at &quot; + text.features[1].properties['quxian']); } }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 面查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;框选查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 var dragbox = new ol.interaction.DragBox(); map.addInteraction(dragbox); dragbox.on(&quot;boxend&quot;, function () { var extent = dragbox.getGeometry().getExtent(); var point = extent[0] + &quot;,&quot; + extent[1] + &quot;,&quot; + extent[0] + &quot;,&quot; + extent[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;You clicked the map at &quot; + text.features[0].properties['quxian']); } }); }) &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 属性查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;属性查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 map.on(&quot;click&quot;, function (e) { var location = e.coordinate; var point = location[0] + &quot;,&quot; + location[1] + &quot;,&quot; + location[0] + &quot;,&quot; + location[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=&quot;; qurl += &quot; quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); alert(&quot;根据属性选择的是&quot; + text.features[0].properties['quxian']); } }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 组合查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;title&gt;组合查询&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/css/ol.css&quot; type=&quot;text/css&quot; /&gt; &lt;script src=&quot;http://172.18.109.232:8082/openlayer/v3.20.1/build/ol.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://172.18.109.232:8082/jquery/jquery.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;map&quot; class=&quot;map&quot;&gt;&lt;/div&gt; &lt;script&gt; var format = &quot;image/png&quot;; var bounds = [ 119.347618103027, 29.7052669525146, 120.698570251465, 30.5366992950439, ]; var projection = new ol.proj.Projection({ code: &quot;EPSG:4490&quot;, units: &quot;degrees&quot;, axisOrientation: &quot;neu&quot;, global: true, }); var fullExtent = [ 118.33968849655, 29.188589472, 120.71408849655, 30.5651894750343, ]; var resolutions = [ 0.00549933137239034, 0.00274966568619517, 0.00137483284309758, 0.000687416421548792, 0.000343708210774396, 0.000171854105387198, 8.5927052693599e-5, 4.29635263467995e-5, 2.14817631733998e-5, 1.07408815866999e-5, 5.37044079334994e-6, 2.68522039667497e-6, 1.34261019833748e-6, ]; var tileGrid = new ol.tilegrid.TileGrid({ tileSize: 256, origin: [118.122911693886, 31.2869311022836], extent: fullExtent, resolutions: resolutions, }); // ol.source.XYZ添加瓦片地图的层 var tileLayer = new ol.layer.Tile({ source: new ol.source.XYZ({ tileGrid: tileGrid, tileUrlFunction: function (tileCoord) { var z = tileCoord[0]; var x = tileCoord[1]; var y = -tileCoord[2] - 1; var url = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/tile/&quot; + z + &quot;/&quot; + y + &quot;/&quot; + x; //console.log(url); return url; }, projection: &quot;EPSG:4490&quot;, }), }); var vectorSource = new ol.source.Vector({ format: new ol.format.GeoJSON(), url: function (extent) { return &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&quot;; }, //strategy: bboxStrategy, }); var vector = new ol.layer.Vector({ source: vectorSource, style: new ol.style.Style({ stroke: new ol.style.Stroke({ color: &quot;rgba(0, 0, 255, 1.0)&quot;, width: 2, }), }), }); var map = new ol.Map({ target: &quot;map&quot;, layers: [tileLayer, vector], view: new ol.View({ projection: projection, }), }); map.getView().fit(bounds, map.getSize()); //非常重要 map.on(&quot;click&quot;, function (e) { var location = e.coordinate; var point = location[0] + &quot;,&quot; + location[1] + &quot;,&quot; + location[0] + &quot;,&quot; + location[1];// &quot;120.16271670916996,30.251675868508478,120.1713094144393,30.25591314629446&quot;; var qurl = &quot;http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.0.0&amp;request=getfeature&amp;typename=lsyt_dmdz_qxbj0619&amp;outputFormat=application/json&amp;CQL_FILTER=BBOX(geom,&quot; + point + &quot;)&quot;; qurl += &quot; and quxian='上城区'&quot;; $.ajax({ url: qurl, success: function (data) { console.log(data); var text = JSON.parse(data); if (!text.features.length) { alert(&quot;你选择的是非上城区，无查询结果！&quot;); } else { alert(&quot;你选择的是 &quot; + text.features[0].properties[&quot;quxian&quot;]); } } }); }); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; Cygwin windows使用linux环境（部分机型失败） Tippecanoe Tippecanoe是Mapbox的一个开源切片工具，项目地址：https://github.com/mapbox/tippecanoe，Mapbox常规的切片方法tilelive-copy参见另一篇博客。Tippecanoe主要在处理大数据量时有很大的优势，具有很高的效率，并且有很多参数可以控制。Tippecanoe只能处理GeoJSON，因此在切片前需要将矢量数据转换为GeoJSON，推荐使用ogr2ogr工具转换。切片以后的格式为mbtiles，可自行导入mongodb等数据库。 目的：根据你的数据创造一个可自由缩放的视图 引用地址：https://my.oschina.net/u/1464512/blog/1631972 常用tippecanoe参数设置 GEOJSON GeoSever GeoServer是OGC Web服务器规范的J2EE实现，利用GeoServer可以方便地发布地图数据，允许用户对特征数据进行更新、删除、插入操作，通过GeoServer可以比较容易地在用户之间迅速共享空间地理信息。GeoServer是开源软件。 GeoServer主要包含如下一些特点： 兼容WMS和WFS特性 支持PostGIS、Shapefile、ArcSDE、Oracle、VPF、MySQL、MapInfo 支持上百种投影 能够将网络地图输出为JPEG、GIF、PNG、SVG、KML等格式 能够运行在任何基于J2EE/Servlet容器之上 嵌入MapBuilder支持AJAX的地图客户端OpenLayers 引用地址 WMS &amp;&amp; WFS WMS是由服务器将一地图图像发送给客户端，而WFS是服务器将矢量数据发送给客户端，也就是在使用WMS时地图由服务器绘制，在使用WFS时地图由客户端绘制。 WMS 1.2.1 WMS服务简介 Web Map Service（网络地图服务），简称WMS，由开放地理信息联盟（Open GeoSpatial Consortium，OGC）制定。该规范定义了Web客户端从网络地图服务器获取地图的接口标准。一个WMS可以动态地生成具有地理参考数据的地图，这些地图通常用GIF、JPEG或PNG等图像格式等，使用者通过指定的参数获取相应的地图图片。 1.2.2 服务操作列表 WMS实现规范由三个基础性操作协议(GetCapabilities，GetMap和GetFeatureInfo)组成，这些协议共同构成了利用WMS创建和叠加显示不同来源的远程异构地图服务的基础。WMS服务操作列表见下表所示。 操作 实现要求 描述 GetCapabilities 强制实现 暂关闭 GetMap 强制实现 获取地图图片。该操作根据客户端发出的请求参数在服务端进行检索，服务器端返回一个地图图像，其地理空间参数和大小参数是已经明确定义的，返回的地图图像可以是GIF、JPEG、PNG或SVG格式。 GetFeatureInfo 选择实现 1.2.3服务操作的参数列表 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WMS” request 1个(必选) 字符类型，请求的操作名称，值为“GetCapabilities” version 0或1个(可选) 字符类型，值为请求的WMS的版本号 format 0或1个(可选) MIME类型，值为服务元数据的输出格式 GetMap操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型标识值为“WMS” request 1个(必选) 字符类型，值为“GetMap” version 1个(必选) 字符类型，值为请求的WMS的版本号 layers 1个(必选) 字符类型，值为一个或多个地图图层列表，多个图层之间用”,”隔开 styles 1个(必选) 字符类型，值为请求图层的地图渲染样式 CRS 1个(必选) 字符类型，值为坐标参照系统 BBOX 1个(必选) Wkt格式，值为某个CRS下的地图边界范围的坐标序列 width 1个(必选) 整型类型，值为地图图片的像素宽度 height 1个(必选) 整型类型，值为地图图片的像素高度 format 1个(必选) 字符类型，值为地图的输出格式 transparent 0或1个(可选) 字符类型，值为true或者false，用来表示地图图层是否透明(默认情况下是不透明的) filter 0或1个(可选) 请求要素的过滤条件 1.2.4 接口调用示例 实例名称 调用实例 GetCapabilities 暂关闭 GetMap http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wms?SERVICE=WMS&amp;VERSION=1.1.1&amp;REQUEST=GetMap&amp;FORMAT=image%2Fpng&amp;TRANSPARENT=true&amp;LAYERS=gpserver%3Alsyt_dmdz_csbj0619&amp;CQL_FILTER=quxian%3D'滨江区'&amp;SRS=EPSG%3A4490&amp;STYLES=&amp;WIDTH=768&amp;HEIGHT=546&amp;BBOX=120.05413055419922%2C30.084686279296875%2C120.31780242919922%2C30.272140502929688 WFS 1.3.1 WFS服务简介 Web Feature Service（网络要素服务），简称WFS，由开放地理信息联盟（Open GeoSpatial Consortium，OGC）制定。该规范主要对OpenGIS简单要素的数据编辑操作进行规范，从而使服务器端和客户端能够在要素层面进行“通讯”。其返回结果的是XML格式的WFS服务元数据文档，通过该文档用户能够了解：WFS服务器支持的所有操作操作列表，GetFeature操作返回的数据格式，可用的坐标参照系统列表，操作异常信息的列表，WFS服务提供商的相关信息，WFS服务器的可用要素类列表等。 1.3.2 服务操作列表 WFS服务接口规范定义了GetCapabilities，DescribeFeatureType、GetFeature、Transaction等操作。其中GetCapabilities，DescribeFeatureType和GetFeature为必须实现的操作，也即只要实现了这三个操作的服务均可称为WFS服务。WFS的操作见下表所示。 操作 实现要求 描述 GetCapabilities 强制实现 GetCapabilities请求用于查询WFS服务的能力信息，包括支持的操作、支持的格式、空间坐标、包含的资源等。它主要的目的是使客户端在使用GetFeature请求前可以对WFS服务有一个基本的了解，从而可以设置正确的参数。 DescribeFeatureType 强制实现 用于生成一个 Schema 描述，该 Schema 描述了 WFS 服务提供的要素类型（Feature Type），以及要素类型的结构信息。该 Schema 还定义了 WFS 服务所期望的要素实例在输入时如何编码以及输出时如何生成要素实例。 GetFeature 强制实现 GetFeature用于向WFS的客户端程序提供查询特定地理信息的能力，通过GetFeature操作可以由指定的属性条件、空间条件或者两者叠加的条件进行空间查询。 Transaction 选择实现 允许Transaction操作，使客户端可对服务器端所提供的地图要素类执插入，更新，删除等命令 1.3.3服务操作的参数列表 GetCapabilities操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WFS” request 1个(必选) 字符类型，请求的操作名称，值为“GetCapabilities” versions 0或1个(可选) 字符类型，值为请求的WFS的版本号 DescribeFeatureType操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WFS” request 1个(必选) 字符类型，请求的操作名称，值为“DescribeFeatureType” typeName 0或1个(可选) 字符类型，值为要素类型的列表，多个值之间用“，”隔开，默认解析包括的全部要素类型 outputFormat 0或1个(可选) MIME类型，值为输出格式 GetFeature操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型标识值为“WFS” request 1个(必选) 字符类型，请求的操作值为“GetFeature” typeName 1个(必选) 字符类型，值为请求的要素类型的名称，多个名称之间用“，”隔开 version 0或1个(可选) 字符类型，值为请求的WFS的版本号 outputFormat 0或1个(可选) MIME类型，值为输出格式 resultType 0或1个(可选) 字符类型，值为请求的结果类型 propertyName 0或1个(可选) 字符类型，值为请求要素的属性名，多个值之间用“，”隔开 featureVersion 0或1个(可选) 字符类型，值为要素的版本，值为ALL返回请求的要素的所有版本，没有值默认为返回请求要素的最新版本 maxFeature 0或1个(可选) 整型类型，值为请求要素的最大数，默认值为满足查询的所有结果集 expiry 0或1个(可选) 数字类型，要素被锁定的时间 SRSName 0或1个(可选) 字符类型，值为坐标系统名 featureID 0或1个(可选) 字符类型，值为要素的ID，多个ID之间用“，”隔开 filter 0或1个(可选) 请求要素的过滤条件 bBox 0或1个(可选) Wkt格式，请求指定要素查询范围，可以替代featureId和filter参数 1.3.4 接口调用示例 实例名称 调用实例 GetCapabilities http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wfs?service=wfs&amp;version=1.1.0&amp;request=getcapabilities DescribeFeatureType http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_qxbj0619/MapServer/wfs?service=wfs&amp;version=1.1.0&amp;request=describefeaturetype&amp;typename=lsyt_dmdz_csbj0619 GetFeature http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/lsyt_dmdz_csbj0619/MapServer/wfs?service=wfs&amp;version=1.1.0&amp;request=getfeature&amp;typename=lsyt_dmdz_csbj0619&amp;outputFormat=json&amp;CQL_FILTER=zhenjie='闲林街道' WMTS WMTS 1.1 WMTS服务简介 ​ Web Map Tile Service（网络地图瓦片服务），简称WMTS，由开放地理信息联盟（Open GeoSpatial Consortium，OGC）制定，是和WMS并列的重要OGC规范之一。WMTS标准定义了一些操作，这些操作允许用户访问切片地图，是OGC首个支持RESTful访问的服务标准。WMTS不同于WMS,它最重要的特征是采用缓存技术能够缓解WebGIS服务器端数据处理的压力，提高交互响应速度，大幅改善在线地图应用客户端的用户体验。WMTS是OGC主推的缓存技术规范，是目前各种缓存技术相互兼容的一种方法。 WMTS的切片坐标系统和其组织方式可参考下图： 1.2 服务操作列表 WMTS服务支持RESTful访问，其接口包括GetCapabilities、GetTile和GetFeatureInfo3个操作，这些操作允许用户访问切片地图。 操作 操作 描述 GetCapabilities 强制实现 获取WMTS的能力文档（即元数据文档），里面包含服务的信息 GetTile 强制实现 该操作根据客户端发出的请求参数在服务端进行检索，服务器端返回地图瓦片图像 GetFeatureInfo 选择实现 1.3 服务操作的参数列表 GetCapabilities操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型值为“WMTS” request 1个(必选) 字符类型，请求的操作名称，值为“GetCapabilities” GetTile操作请求方法实现参数 参数名称 参数个数 参数类型和值 service 1个(必选) 字符类型，服务类型标识值为“WMTS” request 1个(必选) 字符类型，请求的操作值为“GetTile” version 1个(必选) 字符类型，值为请求的WMTS的版本号 layer 1个(必选) 字符类型，值为请求的图层名称 style 1个(必选) 字符类型，值为请求图层的渲染样式 format 1个(必选) 字符类型，值为瓦片地图的输出格式 tileMatrixSet 1个(必选) 字符类型，瓦片矩阵数据集，其值在服务的元数据文档中指定 tileMatrix 1个(必选) 字符类型，瓦片矩阵，其值在服务的元数据文档中指定 tileRow 1个(必选) 整型类型，值为大于0的整数，表示瓦片矩阵的行号 tileCol 1个(必选) 整型类型，值为大于0的整数，表示瓦片矩阵的列号 1.4接口调用示例 实例名称 调用实例 GetCapabilities操作 http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/WMTS?service=wmts&amp;request=GetCapabilities GetTile操作 http://172.18.109.232:8080/68DE38F71E38CD8C508FAB3035752EA8ED8F9221EDE18FDBE593B01FCDD12BD296F09FE27CD7D2733AD075EAF994851B/PBS/rest/services/hzsyvector/Mapserver/WMTS?service=wmts&amp;request=getTile&amp;version=1.0.0&amp;layer=hzsyvector&amp;Style=&amp;tilematrixset=hzsyvector&amp;tilematrix=3&amp;tilerow=5&amp;tilecol=7&amp;format=png ****","link":"/2021/02/22/Draft/2021/GIS/"},{"title":"JVM 与上层技术","text":"JVM 与上层技术 新学四问 **WHY【与前代优化了什么，弥补了什么空白】：**了解底层，优化，面试，解决底层BUG **WHAT【框架，思维导图，主题框架】：**结构， HOW【如何记忆，学习资源】：学习资源：尚硅谷JVM，ANKI记忆，文档 **LEVEL【不是每个都学精】：**了解 进度：上篇 【完12.19】 上篇：内存与垃圾回收 速查 -XX选项表 堆 选项名 选项作用 默认值 备注 -XX: +PrintFlagsInitial 查看所有的参数的默认初始值 -XX: +PrintFlagsFinal 查看所有的参数的最终值（可能会存在修改，不再是初始值) jps 查看当前运行中的进程 jinfo -flag 参数名 进程id 当前进程中参数值 -Xms 初始雄空向内存 物理内存的1/64 -Xmx :最大雄空间内存 物理内存的1/4 -Xmn :设置新生代的大小。（初始值及最大值） -XX: NewRatio :配置新生代与老年代在堆结构的占比 新老比：1:2 -XX: SurvivorRatio :设置新生代中Eden和S0/S1空间比例 8 -XX: MaxTenuringThreshold :设置新生代垃扱的最大年龄 -XX: +PrintGCdetails :输出详细GC处理日志 🌞 -XX: +PrintGC 🌞 - verbose:GC :打印筒要信息 -XX: HandlePromotionFailure :是否设置空问分配担保 -XX: Doescapeanalysis 显式开启逃逸分析 JDK 6u23后默认开启 -XX: Printescapeanalysis 查看逃逸分析的筛选结果 -XX:+EliminateAllocations :开启标量替换，允许将对象打散分配在栈上。 默认打开 方法区 选项名 选项作用 默认值 备注 垃圾回收器 选项名 选项作用 默认值 备注 一、JVM与JAVA体系结构 1.JAVA与JVM Java大事件 1990年，在sun公司中，由Patrick naughton、mikesheridan以及james Gosling领导的小组Green Team，开发出新的程序语言，命名为OAK，后期更名为Java 1995年，sun正式发布Java和hotJAVA产品，Java首次公开亮相。 1996年1月23日sun Microsystems发布了JDK1.0. 1998年，JDK1.2版本发布。同时，sun发布了JSP/Servlet、EJB规范，以及将Java分成了J2EE、J2SE和J2ME。这表明Java开始向企业、桌面应用和移动设备应用3大领域挺进。 2000年，JDK1.3发布，Java HotSpot Virtual Machine正式发布，成为Java默认的虚拟机。 2002年，JDK 1.4发布，古老的classic虚拟机退出历史舞台。 2003年底，Java平台的scala正式发布，同年Groovy也加入了Java阵营。 2004年，JDK1.5发布，同时JDK1.5改名为JavaSE5.0. 2006年，JDK 6发布，同年Java开源并建立了openJDK，顺理成章，Hotspot虚拟机成为了OpenJDK中默认的虚拟机。 2007年，Java平台迎来了新伙伴Clojure。 2008年，Oracle收购了BEA，得到了JRockit虚拟机。 2009年，Twitter宣布将后台大部分程序从ruby迁移到Scala，这是Java平台的有一次大规模应用。 2010年，Oracle收购了sun，获得Java商标和最具价值的hotspot虚拟机。此时Oracle拥有市场占用率最高的两款虚拟机hotspot和JRockit，并且计划未来进行整合：HotRockit。 2011年，JDK7发布，在JDK１.7ｕ4中，正式启用了新的垃圾回收器G1. 2017年，JDK9发布，将G1设置为默认GC，替代CMS。 2017同年，IBM的J9开源，形成了现在的open J9社区。 2018年，Android的Java侵权案判决，Google公司赔偿Oracle总计88亿美元。 2018同年，Oracle宣布JavaEE成为历史名词，JDBC、JMS、Servlet赠与Eclipse基金会。 2018同年，JDK11发布，LTS版本的JDK，发布革命性的ZGC，调整JDK授权许可。 2019年，JDK12发布，加入RedHat领导开发的shenandoah GC。 在JDK11之前，OracleJDK还会存在一些openJDK中没有的、闭源的功能。但在JDK11中，openJDK和OracleJDK代码实质上已经达到完全一致的程度。 JVM介绍 ​ 所谓虚拟机(Virtual Machine)，就是一台虚拟的计算机。它是一款软件，用来执行一系列虚拟计算机指令。大体上，虚拟机可以分为系统虚拟机和程序虚拟机。大名鼎鼎的Visual Box,VMware就属于系统虚拟机，它们完全是对物理计算机的仿真，提供了一个可运行完整操作系统的软件平台。程序虚拟机的典型代表就是Java虚拟机，它专门为执行单个计算机程序而设计，在Java虚拟机中执行的指令我们称为Java字节码指令。无论是系统虚拟机还是程序虚拟机，在上面运行的软件都被限制于虚拟机提供的资源中。 ​ Java虚拟机是一台执行Java字节码的虚拟计算机，它拥有独立的运行机制,其运行的Java字节码也未必由Java语言编译而成。JVM平台的各种语言可以共享Java虚拟机带来的跨平台性、优秀的垃圾回器，以及可靠的即时编译器。Java技术的核心就是Java虚拟机（JVM，Java Virtual Machine) ,因为所有的Java程序都运行在Java虚拟机内部。 **作用：**Java虚拟机就是二进制字节码的运行环境，负责装载字节码到其内部，解释/编译为对应平台上的机器指令执行。每一条Java指令，Java虚拟机规汜甲都有详细定义，如怎么取操作数，怎么处理操作数，处理结果放在哪里。 **特点：**一次编译，到处运行；自动内存管理；自动垃圾回收功能 位置： JVM整体结构 Java代码执行流程 ​ JVM架构模型 Java编译器输入的指令流基本上是一种基于栈的指令集架构，另外一种指令集架构则是基于寄存器的指令集架构。 区别: ·基于栈式架构的特点 设计和实现更简单，适用于资源受限的系统; 避开了寄存器的分配难题:使用零地址指令方式分配。 指令流中的指令大部分是零地址指令，其执行过程依赖于操作栈。指令集更小，编译器容易实现。 不需要硬件支持，可移植性更好，更好实现跨平台 基于寄存器架构的特点 典型的应用是x86的二进制指令集:比如传统的pc以及Android的Davlik虚拟机。 指令集架构则完全依赖硬件,可移植性差 性能优秀和执行更高效; 花费更少的指令去完成一项操作。 在大部分情况下，基于寄存器架构的指令集往往都以一地址指令、二地址指令和三地址指令为主，而基于栈式架构的指令集却是以零地址指令为主。 总结: 由于跨平台性的设计，Java的指令都是根据栈来设计的。不同平台CPu架构不同，所以不能设计为基于寄存器的。优点是跨平台，指令集小，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。 时至今日，尽管嵌入式平台已经不是Java程序的主流运行平台了（准确的来说HotSpotVM的宿主环境已经不局限于嵌入式平台了)，那么为什么不将 架构更换为基于寄存器的架构呢? 栈: 跨平台性、指令集小、指令多;执行性能比寄存器差 JVM生命周期 启动 Java虚拟机的启动是通过引导类加载器(bootstrap class loader)创建一个初始类(initial class)来完成的，这个类是由虚拟机的具体实现指定的 执行 一个运行中的Java虚拟机有着一个清晰的任务:执行Java程序。 程序开始执行时他才运行，程序结束时他就停止。 执行一个所谓的Java程序的时候，真真正正在执行的是一个叫做Java虚拟机的进程。 退出 有如下的几种情况:。程序正常执行结束 ·程序在执行过程中遇到了异常或错误而异常终止·由于操作系统出现错误而导致Java虚拟机进程终止 ·某线程调用Runtime类或system类的exit方法，或Runtime类的halt方法，并且Java安全管理器也允许这次exit或halt操作。 ·除此之外，JNI ( Java Native Interface)规范描述了用JNI Invocation API来加载或卸载Java虚拟机时，Java虚拟机的退出情况。 JVM发展历程 SUN Classic VM ·早在1996年Java1.0版本的时候，sun公司发布了一款名为Sun Classic VM的Java虚拟机，它同时也是世界上第一款商用Java虚拟机，JDK1.4时完全被淘汰。 ·这款虚拟机内部只提供解释器。 ·如果使用JIT编译器，就需要进行外挂。但是一旦使用了JIT编译器JIT就会接管虚拟机的执行系统。解释器就不再工作。解释器和编译器不能配合工作。 ·现在hotspot内置了此虚拟机。 Exact VM ·为了解决上一个虚拟机问题，jdk1.2时，sun提供了此虚拟机。 Exact Memory Management:准确式内存管理 ·也可以叫Non-conservative/Accurate Memory Management：虚拟机可以知道内存中某个位置的数据具体是什么类型。 具备现代高性能虚拟机的雏形 ·热点探测 ·编译器与解释器混合工作模式I 只在solaris平台短暂使用，其他平台上还是classic vm ·英雄气短，终被Hotspot虚拟机替换 SUN公司的 HotSpot VM HotSpot历史 ·最初由一家名为“Longview Technologies&quot;的小公司设计1997年，此公司被sun收购;2009年，sun公司被甲骨文收购。 ·JDK1.3时，HotSpot VM成为默认虚拟机 ·目前Hotspot占有绝对的市场地位，称霸武林。 不管是现在仍在广泛使用的JDK6，还是使用比例较多的JDK8中，默认的虚拟机都是HotSpot sun/ oracle JDK 和 OpenJDK的默认虚拟机 因此本课程中默认介绍的虚拟机都是HotSpot，相关机制也主要是指HotSpot的GC机制。(比如其他两个商用虚拟机都没有方法区的概念) ·从服务器、桌面到移动端、嵌入式都有应用。 ·名称中的HotSpot指的就是它的热点代码探测技术。 通过计数器找到最具编译价值代码，触发即时编译或栈上替换 通过编译器与解释器协同工作，在最优化的程序响应时间与最佳执行性能中取得平衡 BEA 的JRockit ·专注于服务器端应用 它可以不太关注程序启动速度，因此JRockit内部不包含解析器实现，全部代码都靠即时编译器编译后执行。 ·大量的行业基准测试显示，JRockit JVM是世界上最快的JVM。 使用JRockit产品，客户已经体验到了显著的性能提高（一些超过了70% ）和 硬件成本的减少(达50%）。 ·优势:全面的Java运行时解决方案组合 JRockit面向延迟敏感型应用的解决方案JRockit Real Time提供以毫秒或 微秒级的JVM响应时间，适合财务、军事指挥、电信网络的需要 MissionControl服务套件，它是一组以极低的开销来监控、管理和分析生产 环境中的应用程序的工具。 2008年，BEA被oracle收购。 oracle表达了整合两大优秀虚拟机的工作，大致在JDK 8中完成。整合的方式是在HotSpot的基础上，移植JRockit的优秀特性。 ·高斯林:目前就职于谷歌，研究人工智能和水下机器人 IBM 的J9 全称:IBM Technology for Java virtual Machine，简称IT4J，内部代号:J9 ·市场定位与HotSpot接近，服务器端、桌面应用、嵌入式等多用途VM。 ·广泛用于IBM的各种Java产品。 ·目前，有影响力的三大商用虚拟机之一，也号称是世界上最快的Java虚拟机。 ·2017年左右，IBM发布了开源J9 VM，命名为openJ9，交给Eclipse基金会管理，也称为Eclipse OpenJ9 KVM和cDC/CL.DC Hotspot oracle在Java ME产品线上的两款虚拟机为:CDC/CLDC HotSpot Implementation VM KVM (Kilobyte）是CLDC-HI早期产品 ·目前移动领域地位尴尬，智能手机被Android和ioS二分天下。 .KVM简单、轻量、高度可移植，面向更低端的设备上还维持自己的一片市场 智能控制器、传感器I 老人手机、经济欠发达地区的功能手机 .所有的虚拟机的原则:一次编译，到处运行。 Azul VM ·前面三大“高性能Java虚拟机”使用在通用硬件平台上 ·这里Azul VM和BEA Liquid VM是与特定硬件平台绑定、软硬件配合的专有虚拟机 高性能Java虚拟机中的战斗机。 Azul VM是Azul systems公司在HotSpot基础上进行大量改进，运行于Azul systems公司的专有硬件vega系统上的Java虚拟机。 ·每个Azul VM实例都可以管理至少数十个CPu和数百GB内存的硬件资源，并提供在巨大内存范围内实现可控的GC时间的垃圾收集器、专有硬件优化的线程调度等优秀特性。 2010年，Azul systems公司开始从硬件转向软件，发布了自己的ZingJVM，可以在通用x86平台上提供接近于Vega系统的特性。T Liquid VM ·高性能Java虚拟机中的战斗机。 ·BEA公司开发的，直接运行在自家Hypervisor系统上 ·Liquid VM即是现在的JRockit VE(Virtual Edition） ,LiquidVM不需要操作系统的支持，或者说它自己本身实现了一个专用操作系统的必要功能，如线程调度、文件系统、网络支持等。 ·随着JRockit虚拟机终止开发，Liquid VM项目也停止了。 Apache Harmony Apache也曾经推出过与JDK 1.5和JDK 1.6兼容的Java运行平台Apache Harmony。 ·它是IBM和Intel联合开发的开源JVM，受到同样开源的openJDK的压制， sun坚决不让Harmony获得JCP认证，最终于2011年退役，IBM转而参与OpenJDK ·虽然目前并没有Apache Harmony被大规模商用的案例，但是它的Java 类库代码吸纳进了Android SDK。 Microsoft JVM ·微软为了在IE3浏览器中支持Java Applets，开发了Microsoft JVM。·只能在window平台下运行。但确是当时windows下性能最好的Java VM.. 1997年，sun以侵犯商标、不正当竞争罪名指控微软成功，赔了sun很多钱。微软在windowsXP SP3中抹掉了其VM。现在windows上安装的jdk都是HotSpot。l TaobaoJVM ·由AliJVM团队发布。阿里，国内使用Java最强大的公司，覆盖云计算、金雷生切一电商等众多领域，需要解决高并发、高可用、分布式的复合问题。有大重的开源广的。 ·基于openJDK开发了自己的定制版本AlibabaJDK，简称AJDK。是整个阿里Java体系的基石。 ·基于openJDK HotSpot VM 发布的国内第一个优化、深度定制且开源的高性能服务器版Java虚拟机。 创新的GCIH (GC invisible heap ）技术实现了off-heap ，即将生命周期较长的Java对象从heap中移到heap之外，并且GC不能管理GCIH内部的Java 对象，以此达到降低GC的回收频率和提升GC 的回收效率的目的。 GCIH 中的对象还能够在多个Java虚拟机进程中实现共享 使用crc32指令实现JVM intrinsic降低JNI 的调用开销 PMU hardware 的Java profiling tool和诊断协助功能 针对大数据场景的ZenGC . taobao vm应用在阿里产品上性能高，硬件严重依赖intel的cpu，损失了兼容性，但提高了性能 目前已经在淘宝、天猫上线，把oracle 官方JVM版本全部替换了。 Dalvik VM : ·谷歌开发的，应用于Android系统，并在Android2.2中提供了JIT，发展迅猛。.Dalvik VM只能称作虚拟机，而不能称作“Java虚拟机”，它没有遵循 Java虚拟机规范 ·不能直接执行Java 的class 文件·基于寄存器架构，不是jvm的栈架构。 ·执行的是编译以后的dex(Dalvik Executable）文件。执行效率比较高 它执行的dex (Dalvik Executable）文件可以通过class文件转化而来，使用Java语法编写应用程序，可以直接使用大部分的Java API等。 ·Android 5.0使用支持提前编译(Ahead of Time Compilation，AOT）的ARTVM替换Dalvik VM。 Graal VM . 2018年4月，oracle Labs公开了Graal VM，号称&quot;Run Programs Faster Anywhere&quot;，勃勃野心。与1995年java的”write once，run anywhere&quot;遥相呼应。 Graal VM在HotSpot VM基础上增强而成的跨语言全栈虚拟机，可以作为“任何语言”的运行平台使用。语言包括: Java、Scala、Groovy、Kotlin;C、C++ Javascript、Ruby、Python、R等 ·支持不同语言中混用对方的接口和对象，支持这些语言使用已经编写好的本地库文件工作原理是将这些语言的源代码或源代码编译后的中间格式，通过解释器转换为能被Graal VM接受的中间表示。Graal VM提供Truffle工具集快速构建面向一种新语言的解释器。在运行时还能进行即时编译优化，获得比原生编译器更优秀的执行效率。 如果说HotSpot有一天真的被取代，Graal VM希望最大。但是Java的软件生态没有丝毫变化。 二、类加载子系统 1.内存简图 2.类加载器 ​ 类加载器子系统负责从文件系统或者网络中加载class文件，class文件在文件开头有特定的文件标识。 ClassLoader只负责class文件的加载，至于它是否可以运行，则由ExecutionEngine决定。 加载的类信息存放于一块称为方法区的内存空间。除了类的信息外，方法区中还会存放运行时常量池信息，可能还包括字符串字面量和数字常量（这部分常量信息是Class文件中常量池部分的内存映射) 3.类加载器ClassLoader角色 class file 存在于本地硬盘上，可以理解为设计师画在纸上的模板，而最终这个模板在执行的时候是要加载到JVM当中来，根据这个文件实例化出n个一模一样的实例。 class file 加载到JVM中，被称为DNA元数据模板，放在方法区。 在.class文件-&gt; JVM -&gt;最终成为元数据模板，此过程就要一个运输工具(类装载器class Loader)，扮演一个快递员的角色。 获取ClassLoader途经 4.类加载过程 加载(Loading): 1．通过一个类的全限定名获取定义此类的二进制字节流 2．将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构 3．在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口 补充:加载.class文件的方式 ·从本地系统中直接加载 ·通过网络获取，典型场景: web Applet ·从zip压缩包中读取，成为日后jar、 war格式的基础·运行时计算生成，使用最多的是:动态代理技术 ·由其他文件生成，典型场景:JSP应用 ·从专有数据库中提取.class文件,比较少见 ·从加密文件中获取，典型的防class文件被反编译的保护措施 链接 验证(Verify): ·目的在于确保class文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机自身安全。 ·主要包括四种验证，文件格式验证，元数据验证，字节码验证，符号引用验证。 准备(Prepare): ·为类变量分配内存并且设置该类变量的默认初始值，即零值。 这里不包含用final修饰的static，因为final在编译的时候就会分配了，准备阶段会显式链初始化; ·这里不会为实例变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中。 解析(Resolve) : ·将常量池内的符号引用转换为直接引用的过程。 ·事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行。 ·符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《java虚拟机规范》的class文件格式中。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 ·解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的 CONSTANT Class info、CONSTANT Fieldref_info、CONSTANT _Methodref_info等.。 初始化: ·初始化阶段就是执行类构造器方法 ()的过程。 ·此方法不需定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。 ·构造器方法中指令按语句在源文件中出现的顺序执行。 . ()不同于类的构造器。(关联:构造器是虚拟机视角下的( ))·若该类具有父类，JVM会保证子类的()执行前，父类的 ()已经执行完毕。 ·虚拟机必须保证一个类的 ()方法在多线程下被同步加锁。 5.类加载器分类 JVM支持两种类型的类加载器，分别为引导类加载器（Bootstrap ClassLoader）和自定义类加载器(User-Defined ClassLoader)。 ·从概念上来讲，自定义类加载器一般指的是程序中由开发人员自定义的一类类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 ·无论类加载器的类型如何划分，在程序中我们最常见的类加载器始终只有3个，如下所示: 虚拟机自带的加载器 ·启动类加载器（引导类加载器，Bootstrap classLoader) 这个类加载使用**C/C++**语言实现的，嵌套在JVM内部。 它用来加载Java的核心库（JAVA HOME/jre/lib/rt.jar、resources.jar或sun.boot.class.path路径下的内容），用于提供JVM自身需要的类 并不继承自java.lang.classLoader，没有父加载器。加载扩展类和应用程序类加载器，并指定为他们的父类加载器。 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 ·扩展类加载器（Extension ClassLoader) Java语言编写，由sun.misc.Launcher$ExtClassLoader实现。派生于classLoader类 父类加载器为启动类加载器 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre/lib/ext子目录（扩展目录)下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载。 ·应用程序类加载器（系统类加载器，AppClassLoader) java语言编写，由sun.misc.Launcher$AppclassLoader实现&gt;派生于classLoader类 父类加载器为扩展类加载器 它负责加载环境变量classpath或系统属性java.class.path指定路径下的类库 该类加载是程序中默认的类加载器，一般来说，Java应用的类都是由它来完成加载 通过classLoader#getSystemClassLoader ()方法可以获取到该类加载器 用户自定义类加载器 ·在Java的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互 配合执行的，在必要时，我们还可以自定义类加载器，来定制类的加载方式。 .为什么要自定义类加载器? 隔离加载类 修改类加载的方式 扩展加载源 防止源码泄漏 用户自定义类加载器实现步骤: 1.开发人员可以通过继承抽象类java.lang.classLoader类的方式，实现自己的类加载器，以满足一些特殊的需求 2.在JDK1.2之前，在自定义类加载器时，总会去继承classLoader类并重写loadClass ()方法，从而实现自定义的类加载类，但是在JDK1.2之后已不再建议用户去覆盖loadclass ()方法，而是建议把自定义的类加载逻辑写在findclass ()方法中 3．在编写自定义类加载器时，如果没有太过于复杂的需求，可以直接继承URLClassLoader类，这样就可以避免自己去编写findclass ()方法及其获取字节码流的方式，使自定义类加载器编写更加简洁。 6.双亲委派机制 ​ Java虚拟机对class文件采用的是按需加载的方式，也就是说当需要使用该类时才会将它的class文件加载到内存生成class对象。而且加载某个类的class文件时，Java虚拟机采用的是双亲委派模式，即把请求交由父类处理,它是一种任务委派模式。 工作原理 1)如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行; 2)如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器; 3)如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式。 即向上委托，向下甩锅 双亲委派优势 避免类的重复加载 保护程序安全，防止核心API被随意篡改 自定义类:java. lang. string 自定义类: java . lang. shkStart java.lang. securityException: Prohibited package name: java.lang 沙箱安全机制 ​ 自定义string类，但是在加载自定义String类的时候会率先使用引导类加载器加载，而引导类加载器在加载的过程中会先加载jdk自带的文件(rt.jar包中java\\lang\\String.class)，报错信息说没有main方法，就是因为加载的是rt.jar包中的String类。这样可以保证对java核心源代码的保护，这就是沙箱安全机制。即先用java核心源码再往外层用 7.其他 ·在JVM中表示两个class对象是否为同一个类存在两个必要条件: 类的完整类名必须一致，包括包名。 加载这个类的classLoader(指classLoader实例对象)必须相同。 ·换句话说，在VM中，即使这两个类对象(class对象)来源同一个class文件，被同一个虚拟机所加载，但只要加载它们的ClassLoader实例对象不同，那么这两个类对象也是不相等的。 对类加载其的引用 JVM必须知道一个类型是由启动加载器加载的还是由用户类加载器加载的。如果一个类型是由用户类加载器加载的，那么JVM会将这个类加载器的一个引用作为类型信息的一部分保存在方法区中。当解析一个类型到另一个类型的引用的时候，JVM需要保证这两个类型的类加载器是相同的。 类的主动使用和被动使用 Java程序对类的使用方式分为:主动使用和被动使用。·主动使用，又分为七种情况: ·创建类的实例 ·访问某个类或接口的静态变量，或者对该静态变量赋值 ·调用类的静态方法 ·反射（比如: Class.forName ( &quot;com.atguigu . Test&quot;) )&gt; ·初始化一个类的子类 ·Java虚拟机启动时被标明为启动类的类 ·JDK 7开始提供的动态语言支持:java . lang.invoke.MethodHandle实例的解析结果REF getstatic、REF putstatic、REF_invokestatic句柄对应的类没有初始化，则初始化 除了以上七种情况，其他使用Java类的方式都被看作是对类的被动使用，都不会导致类的初始化。 三、运行时数据区概述及线程 1.运行时数据区内部结构 ​ 内存是非常重要的系统资源，是硬盘和CPU的中间仓库及桥梁，承载着操作系统和应用程序的实时运行。JVM内存布局规定了Java在运行过程中内存申请、分配、管理的策略，保证了JVM的高效稳定运行。不同的JVM对于内存的划分方式和管理机制存在着部分差异。结合JVM虚拟机规范，来探讨一下经典的JVM内存布局。 Error GC(Garbage Collection) OOM(OutOfMemoryError) StackOverflowError 程序计数器 ❌ ❌ ❌ 本地方法栈 ✔️ ❌ ✔️ ✔️ 虚拟机栈 ✔️ ❌ ✔️ ✔️ 堆 ✔️ ✔️ ✔️ 方法区 ✔️ ✔️ ✔️ ​ Java虚拟机定义了若干种程序运行期间会使用到的运行时数据区，其中有一些会随着虚拟机启动而创建，随着虚拟机退出而销毁。另外一些则是与线程一一对应的，这些与线程对应的数据区域会随着线程开始和结束而创建和销毁。灰色的为单独线程私有的，红色的为多个线程共享的。即:每个线程独立：包括程序计数器、栈、本地栈。每个线程共享：堆、方法区 2.线程 ​ 线程是一个程序里的运行单元。JVM允许一个应用有多个线程并行的执行。在Hotspot JVM里，每个线程都与操作系统的本地线程直接映射。 ​ 当一个Java线程准备好执行以后，此时一个操作系统的本地线程也同时创建。Java线程执行终止后，本地线程也会回收。操作系统负责所有线程的安排调度到任何一个可用的CPU上。一旦本地线程初始化成功，它就会调用Java线程中的run ()方法。 ​ 如果你使用jconsole或者是任何一个调试工具，都能看到在后台有许多线程在运行。这些后台线程不包括调用public static void main (string[])的main线程以及所有这个main线程自己创建的线程。这些主要的后台系统线程在Hotspot JVM里主要是以下几个: **虚拟机线程:**这种线程的操作是需要JVM达到安全点才会出现。这些操作必须在不同的线程中发生的原因是他们都需要JVM达到安全点，这样堆才不会变化。这种 线程的执行类型包括&quot;stop-the-world&quot;的垃圾收集，线程栈收集，线程挂起以及偏向锁撤销。 **周期任务线程:**这种线程是时间周期事件的体现(比如中断)，他们一般用于周期性操作的调度执行。 **GC线程:**这种线程对在JVM里不同种类的垃圾收集行为提供了支持。&gt;编译线程:这种线程在运行时会将字节码编译成到本地代码。 **信号调度线程:**这种线程接收信号并发送给JVM，在它内部通过调用适当的方法进行处理。 ·线程是一个程序里的运行单元。JVM允许一个应用有多个线程并行的执行。 .在Hotspot JVM里，每个线程都与操作系统的本地线程直接映射。 当一个Java线程准备好执行以后，此时一个操作系统的本地线程也同时创建。Java线程执行终止后，本地线程也会回收。 ·操作系统负责所有线程的安排调度到任何一个可用的CPU上。一旦本地线程初始化成功，它就会调用Java线程中的run ()方法。 ●守护线程、普通线程 四、程序计数器 1.Program Counter Register介绍 ​ JVM中的程序计数寄存器(Program counter Register)中， Register的命名源于CPU的寄存器，寄存器存储指令相关的现场信息。CPU只有把数据装载到寄存器才能够运行。这里，并非是广义上所指的物理寄存器，或许将其翻译为PC计数器（或指令计数器）会更加贴切(也称为程序钩子)，并且也不容易引起一些不必要的误会。JVM中的PC寄存器是对物理PC寄存器的一种抽象模拟。 作用: PC寄存器用来存储指向下一条指令的地址,也即将要执行的指令代码。由执行引擎读取下一条指令(指令代码在java栈中对应的指令)。 它是一块很小的内存空间，几乎可以忽略不记。也是运行速度最快的存储区域。 在JVM规范中，每个线程都有它自己的程序计数器，是线程私有的，生命周期与线程的生命周期保持一致。 任何时间一个线程都只有一个方法在执行，也就是所谓的当前方法。程序计数器会存储当前线程正在执行的Java方法的TVM指令地址;或者，如果是在执行native方法，则是未指定值（undefned) 。 它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令。 它是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 2.程序示例 1javap -v/verbose （class名） //反编译 3.两个问题 （1）使用PC寄存器存储字节码指令地址有什么用呢?为什么使用PC寄存器记录当前线程的执行地址呢？ 因为CPU需要不停的切换各个线程，这时候切换回来以后，就得知道接着从哪开始继续执行。 JVM的字节码解释器就需要通过改变PC寄存器的值来明确下一条应该执行什么样的字节码指令。 (2)PC寄存器为什么会被设定为线程私有? ​ 我们都知道所谓的多线程在一个特定的时间段内只会执行其中某一个线程的方法，CPU会不停地做任务切换，这样必然导致经常中断或恢复，如何保证分毫无差呢?为了能够准确地记录各个线程正在执行的当前字节码指令地址，最好的办法自然是为每一个线程都分配一个PC寄存器，这样一来各个线程之间便可以进行独立计算，从而不会出现相互千扰的情况。 ​ CPU时间片即CPU分配给各个程序的时间，每个线程被分配一个时间段，称作它的时间片。在宏观上:我们可以同时打开多个应用程序，每个程序并行不悖，同时运行。但在微观上:由于只有一个CPU，一次只能处理程序要求的一部分，如何处理公平，一种方法就是引入时间片，每个程序轮流执行。 五、虚拟机栈 1.栈主要特点 ​ 由于跨半台性的设计，Java的指令都是根据栈来设计的。不同平台CPU架构不同，所以不能设计为基于寄存器的。优点是跨平台，指令集小，编译器容易实现，缺点是性能下降，实现同样的功能需要更多的指令。**栈是运行时的单位，而堆是存储的单位。**即：栈解决程序的运行问题，即程序如何执行，或者说如何处理数据。堆解决的是数据存储的问题，即数据怎么放、放在哪儿。 ・Java虚拟机栈是什么？ Java虚拟机栈( Java Virtual Machine Stack),早期也叫Java栈。每个线程在创建时都会创建一个虚拟机栈，其内部保存一个个的栈帧( Stack Frame),对应着一次次的Java方法调用，是线程私有的。生命周期和线程一致。 ・作用 主管Java程序的运行，它保存方法的局部变量(8种基本数据类型、对象的引用地址)、部分结果，并参与方法的调用和返回。 局部变量vs成员变量（或属性） 基本数据变量vs引用类型变量(类、数组、接口) ・栈的特点（优点） 栈是一种快速有效的分配存储方式，访问速度仪次于程序计数器。 JVM直接对Java栈的操作只有两个： 每个方法执行，伴随着进栈(入栈、压栈) 执行结束后的出栈工作 对于栈来说不存在垃圾回收问题 2.栈中可能出现的异常 ​ Java虚拟机规范允许Java栈的大小是动态的或者是固定不变的。 ​ 如果采用固定大小的Java虚拟机栈，那每一个线程的Java虚拟机栈容量可以在线程创建的时候独立选定。如果线程请求分配的栈容量超过Java虚拟机栈允许的最大容量，Java虚拟机将会抛出一个StackOverflowError异常。 ​ 如果Java虚拟机栈可以动态扩展，并且在尝试扩展的时候无法中请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的虚拟机栈，那Java虚拟机将会抛出一个 OutOfMemoryError异常。 设置栈大小 Xss size Sets the thread stack size( in bytes). Append the letter k or K to indicate KB, m or M to indicate MB, and g or G to indicate GB. The default value depends on the platform Linux/x64(64-bit): 1024 KB macos(64-bit): 1024 KB Oracle Solaris/x64(64-bit): 1024 KB Windows: The default value depends on virtual memory The following examples set the thread stack size to 1024 KB in different units -Xss1m -Xss1024k Xss1048576 3.栈存储单位 栈中存储什么 ・每个线程都有自己的栈，栈中的数据都是以栈帧( Stack Frame)的格式存在。 ・在这个线程上正在执行的栈帧( Stack Frame) 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中的各种数据信息 复习 ・OOP的基本概念：类、对象 ・类中基本结构：field(属性、字段、域)、 method JVM直接对Java栈的操作只有两个，就是对栈帧的栈和出栈，遵循“先进后出”/“后进先出”原则。 在一条活动线程中，一个时间点上，只会有一个活动的栈帧。即只有当前正在执行的方法的栈帧（栈顶栈帧）是有效的，这个栈帧被称为当前栈帧( Current Frame),与当前栈帧相对应的方法就是当前方法（ Current Method),定义这个方法的类就是当前类( Current Class). 执行引擎运行的所有字节码指令只针对当前栈帧进行操作。 如果在该方法中调用了其他方法，对应的新的栈帧会被创建出来，放在栈的顶端，成为新的当前帧。 ・不同线程中所包含的栈帧是不允许存在相互引用的，即不可能在一个栈帧之中引用另外一个线程的栈帧 ・如果当前方法调用了其他方法，方法返回之际，当前栈帧会传回此方法的执行结果给前一个栈帧，接着，虚拟机会丢弃当前栈帧，使得前一个栈帧重新成为当前栈帧。 ・Java方法有两种返回函数的方式，一种是正常的函数返回，使用 return指令；另外一种是抛出异常。不管使用哪种方式，都会导致栈帧被弹出。 4.栈帧结构 ・局部变量表(Local variables) ・操作数栈( Operand Stack)（或表达式栈） ・动态链接( Dynamic Linking)（或指向运行时常量池的方法引用） ・方法返回地址( Return Address)（或方法正常退出或者异常退出的定义） ・一些附加信息 1.局部变量表 ・局部变量表也被称之为局部变量数组或本地变量表 ・定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量,这些数据类型包括各类基本数据类型、对象引用( reference),以及returnAddress类型。 ・由于局部变量表是建立在线程的栈上，是线程的私有数据，因此不存在数据安全问题 ・局部变量表所需的容量大小是在编译期确定下来的，并保存在方法的code属性的 maximuim local variables数据项中。在方法运行期间是不会改变局部变量表的大小的。 ・方法嵌套调用的次数由栈的大小决定。一般来说，栈越大，方法嵌套调用次数越多。对一个函数而言，它的参数和局部变量越多，使得局部变量表膨胀，它的栈帧就越大，以满足方法调用所需传递的信息増大的需求。进而函数调用就会占用更多的栈空间，导致其嵌套调用次数就会减少。 ・局部变量表中的变量只在当前方法调用中有效。在方法执行时，虚拟机通过使用局部变量表完成参数值到参数变量列表的传递过程。当方法调用结束后，随着方法栈帧的销毁，局部变量表也会随之销毁。 2.字节码中方法内部结构解析 3.Slot ・参数值的存放总是在局部变量数组的 index0开始，到数组长度-1的索引结束 ・局部变量表，最基本的存储单元是Slot（变量槽） ・局部变量表中存放编译期可知的各种基本数据类型(8种)，引用类型( reference), returnAddressa类型。 ・在局部变量表里，32位以内的类型只占用一个slot(包括returnAddressa类型），64位的类型(long和 double)占用两个slot. byte、 short、char在存储前被转换为int, boolean也被转换为int,0表示 false,非0表示true。 long和double则占据两个Slot。 ・JVM会为局部变量表中的每一个Slot都分配一个访问素引，通过这个素引即可成功访问到局部变量表中指定的局部变量值. ・当一个实例方法被调用的时候，它的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个Slot上. ・如果需要访问局部变量表中ー个64bit的局部变量值时，只需要使用前一个素引即可。(比如：访问1ong或doub1e类型变量). ・如果当前帧是由构造方法或者实例方法创建的，那么该对象引用this将会存放在 index为0的slot处，其余的参数按照参数表顺序继续排列. ​ 栈帧中的局部变量表中的槽位是可以重用的，如果一个局部变量过了其作用域，那么在其作用域之后申明的新的局部变量就很有可能会复用过期局部变量的槽位，从而达到节省资源的目的。 7.静态变量和局部变量对比 $\\triangleright$ 参数表分配完毕之后，再根据方法体内定义的变量的顺序和作用域分配 $\\triangleright$ 我们知道类变量表有两次初始化的机会，第一次是在“准备阶段”，执行系统初始化，对类变量设置零值，另一次则是在“初始化”阶段，赋予程序员在代码中定义的初始值. $\\triangleright$ 和类变量初始化不同的是，局部变量表不存在系统初始化的过程，这意味着一旦定义了局部变量则必须人为的初始化，否则无法使用。 12345//这样的代码是错误的，没有赋值不能够使用public void test() {int i;System.out .println(i);} 变量的分类： $\\triangleright$ 按照数据类型分：基本数据类型、引用数据类型 $\\triangleright$ 按照在类中声明的位置分：成员变量：在使用前，都经历过默认初始化赋值 类变量： Linking的 prepare阶段：给类变量默认赋值–&gt;initial阶段：给静态代码块赋值 实例变量：随着对象的创建，会在堆空间中分配实例变量空间，并进行默认赋值 $\\triangleright$ 局部变量：在使用前，必须要进行显式赋值的！否则，编译不通过。 补充说明 $\\triangleright$ 在栈帧中 ， 与性能调优关系最为密切的部分就是前面提到的局部变量表 。在方法执行时 ， 虚拟机使用局部变量表完成方法的传递 。 $\\triangleright$ 局部变量表中的变量也是重要的垃圾回收根节点 ， 只要被局部变量表中直接或间接引用的对象都不会被回收 。 8.操作数栈 $\\triangleright$ 每一个独立的栈帧中除了包含局部变量表以外，还包含一个后进先出(Last-In-First-0ut)的操作数栈，也可以称之为表达式栈(Expression Stack). $\\triangleright$ 操作数栈，在方法执行过程中，根据字节码指令，往栈中写入数据或提取数据，即入栈(push)/出栈(pop). 某些字节码指令将值压入操作数栈，其余的字节码指令将操作数取出栈。使用它们后再把结果压入栈。比如：执行复制、交换、求和等操作 $\\triangleright$ 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新PC寄存器中下一条需要执行的字节码指令。 $\\triangleright$ 操作数栈中元素的数据类型必须与字节码指令的序列严格匹配，这由编译器在编译器期间进行验证，同时在类加载过程中的类检验阶段的数据流分析阶段要再次验证。 $\\triangleright$ 另外，我们说Java虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈。 $\\triangleright$ 操作数栈，主要用于保存计算过程的中间结果，同时作为计算过程中变量临时的存储空间。 $\\triangleright$ 操作数栈就是JVM执行引擎的-一个工作区，当一个方法刚开始执行的时候，一个新的栈帧也会随之被创建出来，这个方法的操作数栈是空的。 $\\triangleright$ 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译期就定义好了，保存在方法的Code属性中，为max stack的值。 $\\triangleright$ 栈中的任何一个元素都是可以任意的Java数据类型。 ➢32bit的类型占用一个栈单位深度 ➢64bit的类型占用两个栈单位深度 $\\triangleright$ 操作数栈并非采用访问索引的方式来进行数据访问的，而是只能通过标准的入栈(push) 和出栈(pop)操作来完成一次数据访问。 代码示例： 12345public void testAddOperation(){byte i=15;int j=8;int k=i + j;} 图例： 9.栈顶缓存（Top-of-Stack-Cashing）技术 $\\star$ 前面提过，基于栈式架构的虛拟机所使用的零地址指令更加紧凑，但完成一项操作的时候必然需要使用更多的入栈和出栈指令，这同时也就意味着将需要更多的指令分派( instruction dispatch)次数和内存读/写次数。 $\\star$ 由于操作数是存储在内存中的，因此频繁地执行内存读/写操作必然会影响执行速度。为了解决这个问题， Hotspot JVM的设计者们提出了栈顶缓存(ToS,Top-of- Stack Cashing)技术，将栈顶元素全部缓存在物理CPU的寄存器中，以此降低对内存的读/写次数，提升执行引擎的执行效率。 10.动态链接 $\\star$ 每一个栈帧内部都包含一个指向运行时常量池中该栈帧所属方法的引用,包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接( Dynamic Linking)。比如：invoke dynamic 指令。 $\\star$ 在Java源文件被编译到字节码文件中时，所有的变量和方法引用都作为符号引用( Symbolic Reference)保存在class文件的常量池里。比如：描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 11.方法的调用 在JVM中，将符号引用转换为调用方法的直接引用与方法的绑定机制相关。 静态链接： $\\star$ 当一个字节码文件被装载进JVM内部时，如果被调用的目标方法在编译期可知，且运行期保持不变时。这种情况下将调用方法的符号引用转换为直接引用的过程称之为静态链接 动态链接： $\\star$ 如果被调用的方法在编译期无法被确定下来，也就是说，只能够在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此也就被称之为动态链接。 对应的方法的绑定机制为：早期绑定(Early Binding)和晚期绑定( Late Binding)。绑定是一个字段、方法或者类在符号引用被替换为直接引用的过程，这仅仅发生一次。 早期绑定： $\\star$ 早期绑定就是指被调用的目标方法如果在编译期可知，且运行期保持不变时，即可将这个方法与所属的类型进行绑定，这样一来，由于明确了被调用的目标方法究竟是哪一个，因此也就可以使用静态链接的方式将符号引用转换为直接引用。 晚期绑定： $\\star$ 如果被调用的方法在编译期无法被确定下来，只能够在程序运行期根据实际的类型绑定相关的方法，这种绑定方式也就被称之为晚期绑定。 12.虚方法、非虚方法 非虚方法： 如果方法在编译期就确定了具体的调用版本，这个版本在运行时是不可变的。这样的方法称为非虚方法。静态方法、私有方法、final方法、实例构造器、父类方法都是非虚方法，其他方法称为虚方法。 ​ 随着高级语言的横空出世，类似于Javaー样的基于面向对象的编程语言如今越来越多，尽管这类编程语言在语法风格上存在一定的差别，但是它们彼此之间始终保持着一个共性，那就是都支持封装、继承和多态等面向对象特性，既然这一类的编程语言具备多态特性，那么自然也就具备早期绑定和晚期绑定两种绑定方式。 ​ Java中任何一个普通的方法其实都具备虚函数的特征，它们相当于C语言中的虚函数(C中则需要使用关键字 virtual来显式定义)。如果在Java程序中不希望某个方法拥有虚函数的特征时，则可以使用关键字final来标记这个方法。 虚拟机中提供了以下几条方法调用指令 普通调用指令 invokestatic:调用静态方法，解析阶段确定唯一方法版本 invokespecial:调用****方法、私有及父类方法，解析阶段确定唯一方法版本 involkevirtual:调用所有虚方法 invokeinterface:调用接口方法 动态调用指令 invokedynamic:动态解析出需要调用的方法，然后执行前四条指令固化在虚拟机内部，方法的调用执行不可人为干预，而 invokedynamic指令则支持由用户确定方法版本。其中 invokestatic 指令和 invokespecial 指令调用的方法称为非虚方法，其余的( final 修饰的除外)称为虚方法。 13.Invokedynamic $\\star$ JVM字节码指令集一直比较稳定，一直到Java7中オ増加了一个 invokedynamic 指令，这是 Java 为了实现「动态类型语言」支持而做的种改进。 $\\star$ 但是在 Java7 中并没有提供直接生成 invokedynamic 指令的方法，需要借助 ASM 这种底层字节码工具来产生 invokedynamic 指令。直到 Java8 的 Lambda 表达式的出现，nvokedynamic 指令的生成，在 Java 中才有了直接的生成方式。 $\\star$ Java7 中增加的动态语言类型支持的本质是对 Java 虚拟机规范的修改，而不是对 Java 语言规则的修改，这一块相对来讲比较复杂，增加了虚拟机中的方法调用，最直接的受益者就是运行在 Java 平台的动态语言的编译器。 动态类型语言和静态类型语言 动态类型语言和静态类型语言两者的区别就在于对类型的检查是在编译期还是在运行期，满足前者就是静态类型语言，反之是动态类型语言。静态类型语言是判断变量自身的类型信息；动态类型语言是判断变量值的类型信息，变量没有类型信息，变量值才有类型信息，这是动态语言（比如Python）的一个重要特征。 14.方法重写本质 Java语言中方法重写的本质 1.找到操作数栈顶的第一个元素所执行的对象的实际类型，记作C 2.如果在类型C中找到与常量中的描述符合简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用，査找过程结束；如果不通过，则返回java.lang.Illegal AccessError异常。 3.否则，按照继承关系从下往上依次对C的各个父类进行第2步的搜索和验证过程。 4.如果始终没有找到合适的方法，则抛出java.lang. AbstractMethodError异常。 Illegal Accessarror介绍： 程序试图访问或修改一个属性或调用一个方法，这个属性或方法，你没有权限访问。一般的，这个会引起编译器异常。这个错误如果发生在运行时，就说明一个类发生了不兼容的改变。 ・在面向对象的编程中，会很频繁的使用到动态分派，如果在每次动态分派的过程中都要重新在类的方法元数据中搜索合适的目标的话就可能影响到执行效率。因此，为了提高性能，JVM采用在类的方法区建立一个虚方法表( virtual method tabble)（非虚方法不会出现在表中）来实现。使用索引表来代替查找。 ・每个类中都有一个虚方法表，表中存放着各个方法的实际入口 ・那么虚方法表什么时候被创建？ 虚方法表会在类加载的链接阶段被创建并开始初始化，类的变量初始值准备完成之后，JVM会把该类的方法表也初始化完毕。 15.方法返回地址 存放调用该方法的PC寄存器的值。 **一个方法的结束，有两种方式：**1.正常执行完成。2.出现未处理的异常，非正常退出 ​ 无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置。方法正常退出时，调用者的PC计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址。而通过异常退出的，返回地址是要通过异常表来确定，栈帧中一般不会保存这部分信息。 ​ 本质上，方法的退出就是当前栈帧出栈的过程。此时，需要恢复上层方法的局部变量表、操作数栈、将返回值压入调用者栈帧的操作数栈、设置PC寄存器值等，让调用者方法继续执行下去。 ​ 正常完成出口和异常完成出口的区别在于：通过异常完成出口退出的不会给他的上层调用者产生任何的返回值。 当一个方法开始执行后，只有两种方式可以退出这个方法 1、执行引擎遇到任意一个方法返回的字节码指令( return),会有返回值传递给上层的方法调用者，简称正常完成出口 ​ 一个方法在正常调用完成之后究竟需要使用哪一个返回指令还需要根据方法返回值的实际数据类型而定。 ​ 在字节码指令中，返回指令包含 lreturn(当返回值是 boolean、byte、char、shorth和int类型时使用)、lreturn、 freturn、 dreturn以及 areturn,另外还有一个 return指令供声明为void的方法、实例初始化方法、类和接口的初始化方法使用。 2、在方法执行的过程中遇到了异常( Exception),并且这个异常没有在方法内进行处理，也就是只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出。简称异常完成出口。 方法执行过程中抛出异常时的异常处理，存储在一个异常处理表，方便在发生异常的时候找到处理异常的代码。 16.一些附加信息(略) 17.面试题 举例栈溢出的情况？( StackOverflowError):2.栈中可能出现的异常 通过-Xss设置栈的大小；OOM 调整栈大小，就能保证不出现溢出吗？不能 分配的栈内存越大越好吗？不是 垃圾回收是否会涉及到虚拟机栈？不会的！ 方法中定义的局部变量是否线程安全？1.局部变量表 六、本地方法接口 什么是本地方法？ 简单地讲，一个 Native Method 就是一个Java调用非Java代码的接口。一个Native Method是这样一个Java方法：该方法的实现由非Java语言实现，比如C.这个特征并非Java所特有，很多其它的编程语言都有这一机制，比如在 C 中，你可以用 extern&quot;c&quot;告知C 编译器去调用一个C的函数。 A native method is a Java method whose Implementation is provided by non-java code 在定义ー个 native method 时，并不提供实现体(有些像定义ー个Javai nterface),因为其实现体是由非java语言在外面实现的。 本地接口的作用是融合不同的编程语言为Java所用，它的初衷是融合C/C++程序。 为什么要使用 Native Method? Java使用起来非常方便，然而有些层次的任务用Java实现起来不容易，或者我们对程序的效率很在意时，问题就来了。 ・与Java环境外交互：**有时Java应用需要与Java外面的环境交互，这是本地方法存在的主要原因。**你可以想想Java需要与一些底层系统，如操作系统或某些硬件交换信息时的情况。本地方法正是这样一种交流机制：它为我们提供了一个非常简洁的接口而且我们无需去了解Java应用之外的繁琐的细节。 与操作系统交互 JVM支持着Java语言本身和运行时库，它是Java程序赖以生存的平台，它由一个解释器（解释字节码）和一些连接到本地代码的库组成。然而不管怎样，它毕竟不是一个完整的系统，它经常依赖于一些底层系统的支持。这些底层系统常常是强大的操作系统。通过使用本地方法，我们得以用Java实现了jre的与底层系统的交互，甚至JVM的一些部分就是用C写的。还有，如果我们要使用一些Java语言本身没有提供封装的操作系统的特性时，我们也需要使用本地方法。 Sun’s Java **Sun的解释器是用C实现的，这使得它能像一些普通的C一样与外部交互。**jre大部分是用Java实现的，它也通过一些本地方法与外界交互。例如：类java.lang. hread的 setpriority（）方法是用Java实现的，但是它实现调用的是该类里的本地方法setprlority0（）。这个本地方法是用C实现的，并被植入JVM内部，在Windows 95的平台上，这个本地方法最终将调用Win32 Setpriority（）API。这是一个本地方法的具体实现由JVM直接提供，更多的情况是本地方法由外部的动态链接库( external dynamic link library)提供，然后被JVM调用。 现状 目前该方法使用的越来越少了，除非是与硬件有关的应用，比如通过Java程序驱动打印机或者Java系统管理生产设备，在企业级应用中已经比较少见。因为现在的异构领域间的通信很发达，比如可以使用 Socket通信，也可以使用网 Web Service等等，不多做介绍。 七、本地方法栈 本地方法栈( Native Method Stack) ・Java虚拟机栈用于管理Java方法的调用，而本地方法栈用于管理本地方法的调用。 ・本地方法栈，也是线程私有的。 ・允许被实现成固定或者是可动态扩展的内存大小。（在内存溢出方面是相同的） ​ 如果线程请求分配的栈容量超过本地方法栈允许的最大容量，Java虚拟机将会抛出一个 StackOverflowError异常。 ​ 如果本地方法栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的本地方法栈，那么Java虚拟机将会抛出一个 OutOfMemoryError异常。 ・本地方法栈是使用C语言实现的 ・它的具体做法是 Native Method Stack中登记 native 方法，在 Execution Engine 执行时加载本地方法库。 ・当某个线程调用一个本地方法时，它就进入了一个全新的并且不再受虚拟机限制的世界。它和虚拟机拥有同样的权限。 本地方法栈可以通过本地方法接口来访问虚拟机内部的运行时数据区。 它甚至可以直接使用本地处理器中的寄存器 直接从本地内存的堆中分配任意数量的内存。 并不是所有的JVM都支持本地方法。因为Java虚拟机规范并没有明确要求本地方法栈的使用语言、具体实现方式、数据结构等。如果JVM产品不打算支持 native方法，也可以无需实现本地方法栈。 ・在 Hotspot JVM中，直接将本地方法栈和虚拟机栈合二为一。 八、堆 1.堆的核心概述 ・一个JVM实例只存在一个堆内存，堆也是Java内存管理的核心区域。 ・Java堆区在JVM启动的时候即被创建，其空间大小也就确定了。是JVM管理的最大一块内存空间。堆内存的大小是可以调节的。《Java虚拟机规范》规定，堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的。 ・所有的线程共享Java堆，在这里还可以划分线程私有的缓冲区（ Thread Local Allocation Buffer, TLAB) ・《Java虚拟机规范》中对Java堆的描述是：所有的对象实例以及数组都应当在运行时分配在堆上。（ The heap is the run-time data area fromwhich memcry for all class instances and arrays is allocated)我要说的是：“几乎”所有的对象实例都在这里分配内存。一从实际使用角度看的。 ・数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。 ・在方法结束后，堆中的对象不会马上被移除，仅仅在垃圾收集的时候才会被移除。 ・堆，是GC( Garbage Collection,垃圾收集器)执行垃圾回收的重点区域。 2.堆的细分内存结构 3.设置堆内存大小与OOM 1.设置堆内存 Java堆区用于存储Java对象实例，那么堆的大小在JVM启动时就已经设定好了，大家可以通过选项&quot;-Xmx&quot;和&quot;-Xms&quot;来进行设置 ​ “-Xms&quot;用于表示堆区的起始内存，等价于ーXX: Initialheapsize ​ “-Xmx&quot;则用于表示堆区的最大内存，等价于ーXX: Maxheapsize 一旦堆区中的内存大小超过“-Xmx&quot;所指定的最大内存时，将会抛出 OutOfMemoryError 异常。 ・通常会将 -Xms 和 -Xmx 两个参数配置相同的值，其目的是为了能够在java垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小，从而提高性能。默认情况下：初始内存大小：物理电脑内存大小/64，最大内存大小：物理电脑内存大小/4 4.查看设置的参数：方式一：jps/jstat -GC 进程id 方式二：-XX: +PrintGCdetails 2.OOM说明与举例 工具： jvisualvm：查看内存情况，jvisualvm命令可直接打开 安装插件 1234567891011121314151617181920212223public class OOMInstance { public static void main(String[] args){ ArrayList&lt;Picture1&gt; list=new ArrayList&lt;&gt;(); while(true) { try { Thread.sleep(20); } catch (InterruptedException e) { e.printStackTrace(); } list.add(new Picture1(new Random().nextInt(1024*1024))); } }}class Picture1{ private byte[] pixels; public Picture1(int length){ this.pixels = new byte[length]; }} 编译程序 修改堆大小 运行程序查看 jvisualvm Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at pers.lxl.mylearnproject.programbase.jvm.Picture1.(OOMInstance.java:27) at pers.lxl.mylearnproject.programbase.jvm.OOMInstance.main(OOMInstance.java:19) 4.年轻代与老年代 ・存储在JVM中的Java对象可以被划分为两类 一类是生命周期较短的瞬时对象，这类对象的创建和消亡都非常迅速 另外一类对象的生命周期却非常长，在某些极端的情况下还能够与JVM的生命周期保持一致。 ・Java堆区进一步细分的话，可以划分为年轻代( YoungGen)和老年代( OldGen) ・其中年轻代又可以划分为Eden空间、 Survivor0空间和 Survivor1空间(有时也叫做from区、to区)。 参数设置： 配置新生代与老年代在堆结构的占比。 默认**-XX: NewRatio=2**,表示新生代占1,老年代占2,新生代占整个堆的1/3 可以修改-XX: Newratio=4,表示新生代占1,老年代占4,新生代占整个堆的1/5 在 Hot Spot中，Eden究间和另外两个 Survivor'空间缺省所占的比例是8:1:1当然开发人员可以通过选项“-Xx: SurvivorRatio”调整这个空间比例。比如XX: Survivorratio=8,默认8 几乎所有的Java对象都是在Eden区被new出来的绝大部分的Java对象的销毁都在新生代进行了。IBM公司的专门研究表明，新生代中80%的对象都是“朝生夕死”的。 可以使用选项&quot;-Xmn'&quot;设置新生代最大内存大小&gt;这个参数一般使用默认值就可以了。 对象流程： 5.对象分配过程 1.一般过程 ​ Eden 满触发 YGC/Minor GC 对 Eden与 From 区回收，还在用的对象放到 From 区，Age设为1，下次回收触发时复制 From 区转到 To 区（空的区），Age+1，Age 达到阈值（可通过设置 -XX: MaxTenuringThreshold=）放入Old。 为新对象分配内存是一件非常严谨和复杂的任务，JVM的设计者们不仅需要考虑内存如何分配、在哪里分配等问题，并且由于内存分配算法与内存回收算法密切相关，所以还需要考虑GC执行完内存回收后是否会在内存空间中爬生内存碎片。 1.new的对象先放伊甸园区。此区有大小限制。 2.当伊甸园的空间填满时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收( Minor GC),将伊甸园区中的不再被其他对象所引用的对象进行销毁。再加载新的对象放到伊甸园区。 3.然后将伊甸园中的剩余对象移动到幸存者0区 4.如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，如果没有回收，就会放到幸存者1区。 5.如果再次经历垃圾回收，此时会重新放回幸存者0区，接着再去幸存者1区 6.啥时候能去养老区呢？可以设置次数。默认是15次。可以设置参数：-XX: MaxTenuringThreshold=进行设置。 总结 针对幸存者s0,s1区的总结：复制之后有交换，谁空谁是To 关于垃圾回收：频繁在新生区收集，很少在养老区收集，几乎不在永久区/元空间收集。 2.特殊情况 6.常用调优工具 ・JDK命令行 Ecllpse: Memory Analyzer Tool Jconsole Visualvm Jprofiler Java Flight Recorder GCVlewer GC Easy 7.Minor GC、 Maior GC、 Full GC JVM在进行GC时，并非每次都对上面三个内存(新生代、老年代、方法区)区域一起回收的，大部分时候回收的都是指新生代。 针对 HotSpot VM的实现，它里面的GC按照回收区域又分为两大种类型：一种是部分收集( Partial GC),一种是整堆收集(Full GC) 部分收集：不是完整收集整个JAVA堆的垃圾收集。其中又分为： 新生代收集( Minor GC / Young GC):只是新生代的垃圾收集 老年代收集( Major GC / Old GC):只是老年代的垃圾收集 目前，只有 CMS GC会有单独收集老年代的行为。 注意，很多时候 Major GC 会和 Full GC混淆使用，需要具体分辨是老年代回收还是整堆回收。 混合收集( Mixed GC):收集整个新生代以及部分老年代的垃圾收集。 目前，只有G1 GC会有这种行为 整堆收集(Full GC):收集整个java堆和方法区的垃圾收集。 1.年轻代GC( Minor GC)触发机制： ​ 当年轻代空间不足时，就会触发 Minor GC,这里的年轻代满指的是 Eden代满， Survivor满不会引发GC.(每次 Minor GC会清理年轻代的内存。） ​ 因为Java对象大多都具备朝生夕灭的特性，所以 Minor GC非常频繁，一般回收速度也比较快。这一定义既清晰又易于理解。 ​ Minor GC会引发STW,暂停其它用户的线程，等垃圾回收结東，用户线程才恢复运行。 2.老年代GC( Major GC / Full GC) 触发机制 指发生在老年代的GC,对象从老年代消失时，我们说“ Major GC”或“Full GC”发生了。 出现了 Major GC,经常会伴随至少一次的 Minor GC(但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行 Major GCE的策略选择过程） 也就是在老年代空间不足时，会先尝试触发 Minor GC。如果之后空间还不足，则触发 Major GC。 Major GC的速度一般会比 Minor GC 慢10倍以上，STW 的时间更长。 如果 Major GC后，内存还不足，就报OOM了 Major GC 的速度一般会比 Minor GC慢10倍以上。 3.Full GC 触发机制：（后面细讲触发Full GC执行的情况有如下五种： (1)调用 System.GC（）时，系统建议执行Full GC,但是不必然执行 (2)老年代空间不足 (3)方法区空间不足 (4)通过 Minor GC后进入老年代的平均大小大于老年代的可用内存 (5)由Eden区、 survivor space0( From Space)区向 survivor space1(To Space)区复制时，对象大小大于 To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小 说明：Full GC是开发或调优中尽量要避免的。这样暂时时间会短一些。 8.GC举例与日志分析 添加参数：-XX:PrintGCDetails 查看日志： 9.堆空间分代思想 为什么需要把Java堆分代？不分代就不能正常工作了吗？ 其实不分代完全可以，分代的唯一理由就是优化GC性能。如果没有分代，那所有的对象都在一块，就如同把一个学校的人都关在一个教室。GC的时候要找到哪些对象没用，这样就会对堆的所有区域进行扫描。而很多对象都是朝生夕死的，如果分代的话，把新创建的对象放到某一地方，当GC的时候先把这块存储“朝生夕死”对象的区域进行回收，这样就会腾出很大的空间出来。 10.内存分配策略（对象提升【Promotion】规则） ​ 如果对象在 Eden 出生并经过第一次 Miinor GC 后仍然存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为1。对象 Survivor区中每熬过一次 MInorGC,年龄就增加1岁，当它的年龄增加到一定程度(默认为15岁，其实每个JVM、每个GC都有所不同)时，就会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过选项**-XX: MaxTenuringThreshold**来设置。 针对不同年龄段的对象分配原则如下所示： 优先分配到Eden。 大对象直接分配到老年代。 尽量避兔程序中出现过多的大对象。 长期存活的对象分配到老年代。 动态对象年龄判断 如果 Survivor区中相同年龄的所有对象大小的总和大于 Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代，无须等到 MaxTenuringThreshold中要求的年龄 空间分配担保： -XX: HandlePromotionFailure 11.对象分配过程：TLAB 1.为什么有TLAB( Thread Local Allocation Buffer)? 🎧 堆区是线程共享区域，任何线程都可以访问到堆区中的共享数据 🎧 由于对象实例的创建在JVM中非常频繁，因此在并发环境下从堆区中划分内存空间是线程不安全的 🎧 为避免多个线程操作同一地址，需要使用加锁等机制，进而影响分配速度。 2.什么是TLAB? 🐜 从内存模型而不是垃圾收集的角度，对Eden区域继续进行划分，JVM为每个线程分配了一个私有缓存区域，它包含在Eden空间内 🐜 多线程同时分配内存时，使用TLAB可以避免一系列的非线程安全问题，同时还能够提升内存分配的吞吐量，因此我们可以将这种内存分配方式称之为快速分配策略。 🐜 据我所知所有 OPENJDK行生出来的JVM都提供了TLAB的设计 🐜 尽管不是所有的对象实例都能够在TLAB中成功分配内存，但JVM确实是将TLAB作为内存分配的首选。 🐜 在程序中，开发人员可以通过选项“-XX: USETLAB”设置是否开启TLAB空间。 🐜 默认情况下，TLAB空间的内存非常小，仅占有整个Eden空间的1%,当然我们可以通过选项“-XX: TLABWasteTargetPercent”设置TLAB空间所占用Eden空间的百分比大小。 🐜 一旦对象在AB空间分配内存失败时，JVM就会尝试着通过使用加锁机制确保数据操作的原子性，从而直接在Eden空间中分配内存。 12.堆空间常见参数 -XX: +PrintFlagsInitial:查看所有的参数的默认初始值 -XX: +PrintFlagsFinal:查看所有的参数的最终值（可能会存在修改，不再是初始值) -Xms:初始雄空向内存(默认为物理内存的1/64) -Xmx:最大雄空间内存(默认为物理内存的1/4) -Xmn:设置新生代的大小。（初始值及最大值） -XX: NewRatio:配置新生代与老年代在堆结构的占比 -XX: SurvivorRatio:设置新生代中Eden和S0/S1空间比例 -XX: MaxTenuringThreshold:设置新生代垃扱的最大年龄 -XX: +PrintGCdetails:输出详细GC处理日志 打印筒要信息：🌞 -XX: +PrintGC 🌞 - verbose:GC -XX: HandlePromotionFailure:是否设置空问分配担保 是否设置空问分配担保？： 在发生 Minor GC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间。 如果大于，则此次 Minor GC是安全的 如果小于，则虚拟机会查看ーXX: HandlePromotionFailure设置值是否允许担保失败。 如果 HandlepromotionFailure=true,那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小。 如果大于，则尝试进行一次 Minor GC,但这次 Minor GC依然是有风险的。 如果小于，则改为进行一次Full GC。 如果 HandlePromotionFalure= false,则改为进行一次Full GC。 在JDK6 Update24之后， HandlePromotionFailure 参数不会再影响到虚拟机的空间分配担保策略，观察 OpenJDK中的源码变化，虽然源码中还定义了HandlePromotionFailure 参数，但是在代码中已经不会再使用它。JDK6 Update24之后的规则变为 只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小 就会进行 Minor GC,否则将进行Full GC。 13.通过逃逸分析看堆空间的对象分配 堆是分配对象存储的唯一选择吗？ ​ 在《深入理解Java虚拟机》中关于Java堆内存有这样一段描述：随着JIT编译期的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换优化技术将会导致一些微妙的变化，所有的对象都分配到堆上也渐渐变得不那么“绝对”了。在Java虚拟机中，对象是在Java堆中分配内存的，这是一个普遍的常识。但是，有种特殊情况，那就是如果经过逃逸分析( Escape Analysis)后发现，一个对象并没有逃逸出方法**【NEW的对象是否有可能在方法外被调用，调用则发生逃逸】**的话，那么就可能被优化成栈上分配。这样就无需在堆上分配内存，也无须进行垃圾回收了。这也是最常见的堆外存储技术。 ​ 此外，前面提到的基于 OPENJDK深度定制的 TaoBaoVM,其中创新的GCIH(GCinvisible heap)技术实现。off-heap,将生命周期较长的Java对象从heap中移至heap外，并且GC不能管理GCIH内部的Java对象，以此达到降低GC的回收频率和提升GC的回收效率的目的。 [堆不是分配对象存储的唯一选择吗](# 4.堆不是分配对象存储的唯一选择吗) 如何将堆上的对象分配到栈，需要使用逃逸分析手段。 这是一种可以有效减少Java程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。通过逃逸分析， Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。 逃逸分析的基本行为就是分析对象动态作用域： 当一个对象在方法中被定义后，对象只在方法内部使用，则认为没有发生逃逸。 当一个对象在方法中被定义后，它被外部方法所引用，则认为发生逃逸。例如作为调用参数传递到其他地方中。 参数设置： ・在JDK6u23版本之后， HotSpot中默认就已经开启了逃逸分析。如果使用的是较早的版本，开发人员则可以通过 选项“-XX: Doescapeanalysis&quot;显式开启逃逸分析 通过选项“-X: Printescapeanalysis&quot;查看逃逸分析的筛选结果 结论： 开发中能使用局部变量的，就不要使用在方法外定义。 14.逃逸分析：代码优化 ​ 使用逃逸分析，编译器可以对代码做如下优化： 一、栈上分配。将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。 二、同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 三、分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在CPU寄存器中。 1.栈上分配 实验： 1.-Xmx1G -Xms1G -XX:-DoEscapeAnalysis -XX:+PrintGCDetails 2.-Xmx1G -Xms1G -XX:+DoEscapeAnalysis -XX:+PrintGCDetails 3.-Xmx256m -Xms256m -XX:-DoEscapeAnalysis -XX:+PrintGCDetails 4.-Xmx256m -Xms256m -XX:+DoEscapeAnalysis -XX:+PrintGCDetails 1234567891011121314151617181920212223242526272829303132/** * 栈上分配测试 * -Xmx1G -Xms1G -XX:-DoEscapeAnalysis -XX:+PrintGCDetails * @author shkstart shkstart@126.com * @create 2020 10:31 */public class StackAllocation { public static void main(String[] args) { long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000000; i++) { alloc(); } // 查看执行时间 long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为： &quot; + (end - start) + &quot; ms&quot;); // 为了方便查看堆内存中对象个数，线程sleep try { Thread.sleep(1000000); } catch (InterruptedException e1) { e1.printStackTrace(); } } private static void alloc() { User user = new User();//未发生逃逸 } static class User { }} 2.同步省略（锁消除） ​ 线程同步的代价是相当高的，同步的后果是降低并发性和性能。 ​ 在动态编译同步块的时候，JIT编译器可以借助逃逸分析来判断同步块所使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。如果没有，那么JIT编译器在编译这个同步块的时候就会取消对这部分代码的同步。这样就能大大提高并发性和性能。这个取消同步的过程就叫同步省略，也叫锁消除。 3.分离对象或标量替换 ​ 标量(Scalar)是指一个无法再分解成更小的数据的数据。Java中的原始数据类型就是标量。相对的，那些还可以分解的数据叫做聚合量( Aggregate),Java中的对象就是聚合量，因为他可以分解成其他聚合暈和标量 ​ 在JIT阶段，如果经过逃逸分析，发现一个对象不会被外界访问的话，那么经过JIT优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。这个过程就是标量替换。 标量替换参数设置： 参数-XX:+EliminateAllocations:开启标量替換（默认打开），允许将对象打散分配在栈上。 4.堆不是分配对象存储的唯一选择吗？ 上述代码在主函数中进行了1亿次alloc。调用进行对象创建，由于User对象实例需要占据约16字节的空间，因此累计分配空间达到将近1.5GB。如果堆空间小于这个值，就必然会发生GC。使用如下参数运行上述代码 -server -Xmx100m -xms100m -xx:+DoEscapeAnalvsis -XX: Print GC -XX:+EliminateAllocations 这里使用参数如下 ・参数- server:启动 server模式，因为在 Server模式下，オ可以启用逃逸分析。 ・参数-XX: +DoEscapeAnalysis:启用逃逸分析 ・参数-Xmx10m:指定了堆空间最大为10MB ・参数-XX: Print GC:将打印GC日志。 ・参数-XX: EliminateAllocations:开启了标量替换（默认打开），允许将对象打散分配在栈上，比如对象拥有id和name两个字段，那么这两个字段将会被视为两个独立的局部变量进行分配。 ​ 关于逃逸分析的论文在1999年就已经发表了，但直到JDK1.6オ有实现，而且这项技术到如今也并不是十分成熟的。 ​ 其根本原因就是无法保证逃逸分析的性能消耗一定能高于他的消耗。虽然经过逃逸分析可以做标量替换、栈上分配、和锁消除。但是逃逸分析自身也是需要进行一系列复杂的分析的，这其实也是一个相对耗时的过程。 ​ 一个极端的例子，就是经过逃逸分析之后，发现没有一个对象是不逃逸的。那这个逃逸分析的过程就白白浪费掉了 ​ 虽然这项技术并不十分成熟，但是它也是即时编译器优化技术中一个十分重要的手段。注意到有一些观点，认为通过逃逸分析，JVM会在栈上分配那些不会逃逸的对象，这在理论上是可行的，但是取决于JVM设计者的选择。据我所知， Oracle Hotspot JVM中并未这么做，这一点在逃逸分析相关的文档里已经说明，所以可以明确所有的对象实例都是创建在堆上。 ​ 目前很多书籍还是基于JDK7以前的版本，JDK已经发生了很大变化， intern字符串的缓存和静态变量曾经都被分配在永久代上，而永久代已经被元数据区取代。但是，intern:字符串缓存和静态变量并不是被转移到元数据区，而是直接在堆上分配，所以这一点同样符合前面一点的结论：对象实例都是分配在堆上。 九、方法区 1.栈、堆、方法区间的交互关系 2.方法区基本理解 ​ 《Java虚拟机规范》中明确说明：&quot;尽管所有的方法区在逻辑上是属于堆的一部分，但些简单的实现可能不会选择去进行垃圾收集或者进行压缩。”但对于 HotSpotJVM而言，方法区还有一个别名叫做Non-Heap（非堆），目的就是要和堆分开。所以，方法区看作是一块独立于Java堆的内存空间。 ・方法区( Method Area)与JAVA堆一样，是各个线程共享的内存区域。 ・方法区在JVM启动的时侯被创建，并且它的实际的物理内存空间中和Java堆区一样都可以是不连续的。 ・方法区的大小，跟堆空间一样，可以选择固定大小或者可扩展。 ・方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类【比如加载大量的第三方的jar包； Tomcat部署的工程过多(30-50个)；大量动态的生成反射类】，导致方法区溢出，虚拟机同样会抛出内存溢出错误：java.lang. OutOfMemoryError: PermGen space 或者 java. lang.OutOfMemoryError: Metaspace ・关闭JVM就会释放这个区域的内存。 3.HotSpot中方法区的演进 在jdk7及以前，习惯上把方法区，称为永久代。jdk8开始，使用元空间取代了永久代。 In JDK8, classes metadata is now stored in the native heap and this space is called Metaspace 本质上，方法区和永久代并不等价。仅是对 HotSpot而言的。《Java虚拟机规范》对如何实现方法区，不做统一要求。例如： BEA JRockit / IBM J9中不存在永久代的概念。现在来看，当年使用永久代，不是好的idea。导致Java程序更容易OOM(超过XX: MaxPermSize上限) 而到了JDK8,终于完全废弃了永久代的概念，改用与 Jrockit、J9一样在本地内存中实现的元空间( Metaspace)来代替 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过**元空间与永久代最大的区别在于：元空间不在虚拟机设置的内存中，而是使用本地内存。**永久代、元空间二者并不只是名字变了，内部结构也调整了。根据《Java虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出OOM异常。 4.设置方法区大小 方法区的大小不必是固定的，JVM可以根据应用的需要动态调整。 jdk7及以前： 通过**-XX: PermSize**来设置永久代初始分配空间。默认值是20.75M -XX: MaxPermSize来设定永久代最大可分配空间。32位机器默认是64M,64位机器模式是82M 当JVM加载的类信息容量超过了这个值，会报异常 OutOfMemoryError: Permgen Space。 可通过命令行查看：jps————》jinfo -flag PermSize/MaxPermSize ’num‘ jdk8及以后： ​ 元数据区大小可以使用参数**-XX: MetaspaceSize和-XX: MaxMetaspacesSize**指定，替代上述原有的两个参数。 ​ 默认值依赖于平台。 windows下，-XX: MetaspaceSize是21M,-XX: MaxMetaspaceSize的值是-1,即没有限制。 ​ 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存。如果元数据区发生溢出，虚拟机一样会抛出异常OutOfMemoryError:Metaspace ​ -XX: MetaspaceSize:设置初始的元空间大小。对于ー个64位的服务器端JVM来说，其默认的-XX: MetaspaceSize值为21MB。这就是初始的高水位线，一旦触及这个水位线，Full GC将会被触发并卸载没用的类（即这些类对应的类加载器不再存活），然后这个高水位线将会重置。新的高水位线的值取决于GC后释放了多少元空间。如果释放的空间不足，那么在不超过 MaxMetaspaceSize时，适当提高该值。如果释放空间过多，则适当降低该值。 ​ 如果初始化的高水位线设置过低，上述高水位线调整情况会发生很多次。通过垃圾回收器的日志可以观察到FullGC多次调用。为了避免频繁地GC,建议将 -XX: MetaspaceSize设置为一个相对较高的值。 5.OOM举例 1234567891011121314151617181920212223242526272829303132333435import com.sun.xml.internal.ws.org.objectweb.asm.ClassWriter;import jdk.internal.org.objectweb.asm.Opcodes;/** * jdk6/7中： * -XX:PermSize=10m -XX:MaxPermSize=10m * * jdk8中： * -XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m * * @author shkstart shkstart@126.com * @create 2020 22:24 */public class OOMTest extends ClassLoader { public static void main(String[] args) { int j = 0; try { OOMTest test = new OOMTest(); for (int i = 0; i &lt; 10000; i++) { //创建ClassWriter对象，用于生成类的二进制字节码 ClassWriter classWriter = new ClassWriter(0); //指明版本号，修饰符，类名，包名，父类，接口 classWriter.visit(Opcodes.V1_6, Opcodes.ACC_PUBLIC, &quot;Class&quot; + i, null, &quot;java/lang/Object&quot;, null); //返回byte[] byte[] code = classWriter.toByteArray(); //类的加载 test.defineClass(&quot;Class&quot; + i, code, 0, code.length);//Class对象 j++; } } finally { System.out.println(j); } }} 如何解决这些OOM？ 1、要解决OOM异常或 heap space的异常，一般的手段是首先通过内存映像分析工具(如Eclipse Memory Analyzer)对dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，也就是要先分清楚到底是出现了内存泄漏（ Memory Leak)还是内存溢出( Memory Overflow) 2、如果是内存泄漏，可进一步通过工具査看泄漏对象到 GC Roots的引用链。于是就能找到泄漏对象是通过怎样的路径与 GC Roots相关联并导致垃圾收集器无法自动回收它们的。掌握了泄漏对象的类型信息，以及 GC Roots引用链的信息，就可以比较准确地定位出泄漏代码的位置。 3、如果不存在内存泄漏，换句话说就是内存中的对象确实都还必须存活着，那就应当检査虚拟机的堆参数(-Xmx与-Xms),与机器物理内存对比看是否还可以调大，从代码上检查是否存在某些对象生命周期过长、持有状态时间过长的情况，尝试减少程序运行期的内存消耗。 6.方法区内部结构 ​ 《深入理解Java虚拟机》书中对方法区( Method Area)存储内容描述如下它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等。 类型信息： 对每个加载的类型(类class、接口 interface、枚举enum、注解 annotation),JVM必须在方法区中存储以下类型信息： ①这个类型的完整有效名称(全名=包名.类名) ②)这个类型直接父类的完整有效名（对于 interface或是java.lang.Object,都没有父类) ③这个类型的修饰符（pulblic, abstract,fina的某个子集) ④这个类型直接接口的一个有序列表 域（Filed）信息 JVM必须在方法区中保存类型的所有域的相关信息以及域的声明顺序。 域的相关信息包括：域名称、域类型、域修饰符（ public，private，protected，static，fina， volatile，transient的某个子集） 方法（Method）信息 JTVM必须保存所有方法的以下信息，同域信息一样包括声明顺序 🔼 方法名称 🔼 方法的返回类型(或void)方法参数的数量和类型（按顺序 🔼 方法的修饰符（ public, private, protected, static, final,synchronized, native, abstract的一个子集) 🔼 方法的字节码( bytecodes)、操作数栈、局部变量表及大小( abstract和native方法除外) 🔼 异常表( abstract和 native方法除外)：每个异常处理的开始位置、结束位置、代码处理在程序计数器中的偏移地址被捕获的异常类的常量池索引 non- final的类变量 静态变量和类关联在一起，随着类的加载而加载，它们成为类数据在逻辑上的一部分。 类变量被类的所有实例共享，即使没有类实例时你也可以访问它。 全局常量： static final 被声明为 final的类变量的处理方法则不同，每个全局常量在编译的时候就会被分配了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276import java.io.Serializable;/** * 测试方法区的内部构成 * @author shkstart shkstart@126.com * @create 2020 23:39 */public class MethodInnerStrucTest extends Object implements Comparable&lt;String&gt;,Serializable { //属性 public int num = 10; private static String str = &quot;测试方法的内部结构&quot;; //构造器 //方法 public void test1(){ int count = 20; System.out.println(&quot;count = &quot; + count); } public static int test2(int cal){ int result = 0; try { int value = 30; result = value / cal; } catch (Exception e) { e.printStackTrace(); } return result; } @Override public int compareTo(String o) { return 0; }}//运行javap-v-p MethodInnerStrucTest.class &gt; test.txtClassfile /D:/workspace_idea5/JVMDemo/out/production/chapter09/com/atguigu/java/MethodInnerStrucTest.class Last modified 2020-4-22; size 1626 bytes MD5 checksum 69643a16925bb67a96f54050375c75d0 Compiled from &quot;MethodInnerStrucTest.java&quot; //类型信息public class com.atguigu.java.MethodInnerStrucTest extends java.lang.Object implements java.lang.Comparable&lt;java.lang.String&gt;, java.io.Serializable minor version: 0 major version: 51 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #18.#52 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #17.#53 // com/atguigu/java/MethodInnerStrucTest.num:I #3 = Fieldref #54.#55 // java/lang/System.out:Ljava/io/PrintStream; #4 = Class #56 // java/lang/StringBuilder #5 = Methodref #4.#52 // java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V #6 = String #57 // count = #7 = Methodref #4.#58 // java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #8 = Methodref #4.#59 // java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; #9 = Methodref #4.#60 // java/lang/StringBuilder.toString:()Ljava/lang/String; #10 = Methodref #61.#62 // java/io/PrintStream.println:(Ljava/lang/String;)V #11 = Class #63 // java/lang/Exception #12 = Methodref #11.#64 // java/lang/Exception.printStackTrace:()V #13 = Class #65 // java/lang/String #14 = Methodref #17.#66 // com/atguigu/java/MethodInnerStrucTest.compareTo:(Ljava/lang/String;)I #15 = String #67 // 测试方法的内部结构 #16 = Fieldref #17.#68 // com/atguigu/java/MethodInnerStrucTest.str:Ljava/lang/String; #17 = Class #69 // com/atguigu/java/MethodInnerStrucTest #18 = Class #70 // java/lang/Object #19 = Class #71 // java/lang/Comparable #20 = Class #72 // java/io/Serializable #21 = Utf8 num #22 = Utf8 I #23 = Utf8 str #24 = Utf8 Ljava/lang/String; #25 = Utf8 &lt;init&gt; #26 = Utf8 ()V #27 = Utf8 Code #28 = Utf8 LineNumberTable #29 = Utf8 LocalVariableTable #30 = Utf8 this #31 = Utf8 Lcom/atguigu/java/MethodInnerStrucTest; #32 = Utf8 test1 #33 = Utf8 count #34 = Utf8 test2 #35 = Utf8 (I)I #36 = Utf8 value #37 = Utf8 e #38 = Utf8 Ljava/lang/Exception; #39 = Utf8 cal #40 = Utf8 result #41 = Utf8 StackMapTable #42 = Class #63 // java/lang/Exception #43 = Utf8 compareTo #44 = Utf8 (Ljava/lang/String;)I #45 = Utf8 o #46 = Utf8 (Ljava/lang/Object;)I #47 = Utf8 &lt;clinit&gt; #48 = Utf8 Signature #49 = Utf8 Ljava/lang/Object;Ljava/lang/Comparable&lt;Ljava/lang/String;&gt;;Ljava/io/Serializable; #50 = Utf8 SourceFile #51 = Utf8 MethodInnerStrucTest.java #52 = NameAndType #25:#26 // &quot;&lt;init&gt;&quot;:()V #53 = NameAndType #21:#22 // num:I #54 = Class #73 // java/lang/System #55 = NameAndType #74:#75 // out:Ljava/io/PrintStream; #56 = Utf8 java/lang/StringBuilder #57 = Utf8 count = #58 = NameAndType #76:#77 // append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #59 = NameAndType #76:#78 // append:(I)Ljava/lang/StringBuilder; #60 = NameAndType #79:#80 // toString:()Ljava/lang/String; #61 = Class #81 // java/io/PrintStream #62 = NameAndType #82:#83 // println:(Ljava/lang/String;)V #63 = Utf8 java/lang/Exception #64 = NameAndType #84:#26 // printStackTrace:()V #65 = Utf8 java/lang/String #66 = NameAndType #43:#44 // compareTo:(Ljava/lang/String;)I #67 = Utf8 测试方法的内部结构 #68 = NameAndType #23:#24 // str:Ljava/lang/String; #69 = Utf8 com/atguigu/java/MethodInnerStrucTest #70 = Utf8 java/lang/Object #71 = Utf8 java/lang/Comparable #72 = Utf8 java/io/Serializable #73 = Utf8 java/lang/System #74 = Utf8 out #75 = Utf8 Ljava/io/PrintStream; #76 = Utf8 append #77 = Utf8 (Ljava/lang/String;)Ljava/lang/StringBuilder; #78 = Utf8 (I)Ljava/lang/StringBuilder; #79 = Utf8 toString #80 = Utf8 ()Ljava/lang/String; #81 = Utf8 java/io/PrintStream #82 = Utf8 println #83 = Utf8 (Ljava/lang/String;)V #84 = Utf8 printStackTrace{ //域信息 public int num; descriptor: I flags: ACC_PUBLIC private static java.lang.String str; descriptor: Ljava/lang/String; flags: ACC_PRIVATE, ACC_STATIC //方法信息 public com.atguigu.java.MethodInnerStrucTest(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: aload_0 5: bipush 10 7: putfield #2 // Field num:I 10: return LineNumberTable: line 10: 0 line 12: 4 LocalVariableTable: Start Length Slot Name Signature 0 11 0 this Lcom/atguigu/java/MethodInnerStrucTest; public void test1(); descriptor: ()V flags: ACC_PUBLIC Code: stack=3, locals=2, args_size=1 0: bipush 20 2: istore_1 3: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 6: new #4 // class java/lang/StringBuilder 9: dup 10: invokespecial #5 // Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 13: ldc #6 // String count = 15: invokevirtual #7 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 18: iload_1 19: invokevirtual #8 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 22: invokevirtual #9 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 25: invokevirtual #10 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 28: return LineNumberTable: line 17: 0 line 18: 3 line 19: 28 LocalVariableTable: Start Length Slot Name Signature 0 29 0 this Lcom/atguigu/java/MethodInnerStrucTest; 3 26 1 count I public static int test2(int); descriptor: (I)I flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: iconst_0 1: istore_1 2: bipush 30 4: istore_2 5: iload_2 6: iload_0 7: idiv 8: istore_1 9: goto 17 12: astore_2 13: aload_2 14: invokevirtual #12 // Method java/lang/Exception.printStackTrace:()V 17: iload_1 18: ireturn Exception table: from to target type 2 9 12 Class java/lang/Exception LineNumberTable: line 21: 0 line 23: 2 line 24: 5 line 27: 9 line 25: 12 line 26: 13 line 28: 17 LocalVariableTable: Start Length Slot Name Signature 5 4 2 value I 13 4 2 e Ljava/lang/Exception; 0 19 0 cal I 2 17 1 result I StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 12 locals = [ int, int ] stack = [ class java/lang/Exception ] frame_type = 4 /* same */ public int compareTo(java.lang.String); descriptor: (Ljava/lang/String;)I flags: ACC_PUBLIC Code: stack=1, locals=2, args_size=2 0: iconst_0 1: ireturn LineNumberTable: line 33: 0 LocalVariableTable: Start Length Slot Name Signature 0 2 0 this Lcom/atguigu/java/MethodInnerStrucTest; 0 2 1 o Ljava/lang/String; public int compareTo(java.lang.Object); descriptor: (Ljava/lang/Object;)I flags: ACC_PUBLIC, ACC_BRIDGE, ACC_SYNTHETIC Code: stack=2, locals=2, args_size=2 0: aload_0 1: aload_1 2: checkcast #13 // class java/lang/String 5: invokevirtual #14 // Method compareTo:(Ljava/lang/String;)I 8: ireturn LineNumberTable: line 10: 0 LocalVariableTable: Start Length Slot Name Signature 0 9 0 this Lcom/atguigu/java/MethodInnerStrucTest; static {}; descriptor: ()V flags: ACC_STATIC Code: stack=1, locals=0, args_size=0 0: ldc #15 // String 测试方法的内部结构 2: putstatic #16 // Field str:Ljava/lang/String; 5: return LineNumberTable: line 13: 0}Signature: #49 // Ljava/lang/Object;Ljava/lang/Comparable&lt;Ljava/lang/String;&gt;;Ljava/io/Serializable;SourceFile: &quot;MethodInnerStrucTest.java&quot; 运行时常量池VS常量池 常量池 方法区内部包含了运行时常量池 ・字节码文件，内部包含了常量池。 ・要弄清楚方法区，需要理解清楚ClassFile,因为加载类的信息都在方法区。 ・要弄清楚方法区的运行时常量池，需要理解清楚ClassFile中的常量池https://docs.oracle.com/javase/specs/ivms/se8/htm1/jvms-4.html如下： 一个有效的字节码文件中除了包含类的版本信息、字段、方法以及接口等描述信息外，还包含一项信息那就是常量池表( Constant Pool Table),包括各种字面量和对类型、域和方法的符号引用。 为什么需要常量池？ 一个java源文件中的类、接口，编译后产生一个字节码文件。而Java中的字节码需要数据支持，通常这种数据会很大以至于不能直接存到字节码里，换另一种方式，可以存到常量池，这个字节码包含了指向常量池的引用。在动态链接的时候会用到运行时常量池，之前有介绍。 小结 常量池，可以看做是一张表，虚拟机指令根据这张常量表找到要执行的类名、方法名、参数类型、字面量等类型。 运行时常量池 🥑 运行时常量池( Runtime Constant Pool)是方法区的一部分。 🥑 常量池表( Constant Pool Table)是Class文件的一部分，用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。 🥑 运行时常量池，在加载类和接口到虚拟机后，就会创建对应的运行时常量池。 🥑 JVM为每个已加载的类型（类或接口）都维护一个常量池。池中的数据项像数组项一样，是通过索引访问的。 🥑 运行时常量池中包含多种不同的常量，包括编译期就已经明确的数值字面量，也包括到运行期解析后才能够获得的方法或者字段引用。此时不再是常量池中的符号地址了，这里换为真实地址。 🥑 运行时常量池，相对于Class文件常量池的另一重要特征是：具备动态性。运行时常量池类似于传统编程语言中的符号表( symol table),但是它所包含的数据却比符号表要更加丰富一些。 🥑 当创建类或接口的运行时常量池时，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大值，则JVM会抛 OutOfMemoryError异常。 7.方法区的演进细节 1.首先明确：只有 HotSpotオ有永久代。 BEA JRockit、IBM J9等来说，是不存在永久代的概念的。原则上如何实现方法区属于虚拟机实现细节，不受《Java虚拟机规范》管束，并不要求统一。 2, HotSpot中方法区的变化： jak1.6及之前 有永久代( permanent generation),静态变量（变量名）存放在永久代上 jdk. 7 有永久代，但已经逐步“去永久代”，字符串常量池、静态变量移除，保存在堆中 jak1.8及之后 无永久代，类型信息、字段、方法、常量保存在本地内存的元空间，但字符串常量池、静态变量仍在堆 3.永久代为什么要被元空间替换？ ・随着Java8的到来， Hotspot Vy中再也见不到永久代了。但是这并不意味着类的元数据信息也消失了。这些数据被移到了一个与堆不相连的本地内存区域，这个区域叫做元空间( Metaspace). ・由于类的元数据分配在本地内存中，元空间的最大可分配空间就是系统可用内存空 ・这项改动是很有必要的，原因有： 1)为永久代设置空间大小是很难确定的 ​ 在某些场景下，如果动态加载类过多，容易产生Perm 区的OOM。比如某个实际Web工程中，因为功能点比较多，在运行过程中，要不断动态加载很多类，经常出现致命错误。比如Exception in thread ‘dubbo client x.x conner’ java. lang. Outommemoryemor PermGen space 而元空间和永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制。 2)对永久代进行调优是很困难的。 ・判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件： 该类所有的实例都已经被回收，也就是JAVA堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 ・JAVA虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。关于是否要对类型进行回收，HotSpot虚拟机提供了-Xnoclassc参数进行控制，还可以使用 -verbose:class 以及 -XX: TracedClass - Loading、-XX: +TraceClassUnLoading查看类加载和卸载信息。 ・在大量使用反射、动态代理、CGLib等字节码框架，动态生成JSP以及OSGi这类频繁自定义类加载器的场景中，通常都需要Java虚拟机具备类型卸载的能力，以保证不会对方法区造成过大的内存压力。 4、Stringtable为什么要调整？ ​ jdk7中将 string Table放到了堆空间中。因为永久代的回收效率很低，在full GC的时候才会触发。而full GC是老年代的空间不足、永久代不足时才会触发 这就导致 StringTable回收效率不高。而我们开发中会有大量的字符串被创建，回收效率低，导致永久代内存不足。放到堆里，能及时回收内存。 8.静态变量存在哪 工具： 12345678910111213141516public class StaticObjTest { static class Test { static ObjectHolder staticObj = new ObjectHolder(); ObjectHolder instanceObj = new ObjectHolder(); void foo { ObjectHolder localObj = new ObjectH01der(); System.out.println(&quot;done&quot;); private static class ObjectHolder { } public static void main (String[]args){ Test test = new StaticObjTest.Test(); test.foo(); } } staticOBJ随着Test的类型信息存放在方法区， instanceOBJ随着Test的对象实例存放在JAVA堆，localObject则是存放在foo()方法栈帧的局部变量表中。 1234hsdb&gt;scanoops 0x00007f32c7800000 0x00007f32c7b50000 JHSDB_TestCase$ObjectHolder0x00007f32c7a7c458 JHSDB_TestCase$ObjectHolder0x00007f32c7a7c480 JHSDB_TestCase$ObjectHolder0x00007f32c7a7c490 JHSDB_TestCase$ObjectHolder 测试发现：三个对象的数据在内存中的地址都落在Eden区范围内，所以结论：只要是对象实例必然会在Java堆中分配 接着，找到了一个引用该 staticon&gt;对象的地方，是在一个java.1ang.C1ass的实例里，并且给出了这个实例的地址，通过工 nspector查看该对象实例，可以清楚看到这确实是一个java.1ang.C1ass类型的对象实例，里面有一个名为 statical]的实例字段： ​ 从《Java虚拟机规范》所定义的概念模型来看，所有Class相关的信息都应该存放在方法区之中，但方法区该如何实现，《JAVA虚拟机规范》并未做出规定，这就成了一件允许不同虚拟机自己灵活把握的事情。JDK7及其以后版本的 HotSpot虚拟机选择把静态变量与类型在Java语言一端的映射class对象存放在一起，存储于Java堆之中，从我们的实验中也明确验证了这一点。 9.方法区垃圾回收 ​ 有些人认为方法区(如 HotSpot虚拟机中的元空间或者永久)是没有垃圾收集行为的，其实不然。《Java虚拟机规范》对方法区的约束是非常宽松的，提到过可以不要求虚拟机在方法区中实现垃圾收集。事实上也确实有未实现或未能完整实现方法区类型卸载的收集器存在(如JDK11时期的ZGC收集器就不支持类卸载)。 ​ 一般来说这个区域的回收效果比较难令人满意，尤其是类型的卸载，条件相当苛刻。但是这部分区域的回收有时又确实是必要的。以前Sun公司的Bug列表中，曾出现过的若干个严重的Bug就是由于低版本的 HotSpot虚拟机对此区域未完全回收而导致内存泄漏。 ​ 方法区的垃圾收集主要回收两部分内容：常量池中废弃的常量和不再使用的类型。 ​ 先来说说方法区内常量池之中主要存放的两大类常量：字面量和符号引用字面量比较接近JAVA语言层次的常量概念，如文本字符串、被声明为final的常量值等。而符号引用则属于编译原理方面的概念，包括下面三类常量： ​ 1、类和接口的全限定名 ​ 2、字段的名称和描述符 ​ 3、方法的名称和描述符 ​ HotSpot,虚拟机对常量池的回收策略是很明确的，只要常量池中的常量没有被任何地方引用，就可以被回收。 ​ 回收废弃常量与回收JAVA堆中的对象非常类似 ​ 判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件 ​ 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例。 ​ 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。 ​ 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 ​ Java虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。关于是否要对类型进行回收，HotSpot虚拟机提供了-Xnoclassc参数进行控制，还可以使用 -cverbose:class以及 -XX: +TracedClass- Loading、-XX: TracedClassUnLoading查看类加载和卸载信息。 ​ 在大量使用反射、动态代理、CGLib等字节码框架，动态生成JSP以及SGi这类频繁自定义类加载器的场景中，通常都需要Java虚拟机具备类型卸载的能力，以保证不会对方法区造成过大的内存压力。 十、对象的实例化内存 布局与访问定位 1.对象实例化的几种方式 2.创建对象的步骤 1.判断对象对应的类是否加载、链接、初始化 ​ 虚拟机遇到一条new指令，首先去检査这个指令的参数能否在 Metaspace的常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化。（即判断类元信息是否存在）。如果没有，那么在双亲委派模式下，使用当前类加载器以classloader 包名 类名为Key进行查找对应的，class文件。如果没有找到文件，则抛出Classnotfound Exception异常，如果找到，则进行类加载，并生成对应的class类对象 2.为对象分配内存 ​ 首先计算对象占用空间大小，接着在堆中划分一块内存给新对象。如果实例成员变量是引用变量，仅分配引用变量空间即可，即4个字节大小。 如果内存规整，使用指针碰撞 ​ 如果内存是规整的，那么虚拟机将采用的是指针碰撞法( Bump The Pointer)来为对象分配内存。意思是所有用过的内存在一边，空闲的内存在另外一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是把指针向空闲那边挪动一段与对象大小相等的距离罢了。如果垃圾收集器选择的是 Serial、 Pardew这种基于压缩算法的，虚拟机采用这种分配方式。般使用带有 compact（整理）过程的收集器时，使用指针碰撞。 如果内存不规整，虚拟机需要维护一个列表，使用空闲列表分配 ​ 如果内存不是规整的，已使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。意思是虚拟机维护了一个列表，记录上哪些内存块是可用的，再分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的内容。这种分配方式成为“空闲列表( Free List)” 说明：选择哪种分配方式由Java堆是否规整决定，而JAVA堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 3.处理并发安全问题 ​ 在分配内存空间时，另外一个问题是及时保证new对象时候的线程安全性：创建对象是非常频繁的操作，虚拟机需要解决并发问题。虚拟机采用了两种方式解决并发问题: ・CAS( Compare And Swap)失败重试、区域加锁：保证指针更新操作的原子性； ・TLAB把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在JAVA堆中预先分配一小块内存，称为本地线程分配缓冲区，（TLAB, Thread Local All ocation Buffer)虚拟机是否使用TLAB,可以通过-XX:+/- UseTLAB参数来设定。 4,初始化分配到的空间 ​ 内存分配结束，虚拟机将分配到的内存空间都初始化为零值（不包括对象头）。这一步保证了对象的实例字段在JAVA代码中可以不用赋初始值就可以直接使用，程序能访问到这些字段的数据类型所对应的零值。 5,设置对象的对象头 ​ 将对象的所属类（即类的元数据信息）、对象的 Hashcode,和对象的GC信息、锁信息等数 据存储在对象的对象头中。这个过程的具体设置方式取决于JVM实现。 6,执行init方法进行初始化 ​ 在Java程序的视角看来，初始化才正式开始。初始化成员变量，执行实例化代码块，调用类的构造方法，并把堆内对象的首地址赋值给引用变量。因此一般来说(由字节码中是否跟随有invokespecial指令所决定)，new指令之后会接着就是执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全创建出来。 3.对象的内存布局 4.对象访问定位 十一、直接内存 ・不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。 ・直接内存是在Java堆外的、直接向系统申请的内存区间。 来源于NIO,通过存在堆中的 Directbytebuffer操作 Native内存 ・通常，访问直接内存的速度会优于Java堆。即读写性能高。因此出于性能考虑，读写频繁的场合可能会考虑使用直接内存。 Java的NIO库允许Java程序使用直接内存，用于数据缓冲区 非直接缓存：读写文件，需要与磁盘交互，需要由用户态切换到内核态。在内核态时，需要内存如右图的操作。 使用ｴO,见右图。这里需要两份内存存储重复数据，效率低。 直接缓存：使用NIO时，如图。操作系统划出的直接缓存区可以被java代码直接访问，只有一份。NIO适合对大文件的读写操作。 直接内存也可能导致 Outofmemoryerror异常，由于直接内存在Java堆外，因此它的大小不会直接受限于-Xmx指定的最大堆大小，但是系统内存是有限的，Java堆和直接内存的总和依然受限于操作系统能给出的最大内存。 缺点： 分配回收成本较高 不受JVM内存回收管理 直接内存大小可以通过 MaxDlrectMemorySize如果不指定，默认与堆的最大值一Xmx参数值一致 十二、执行引擎 十三、StringTable 1.String的基本特性 String:字符串,使用一对&quot; &quot;引起来表示. ・ string s1=&quot; atgulqu&quot;;//字面量的定义方式 ・ string s2= new String(&quot;hello&quot;)； string声明为final的,不可被继承 ・ string实现了 Serializable接口:表示字符串是支持序列化的. 实现了 Comparable接口:表示 string可以比较大小 ・ String在jdk8及以前内部定义了 final char【】 value用于存储字符串数据.jk9时改为byte【】加上编码标记 以节约空间。StringBuffer与StringBuilder同步修改。 string:代表不可变的字符序列.简称:不可变性. 当对字符串重新赋值时,需要重写指定内存区域赋值,不能使用原有的value进行赋值. 当对现有的字符串进行连接操作时,也需要重新指定内存区域赋值,不能使用原有的value进行赋值. 当调用 string的 replace()方法修改指定字符或字符串时,也需要重新指定内存区域赋值,不能使用原有的value进行赋值 通过字面量的方式(区别于new)给一个字符串赋值,此时的字符串值声明在字符串常量池中. 字符串常量池中是不会存储相同内容的字符串的 Stringl的 String Pool是一个固定大小的 Hashtable,默认值大小长度是1009.如果放进 string Pool的String非常多,就会造成Hash冲严重,从而导致链表会很长,而链表长了后直接会造成的影响就是当调用 String. internl时性能会大幅下降. 使用-XX: StringTableSize可设置 StringTable的长度 在jdk6中 StringTable是固定的,就是1009的长度,所以如果常量池中的字符串过多就会导致效率下降很快. StringTableSize设置没有要求 在jdk7中， Stringtable的长度默认值是60013, StringTableSize设置没有要求 Jdk8开始，设置 StringTable的长度的话，1009是可设置的最小值。 2.String的内存分配 ​ 在Java语言中有8种基本数据类型和一种比较特殊的类型 string。这些类型为了使它们在运行过程中速度更快、更节省内存，都提供了一种常量池的概念。常量池就类似一个Java系统级别提供的缓存。8种基本数据类型的常量池都是系统协调的，String类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 string对象会直接存储在常量池中。比如：Stringinfo=&quot;atqulgu.com&quot;; 如果不是用双引号声明的 string对象，可以使用 string提供的 Intern（）方法。 Java6及以前，字符串常量池存放在永久代。 Java7 中 Oracle的工程师对字符串池的逻辑做了很大的改变，即将字符串常量池的位置调整到Java堆内。 所有的字符串都保存在堆(Heap)中，和其他普通对象一样，这样可以让你在进行调优应用时仅需要调整堆大小就可以了。 字符串常量池概念原本使用得比较多，但是这个改动使得我们有足够的理由让我们重新考虑在Java7中使用 string.intern（） Java8元空间，字符串常量在堆。 3.String的基本操作 4.字符串拼接操作 常量与常量的拼接结果在常量池，原理是编译期优化 2.常量池中不会存在相同内容的常量。 3.只要其中有一个是变量，结果就在堆中。变量拼接的原理是 String Builder 4.如果拼接的结果调用 intern（）方法，则主动将常量池中还没有的字符串对象放入池中，并返回此对象地址。 字符串如何拼接？ 字符审拼接操作不一定使用的是 Stringbuilder! 如果拼接符号左右两边部是字符审常量戦常量引用，则仍然使编译期优化，即非 StringBuilder的方式分对于 final修飾类、方法、蒸本数类率、引用数据类型的量的结构时，能使用上 final的时候建议使用上。 体会执行效率：通过 StringBuilder的append（）的方式添加字符宇的教率要远高于使用 string的字符宇拼接方式！ 洋情： StringBuilder的 append（）的方式：自始至終中只创建过一个StringBuilder的对象 使用String的字符串拼接方式：建过多个 StringBuilder和 String的对象 使用 String的字符串拼接方式：内存中出于创建了较多 StringBuilder和 String的对象，内存占用更大；如果进行GC,需要花费额外的时间。 改进空间：在实际开发中，如果基本确定要前前后后添加的字符串长度不高手某个限定值 highlevel的况下，建议使用构造器实例化 StringBuilder s=new StringBuilder(highlevel);//new char[highlevel] 12345678String s1=&quot;a&quot;;String s2=&quot;b&quot;;String s3=&quot;ab&quot;;//s1+s2细节：//1.StringBuilder s=new StringBuilder();//s.append(&quot;a&quot;);//s.append(&quot;b&quot;);//s.toString()//---&gt;约等于 new String(&quot;ab&quot;) 5.intern（）的使用 ​ 如果不是用双引号声明的 string对象，可以使用 string提供的 intern方法： intern方法会从字符串常量池中査询当前字符串是否存在，若不存在就会将当前字符串放入常量池中。 比如：String myinfo = new string(&quot;I love atguigu&quot;).intern(） ​ 也就是说，如果在任意字符串上调用 string. intern方法，那么其返回结果所指向的那个类实例，必须和直接以常量形式出现的字符串实例完全相同。因此，下列表达式的值必定是true: ​ (“a”+“b”+“c”).intern() ==&quot;abc&quot; ​ 通俗点讲， Interned String/就是确保字符串在内存里只有一份拷贝，这样可以节约内存空间，加快字符串操作任务的执行速度。注意，这个值会被存放在字符串内部池(String Intern Pool) 问题 new String（“ab”）会创建几个对象,new String(&quot;a&quot;) + new String(&quot;b&quot;)呢？ 1234567891011121314151617181920212223242526/** * 题目： * new String(&quot;ab&quot;)会创建几个对象？看字节码，就知道是两个。 * 一个对象是：new关键字在堆空间创建的 * 另一个对象是：字符串常量池中的对象&quot;ab&quot;。 字节码指令：ldc * * * 思考： * new String(&quot;a&quot;) + new String(&quot;b&quot;)呢？ * 对象1：new StringBuilder() * 对象2： new String(&quot;a&quot;) * 对象3： 常量池中的&quot;a&quot; * 对象4： new String(&quot;b&quot;) * 对象5： 常量池中的&quot;b&quot; * * 深入剖析： StringBuilder的toString(): * 对象6 ：new String(&quot;ab&quot;) * 强调一下，toString()的调用，在字符串常量池中，没有生成&quot;ab&quot; */public class StringNewTest { public static void main(String[] args) {// String str = new String(&quot;ab&quot;); String str = new String(&quot;a&quot;) + new String(&quot;b&quot;); }} 如何保证变量s指向的是字符串常量池中的数据呢？ 123456789101112131415161718192021222324252627/** * 如何保证变量s指向的是字符串常量池中的数据呢？ * 有两种方式： * 方式一： String s = &quot;shkstart&quot;;//字面量定义的方式 * 方式二： 调用intern() * String s = new String(&quot;shkstart&quot;).intern(); * String s = new StringBuilder(&quot;shkstart&quot;).toString().intern(); */public class StringIntern { public static void main(String[] args) { String s = new String(&quot;1&quot;); s.intern();//调用此方法之前，字符串常量池中已经存在了&quot;1&quot; String s2 = &quot;1&quot;; System.out.println(s == s2);//jdk6：false jdk7/8：false String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;);//s3变量记录的地址为：new String(&quot;11&quot;) //执行完上一行代码以后，字符串常量池中，是否存在&quot;11&quot;呢？答案：不存在！！ s3.intern();//在字符串常量池中生成&quot;11&quot;。如何理解：jdk6:创建了一个新的对象&quot;11&quot;,也就有新的地址。 // jdk7:此时常量中并没有创建&quot;11&quot;,而是创建一个指向堆空间中new String(&quot;11&quot;)的地址 String s4 = &quot;11&quot;;//s4变量记录的地址：使用的是上一行代码代码执行时，在常量池中生成的&quot;11&quot;的地址 System.out.println(s3 == s4);//jdk6：false jdk7/8：true }} 12345678910public class StringIntern1 { public static void main(String[] args) { String s3 = new String(&quot;1&quot;) + new String(&quot;1&quot;);//new String(&quot;11&quot;) //执行完上一行代码以后，字符串常量池中，是否存在&quot;11&quot;呢？答案：不存在！！ String s4 = &quot;11&quot;;//在字符串常量池中生成对象&quot;11&quot; String s5 = s3.intern(); System.out.println(s3 == s4);//false System.out.println(s5 == s4);//true }} 总结 stringl的 intern（）的使用 ・jdk1.6中，将这个字符串对象尝试放入串池。 如果串池中有，则并不会放入。返回已有的串池中的对象的地址。如果没有，会把此对象复制一份，放入串池，并返回串池中的对象地址 ・Jdk1.7起，将这个字符串对象尝试放入串池。 如果串池中有，则并不会放入。返回已有的串池中的对象的地址。如果没有，则会把对象的引用地址复制一份，放入串池，并返回串池中的引用地址 12345678910111213public class StringExer1 { public static void main(String[] args) { String x = &quot;ab&quot;; String s = new String(&quot;a&quot;) + new String(&quot;b&quot;);//new String(&quot;ab&quot;) //在上一行代码执行完以后，字符串常量池中并没有&quot;ab&quot; String s2 = s.intern();//jdk6中：在串池中创建一个字符串&quot;ab&quot; //jdk8中：串池中没有创建字符串&quot;ab&quot;,而是创建一个引用，指向new String(&quot;ab&quot;)，将此引用返回 System.out.println(s2 == &quot;ab&quot;);//jdk6:true jdk8:true System.out.println(s == &quot;ab&quot;);//jdk6:false jdk8:true }} 123456789public class StringExer2 { public static void main(String[] args) { String s1 = new String(&quot;ab&quot;);//执行完以后，会在字符串常量池中会生成&quot;ab&quot;// String s1 = new String(&quot;a&quot;) + new String(&quot;b&quot;);////执行完以后，不会在字符串常量池中会生成&quot;ab&quot; s1.intern(); String s2 = &quot;ab&quot;; System.out.println(s1 == s2); }} 空间效率测试 123456789101112131415161718192021222324252627282930/** * 使用intern()测试执行效率：空间使用上 * * 结论：对于程序中大量存在存在的字符串，尤其其中存在很多重复字符串时，使用intern()可以节省内存空间。 * 大的网站平台，需要内存中存储大量的字符串。比如社交网站，很多人都存储：北京市、海淀区等信息。这时侯如果字符串都调用intern（）方 * * 法，就会明显降低内存的大小。 */public class StringIntern2 { static final int MAX_COUNT = 1000 * 10000; static final String[] arr = new String[MAX_COUNT]; public static void main(String[] args) { Integer[] data = new Integer[]{1,2,3,4,5,6,7,8,9,10}; long start = System.currentTimeMillis(); for (int i = 0; i &lt; MAX_COUNT; i++) {// arr[i] = new String(String.valueOf(data[i % data.length])); arr[i] = new String(String.valueOf(data[i % data.length])).intern(); } long end = System.currentTimeMillis(); System.out.println(&quot;花费的时间为：&quot; + (end - start)); try { Thread.sleep(1000000); } catch (InterruptedException e) { e.printStackTrace(); } System.GC(); }} 垃圾回收测试 1234567891011121314/** * String的垃圾回收: * -Xms15m -Xmx15m -XX:+PrintStringTableStatistics -XX:+PrintGCDetails * * @author shkstart shkstart@126.com * @create 2020 21:27 */public class StringGCTest { public static void main(String[] args) { for (int j = 0; j &lt; 100000; j++) { String.valueOf(j).intern(); } }} G1的String去重操作 背景： 对许多Java应用（有大的也有小的）做的测试得出以下结果： 堆存活数据集合里面 string,对象占了25% 堆存活数据集合里面重复的 string对象有13.5% String对象的平均长度是45 许多大规模的Java应用的瓶颈在于内存，测试表明，在这些类型的应用里面，Java堆中存活的数据集合差不多25%是 string对象。更进一步，这里面差不多一半 string对象是重复的，重复的意思是说: strinG1. equals( string2)=true。堆上存在重复的 String对象必然是一种内存的浪费。这个项目将在G1垃圾收集器中实现自动持续对重复的 string对象进行去重，这样就能避免浪费内存。 实现 当垃圾收集器工作的时候，会访问堆上存活的对象。对每一个访问的对象都会检查是否是候选的要去重的String对象。 如果是，把这个对象的一个引用插入到队列中等待后续的处理。一个去重的线程在后台运行，处理这个队列。处理队列的一个元素意味着从队列删除这个元素，然后尝试去重它引用的String对象。 使用一个 hashtable来记录所有的被 String对象使用的不重复的char数组。当去重的时候，会査这个 hashtable,来看堆上是否已经存在一个一模一样的char数组。 如果存在， string对象会被调整引用那个数组，释放对原来的数组的引用，最终会被垃圾收集器回收掉。 如果查找失败，char数组会被插入到 hashtable,这样以后的时候就可以共享这个数组了。 命令行选项 UsestringDeduplication(bool):开启 String去重，默认是不开启的，需要手动开启。 PrintstringDeduplicationStatistics(bool):打印详细的去重统计信息 StringdeduplicationAgethreshold( uintx):达到这个年龄的 string对象被认为是去重的候选对象 十四、垃圾回收概述 垃圾收集，不是Java语言的伴生产物。早在1960年，第一门开始使用内存动态分配和垃圾收集技术的Lisp语言诞生 关于垃圾收集有三个经典问题：哪些内存需要回收？&gt;什么时候回收？如何回收？ 垃圾收集机制是Java的招牌能力，极大地提高了开发效率。如今，垃圾收集几乎成为现代语言的标配，即使经过如此长时间的发展，Java的垃圾收集机制仍然在不断的演进中，不同大小的设备、不同特征的应用场景，对垃圾收集提出了新的挑战，这当然也是面试的热点。 题例： 蚂蚁金服： 你知道哪几种垃圾回收器，各自的优缺点，重点讲一下CMS和G1 面： JVM GC算法有哪些，目前的JDK版本采用什么回收算法 面：G1回收器讲下回收过程 GC是什么？为什么要有GC? 面：GC的两种判定方法?CMS收集器与G1收集器的特点。 百度： 说一下GC算法，分代回收说下垃圾收集策略和算法 天猫： 面： JVM GC原理，JVM怎么回收内存 一面：CMS特点，垃圾回收算法有哪些？各自的优缺点，他们共同的缺点是什么？ 滴滴 面：java的垃圾回收器都有哪些，说下G1的应用场景，平时你是如何搭配使用垃圾回收器的。 京东： 你知道哪几种垃圾收集器，各自的优缺点，重点讲下CMS和G1,包括原理，流程，优缺点。垃圾回收算法的实现原理阿里： 讲一讲垃圾回收算法。什么情况下触发垃圾回收？如何选择合适的垃圾收集算法？ JVM有哪三种垃圾回收器？ 字节跳动： 常见的垃圾回收器算法有哪些，各有什么优劣？system.GC（）和 runtime.GC（）会做什么事情？面： Java GC机制？ GC Roots有哪些？ 二面：Java对象的回收方式，回收算法 CMS和G1了解么，CMS解决什么问题，说一下回收的过程。 CMS回收停顿了几次，为什么要停顿两次。 1.什么是垃圾 什么是垃圾( Garbage)呢？ 垃圾是指在运行程序中没有任何指针指向的对象，这个对象就是需要被回收的垃圾。 An object is considered garbage when it can nolonger be reached from any pointer in the runningprogram 如果不及时对内存中的垃圾进行清理，那么，这些垃圾对象所占的内存空间会一直保留到应用程序结束，被保留的空间无法被其他对象使用。甚至可能导致内存溢出。 2.想要学习GC,首先需要理解为什么需要GC? ​ 对于高级语言来说，一个基本认知是如果不进行垃圾回收，内存迟早都会被消耗完因为不断地分配内存空间而不进行回收，就好像不停地生产生活垃圾而从来不打扫一样。 ​ 除了释放没用的对象，垃圾回收也可以清除内存里的记录碎片。碎片整理将所占用的堆内存移到堆的一端，以便JVM将整理出的内存分配给新的对象。 ​ 随着应用程序所应付的业务越来越庞大、复杂，用户越来越多，没有GC就不能保证应用程序的正常进行。而经常造成STW的GC又跟不上实际的需求，所以才会不断地尝试对GC进行优化。 早期垃圾回收 ​ 在早期的C/C 时代，垃圾回收基本上是手工进行的。开发人员可以使用new关键字进行内存申请，并使用 delete关键字进行内存释放。比如以下代码。 ​ 这种方式可以灵活控制内存释放的时间，但是会给开发人员带来频繁申请和释放内存的管理负担。倘若有一处内存区间由于程序员编码的问题忘记被回收，那么就会产生内存泄漏，垃圾对象永远无法被清除，随着系统运行时间的不断增长，垃圾对象所耗内存可能持续上升，直到出现内存溢出并造成应用程序崩溃。现在，除了Java以外，C#、 python、Ruby等语言都使用了自动垃圾回收的思想，也是未来发展趋势。可以说，这种自动化的内存分配和垃圾回收的方式己经成为现代开发语言必备的标准。 Java垃圾回收机制 ​ 自动内存管理，无需开发人员手动参与内存的分配与回收，这样降低内存泄漏和内存溢出的风险。没有垃圾回收器，java也会和cpp一样，各种悬垂指针，野指针，泄露问题让你头疼不已。 ​ 自动内存管理机制，将程序员从繁重的内存管理中释放出来，可以更专心地专注于业务开发 oracle官网关于垃圾回收的介绍：https://docs.oracle.com/javase/8/docs/technotes/guides/vm/GCtuning/toc.html ​ 对于Java开发人员而言，自动内存管理就像是一个黑匣子，如果过度依赖于“自动”，那么这将会是一场灾难，最严重的就会弱化Java开发人员在程序出现内存溢出时定位问题和解决问题的能力。 ​ 此时，了解JVM的自动内存分配和内存回收原理就显得非常重要，只有在真正了解JVM是如何管理内存后，我们才能够在遇见 OutOfMemoryError时，快速地根据错误异常日志定位问题和解决问题。 ​ 当需要排查各种内存溢出、内存泄漏问题时，当垃圾收集成为系统达到更高并发量的瓶颈时，我们就必须对这些“自动化”的技术实施必要的监控和调节。 垃圾回收器可以对年轻代回收，也可以对老年代回收，甚至是全堆和方法区的回收。 其中，Java堆是垃圾收集器的工作重点。 从次数上讲 频繁收集 Young区 较少收集old区 基本不动Perm区 十五、垃圾回收相关算法 1.垃圾标记阶段的算法 1.垃圾标记阶段：对象存活判断 ​ 在堆里存放着几乎所有的Java对象实例，在GC执行垃圾回收之前，首先需要区分出内存中哪些是存活对象，哪些是已经死亡的对象。只有被标记为己经死亡的对象，GCオ会在执行垃圾回收时，释放掉其所占用的内存空间，因此这个过程我们可以称为垃圾标记阶段。 那么在JVM中究竟是如何标记一个死亡对象呢？简单来说，当一个对象已经不再被任何的存活对象继续引用时，就可以宣判为已经死亡。判断对象存活一般有两种方式：引用计数算法和可达性分析算法。 方式一：引用计数算法 引用计数算法( Reference Counting)比较简单，对每个对象保存一个整型的引用计数器属性。用于记录对象被引用的情况。 对于一个对象A,只要有任何一个对象引用了A,则A的引用计数器就加1;当引用失效时，引用计数器就减1。只要对象A的引用计数器的值为0,即表示对象A不可能再被使用，可进行回收。 优点：实现简单，垃圾对象便于辨识；判定效率高，回收没有延退性。 缺点：它需要单独的字段存储计数器，这样的做法增加了存储空间的开销。每次赋值都需要更新计数器，伴随着加法和减法操作，这增加了时间开销。引用计数器有一个严重的问题，即无法处理循环引用的情况。这是一条致命缺陷，导致在Java的垃圾回收器中没有使用这类算法。 方式二：可达性分析(或根搜索算法、追踪性垃圾收集) 相对于引用计数算法而言，可达性分析算法不仅同样具备实现简单和执行高效等特点，更重要的是该算法可以有效地解决在引用计数算法中循环引用的问题，防止内存泄漏的发生。 相较于引用计数算法，这里的可达性分析就是Java、C#选择的。这种类型的垃圾收集通常也叫作追踪性垃圾收集（ Tracing Garbage Collection)。 所谓&quot; GC Roots&quot;根集合就是一组必须活跃的引用。 基本思路： 可达性分析算法是以根对象集合( GC Roots)为起始点，按照从上至下的方式搜索被根对象集合所连接的目标对象是否可达。 使用可达性分析算法后，内存中的存活对象都会被根对象集合直接或间接连接着，搜索所走过的路径称为引用链( Reference Chain) 如果目标对象没有任何引用链相连，则是不可达的，就意味着该对象己经死亡，可以标记为垃圾对象。 在可达性分析算法中，只有能够被根对象集合直接或者间接连接的对象才是存活对象。 GC ROOT 在Java语言中， GC Roots包括以下几类元素： 虚拟机栈中引用的对象。比如：各个线程被调用的方法中使用到的参数、局部变量等。 本地方法栈内JNI（通常说的本地方法）引用的对象 方法区中类静态属性引用的对象&gt;比如：Java类的引用类型静态变量方法区中常量引用的对象。比如：字符串常量池( String Table)里的引用所有被同步锁 synchronized持有的对象 Java虚拟机内部的引用。基本数据类型对应的class对象，一些常驻的异常对象（如：Null Pointerexception、 OutOfMemoryError),系统类加载器。 反映java虚拟机内部情况的 JMXbeam、JVMTI中注册的回调、本地代码缓存等。 除了这些固定的 GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整 GC Roots集合。比如：分代收集和局部回收( Partial GC)。如果只针对Java堆中的某一块区域进行垃圾回收(比如：典型的只针对新生代)，必须考虑到内存区域是虚拟机自己的实现细节，更不是孤立封闭的，这个区域的对象完全有可能被其他区域的对象所引用，这时候就需要一并将关联的区域对象也加入 GC Roots 集合中去考虑，才能保证可达性分析的准确性 小技巧：由于Root采用栈方式存放变量和指针，所以如果一个指针，它保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那它就是一个Root 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。 这点也是导致GC进行时必须 “Stop The World&quot;的一个重要原因。即使是号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 2.对象的 finalization机制 Java语言提供了对象终止( finalization)机制来允许开发人员提供对象被销毁之前的自定义处理逻辑。 当垃圾回收器发现没有引用指向一个对象，即：垃圾回收此对象之前，总会先调用这个对象的 finallize（）方法 finalize()方法允许在子类中被重写，用于在对象被回收时进行资源释放。通常在这个方法中进行一些资源释放和清理的工作，比如关闭文件、套接字和数据库连接等。 永远不要主动调用某个对象的finalize()方法，应该交给垃圾回收机制调用。理由包括下面三点： finalize() 时可能会导致对象复活。 finalize() 方法的执行时间是没有保障的，它完全由GC线程决定，极端情况下，若不发生GC,则finalize()方法将没有执行机会。 一个糟糕的 finalize() 会严重影响GC的性能 从功能上来说， finalize() 方法与 C 中的析构函数比较相似，但是Java采用的是基于垃圾回收器的自动内存管理机制，所以 finalize() 方法在本质上不同于C中的析构函数。由于 finalize() 方法的存在，虚拟机中的对象一般处于三种可能的状态。 如果从所有的根节点都无法访问到某个对象，说明对象己经不再使用了。一般来说，此对象需要被回收。但事实上，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段。一个无法触及的对象有可能在某一个条件下“复活”自己，如果这样，那么对它的回收就是不合理的，为此，定义虚拟机中的对象可能的三种状态。如下： 可触及的：从根节点开始，可以到达这个对象。 可复活的：对象的所有引用都被释放，但是对象有可能 finalize() 中复活。 不可触及的：对象的 finalize() 被调用，并且没有复活，那么就会进入不可触及状态。不可触及的对象不可能被复活，因为 finalize() 只会被调用一次。 以上3种状态中，是由于fina1ize（）方法的存在，进行的区分。只有在对象不可触及时才可以被回收。 具体过程： 判定一个对象 objA 是否可回收，至少要经历两次标记过程 1.如果对象。objA到 GC Roots没有引用链，则进行第一次标记。 2.进行筛选，判断此对象是否有必要执行finalize()方法。 ①如果对象objA没有重写 finalize()方法，或者 finalize() 方法已经被虚拟机调用过，则虚拟机视为“没有必要执行”，objA被判定为不可触及的。 ②如果对象objA重写了finalize() 方法，且还未执行过，那么objA会被插入到F- Queue队列中，由一个虚拟机自动创建的、低优先级的Finalizer线程触发其 finalize() 方法执行。 ③ finalize() 方法是对象逃脱死亡的最后机会，稍后GC会对F- Queue队列中的对象进行第二次标记。如果objA在finalize() 方法中与引用链上的任何一个对象建立了联系，那么在第二次标记时，objA会被移出“即将回收”集合。之后，对象会再次出现没有引用存在的情况。在这个情况下，finalize方法不会被再次调用，对象会直接变成不可触及的状态，也就是说，一个对象的finalize方法只会被调用一次。 3.MAT与 Jprofiler的GC Roots湖源 MAT MAT是 Memory Analyzer的简称，它是一款功能强大的Java堆内存分析器。用于査找内存泄漏以及査看内存消耗情况。MAT是基于 Eclipse开发的，是一款免费的性能分析工具大家可以在https://www.eclipse.org/mat/下载并使用MAT。 ​ 获取 dump 文件 Jprofiler 4.垃圾清除阶段算法之标记一清除算法 当成功区分出内存中存活对象和死亡对象后，GC接下来的任务就是执行垃圾回收，释放掉无用对象所占用的内存空间，以便有足够的可用内存空间为新对象分配内存。 目前在JVM中比较常见的三种垃圾收集算法是标记 - 清除算法（Mark Sweep)、复制算法( Copying)、标记 - 压缩算法（Mark Compact） 标记 - 清除算法（Mark Sweep) 停止-标记-清除 背景：标记一清除算法(Mark- Sweep)是一种非常基础和常见的垃圾收集算法，该算法被J. Mccarthy等人在1960年提出并并应用于Lisp语言。 执行过程：当堆中的有效内存空间(avail able memory)被耗尽的时候，就会停止整个程序(也被称为 stop the world),然后进行两项工作，第一项则是标记，第二项则是清除。 标记：Collector从引用根节点开始遍历，标记所有被引用的对象。一般是在对象的 Header中记录为可达对象。 清除：Collector对堆内存从头到尾进行线性的遍历，如果发现某个对象在其 Headerl中没有标记为可达对象，则将其回收. 缺点 效率不算高 在进行GC的时候，需要停止整个应用程序，导致用户体验差。 这种方式清理出来的空闲内存是不连续的，产生内存碎片。需要维护一个空闲列表注意。 何为清除？ 这里所谓的清除并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里。下次有新对象需要加载时，判断垃圾的位置空间是否够如果够，就存放。 复制算法( Copying) 两块-活连续复制到另一块-清除 背景 为了解决标记一清除算法在垃圾收集效率方面的缺陷,M,工. Minsky于1963年发表了著名的论文,&quot;使用双存储区的Lisp语言垃圾收集器CALISP Garbage Collector Algorithm Using Serial Secondary Storage)&quot;。M.L. Minsky在该论文中描述的算法被人们]称为复制( Copying)算法,它也被M.L. Minsky本人成功地引入到了Lisp语言的一个实现版本中。 执行过程 将活着的内存空间分为两块,每次只使用其中一块,在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中,之后清除正在使用的内存块中的所有对象,交换两个内存的角色,最后完成垃圾回收. 优点： 没有标记和清除过程，实现简单，运行高效 复制过去以后保证空间的连续性，不会出现“碎片”问题缺点 缺点：需要两倍的内存空间。 对于G1这种分拆成为大量 region的GC,复制而不是移动，意味着GC需要维护 region之间对象引用关系，不管是内存占用或者时间开销也不小。 特别的：如果系统中的垃圾对象很多，复制算法不会很理想，因为复制算法需要复制的存活对象数量并不会太大或者说非常低才行。 标记 - 压缩算法（Mark Compact） 标记-引用对象压缩到一端-清除 背景 复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代，更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活对象较多，复制的成本也将很高。因此，基于老年代垃圾回收的特性，需要使用其他的算法。 标记一清除算法的确可以应用在老年代中，但是该算法不仅执行效率低下，而且在执行完内存回收后还会产生内存碎片，所以JM的设计者需要在此基础之上进行改进。标记压缩(Mark- Compact)算法由此诞生。 1970年前后，G.L. Steele、C.J. Chene和D.S.Wise等研究者发布标记压缩算法。在许多现代的垃圾收集器中，人们都使用了标记-压缩算法或其改进版本。 执行过程 第一阶段和标记清除算法一样从根节点开始标记所有被引用对象 第二阶段将所有的存活对象压缩到内存的一端，按顺序排放。 之后，清理边界外所有的空间。 ​ 标记一压缩算法的最终效果等同于标记一清除算法执行完成后，再进行一次内存碎片整理，因此，也可以把它称为标记一清除一压缩（Mark- Sweep- Compact)算法。二者的本质差异在于标记一清除算法是一种非移动式的回收算法，标记-压缩是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策。可以看到，标记的存活对象将会被整理，按照内存地址依次排列，而未被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销。 优点： 消除了标记一清除算法当中，内存区域分散的缺点，我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可。消除了复制算法当中，内存减半的高额代价 缺点： 从效率上来说，标记一整理算法要低于复制算法移动对象的同时，如果对象被其他对象引用，则还需要调整引用的地址。移动过程中，需要全程暂停用户应用程序。即：STW 总结 ​ 效率上来说，复制算法是当之无愧的老大，但是却浪费了太多内存。而为了尽量兼顾上面提到的三个指标，标记一整理算法相对来说更平滑一些，但是效率上不尽如人意，它比复制算法多了一个标记的阶段，比标记一清除多了一个整理内存的阶段。 前面所有这些算法中，并没有一种算法可以完全替代其他算法，它们都具有自己独特的优势和特点。分代收集算法应运而生。 分代收集算法，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的收集方式，以便提高回收效率。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点使用不同的回收算法，以提高垃圾回收的效率 在Java程序运行的过程中，会产生大量的对象，其中有些对象是与业务信息相关，比如Http请求中的 Session对象、线程、 Socket连接，这类对象跟业务直接挂钩，因此生命周期比较长。但是还有一些对象，主要是程序运行过程中生成的临时变量，这些对象生命周期会比较短，比如： string,对象，由于其不变类的特性，系统会产生大量的这些对象，有些对象甚至只用一次即可回收。 ​ 目前几乎所有的GC都是采用分代收集( Generational Collecting)算法执行垃圾回收的。在 Hotspot中，基于分代的概念，GC所使用的内存回收算法必须结合年轻代和老年代各自的特点。 年轻代( Young Gen) ​ 年轻代特点：区域相对老年代较小，对象生命周期短、存活率低，回收频繁。这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对象大小有关，因此很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过 hotspot中的两个 survivor的设计得到缓解。 老年代( Tensured Gen) ​ 老年代特点：区域较大，对象生命周期长、存活率高，回收不及年轻代频繁。这种情况存在大量存活率高的对象，复制算法明显变得不合适。一般是由标记一清除或者是标记一清除与标记整理的混合实现。Mark阶段的开销与存活对象的数量成正比。Sweep阶段的开销与所管理区域的大小成正相关。Compact阶段的开销与存活对象的数据成正比。 ​ 以 Hotspot中的CMS回收器为例，CMS是基于Mark- Sweep实现的，对于对象的回收效率很高。而对于碎片问题，CMS采用基于Mark- Compact算法的 Serial old 回收器作为补偿措施：当内存回收不佳（碎片导致的 Concurrent Mode Failure时），将采用 Seralold执行FullGC以达到对老年代内存的整理。分代的思想被现有的虚拟机广泛使用。几乎所有的垃圾回收器都区分新生代和老年代。 分区算法 ​ 一般来说，在相同条件下，堆空间越大，一次GC时所需要的时间就越长，有关GC产生的停顿也越长。为了更好地控制GC产生的停顿时间，将一块大的内存区域分割成多个小块，根据目标的停顿时间，每次合理地回收若干个小区间，而不是整个堆空间，从而减少ー次GC所产生的停顿。分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同小区间。每一个小区间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。 增量收集算法 缺点 使用这种方式，由于在垃圾回收过程中，间断性地还执行了应用程序代码，所以能减少系统的停顿时间。但是，因为线程切换和上下文转换的消耗，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 十六、垃圾回收相关概念 1.System.GC()的理解 在默认情况下，通过 System.GC)或者 Runtime. getruntime（）.GC（）的调用，会显式触发Full GC,同时对老年代和新生代进行回收，尝试释放被丢弃对象占用的内存。 然而 system.GC（）调用附带一个免责声明，无法保证对垃圾收集器的调用。 JVM实现者可以通过 System.GC（）调用来决定JVM的GC行为。而一般情况下，垃圾回收应该是自动进行的，无须手动触发，否则就太过于麻烦了。在一些特殊情况下，如我们正在编写一个性能基准，我们可以在运行之间调用System.GC（）。 2.内存溢出与内存泄漏 内存溢出 (满了) 内存溢出相对于内存泄漏来说，尽管更容易被理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。 由于GC一直在发展，所有一般情況下，除非应用程序占用的内存增长速度非常快，造成垃圾回收已经跟不上内存消耗的速度，否则不太容易出现OOM的情况。 大多数情况下，GC会进行各种年龄段的垃圾回收，实在不行了就放大招，来一次独占式的Full GC操作，这时候会回收大量的内存，供应用程序继续使用。 javadoc中对 OutOfMemoryError的解释是，没有空闲内存，并且垃圾收集器也无法提供更多内存。 ・没有空闲内存的情况:说明java虚拟机的堆内存不够.原因有二： (1)Java虚拟机的堆内存设置不够 比如:可能存在内存泄漏问题:地很有可能就是堆的大小不合理,比如我们要处理比较可观的数据量,但是没有显式指定JVM堆大小或者指定数值偏小.我们可以通过参数-Xms、Xmx来调整. (2)代码中创建了大量大对象,并且长时间不能被垃圾收集器收集(存在被引用) s对于老版本的 Oracle JDK,因为永久代的大小是有限的,并且JVM对永久代垃圾回收(如,常量池回收、卸载不再需要的类型)非常不积极,所以当我们不断添加新类型的时候,永久代出现 OutOfMemoryError也非常多见,尤其是在运行时存在大量动态类型生成的场合;类似 l tern字符串缓存占用太多空间,也会导致OOM问题.对应的异常信息,会标记出来和永久代相关:&quot;java.lang.OutOfMemoryError: Permgen space&quot;. 随着元数据区的引入,方法区内存已经不再那么窘迫,所以相应的OOM有所改观,出现OOM,异常信息则变成了:&quot;java.lang. OutOfMemoryError: Metaspace&quot;.直接内存不足,也会导致OOM. ​ 这里面隐含着一层意思是,在抛出 OutOfMemoryError之前,通常垃圾收集器会被触发,尽其所能去清理出空间。例如:在引用机制分析中,涉及到JVM会去尝试回收软引用指向的对象等。在java.nio.BTts. reservememory()方法中,我们能清楚的看到, System.GC()会被调用,以清理空间. ​ 当然,也不是在任何情况下垃圾收集器都会被触发的。比如,我们去分配一个超大对象,类似一个超大数组超过堆的最大值,JVM可以判断出垃圾收集并不能解决这个问题,所以直接抛出 OutOfMemoryError. 内存泄漏（不能回收） ​ 也称作“存储渗漏”。严格来说，只有对象不会再被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。但实际情况很多时候一些不太好的实践（或疏忽）会导致对象的生命周期变得很长甚至导致OOM,也可以叫做宽泛意义上的“内存泄漏”。 ​ 尽管内存泄漏并不会立刻引起程序崩溃，但是一旦发生内存泄漏，程序中的可用内存就会被逐步蚕食，直至耗尽所有内存，最终出现 OutOfMemory异常，导致程序崩溃。注意，这里的存储空间并不是指物理内存，而是指虚拟内存大小，这个虚拟内存大小取决于磁盘交换区设定的大小。 举例 1、单例模式 单例的生命周期和应用程序是一样长的，所以单例程序中，如果持有对外部对象的引用的话，那么这个外部对象是不能被回收的，则会导致内存泄漏的产生。 2、一些提供close的资源未关闭导致内存泄漏 数据库连接( datasource. getconnection（）),网络连接( socket)和io连接必须手动close,否则是不能被回收的。 3.Stop The World Stop-the-World,简称 STW,指的是GC事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被暂停，没有任何响应，有点像卡死的感觉，这个停顿称为STW. 可达性分析算法中枚举根节点( GC Roots)会导致所有Java执行线程停顿。分析工作必须在一个能确保一致性的快照中进行。一致性指整个分析期间整个执行系统看起来像被冻结在某个时间点上如果出现分析过程中对象引用关系还在不断变化，则分析结果的准确性无法保证。 被STW中断的应用程序线程会在完成GC之后恢复，频繁中断会让用户感觉像是网速不快造成电影卡带一样，所以我们需要减少STW的发生 STW事件和采用哪款GC无关，所有的GC都有这个事件。哪怕是G1也不能完全避免Stop-the-world情况发生，只能说垃圾回收器越来越优秀，回收效率越来越高，尽可能地缩短了暂停时间。 STW是JVM在后台自动发起和自动完成的。在用户不可见的情况下，把用户正常的工作线程全部停掉。 开发中不要用 system.GC() 会导致Stop-the- world的发生。 4.垃圾回收的并发与并行 并发和并行，在谈论垃圾收集器的上下文语境中，它们可以解释如下： **并行( Parallel)😗*指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态。如 Pardew、 Parallel Scavenge、 Parallel Old **串行( Serial)😗*相较于并行的概念，单线程执行。如果内存不够，则程序暂停，启动JVM垃圾回收器进行垃圾回收。回收完，再启动程序的线程。 并发和并行,在谈论垃圾收集器的上下文语境中,它们可以解释如下: 并发( Concurrent):指用户线程与垃圾收集线程同时执行(但不一定是并行的,可能会交替执行),垃圾回收线程在执行时不会停顿用户程序的运行。用户程序在继续运行,而垃圾收集程序线程运行于另ー个CPU上，如:CMS、G1。 5.安全点与安全区域 安全点 程序执行时并非在所有地方都能停顿下来开始GC,只有在特定的位置才能停顿下来开始GC,这些位置称为“安全点( Safepoint)”。 Safe Pointl的选择很重要，如果太少可能导致GC等待的时间太长，如果太频繁可能导致运行时的性能问题。大部分指令的执行时间都非常短暂通常会根据“是否具有让程序长时间执行的特征”为标准。比如：选择些执行时间较长的指令作为 Safe Point,如方法调用、循环跳转和异常跳转等。 如何在GC发生时，检査所有线程都跑到最近的安全点停顿下来呢？ 抢先式中断：（目前没有虚拟机采用了) 首先中断所有线程。如果还有线程不在安全点，就恢复线程，让线程跑到安全点。 主动式中断 设置一个中断标志，各个线程运行到 Safe Point的时候主动轮询这个标志，如果中断标志为真，则将自己进行中断挂起。 安全区域 Safepoint机制保证了程序执行时，在不太长的时间内就会遇到可进入GC的 Safepoint。但是，程序“不执行”的时候呢？例如线程处于Sleep状态或Blocked状态，这时候线程无法响应JVM的中断请求，“走”到安全点去中断挂起，JVM也不太可能等待线程被唤醒。对于这种情况，就需要安全区域( Safe Region)来解决。 安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始GC都是安全的。我们也可以把 Safe Region看做是被扩展了的 Safepoint。 实际执行时： 1、当线程运行到 Safe Region的代码时，首先标识已经进入了 Safe Region,如果这段时间内发生GC,JVM会忽略标识为 Safe Region状态的线程 2、当线程即将离开 Safe Region时，会检査JVM是否已经完成GC,如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开 Safe Regiong的信号为止； 6.引用 ​ 我们希望能描述这样一类对象：当内存空间还足够时，则能保留在内存中；如果内存空间在进行垃圾收集后还是很紧张，则可以抛弃这些对象。 【既偏门又非常高频的面试题】强引用、软引用、弱引用、虚引用有什么区别？具体使用场景是什么？ ​ 在JDK1.2版之后，Java对引用的概念进行了扩充，将引用分为强引用（ Strong Reference)、软引用( Soft Reference)、弱引用( Weak Reference)和虚引用(Phantom Reference)4种，这4种引用强度依次逐渐减弱。 ​ 除强引用外，其他3种引用均可以在java.lang. ref包中找到它们的身影。如下图，显示了这3种引用类型对应的类，开发人员可以在应用程序中直接使用它们。 Reference子类中只有终结器引用是包内可见的，其他3种引用类型均为public可以在应用程序中直接使用 强引用( StrongReference)😗*【引用在就别管我】**最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似“ Object obj= new Object（）”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用( SoftReference)😗*【快满了回收我】**在系统将要发生内存溢出之前，将会把这些对象列入回收范围之中进行第二次回收。如果这次回收后还没有足够的内存，才会抛出内存溢出异常。 弱引用( WeakReference)😗*【看到就回收我】**被弱引用关联的对象只能生存到下一次垃圾收集之前。当垃圾收集器工作时，无论内存空间是否足够,都会回收掉被弱引用关联的对象. 虚引用( PhantomReference)😗*【不影响你】**一个对象是否有虚引用的存在,完全不会对其生存时间构成影响,也无法通过虚引用来获得一个对象的实例.为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知 强引用 在Java程序中，最常见的引用类型是强引用(普通系统99%以上都是强引用)，也就是我们最常见的普通对象引用，也是默认的引用类型。 当在JAVA语言中使用new操作符创建一个新的对象，并将其赋值给一个变量的时候，这个变量就成为指向该对象的一个强引用。 强引用的对象是可触及的，垃圾收集器就永远不会回收掉被引用的对象 对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为null,就是可以当做垃圾被收集了，当然具体回收时机还是要看垃圾收集策略。 相对的，软引用、弱引用和虚引用的对象是软可触及、弱可触及和虚可触及的，在一定条件下，都是可以被回收的。所以，强引用是造成Java内存泄漏的主要原因之 软引用 软引用是用来描述一些还有用，但非必需的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，オ会抛出内存溢出异常。 软引用通常用来实现内存敏感的缓存。比如：高速缓存就有用到软引用。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 垃圾回收器在某个时刻决定回收软可达的对象的时候，会清理软引用，并可选地把引用存放到一个引用队列( Reference Queue)。 类似弱引用，只不过Java虚拟机会尽量让软引用的存活时间长一些，迫不得已才清理。 弱引用 弱引用也是用来描述那些非必需对象，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。在系统GC时，只要发现弱引用，不管系统堆空间使用是否充足，都会回收掉只被弱引用关联的对象 但是，由于垃圾回收器的线程通常优先级很低，因此，并不一定能很快地发现持有弱引用的对象。在这种情况下，弱引用对象可以存在较长的时间。 弱引用和软引用一样，在构造弱引用时，也可以指定一个引用队列，当弱引用对象被回收时，就会加入指定的引用队列，通过这个队列可以跟踪对象的回收情况。 软引用、弱引用都非常适合来保存那些可有可无的缓存数据。如果这么做，当系统内存不足时，这些缓存数据会被回收，不会导致内存溢出。而当内存资源充足时，这些缓存数据又可以存在相当长的时间，从而起到加速系统的作用。 弱引用对象与软引用对象的最大不同就在于，当GC在进行回收时，需要通过算法检查是否回收软引用对象，而对于弱引用对象，GC总是进行回收。弱引用对象更容易、更快被GC回收。面试题：你开发中使用过 WeakHashMap吗？ 虚引用 ​ 也称为“幽灵引用”或者“幻影引用”，是所有引用类型中最弱的一个。一个对象是否有虚引用的存在，完全不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它和没有引用几乎是一样的，随时都可能被垃圾回收器回收。它不能单独使用，也无法通过虚引用来获取被引用的对象。当试图通过虚引用的get（）方法取得对象时，总是null。为一个对象设置虚引用关联的唯一目的在于跟踪垃圾回收过程。比如：能在这个对象被收集器回收时收到一个系统通知。 ​ 虚引用必须和引用队列一起使用。虚引用在创建时必须提供一个引用队列作为参数。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象后，将这个虚引用加入引用队列，以通知应用程序对象的回收情况。由于虚引用可以跟踪对象的回收时间，因此，也可以将一些资源释放操作放置在虚引用中执行和记录。在JDK1.2版之后提供了 PhantomReference类来实现虚引用。 终结器引用 ​ 它用以实现对象的finalize()方法，也可以称为终结器引用。无需手动编码，其内部配合引用队列使用。在GC时，终结器引用入队。由Finalizer线程通过终结器引用找到被引用对象并调用它的finalize()方法，第二次GC时才能回收被引用对象。 十七、垃圾回收器 1.GC分类 ​ 垃圾收集器没有在规范中进行过多的规定，可以由不同的厂商、不同版本的JVM来实现。由于JDK的版本处于高速送代过程中，因此Java发展至今已经衍生了众多的GC版本。从不同角度分析垃圾收集器，可以将GC分为不同的类型。 ​ 按线程数分，可以分为串行垃圾回收器和并行垃圾回收器。 ​ 串行回收指的是在同一时间段内只允许有一个CPU用于执行垃圾回收操作，此时工作线程被暂停，直至垃圾收集工作结束。在诸如单CPU处理器或者较小的应用内存等硬件平台不是特别优越的场合，串行回收器的性能表现可以超过并行回收器和并发回收器。所以，串行回收默认被应用在客户端的Client模式下的JVM中在并发能力比较强的CPU上，并行回收器产生的停顿时间要短于串行回收器。 ​ 和串行回收相反，并行收集可以运用多个CPU同时执行垃圾回收，因此提升了应用的吞吐量，不过并行回收仍然与串行回收一样，采用独占式，使用了“Stop-the- world”机制。 ​ **按照工作模式分，**可以分为并发式垃圾回收器和独占式垃圾回收器。 ​ 并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间。独占式垃圾回收器( Stop the world)一旦运行，就停止应用程序中的所有用户线程，直到垃圾回收过程完全结束 ​ 按碎片处理方式分，可分为压缩式垃圾回收器和非压缩式垃圾回收器 ​ 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。非压缩式的垃圾回收器不进行这步操作。 ​ 按工作的内存区间分，又可分为年轻代垃圾回收器和老年代垃圾回收器。 2.GC性能评估指标 吞吐量：运行用户代码的时间占总运行时间的比例(总运行时间：程序的运行时间十内存回收的时间) 垃圾收集开销：吞吐量的补数，垃圾收集所用时间与总运行时间的比例 暂停时间：执行垃圾收集时，程序的工作线程被暂停的时间 收集频率：相对于应用程序的执行，收集操作发生的频率。 内存占用：JAVA堆区所占的内存大小。 快速：一个对象从诞生到被回收所经历的时间。 ​ 这三者共同构成一个“不可能三角”。三者总体的表现会随着技术进步而越来越好。一款优秀的收集器通常最多同时满足其中的两项。 ​ 这三项里，暂停时间的重要性日益凸显。因为随着硬件发展，内存占用多些越来越能容忍，硬件性能的提升也有助于降低收集器运行时对应用程序的影响，即提高了吞吐量。而内存的扩大，对延退反而带来负面效果。简单来说，主要抓住两点：吞吐量、暂停时间。 3.吞吐量与暂停时间 ​ 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)。比如：虚拟机总共运行了180分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 这种情况下，应用程序能容忍较高的暂停时间，因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的吞吐量优先，意味着在单位时间内，STW的时间最短。 ​ “暂停时间”是指一个时间段内应用程序线程暂停，让GC线程执行的状态。例如，GC期间100毫秒的暂停时间意味着在这100毫秒期间内没有应用程序线程是活动的。暂停时间优先，意味着尽可能让单次STW的时间最短。 ​ 高吞吐量较好因为这会让应用程序的最终用户感觉只有应用程序线程在做“生产性”工作。直觉上，吞吐量越高程序运行越快。 ​ 低暂停时间（低延迟）较好因为从最终用户的角度来看不管是GC还是其他原因导致一个应用被挂起始终是不好的。这取决于应用程序的类型，有时候甚至短暂的260毫秒暂停都可能打断终端用户体验。因此，具有低的较大暂停时间是非常重要的，特别是对于一个交互式应用程序。 ​ 不幸的是”高吞吐量”和”低暂停时间”是一对相互竞争的目标（矛盾）因为如果选择以吞吐量优先，那么必然需要降低内存回收的执行频率，但是这样会导致GC需要更长的暂停时间来执行内存回收。相反的，如果选择以低延迟优先为原则，那么为了降低每次执行内存回收时的暂停时间，也只能频繁地执行内存回收，但这又引起了年轻代内存的缩减和导致程序吞吐量的下降。 ​ 在设计（或使用）GC算法时，我们必须确定我们的目标：一个GC算法只可能针对两个目标之一（即只专注于较大吞吐量或最小暂停时间），或尝试找到一个二者的折衷。现在标准：在最大吞吐量优先的情况下，降低停顿时间。 4，垃圾回收器发展史 有了虚拟机，就一定需要收集垃圾的机制，这就是 Garbage Collection,对应的产品我们称为 Garbage Collector. ・1999年随JDK1.3.1一起来的是串行方式的 Serial GC,它是第一款GC. Pardew垃圾收集器是 Seria收集器的多线程版本 ・2002年2月26日，Paralle GC和 Concurrent Mark Sweep GC跟随JDK1.4.2起发布Parallel GC在JDK6之后成为 Hotspot默认GC.2012年，在JDK1.7u4版本中，G1可用。 ・2017年，JDK9中G1变成默认的垃圾收集器，以替代CMS. ・2018年3月，JDK10中G1垃圾回收器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 ・2018年9月，JDK11发布。引入Bpsi1on垃圾回收器，又被称为&quot;No-Op（无操作）回收器。同时，引入zGC:可伸缩的低延迟垃圾回收器( Experimental). ・2019年3月，JDK12发布。增强G1,自动返回未用堆内存给操作系统。同时，引入Sh. enandoah GC:低停顿时间的GC( Experimenta1). ・2019年9月，JDK13发布。增强ZGC,自动返回未用堆内存给操作系统 ・2020年3月，JDK14发布。删除CMS垃圾回收器。扩展zGC在 macos,和 Windows上的应用 串行回收器： Serial、 Serial Old 并行回收器： Pa rnew、Parallel Scavenge、 Parallel Old 并发回收器：CMS、G1 垃圾收集器的组合关系 新生代收集器： Serial、 Pardew、Parallel Scavenge 老年代收集器： Serial 0ld、Parallel 0ld、CMS 整堆收集器：G1 JDK 8—&gt;JDK 9 实线----》红虚线 —&gt;JDK 14 实线----》绿虚线 并废弃CMS 默认G1 ​ 为什么要有很多收集器，一个不够吗？因为Java的使用场景很多，移动端，服务器等。所以就需要针对不同的场景，提供不同的垃圾收集器，提高垃圾收集的性能。 ​ 虽然我们会对各个收集器进行比较，但并非为了挑选一个最好的收集器出来。没有一种放之四海皆准、任何场景下都适用的完美收集器存在更加没有万能的收集器。所以我们选择的只是对具体应用最合适的收集器。 5.查看默认垃圾回收器 **-XX: PrintCommandLineFlags:**查看命令行相关参数（包含使用的垃圾收集器） 使用命令行指令：jinfo -flag 相关垃圾回收器参数 进程ID Serial 串行回收 Serial 收集器是最基本、历史最悠久的垃圾收集器了。JDK1.3之前回收新生代唯一的选择。 Serial 收集器作为 Hotspot中Client模式下的默认新生代垃圾收集器。 Serial收集器采用复制算法、串行回收和&quot;Stop-the-World&quot;机制的方式执行内存回收。 除了年轻代之外， Serial 收集器还提供用于执行老年代垃圾收集的 Serial Old收集器。 Serial Old收集器同样也采用了串行回收和&quot; Stop the World&quot;机制，只不过内存回收算法使用的是标记一压缩算法。 Serial Old是运行在Client 模式下默认的老年代的垃圾回收器。 Serial Old在 Server,模式下主要有两个用途：①与新生代的Parallel Scavenge配合使用。②作为老年代CMS收集器的后备垃圾收集方案。 优势：简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说， Serial 收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。运行在Client 模式下的虚拟机是个不错的选择。 在用户的桌面应用场景中，可用内存一般不大(几十MB至一两百MB),可以在较短时间内完成垃圾收集(几十ms至一百多ms),只要不频繁发生使用串行回收器是可以接受的。 在 Hotspot虚拟机中，使用 -XX: UseSerialGC参数可以指定年轻代和老年代都使用串行收集器。等价于新生代用 Seral GC,且老年代用 Serial Old GC 总结：这种垃圾收集器大家了解，现在已经不用串行的了。而且在限定单核cpuオ可以用。现在都不是单核的了。对于交互较强的应用而言，这种垃圾收集器是不能接受的。一般在Java Web应用程序中是不会采用串行垃圾收集器的。 ParNew 并行回收 ​ 如果说 Serial GC是年轻代中的单线程垃圾收集器，那么 ParNewl收集器则是 Seral 收集器的多线程版本。Par是 Parallel 的缩写，New:只能处理的是新生代。ParNew收集器除了采用并行回收的方式执行内存回收外，两款垃圾收集器之间几乎没有任何区别。 ParNew收集器在年轻代中同样也是采用复制算法、&quot;Stop-the-world&quot;机制。ParNew是很多JVM运行在 Server模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行方式高效。 对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源） 由于 Pardew收集器是基于并行回收，那么是否可以断定 Parnewl收集器的回收效率在任何场景下都会比 Seral 收集器更高效？ Parnew收集器运行在多CPU的环境下，由于可以充分利用多CPU、多核心等物理硬件资源优势，可以更快速地完成垃圾收集，提升程序的吞吐量。 但是在单个CPU的环境下， Parnewl收集器不比 Seral 收集器更高效。虽然 Serlal收集器是基于串行回收，但是由于CPU不需要频繁地做任务切换，因此可以有效避免多线程交互过程中产生的一些额外开销。 因为除 Seral外，目前只有 PardNew GC能与CMS收集器配合工作。 在程序中，开发人员可以通过选项”-XX: UseParNewGC&quot;手动指定使用ParNewl收集器执行内存回收任务。它表示年轻代使用并行收集器，不影响老年代。 -XX: ParallelGCThreads限制线程数量，默认开启和CPU数据相同的线程数。 Parallel Scavenge 吞吐量优先 HotSpot的年轻代中除了拥有 ParNew收集器是基于并行回收的以外，Parallel Scavenge收集器同样也采用了复制算法、并行回收和&quot;Stop The World&quot;机制。 那么 Parallel 收集器的出现是否多此一举？ 和 Pardew收集器不同， Parallel Scavenge收集器的目标则是达到个可控制的吞吐量( Throughput),它也被称为吞吐量优先的垃圾收集器。 自适应调节策略也是 Parallel Scavenge与 ParNew一个重要区别。 高吞吐量则可以高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如，那些执行批量处理、订单处理、工资支付、科学计算的应用程序。 Parallel 收集器在JDK1.6时提供了用于执行老年代垃圾收集的 Parallel Old 收集器，用来代替老年代的 Seral Old收集器。 Parallel Old 收集器采用了标记-压缩算法，但同样也是基于并行回收和&quot;Stop-the- World&quot;机制。 ​ 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间/(运行用户代码时间 垃圾收集时间)。比如：虚拟机总共运行了160分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 ​ 这种情况下，应用程序能容忍较高的暂停时间，因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的。吞吐量优先，意味着在单位时间内，ST的时间最短。在程序吞吐量优先的应用场景中，Parallel 收集器和 Parallel Old收集器的组合，在 Server模式下的内存回收性能很不错。在Java8中，默认是此垃圾收集器。 参数设置: -XX: +UseParallelGC手动指定年轻代使用Para1le1并行收集器执行内存回收任务 -XX: +UseParallelOldGC手动指定老年代都是使用并行回收收集器。分别适用于新生代和老年代。默认jdk8是开启的。上面两个参数，默认开启一个，另一个也会被开启。（互相激活） -XX: +ParallelGCThreads设置年轻代并行收集器的线程数。一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能。在默认情况下，当CPU数量小于8个，ParallelGCThreads的值等于CPU数量。当CPU数量大于8个，ParallelGCThreads的值等于3+[5*CPU_Count] / 8]。 -XX: MaxGCPauseMillis 设置垃圾收集器最大停顿时间(即STW的时间)。单位是毫秒。 为了尽可能地把停顿时间控制在 MaxGCPauseMillis 以内，收集器在工作时会调整Java堆大小或者其他一些参数。对于用户来讲，停顿时间越短体验越好。但是在服务器端，我们注重高并发，整体的吞吐量。所以服务器端适合 Paralle1,进行控制。该参数使用需谨慎。 -XX: GCTimeRatio 垃圾收集时间占总时间的比例 (=1/(N+1))用于衡量吞吐量的大小。 取值范围(0,100)。默认值99,也就是垃圾回收时间不超过1%。与前一个 -XX: MaxGCPauseMillis参数有一定矛盾性。暂停时间越长， Radio参数就容易超过设定的比例。 -XX: UseAdaptiveSizePoltcy 设置 Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小、Eden和 Survivor的比例、晋升老年代的对象年龄等参数会被自动调整，已达到在堆大小、吞吐量和停顿时间之间的平衡点。在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、日标的吞吐量( GCTimeRatio )和停顿时间( MaxGCPauseMillis ),让虚拟机自己完成调优工作。 CMS 低延迟 在JDK1.5时期， HotSpot推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器：CMS( Concurrent - Mark - Sweep)收集器，这款收集器是 HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。 CMS收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短（低延迟）就越适合与用户交互的程序，良好的响应速度能提升用户体验。目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 CMS的垃圾收集算法采用标记一清除算法，并且也会&quot;Stop-the-world 不幸的是，CMS作为老年代的收集器，却无法与JDK1.4.8中已经存在的新生代收集器Parallel Scavenge配合工作，所以在]DK1.5中使用CMS来收集老年代的时候，新生代只能选择 Pardew或者 Sera1收集器中的一个。在G1出现之前，CMS使用还是非常广泛的。一直到今天，仍然有很多系统使用 CMS GC。 工作原理 CMS整个过程比之前的收集器要复杂，整个过程分为4个主要阶段，即初始标记阶段、并发标记阶段、重新标记阶段和并发清除阶段。 **初始标记( Initial-Mark)阶段：**在这个阶段中，程序中所有的工作线程都将会因为“Stop-the-World”机制而出现短暂的暂停，这个阶段的主要任务仅仅只是标记出GC Roots能直接关联到的对象。一旦标记完成之后就会恢复之前被暂停的所有应用线程。由于直接关联对象比较小，所以这里的速度非常快。 **并发标记( Concurrent-Mark)阶段：**从 GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 **重新标记( Remark)阶段：**由于在并发标记阶段中，程序的工作线程会和垃圾收集线程同时运行或者交叉运行，因此为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。 **并发清除( Concurrent- Sweep)阶段：**此阶段清理删除掉标记阶段判断的己经死亡的对象，释放内存空间。由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。 特点与弊端 尽管CMS收集器采用的是并发回收（非独占式），但是在其初始化标记和再次标记这两个阶段中仍然需要执行“Stop-The-Wolrd”机制暂停程序中的工作线程，不过暂停时间并不会太长，因此可以说明目前所有的垃圾收集器都做不到完全不需要 “Stop-The-Wolrd”，只是尽可能地缩短暂停时间。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 另外，由于在垃圾收集阶段用户线程没有中断，所以在CMS回收过程中，还应该确保应用程序用户线程有足够的内存可用。因此，CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在CMS工作过程中依然有足够的空间支持应用程序运行。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“ Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用 Seral Old收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。 CMS收集器的垃圾收集算法采用的是标记一清除算法，这意味着每次执行完内存回收后，由于被执行内存回收的无用对象所占用的内存空间极有可能是不连续的一些内存块，不可避免地将会产生一些内存碎片。那么CMS在为新对象分配内存空间时，将无法使用指针碰撞( Bump the Pointer)技术，而只能够选择空闲列表( Free List)执行内存分配。为什么不使用Mark Compact？因为当并发清除的时候，用 Compact 整理内存的话，原来的用户线程使用的内存还怎么用呢？要保证用户线程能继续执行，前提是它运行的资源不受影响。 Mark Compact更适合“ Stop the World”这种场景下使用。 CMS的优点： 并发收集、低延迟 CMS的弊端： **1)会产生内存碎片，**导致并发清除后，用户线程可用的空间不足。在无法分配大对象的情况下，不得不提前触发Full GC。 **2)CMS收集器对CPU资源非常敏感。**在并发阶段，它虽然不会导致用户停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。 **3)CMS收集器无法处理浮动垃圾。**可能出现“ Concurrent Mode Failure&quot;失败而导致另一次Full GC的产生。在并发标记阶段由于程序的工作线程和垃圾收集线程是同时运行或者交叉运行的，那么在并发标记阶段如果产生新的垃圾对象，CMS将无法对这些垃圾对象进行标记，最终会导致这些新产生的垃圾对象没有被及时回收，从而只能在下一次执行GC时释放这些之前未被回收的内存空间。 参数设置 **-XX: UseConcMarkSweepGC **手动指定使用CMS收集器执行内存回收任务。开启该参数后会自动将 -XX: UseParNEWGC打开。即： ParNew( Young区用) +CMS(Old区用) Serial Old的组合。 -XX: CMSLnitiatingOccupanyFraction 设置堆内存使用率的阈值，一旦达到该阈值，便开始进行回收。JDK5及以前版本的默认值为68,即当老年代的空间使用率达到68%时，会执行一次CMS回收。JDK6及以上版本默认值为92%。如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低CMS的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。因此通过该选项便可以有效降低Full GC的执行次数。 **-XX: UseCMSCompactAtFullCollection **用于指定在执行完Full GC后对内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。 -XX: CMSFullGCsBeforeCompaction 设置在执行多少次Full GC后对内存空间进行压缩整理。 -XX: ParalellCMSThreads 设置CMS的线程数量。CMS默认启动的线程数是(ParalellCMSThreads+3)/4。ParalellCMSThreads是年轻代并行收集器的线程数。当CPU资源比较紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。 总结 小结： HotSpot有这么多的垃圾回收器，那么如果有人问， Serial GC、ParalleGC、Concurrent Mark Sweep GC.这三个GC有什么不同呢？ 请记住以下口令： 如果你想要最小化地使用内存和并行开销，请选 Serial GC; 如果你想要最大化应用程序的吞吐量，请选 Paralle GC; 如果你想要最小化GC的中断或停顿时间，请选 CMS GC。 JDK9新特性：CMS 被标记为 Deprecate(JEP291)。如果对JDK9及以上版本的 Hotspot,虚拟机使用参数 -XX: UseConcMarkSweepGC 来开启CMS收集器的话，用户会收到一个警告信息，提示CMS未来将会被废弃。 DK14新特性：删除CMS垃圾回收器(JEP363)移除了CMS垃圾收集器，如果在JDK14中使用 -XX: UseConcMarkSweepGC的话，JVM不会报错，只是给出一个warning信息，但是不会exit。JVM会自动回退以默认GC方式启动JVM。 G1 区域分代化 为什么名字叫做 Garbage First(G1)呢？ 因为G1是一个并行回收器，它把堆内存分割为很多不相关的区域( Region)（物理上不连续的）。使用不同的 Region来表示Eden、幸存者0区，幸存者1区，老年代等。 G1 GC有计划地避免在整个Java堆中进行全区域的垃圾收集。G1跟踪各个 Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region由于这种方式的侧重点在于回收垃圾最大量的区间( Region),所以我们给G1一个名字：垃圾优先(Garbage First)。 既然我们已经有了前面几个强大的GC,为什么还要发布 Garbage First(G1)GC? 原因就在于应用程序所应对的业务越来越庞大、复杂，用户越来越多，没有GC就不能保证应用程序正常进行，而经常造成STW的GC又跟不上实际的需求，所以才会不断地尝试对GC进行优化。G1( Garbage- First)垃圾回收器是在Java7 update4之后引入的一个新的垃圾回收器，是当今收集器技术发展的最前沿成果之一。 与此同时，为了适应现在不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间( pause time),同时兼顾良好的吞吐量。官方给G1设定的目标是在延退可控的情况下获得尽可能高的吞吐量，所以才担当起“全功能收集器”的重任与期望。 ​ G1( Garbage- First)是一款面向服务端应用的垃圾收集器，主要针对配备多核CPU及大容量内存的机器，以极高概率满足GC停顿时间的同时，还兼具高吞吐量的性能特征。 ​ 在JDK1.7版本正式启用，移除了 Experimental的标识，是JDK9以后的默认垃圾回收器，取代了CMS回收器以及Paralel Parallel Old组合。被 Oracle官方称为“全功能的垃圾收集器”。与此同时，CMS已经在JDK9中被标记为废弃( deprecated)。在jdk8中还不是默认的垃圾回收器，需要使用 -XX: UseG1GC来启用。 优势 与其他GC收集器相比，G1使用了全新的分区算法，其特点如下所示： 并行与并发 并行性：G1在回收期间，可以有多个GC线程同时工作，有效利用多核计算能力。此时用户线程STW 并发性：G1拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会在整个回收阶段发生完全阻塞应用程序的情况。 分代收集 从分代上看，G1依然属于分代型垃圾回收器，它会区分年轻代和老年代，年轻代依然有Eden区和 Survivor区。但从堆的结构上看，它不要求整个Eden区、年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。将堆空间分为若干个区域( Region),这些区域中包含了逻辑上的年轻代和老年代。和之前的各类回收器不同，它同时兼顾年轻代和老年代。对比其他回收器，或者工作在年轻代，或者工作在老年代； 空间整合 ​ CMS:“标记-清除”算法、内存碎片、若干次GC后进行一次碎片整理。G1将内存划分为一个个的 region。内存的回收是以 regiont作为基本单位的。Region之间是复制算法，但整体上实际可看作是标记一压缩（Mark- Compact算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。尤其是当Java堆非常大的时候，G1的优势更加明显。 可预测的停顿时间模型(即：软实时 soft real-time) ​ 这是G1相对于CMS的另一大优势，G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。由于分区的原因，G1可以只选取部分区域进行内存回收，这样缩小了回收的范围，因此对于全局停顿情况的发生也能得到较好的控制。 ​ G1跟踪各个 Region里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。保证了G1收集器在有限的时间内可以获取尽可能高的收集效率。相比于 CMS GC,G1未必能做到CMS在最好情况下的延时停顿，但是最差情况要好很多。 ​ 相较于CMS,G1还不具备全方位、压倒性优势。比如在用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用( Footprint)还是程序运行时的额外执行负载( overload)都要比CMS要高。从经验上来说，在小内存应用上CMS的表现大概率会优于G1,而G1在大内存应用上则发挥其优势。平衡点在6-8GB之间。 参数设置 -XX:+UseG1GC手动指定使用G1收集器执行内存回收任务 -XX:G1HeapRegionSize设置每个 Regionl的大小。值是2的幂，范围是1MB到32MB之间，目标是根据最小的Java堆大小划分出约2048个区域。默认是堆内存的1/2000 -XX: MaxGCPauseMilllis设置期望达到的最大GC停顿时间指标（JVM会尽力实现，但不保证达到）,默认值是200ms。 -XX:ParallelGCThreads设置STW工作线程数的值。最多设置为8。 -XX:ConcGCThreads设置并发标记的线程数。将n设置为并行垃圾回收线程数(ParallelGCThreadS)的1/4左右。 -XX: InitiatingHeapOccupancyPercent设置触发并发GC周期的Java。堆占用率阈值。超过此值，就触发GC。默认值是45。 G1回收器常见步骤 G1的设计原则就是简化JVM性能调优，开发人员只需要简单的三步即可完成调优 第一步：开启G1垃圾收集器 第二步：设置堆的最大内存 第三步：设置最大的停顿时间 G1中提供了三种垃圾回收模式： YoungGC、 Mixed GC和FullGC,在不同的条件下被触发。 使用场景 面向服务端应用，针对具有大内存、多处理器的机器。（在普通大小的堆里表现并不惊喜） 最主要的应用是需要低GC延迟，并具有大堆的应用程序提供解决方案；如：在堆大小约6GB或更大时，可预测的暂停时间可以低于8.5秒；（G1通过每次只清理一部分而不是全部的 Region的增量式清理来保证每次G停顿时间不会过长）用来替换掉]DK1.5中的CMS收集器 在下面的情况时，使用G1可能比CMS好： ①超过50%的Java堆被活动数据占用； ②对象分配频率或年代提升频率变化很大； ③GC停顿时间过长(长于6.5至1秒)； Hotspot垃圾收集器里，除了G1以外，其他的垃圾收集器使用内置的JVM线程执行。GC的多线程操作，而G1GC可以采用应用线程承担后台运行的GC工作，即当JVM的GC线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾回收过程。 Region ​ 使用G1收集器时，它将整个Java堆划分成约2048个大小相同的独立 Region块，每个 Region块大小根据堆空间的实际大小而定，整体被控制在1MB到32MB之间，且为2的N次幂，即1MB,2MB,4MB,8MB,16MB,32MB。可以通过**-XX:G1HeapRegionSize**设定。所有的 Region大小相同，且在JVM生命周期内不会被改变。 ​ 虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分 Region（不需要连续）的集合。通过 Region的动态分配方式实现逻辑上的连续。 ​ 一个 region有可能属于Eden, Survivor或者Old/ Tenured内存区域。但是 一个 region只可能属于一个角色。图中的E表示该 region属于Eden内存区域，S表示属于 Survivor内存区域，O表示属于o1d内存区域。图中空白的表示未使用的内存空间。 ​ G1垃圾收集器还增加了一种新的内存区域，叫做 Humongous内存区域，如图中的H块。主要用于存储大对象，如果超过1.5个 region,就放到H。 **设置H的原因：**对于堆中的大对象，默认直接会被分配到老年代，但是如果它是一个短期存在的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个 Humongous区。它用来专门存放大对象。如果一个H区装不下ー个大对象，那么G1会寻找连续的H区来存储。为了能找到连续的H区，有时候不得不启动Full GC.G1的大多数行为都把H区作为老年代的一部分来看待。 G1 回收过程 G1 GC的垃圾回收过程主要包括如下三个环节 年轻代GC( Young GC) 老年代并发标记过程( Concurrent Marking) 混合回收( Mixed GC) (如果需要，单线程、独占式、高强度的FullGC还是继续存在的。它针对GC的评估失败提供了一种失败保护机制，即强力回收。） ​ 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到 Survivor区间或者老年区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值(默认45%)时，开始老年代并发标记过程。标记完成马上开始混合回收过程。对于一个混合回收期，G1GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，G1的老年代回收器不需要整个老年代被回收，一次只需要扫描/回收小部分老年代的 Region就可以了。同时，这个老年代 Region是和年轻代一起被回收的。举个例子：一个Web服务器，Java进程最大堆内存为4G,每分钟响应1500个请求，每45秒钟会新分配大约2G的内存。G1会每45秒钟进行一次年轻代回收，每31个小时整个堆的使用率会达到45%,会开始老年代并发标记过程，标记完成后开始四到五次的混合回收。 Remembered Set 一个对象被不同区域引用的问题。一个 Region不可能是孤立的，一个 Region中的对象可能被其他任意 Region中对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确。在其他的分代收集器，也存在这样的问题(而G1更突出)。回收新生代也不得不同时扫描老年代？这样的话会降低 Minor GC的效率。 解决方法： 无论G1还是其他分代收集器，JVM都是使用 Remembered Set来避免全局扫描：每个 Region都有一个对应的 Remembered Set;每次 Reference类型数据写操作时，都会产生一个 Write Barrier暂时中断操作；然后检查将要写入的引用指向的对象是否和该 Referencea类型数据在不同的 Region(其他收集器：检査老年代对象是否引用了新生代对象）；如果不同，通过 Cardtable把相关引用信息记录到引用指向对象的所在 Region对应的 Remembered Set中；当进行垃圾收集时，在GC根节点的枚举范围加入 Remembered Set:就可以保证不进行全局扫描，也不会有遗漏。 回收过程一：年轻代GC ​ JVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。年轻代垃圾回收只会回收Eden区和 Survivor区。首先61停止应用程序的执行(Stop-The-World),G1创建回收集(Collection Set),回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和 Survivor区所有的内存分段。 然后开始如下回收过程： 第一阶段，扫描根。 根是指 static变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同RSet 记录的外部引用作为扫描存活对象的入口 第二阶段，更新RSet. 处理 dirty card queue（见备注）中的card,更新RSet。此阶段完成后，RSet可以准确的反 映老年代对所在的内存分段中对象的引用。 第三阶段，处理RSet. 识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。 第四阶段，复制对象。 此阶段，对象树被遍历，Eden区内存段中存活的对象会被复制到 Survivor区中空的内存分段，Survivor区内存段中存活的对象如果年龄未达阈值，年龄会加1,达到阀值会被会被复制到0ld区中空的内存分段。如果 Survivor空间不够，Eden空间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理Soft,Weak, Phantom,Fina1, JNI Weak等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 回收过程二：并发标记过程 初始标记阶段：标记从根节点直接可达的对象。这个阶段是STW的，并且会触发一次年轻代GC. 根区域扫描( Root Region Scanning):G1 GC扫描 Survivor区直接可达的老年代区域对象，并标记被引用的对象。这一过程必须在 young GC之前完成 并发标记( Concurrent Marking):在整个堆中进行并发标记（和应用程序并发执行），此过程可能被 young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性（区域中存活对象的比例）。 再次标记( Remark):由于应用程序持续进行，需要修正上一次的标记结果。是STW的。G1中采用了比CMS更快的初始快照算法： snapshot-at-the- beginning(SATB)。 独占清理(cleanup,STW):计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是ST的。这个阶段并不会实际上去做垃圾的收集并发清理阶段：识别并清理完全空闲的区域。 回收阶段三：混合回收 ​ 当越来越多的对象晋升到老年代。old region时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即 Mixed GC,该算法并不是一个Old GC,除了回收整个 Young Region,还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集，从而可以对垃圾回收的耗时时间进行控制。也要注意的是 Mixed GC并不是Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存分段被回收了，部分为垃圾的内存分段被计算了出来。默认情况下，这些老年代的内存分段会分8次(可以通过XX:G1 MixedGCCountTarget设置)被回收。 混合回收的回收集(Collection Set)包括八分之一的老年代内存分段，Eden区内存分段， Survivor区内存分段。混合回收的算法和年轻代回收的算法完全一样，只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1 MixedGCliveThresholPercent,默认为65%,意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行8次。有一个阈值-XX:G1HeapWastePercent,默认值为10%,意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%,则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。 总结 G1的初衷就是要避免Full GC的出现。但是如果上述方式不能正常工作，G1会停止应用程序的执行(Stop-The-World),使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢？比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用则会回退到Full GC,这种情况可以通过増大内存解决。导致G1 Full GC的原因可能有两个：Evacuation的时候没有足够的to- space来存放晋升的对象。并发处理过程完成之前空间耗尽。 ​ 从 Oracle官方透露出来的信息可获知，回收阶段( Evacuation)其实本也有想过设计成与用户程序一起并发执行，但这件事情做起来比较复杂，考虑到G1只是回收一部分 Reglon,停顿时间是用户可控制的，所以并不迫切去实现，而选择把这个特性放到了G1之后出现的低延迟垃圾收集器(即ZGC)中。另外，还考虑到G1不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案。 优化建议 年轻代大小 避免使用-Xmn或-XX: NewRatio等相关选项显式设置年轻代大小。固定年轻代的大小会覆盖暂停时间目标暂停时间目标不要太过严苛 G1 GC的吞吐量目标是90%的应用程序时间和1%的垃圾回收时间评估G1GC的吞吐量时，暂停时间目标不要太严苛。目标太过严苛表 示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 垃圾回收器总结 Java垃圾收集器的配置对于]W써优化来说是一个很重要的选择，选择合适的垃圾收集器可以让V的性能有一个很大的提升。 怎么选择垃圾收集器？ 优先调整堆的大小让JVM自适应完成 如果内存小于100M,使用串行收集器 如果是单核、单机程序，并且没有停顿时间的要求，串行收集器 如果是多CPU、需要高吞吐量、允许停顿时间超过1秒，选择并行或者JVM自己选择 如果是多CPU、追求低停顿时间，需快速响应（比如延迟不能超过1秒，如互联网应用），使用并发收集器 官方推荐G1,性能高。现在互联网的项目，基本都是使用G1。 最后需要明确一个观点 1.没有最好的收集器，更没有万能的收集； 2.调优永远是针对特定场景、特定需求，不存在一劳永逸的收集器 对于垃圾收集，面试官可以循序渐进从理论、实践各种角度深入，也未必是要求面试者什么都懂。但如果你懂得原理，一定会成为面试中的加分项。 这里较通用、基础性的部分如下 垃圾收集的算法有哪些？如何判断一个对象是否可以回收？ 垃圾收集器工作的基本流程。 另外，大家需要多关注垃圾回收器这一章的各种常用的参数。 GC日志分析 通过阅读GC日志，我们可以了解Java虚拟机内存分配与回收策略。 内存分配与垃圾回收的参数列表 -XX:+PriintGC 输出GC日志。类似：- verbose:GC -XX:+ PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps输出GC的时间戳（以基准时间的形式） -XX:+ PrintGCDateStamps输出GC的时间戳（以日期的形式，如2013-05-04T21:53:59.234+0800) -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -XlogGC:../logs/GC.log日志文件的输出路径 &quot;[GC&quot;和&quot;[Full GC&quot;说明了这次垃圾收集的停顿类型,如果有&quot;Full&quot;则说明GC发生了Stop The World 使用 Serial收集器在新生代的名字是 Default New Generation,因此显示的是&quot;[ ParNew 使用 ParNew收集器在新生代的名字会变成&quot;[Pardew&quot;,意思是&quot; Parallel New Generation&quot; 使用Parallel Scavenge收集器在新生代的名字是&quot;[ Psyounggen&quot; 老年代的收集和新生代道理一样,名字也是收集器决定的 使用G1收集器的话,会显示为&quot; garbage- first heap&quot; Allocation Failure 表明本次引起GC的原因是因为在年轻代中没有足够的空间能够存储新的数据了。 [ PSZoungGen:5986K-&gt;696K(8704K)]5986K--704K(9216K) 中括号内：GC回收前年轻代大小，回收后大小，（年轻代总大小） 括号外：GC回收前年轻代和老年代大小，回收后大小，（年轻代和老年代总大小） user代表用户态回收耗时，sys内核态回收耗时，rea实际耗时。由于多核的原因，时间总和可能会超过real时间 日志分析工具 可以用一些工具去分析这些GC日志。常用的日志分析工具有： GCVlewer、 GCEsay、 GCHisto、 GCCLogViewer、Hpjmeter、 garbagecat等。 垃圾回收器的新时代发展 GC仍然处于飞速发展之中，目前的默认选项G1GC在不断的进行改进，很多我们原来认为的缺点，例如串行的FullGC、 Card Table扫描的低效等，都已经被大幅改进，例如，JDK10以后，FullGC已经是并行运行，在很多场景下，其表现还略优于 ParallelGC的并行FullGC实现。即使是 SerialGC,虽然比较古老，但是简单的设计和实现未必就是过时的，它本身的开销，不管是GC相关数据结构的开销，还是线程的开销，都是非常小的，所以随着云计算的兴起，在 Serverless:等新的应用场景下， SeralGC找到了新的舞台。比较不幸的是 CMS GC,因为其算法的理论缺陷等原因，虽然现在还有非常大的用户群体，但在3DK9中已经被标记为废弃，并在JDK14版本中移除。 Open JDK12的 Shenandoah GC:低停颠时间的cc（实验性） Shenandoah,无疑是众多GC中最孤独的一个。是第一款不由 Oracle公司团队领导开发的 Hotspot垃圾收集器。不可避免的受到官方的排挤。比如号称openJDK和oracleJDK没有区别的oracle公司仍拒绝在oracleJDK12中支持 Shenandoah。Shenandoah垃圾回收器最初由 Redhat进行的一项垃圾收集器研究项目 Pauseless GC的实现，旨在针对]WM上的内存回收实现低停顿的需求。在2014年贡献给OPENJDK. Red Hat研发 Shenandoah团队对外宣称， Shenandoah垃圾回收器的暂停时间与堆大小无关，这意味着无论将堆设置为200MB还是200GB,99.9%的目标都可以把垃圾收集的停顿时间限制在十毫秒以内。不过实际使用性能将取决于实际工作堆的大小和工作负载。 总结： Shenandoah GC的弱项：高运行负担下的吞吐量下降。 Shenandoah GC的强项：低延迟时间。 Shenandoah GC的工作过程大致分为九个阶段，这里就不再赘述。在之前 Java12新特性视频里有过介绍。 ZGC ​ ZGC与 Shenandoah目标高度相似，在尽可能对吞吐量影响不大的前提下实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。《深入理解Java虚拟机》一书中这样定义zGC:2GC收集器是一款基于Region内存布局的，（暂时）不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记一压缩算法的，以低延迟为首要目标的一款垃圾收集器2GC的工作过程可以分为4个阶段：并发标记一并发预备重分配一并发重分配并发重映射等。ZGC几乎在所有地方并发执行的，除了初始标记的是STW的。所以停顿时间几乎就耗费在初始标记上，这部分的实际时间是非常少的。 JBP364:ZGC应用在 macos上 JEP365:ZGC应用在 Windows上 JDK14之前，ZGC仅inuxオ支持。 ・尽管许多使用ZGC的用户都使用类エinux的环境，但在 Windows和 macos上，人们也需要ZGC进行开发部署和测试。许多桌面应用也可以从ZGC中受益。因此，ZGC特性被移植到了 Windows和 macos上。 ・现在mac或 Windows上也能使用ZGC了，示例如下：XX: +UnlockExperimentalVMOptions -XX: +UseZGC 复习【processon作图，完善流程过程中再次熟悉】 完成：JVM总图 - ProcessOn","link":"/2021/11/10/Draft/2021/JVM%20%E4%B8%8E%E4%B8%8A%E5%B1%82%E6%8A%80%E6%9C%AF/"},{"title":"Java基础深入","text":"新学四问 WHY【与前代优化了什么，弥补了什么空白】筑基，越深越稳 WHAT【框架，思维导图，主题框架】容器、并发、IO、NET、JVM、其他 HOW【如何记忆，学习资源】:https://github.com/CyC2018、博客完善、书籍深入、ANKI稳固 LEVEL【不是每个都学精】精通 建议：熟悉基础数据结构，二进制移位运算，耐心+时间 基础数据结构【Java】 哈希 **核心理论：**Hash也称散列、哈希，对应的英文都是Hash。基本原理就是把任意长度的输入，通过Hash算法变成固定长度的输出。这个映射的规则就是对应的Hash算法，而原始数据映射后的二进制串就是哈希值。 Hash的特点： 1.hash值不可以反向推导出原始的数据 2.输入数据的微小变化会得到完全不同的hash值，相同的数据会得到相同的值 3.哈希算法的执行效率要高效，长的文本也能快速地计算出哈希值 4.hash算法的冲突概率要小 由于hash的原理是将输入空间的值映射成hash空间内，而hash值的空间远小于输入的空间。根据抽屉原理，一定会存在不同的输入被映射成相同输出的情况。抽屉原理：桌上有十个苹果，要把这十个苹果放到九个抽屉里，无论怎样放，我们会发现至少会有一个抽屉里面放不少于两个苹果，这一现象就是我们所说的抽屉原理。 红黑树 R-B Tree，全称是Red-Black Tree，又称为“红黑树”，它一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红(Red)或黑(Black)。 红黑树的特性: （1）每个节点或者是黑色，或者是红色。 （2）根节点是黑色。 （3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！] （4）如果一个节点是红色的，则它的子节点必须是黑色的。 （5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 注意： (01) 特性(3)中的叶子节点，是只为空(NIL或null)的节点。 (02) 特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。 红黑树示意图如下： 计划：【一天一类11.17】 进度：【2.23NET】 如何阅读源码？ 方法： 1.实践断点 2.顺序：接口$\\longrightarrow$实现类（(方法【属性--》判断】)构造器方法$\\longrightarrow$常用方法$\\longrightarrow$其他方法） 3.工具：IDEA 的 Diagram 图标化查看继承结构，所有类的所有结构，双击跳转对应类.左下角Structure查看类结构。ctrl+H查看继承树。 4.注释：查看注释 5.快捷键： 6.根据已知看未知：比如已知底层看源码，线程安全看原理 项目 JDK 常用包：【java.io java.lang java.util】 【java.lang.reflect java.net javax.net.* java.nio.* java.util.concurrent.】 框架 容器 学习流程 进度： collection$\\longrightarrow$list$\\longrightarrow$set$\\longrightarrow$map$\\longrightarrow$queue 方法： 总结特点$\\longrightarrow$根据特点查看源码$\\longrightarrow$笔记 总结 总关系图 表格总结 集合名称 结构 是否可重复 是否线程安全/不保证同步 是否可为空 是否有序 增 删 改 查 List Vector 动态数组【Object [ ] 】 可重 安 多空 有序 ArrayList 动态数组【Object [ ] 】 可重 不安 多空 有序 LinkedList 双向循环链表 可重 不安 多空 有序 Set TreeSet 红黑树【red-black tree】 不重 不安 单空 有序 O(logN) HashSet 哈希表【HashMap】 不重 不安 单空 无序 O(1) LinkedHashSet 双向链表【LinkedHashMap】 不重 不安 单空 有序 Map HashMap 数组+链表（1.8之后链表长度大于默认8且数组长度大于64转换为红黑树），散列（哈希）表 key唯一，值可重 不安 多空 无序 TreeMap 红黑树（自平衡的排序二叉树） key唯一，值可重 不安 有序 LinkedHashMap 双向链表 不安 多空 有序 ConcurrentHashMap 1.8之后 数组、链表/红黑树 适合多线程 安 Hashtable 【保留】 安 Queue PriorityQueue 二叉小顶堆，每次取出权值最小元素 安 否 Collection 集合根接口，定义子类基础操作 遍历方式 foreach Iterator listiterator:区别：List专用、遍历时可添加元素、可逆向遍历、可定位当前索引位置、可修改遍历对象。 12345Collection&lt;Person&gt; persons = new ArrayList&lt;Person&gt;();Iterator iterator = persons.iterator();while (iterator.hasNext()) { System.out.println(iterator.next); } aggregate operations 123456789Collection&lt;Person&gt; persons = new ArrayList&lt;Person&gt;();persons .stream() .forEach(new Consumer&lt;Person&gt;() { @Override public void accept(Person person) { System.out.println(person.name); } }); 1234567891011121314//在 JDK 8 以后，推荐使用聚合操作对一个集合进行操作。聚合操作通常和 lambda 表达式结合使用，让代码看起来更简洁（因此可能更难理解）。下面举几个简单的栗子：//1.使用流来遍历一个 ShapesCollection，然后输出红色的元素：myShapesCollection.stream() .filter(e -&gt; e.getColor() == Color.RED) .forEach(e -&gt; System.out.println(e.getName()));//你还可以获取一个并行流（parallelStream），当集合元素很多时使用并发可以提高效率：myShapesCollection.parallelStream() .filter(e -&gt; e.getColor() == Color.RED) .forEach(e -&gt; System.out.println(e.getName())); //聚合操作还有很多操作集合的方法，比如说你想把 Collection 中的元素都转成 String 对象，然后把它们 连起来：String joined = elements.stream() .map(Object::toString) .collect(Collectors.joining(&quot;, &quot;));//Thanks:https://blog.csdn.net/u011240877/article/details/52773577 List 插入有序可重可多空 Vector 底层数组，同步，同步让其比AL慢，内存不够默认扩100% 特点 原因（源码） 与 ArrayList 相似 线程安全 方法中有synchronized，所以性能不如ArrayList 动态扩容为原来的2倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); 可排序扩容为原来两倍 123456789101112private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; //扩容为原来两倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);} ArrayList 特点 原因（源码） 底层数组 new Object[initialCapacity]; 查优，尾增删快，其他地方增删慢 底层数组 动态扩容原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 线程不安全 方法中没有synchronized/**This class is roughly equivalent to * Vector, except that it is unsynchronized.*/ 支持快速随机访问 RandomAccess 动态扩容 123456789101112131415161718/** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */ private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; //====== 动态扩容：位运算 &gt;&gt;：右移运算符 oldCapacity &gt;&gt; 1 ---》 M &gt;&gt; n = M / 2^n 即扩容50%====== int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } 参考：位运算：M &gt;&gt; n = M / 2^n 总结 LinkedList 特点 原因（源码） 底层双向链表，增删优，可操作头尾，用作栈、（双向）队列 底层双向链表 可插入空 底层双向链表 查询慢 底层双向链表，会遍历整个链表 线程不安全 方法中没有synchronized 底层双向链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172 /** * Returns the (non-null) Node at the specified element index. */ Node&lt;E&gt; node(int index) { // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) { Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; } }/** * Inserts all of the elements in the specified collection into this * list, starting at the specified position. Shifts the element * currently at that position (if any) and any subsequent elements to * the right (increases their indices). The new elements will appear * in the list in the order that they are returned by the * specified collection's iterator. * * @param index index at which to insert the first element * from the specified collection * @param c collection containing elements to be added to this list * @return {@code true} if this list changed as a result of the call * @throws IndexOutOfBoundsException {@inheritDoc} * @throws NullPointerException if the specified collection is null */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) { checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; Node&lt;E&gt; pred, succ; if (index == size) { succ = null; pred = last; } else { succ = node(index); pred = succ.prev; } for (Object o : a) { @SuppressWarnings(&quot;unchecked&quot;) E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; } if (succ == null) { last = pred; } else { pred.next = succ; succ.prev = pred; } size += numNew; modCount++; return true; } 总结 Stack 后进先出的堆栈 Set 无序不重单空，常用于去重 TreeSet 特点 原因（源码） 底层TreeMap 底层、可排序 可排序【自然排序，定制排序】 底层、可排序 线程不安全 方法中没有synchronized 底层、可排序底层、可排序 1234567891011121314151617181920212223public interface NavigableSet&lt;E&gt; extends SortedSet&lt;E&gt; {}public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable{}//可以对对象元素进行排序，但是自定义类需要实现comparable接口，重写comparaTo() 方法,否则：{@code ClassCastException}报ClassCastException异常。所有元素必须可以相互比较（相同类型），否则将会报类型转换异常ClassCastExection /** * Constructs a new, empty tree set, sorted according to the * natural ordering of its elements. All elements inserted into * the set must implement the {@link Comparable} interface. * Furthermore, all such elements must be &lt;i&gt;mutually * comparable&lt;/i&gt;: {@code e1.compareTo(e2)} must not throw a * {@code ClassCastException} for any elements {@code e1} and * {@code e2} in the set. If the user attempts to add an element * to the set that violates this constraint (for example, the user * attempts to add a string element to a set whose elements are * integers), the {@code add} call will throw a * {@code ClassCastException}. */ public TreeSet() { this(new TreeMap&lt;E,Object&gt;()); } TreeSet的排列顺序与重写的compareTo()方法的返回值有关。 return 0:元素每次进行比较，都认为是相同的元素，这是就不再向TreeSet里面插入除第一个元素以外的元素，所以TreeSet中就只插入了一个元素。 return 1:元素每次进行比较，都认为新插入的元素比上一个元素大，于是二叉树存储时，会储存在根的右侧，读取时就是正序排列，先进先出。 return -1:元素每次进行比较，都认为新插入的元素比上一个元素小，于是二叉树存储时，会储存在根的左侧，读取时就是倒序排列，先进后出。 HashSet 学习前最好先学习 HashMap 特点 原因（源码） 底层HashMap 底层HashMap 可单空 Map的key唯一 善存取 散列表定位 无序 没继承SortedSet 不安 方法中没有synchronized 不重复 不重复 底层HashMap 123456789101112131415161718192021 private transient HashMap&lt;E,Object&gt; map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object();/** * Constructs a new set containing the elements in the specified * collection. The &lt;tt&gt;HashMap&lt;/tt&gt; is created with default load factor * (0.75) and an initial capacity sufficient to contain the elements in * the specified collection. * * @param c the collection whose elements are to be placed into this set * @throws NullPointerException if the specified collection is null */public HashSet(Collection&lt;? extends E&gt; c) { //初始16，加载因子0.75 map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c);}//HashSet中的元素都存放在HashMap的key上面，而value中的值都是统一的一个固定对象private static final Object PRESENT = new Object(); 不重复 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * Adds the specified element to this set if it is not already present. * More formally, adds the specified element &lt;tt&gt;e&lt;/tt&gt; to this set if * this set contains no element &lt;tt&gt;e2&lt;/tt&gt; such that * &lt;tt&gt;(e==null&amp;nbsp;?&amp;nbsp;e2==null&amp;nbsp;:&amp;nbsp;e.equals(e2))&lt;/tt&gt;. * If this set already contains the element, the call leaves the set * unchanged and returns &lt;tt&gt;false&lt;/tt&gt;. * * @param e element to be added to this set * @return &lt;tt&gt;true&lt;/tt&gt; if this set did not already contain the specified * element */public boolean add(E e) { return map.put(e, PRESENT)==null;} /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) { return putVal(hash(key), key, value, false, true);} /** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;} LinkedHashSet 特点 原因（源码） 底层链表【LinkedHashMap】 底层 不安 方法中没有synchronized 插入有序 LinkedHashSet在遍历时获得的迭代器是LinkedHashSet所实现的LinkedKeyIterator，它的遍历是从双向链表头开始顺序遍历，实现了有序输出。 不重复 类似不重复 底层 1234567891011121314151617181920212223242526272829303132333435/** * Constructs a new linked hash set with the same elements as the * specified collection. The linked hash set is created with an initial * capacity sufficient to hold the elements in the specified collection * and the default load factor (0.75). * * @param c the collection whose elements are to be placed into * this set * @throws NullPointerException if the specified collection is null */ public LinkedHashSet(Collection&lt;? extends E&gt; c) { super(Math.max(2*c.size(), 11), .75f, true); addAll(c); } public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable {} /** * Constructs a new, empty linked hash set. (This package private * constructor is only used by LinkedHashSet.) The backing * HashMap instance is a LinkedHashMap with the specified initial * capacity and the specified load factor. * * @param initialCapacity the initial capacity of the hash map * @param loadFactor the load factor of the hash map * @param dummy ignored (distinguishes this * constructor from other int, float constructor.) * @throws IllegalArgumentException if the initial capacity is less * than zero, or if the load factor is nonpositive */ HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } Map 键值对，键不重 HashMap 推荐资源：博客1 特点 原因（源码） 底层为数组称之为哈希桶，每个桶里面放的是链表，链表中的每个节点，就是哈希表中的每个元素。JDK8后，链表容量大于8且桶的容量大于64，转化成红黑树 底层 线程不安全 方法中没有synchronized 默认长度16 初始容量 扩容为原来的两倍 扩容 无序 无序,可空 可多空 无序,可空 底层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475 /** Node 单向链表 * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; //碰撞之后形成链表 Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + &quot;=&quot; + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; }}/**红黑树 * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn * extends Node) so can be used as extension of either regular or * linked node. */static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; { TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) { super(hash, key, val, next); } /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() { for (TreeNode&lt;K,V&gt; r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } } ......} 扩容 参考 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118 /** 为什么需要扩容：哈希冲突导致的链化影像查找效率，数组以空间换时间 由power-of-two expansion newCap = oldCap &lt;&lt; 1 可知扩容为原来的两倍1010 十进制：10 原始数 number10100 十进制：20 左移一位 number = number &lt;&lt; 1; 1010 十进制：10 右移一位 number = number &gt;&gt; 1; 即 oldCap &lt;&lt; 1 即二进制向左移动两位：oldCap &lt;&lt; 1=oldCap*2 同理oldCap &gt;&gt; 1=oldCap/2 补充&gt;&gt;&gt;：无符号右移，忽略符号位，空位都以0补齐 * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * * @return the table */ final Node&lt;K,V&gt;[] resize() { //oldTab扩容前哈希表 Node&lt;K,V&gt;[] oldTab = table; //扩容之前数组长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; //扩容之前的阈值，触发本次扩容的阈值 int oldThr = threshold; //扩容之后的数组大小 //下次触发扩容的条件 int newCap, newThr = 0; //散列表已初始化，这是一次正常的扩容 if (oldCap &gt; 0) { //扩容之前数组大小达到最大阈值，则不扩容且设扩容条件为int最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } //oldCap左移一位实现数值翻倍，并且复制给newCap，newCap小于最大限制且扩容之前的数组长度 &gt;= 16 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } //oldCap==0,说明HashMap中的散列表是null //1.new HashMap（initCap，loadFactor）； //2.new HashMap（initCap）； //3.new HashMap（map）；且map有数据 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; //oldCap==0，oldThr==0 //new HashMap（map）; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;//16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//12 } //newThr为0时，通过newCap和loadFactor计算出一个newThr if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; //第一次或创建一个更大的数组 @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //扩容前数组不为空 if (oldTab != null) { for (int j = 0; j &lt; oldCap; ++j) { //当前node节点 Node&lt;K,V&gt; e; //当前桶位中有数据，但是数据具体是单个数据或链表或红黑树还不明确 if ((e = oldTab[j]) != null) { //方便JVM GC回收 oldTab[j] = null; //当前节点为单数据 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //当前节点已经树化 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //当前节点为链表 else { // preserve order //loHead低位链表，存放在扩容之后的数组的下标位置，与当前数组的下标位置一致。 //hiHead高位链表，存放在扩容之后的数组的下标位置为当前数组的下标位置+扩容之前的长度。 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); //低位链表有数据 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } 初始容量 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/**默认（缺省）数组长度 * The default initial capacity - MUST be a power of two. */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /**最大数组长度 * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /**缺省负载因子大小 * The load factor used when none specified in constructor. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /**转换为红黑树的单个链表元素阈值 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash);看下面方法 } * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */ static final int TREEIFY_THRESHOLD = 8;/** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. */ final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { TreeNode&lt;K,V&gt; hd = null, tl = null; do { TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } /**从红黑树降为链表的阈值 * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. */ static final int UNTREEIFY_THRESHOLD = 6; /**转换为红黑树的数组阈值 * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; /** 哈系桶（表） * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */ transient Node&lt;K,V&gt;[] table; /** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; /**当前哈希表元素个数 * The number of key-value mappings contained in this map. */ transient int size; /**修改次数，不包含替换 * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). */ transient int modCount; /**扩容阈值，当哈希表元素超过阈值时触发扩容， * The next size value at which to resize (capacity * load factor). * * @serial */ // (The javadoc description is true upon serialization. // Additionally, if the table array has not been allocated, this // field holds the initial array capacity, or zero signifying // DEFAULT_INITIAL_CAPACITY.) int threshold; /**负载因子，threshold = capacity * loadFactor * The load factor for the hash table. * * @serial */ final float loadFactor; 无序,可空 未解决问题:为什么有时候看起来有序？ 答： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124 /** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */ public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } //====Key为null情况//假设 putVal(hash(null), null, value, false, true);//即 putVal(0, 0, value, false, true);//由 if ((p = tab[i = (n - 1) &amp; hash]) == null)// tab[i] = newNode(hash, key, value, null);//得tab[0]==null --&gt; tab[0] = newNode(hash, key, value, null);//key为null放在tab[0] /** * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don't benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. *///作用：在table长度不长的情况下，能让key的hash值的高16位也参与路由运算,//异或：同0异1//h=0b 0010 0101 1010 1100 0011 1111 0010 1110// 0b 0010 0101 1010 1100 0011 1111 0010 1110// ^// 0b 0000 0000 0000 0000 0010 0101 1010 1100//=&gt; 0010 0101 1010 1100 0001 1010 1000 0010 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); }/** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * onlyIfAbsent true==》key存在，不插入 * @param onlyIfAbsent if true, don't change existing value * @param evict if false, the table is in creation mode. * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { //tab 当前散列表 //p 当前散列表元素 //n 当前散列表数组长度 //i 路由寻址结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //延迟初始化，第一次调用putVal时会初始化hashMap对象中最消耗内存的散列表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //最简单情况，寻址找到的桶位为null，直接放入数据 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { //e 不为null 找到了一个与当前要插入的key-value一致的 key元素 Node&lt;K,V&gt; e; K k; //表示桶位中的该元素与当前插入元素key完全一致，后续将替换 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //该元素已经树化时 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //链表时且不等于头元素，元素依次比较 else { for (int binCount = 0; ; ++binCount) { //最后一个元素还找不到同插入key相同的node if ((e = p.next) == null) { //插入最后 p.next = newNode(hash, key, value, null); //链表元素数量大于8，树化 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } //链表中找到同插入key的node，替换 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } //上面的e存在替换值 if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } //记录修改次数，替换不算 ++modCount; //插入后容量大于扩容阈值则扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; } GET() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Returns the value to which the specified key is mapped, * or {@code null} if this map contains no mapping for the key. * * &lt;p&gt;More formally, if this map contains a mapping from a key * {@code k} to a value {@code v} such that {@code (key==null ? k==null : * key.equals(k))}, then this method returns {@code v}; otherwise * it returns {@code null}. (There can be at most one such mapping.) * * &lt;p&gt;A return value of {@code null} does not &lt;i&gt;necessarily&lt;/i&gt; * indicate that the map contains no mapping for the key; it's also * possible that the map explicitly maps the key to {@code null}. * The {@link #containsKey containsKey} operation may be used to * distinguish these two cases. * * @see #put(Object, Object) */public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;}/** * Implements Map.get and related methods. * * @param hash hash for key * @param key the key * @return the node, or null if none */final Node&lt;K,V&gt; getNode(int hash, Object key) { //tab:引用当前 hashmap的散列表 //first:桶位中的头元素 //e:临时node元素 //n: table数组长度 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { //第一种情况：定位出来的桶位元素即为咱们要get的数据 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //说明当前桶位不止一个元素，可能是链表也可能是红黑树 if ((e = first.next) != null) { //第二种情况，桶升级成了红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //桶形成了链表 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null;} Remove（） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * Implements Map.remove and related methods. * * @param hash hash for key * @param key the key * @param value the value to match if matchValue, else ignored * @param matchValue if true only remove if value is equal * @param movable if false do not move other nodes while removing * @return the node, or null if none */final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { //tab:引用当前 hashmap中的散列表 //p:当前node元素 //n:表示散列表数组长度 //index:表示寻址结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) { //说明路由的桶位是有数据的，需要进行查找操作，并且删除 //node:查找到的结果 //e:当前Node的下一个元素 Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) { //说明，当前桶位要么是链表要么是红黑树 if (p instanceof TreeNode)//判断当前桶位是否升级为红黑树了 //第二种情况：红黑树查找 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else { //第三种链表 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = e; break; } p = e; } while ((e = e.next) != null); } } //判断node不为空的话，说明按照key査找到需要删除的数据了 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) { //第一种情況：node是树节点，说明需要进行树节点移除操作 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //第二种情況：桶位元素即为査找结果，则将该元素的下一个元素放至桶位中 else if (node == p) tab[index] = node.next; else //第三种情況：将当前元素p的下一个元素设置成要删除元素的下一个元素 p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; } } return null;} 构造方法 ：&gt;&gt;&gt;无符号右移，忽略符号位，空位都以0补齐 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/* ---------------- Public operations -------------- */ /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) { //限制数组与loadFactor 0&lt;initialCapacity&lt;MAXIMUM_CAPACITY // loadFactor&gt;0 if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; //tableSizeFor 返回一个大于等于当前cap的一个数字，并且这个数字一定是2的次方数，源码如下： this.threshold = tableSizeFor(initialCapacity); } /**返回一个大于等于当前cap的一个数字，并且这个数字一定是2的次方数： * &gt;&gt;&gt;：无符号右移，忽略符号位，空位都以0补齐 * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */ public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } /** * Constructs a new &lt;tt&gt;HashMap&lt;/tt&gt; with the same mappings as the * specified &lt;tt&gt;Map&lt;/tt&gt;. The &lt;tt&gt;HashMap&lt;/tt&gt; is created with * default load factor (0.75) and an initial capacity sufficient to * hold the mappings in the specified &lt;tt&gt;Map&lt;/tt&gt;. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public HashMap(Map&lt;? extends K, ? extends V&gt; m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); } LinkedHashMap 特点 原因（源码） 底层哈希表+双向链表，HashMap子类 底层 线程不安全 方法中没有synchronized 有序 有序 可空键值 同HashMap 底层 put、resize扩容均使用HashMap的方法，拥有HashMap所有特性。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt;{} /** * HashMap.Node subclass for normal LinkedHashMap entries. */ static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); } } /**双向链表头 * The head (eldest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; head; /**双向链表尾 * The tail (youngest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; tail; /** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */ final boolean accessOrder;/** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the specified initial capacity and a default load factor (0.75). * * @param initialCapacity the initial capacity * @throws IllegalArgumentException if the initial capacity is negative */ public LinkedHashMap(int initialCapacity) { super(initialCapacity); accessOrder = false; } /** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the default initial capacity (16) and load factor (0.75). */ public LinkedHashMap() { super(); accessOrder = false; } /** * Constructs an insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with * the same mappings as the specified map. The &lt;tt&gt;LinkedHashMap&lt;/tt&gt; * instance is created with a default load factor (0.75) and an initial * capacity sufficient to hold the mappings in the specified map. * * @param m the map whose mappings are to be placed in this map * @throws NullPointerException if the specified map is null */ public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) { super(); accessOrder = false; putMapEntries(m, false); } /** * Constructs an empty &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance with the * specified initial capacity, load factor and ordering mode. * * @param initialCapacity the initial capacity * @param loadFactor the load factor *=====此构造方法accessOrder为true时实现了按访问顺序存储元素====== *@param accessOrder the ordering mode - &lt;tt&gt;true&lt;/tt&gt; for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder; } 有序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105 /** * HashMap.Node subclass for normal LinkedHashMap entries. */ static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { //记录相邻两个key-value对象 Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); } } /**双向链表头 * The head (eldest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; head; /**双向链表尾 * The tail (youngest) of the doubly linked list. */ transient LinkedHashMap.Entry&lt;K,V&gt; tail;/**====false则按插入顺序存储元素，如果是true则按访问顺序存储元素 * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */ final boolean accessOrder; Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) { LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p; } /** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */ final boolean accessOrder; // internal utilities//实现有序的方法//举例:放入1,2,3//1进：head=tail=1 last=null // 3进：tail=last=1--》tail=3 3.before=1 --》 1.after=3 // 2进：tail=last=3--》tail=2 2.before=3 --》 3.after=2 // link at the end of list private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) { LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; if (last == null) head = p; else { p.before = last; last.after = p; } }//HashMap的Node===============/** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */ static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + &quot;=&quot; + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; } } TreeMap 特点 原因（源码） 底层二叉树【红黑树】 底层红黑树 线程不安全 方法中没有synchronized 有序，比较通过key 底层红黑树 底层红黑树 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697//NavigableMap接口提供针对Key的有序访问，public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable, java.io.Serializable{ /** * The comparator used to maintain order in this tree map, or * null if it uses the natural ordering of its keys. * * @serial */ private final Comparator&lt;? super K&gt; comparator; private transient Entry&lt;K,V&gt; root; /** * The number of entries in the tree */ private transient int size = 0; /** * The number of structural modifications to the tree. */ private transient int modCount = 0; /** * Node in the Tree. Doubles as a means to pass key-value pairs back to * user (see Map.Entry). */ static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color = BLACK; /** * Make a new cell with given key, value, and parent, and with * {@code null} child links, and BLACK color. */ Entry(K key, V value, Entry&lt;K,V&gt; parent) { this.key = key; this.value = value; this.parent = parent; } /** * Returns the key. * * @return the key */ public K getKey() { return key; } /** * Returns the value associated with the key. * * @return the value associated with the key */ public V getValue() { return value; } /** * Replaces the value currently associated with the key with the given * value. * * @return the value associated with the key before this method was * called */ public V setValue(V value) { V oldValue = this.value; this.value = value; return oldValue; } public boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; return valEquals(key,e.getKey()) &amp;&amp; valEquals(value,e.getValue()); } public int hashCode() { int keyHash = (key==null ? 0 : key.hashCode()); int valueHash = (value==null ? 0 : value.hashCode()); return keyHash ^ valueHash; } public String toString() { return key + &quot;=&quot; + value; } }} HashTable 线程安全，内部的方法基本都经过 synchronized，保留类不建议使用 ConcurrentHashMap 特点 原因（源码） 底层数组、链表、红黑树 底层 线程安全 线程安全 动态扩容为原来的2倍 int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); 都安全但是ConcurrentHashMap 比 HashTable 效率要高 ConcurrentHashMap 锁粒度更细 JDK1.7 : 【数组（Segment） + 数组（HashEntry） + 链表（HashEntry节点）】 ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。Segment是一种可重入锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。 JDK1.8 : Node数组+链表 / 红黑树 利用CAS+Synchronized来保证并发更新的安全，底层依然采用数组+链表+红黑树的存储结构。 底层 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. */ transient volatile Node&lt;K,V&gt;[] table; /** * The next table to use; non-null only while resizing. */ private transient volatile Node&lt;K,V&gt;[] nextTable; /** * Base counter value, used mainly when there is no contention, * but also as a fallback during table initialization * races. Updated via CAS. */ private transient volatile long baseCount; /** * hash表初始化或扩容时的一个控制位标识量。 * 负数代表正在进行初始化或扩容操作 * -1代表正在初始化 * -N 表示有N-1个线程正在进行扩容操作 * 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小 * Table initialization and resizing control. When negative, the * table is being initialized or resized: -1 for initialization, * else -(1 + the number of active resizing threads). Otherwise, * when table is null, holds the initial table size to use upon * creation, or 0 for default. After initialization, holds the * next element count value upon which to resize the table. */ private transient volatile int sizeCtl; /** * The next table index (plus one) to split while resizing. */ private transient volatile int transferIndex; /** * Spinlock (locked via CAS) used when resizing and/or creating CounterCells. */ private transient volatile int cellsBusy; /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; Queue 先进先出 PriorityQueue 特点 原因（源码） 底层二叉小顶堆 保证每次取出的元素是队列中权值最小的 方法中有synchronized，所以性能不如ArrayList 非null int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); 工具类 Collections Arrays Comparable和Comparator ​ Comparable是排序接口；若一个类实现了Comparable接口，就意味着“该类支持排序”。而Comparator是比较器；我们若需要控制某个类的次序，可以建立一个“该类的比较器”来进行排序。Comparable相当于“内部比较器”，而Comparator相当于“外部比较器”。 容器综合问题 JUC 问题：什么时候该用多线程？ ​ 在Java中，线程部分是一个重点，本篇文章说的JUC也是关于线程的。JUC就是java.util .concurrent工具包的简称。这是一个处理线程的工具包，JDK 1.5开始出现的。 多线程基础 进程(Process) 把一个任务称为一个进程 进程内部还需要同时执行多个子任务称为线程 进程和线程的关系就是：一个进程可以包含一个或多个线程（Thread），但至少会有一个线程。 多进程模式（每个进程只有一个线程） 多线程模式（一个进程有多个线程） 多进程＋多线程模式（复杂度最高） 和多线程相比， 多进程缺点在于：创建进程比创建线程开销大，尤其是在Windows系统上；进程间通信比线程间通信要慢，因为线程间通信就是读写同一个变量，速度很快。 多进程的优点在于：多进程稳定性比多线程高，因为在多进程的情况下，一个进程崩溃不会影响其他进程，而在多线程的情况下，任何一个线程崩溃会直接导致整个进程崩溃。 多线程 Java语言内置了多线程支持：一个Java程序实际上是一个JVM进程，JVM进程用一个主线程来执行main()方法，在main()方法内部，我们又可以启动多个线程。此外，JVM还有负责垃圾回收的其他工作线程等。 因此，对于大多数Java程序来说，我们说多任务，实际上是说如何使用多线程实现多任务。 ​ 和单线程相比，多线程编程的特点在于：多线程经常需要读写共享数据，并且需要同步。例如，播放电影时，就必须由一个线程播放视频，另一个线程播放音频，两个线程需要协调运行，否则画面和声音就不同步。因此，多线程编程的复杂度高，调试更困难。 ​ Java多线程编程的特点又在于：多线程模型是Java程序最基本的并发模型；后续读写网络、数据库、Web开发等都依赖Java多线程模型。 管程 ​ 管程(monitor)是保证了同一时刻只有一个进程在管程内活动,即管程内定义的操作在同一时刻只被一个进程调用(由编译器实现).但是这样并不能保证进程以设计的顺序执行 ​ JVM中同步是基于进入和退出管程(monitor)对象实现的，每个对象都会有一个管程(monitor)对象，管程(monitor)会随着java对象一同创建和销毁 ​ 执行线程首先要持有管程对象，然后才能执行方法，当方法完成之后会释放管程，方法在执行时候会持有管程，其他线程无法再获取同一个管程 创建新线程 继承Thread类，实例化一个Thread实例并重写run（），然后调用它的start()方法： 希望新线程能执行指定的代码 方法一：从Thread派生一个自定义类，然后覆写run()方法： 方法二：创建Thread实例时，传入一个Runnable实例： 线程的优先级 可以对线程设定优先级，设定优先级的方法是： Thread.setPriority(int n) // 1~10, 默认值5 实现Runnable接口 与Callable不同 无返回值 无返回异常 实现run() 实现Callable接口（jdk1.5） 实现 Callable接口 1.实现 Callable接口,需要返回值类型 2.重写call方法,需要抛出异常 3.创建目标对象 4.创建执行服务: Executorservice ser= Executors. newfixed Threadpool(1) 5.提交执行: Future&lt; Boolean&gt; result1=ser. submit(t1); 6.获取结果: boolean r1= result1.get() 7.关闭服务:ser. shutdownNow() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118package pers.lxl.mylearnproject.javase.thread;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;/** * 快速交替执行看起来像是同时执行 * 进程和线程的关系：一个进程可以包含一个或多个线程，但至少会有一个线程 * 多进程模式（每个进程只有一个线程） * 多线程模式（一个进程有多个线程） * 多进程＋多线程模式（复杂度最高） * 和多线程相比，多进程的缺点在于： * 创建进程比创建线程开销大，尤其是在Windows系统上； * 进程间通信比线程间通信要慢，因为线程间通信就是读写同一个变量，速度很快。 * 多进程的优点在于： * 多进程稳定性比多线程高，因为在多进程的情况下，一个进程崩溃不会影响其他进程，而在多线程的情况下，任何一个线程崩溃会直接导致整个进程崩溃。 * Java语言内置了多线程支持：一个Java程序实际上是一个JVM进程，JVM进程用一个主线程来执行main()方法，在main()方法内部，我们又可以启动多个线程。此外，JVM还有负责垃圾回收的其他工作线程等。 * 因此，对于大多数Java程序来说，我们说多任务，实际上是说如何使用多线程实现多任务。 * 和单线程相比，多线程编程的特点在于：多线程经常需要读写共享数据，并且需要同步。例如，播放电影时，就必须由一个线程播放视频，另一个线程播放音频，两个线程需要协调运行，否则画面和声音就不同步。因此，多线程编程的复杂度高，调试更困难。 * Java多线程编程的特点又在于： * 多线程模型是Java程序最基本的并发模型； * 后续读写网络、数据库、Web开发等都依赖Java多线程模型。 * * @author lxl **/public class HelloClass { //希望新线程能执行指定的代码，有以下几种方法： /** * 方法一：从Thread派生一个自定义类，然后覆写run()方法，启动：子类对象.start(),为避免OOP单继承局限性，不建议使用 */ static class MyThread extends Thread { @Override public void run() { System.out.println(&quot;start new thread!extends Thread&quot;); } } /** * 方法二：创建Thread实例时，传入一个Runnable实例：启动：传入目标对象+Thread对象.start()，建议使用:避免单继承局限性，灵活方便同一个对象被多个线程使用 */ static class MyRunnable implements Runnable { @Override public void run() { System.out.println(&quot;start new thread!implements Runnable&quot;); try { Thread.sleep(10); } catch (InterruptedException e) { } System.out.println(&quot;thread end.&quot;); } } /** * 方法三：实现Callable接口,与Runnable相比,Callable可以有返回值,返回值通过FutureTask【未来任务】进行封装 */ static class MyCallable implements Callable&lt;Integer&gt; { @Override public Integer call() { return 123; } }//相比之下,接口可以实现多个,而Thread只能单继承,// 继承整个Threa类开销过大,所以实现接口方式更好一些 /** * 当Java程序启动的时候，实际上是启动了一个JVM进程，然后，JVM启动主线程来执行main()方法。在main()方法中，我们又可以启动其他线程。 */ public static void main(String[] args) throws ExecutionException, InterruptedException { //创建新线程 System.out.println(&quot;main start...&quot;); //Thread thread = new Thread();//不能执行指定代码//extends Thread============== // Thread thread = new MyThread();//implements Callable============= MyCallable mc = new MyCallable(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread thread1 = new Thread(ft); thread1.start(); System.out.println(ft.get());//lambda Callbale FutureTask&lt;Integer&gt; futureTask2 = new FutureTask&lt;&gt;(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot;come in callbale&quot;); return 111; }); new thread(futureTask2,&quot;hahha&quot;).start();//hahha come in callbale while(!futuretask2.isDone()){ System.out.println(&quot;wait....&quot;) } System.out.println(futureTask2.get());//111//implements Runnable=============== Thread thread = new Thread(new MyRunnable()); //Thread thread = new Thread(() -&gt; {System.out.println(&quot;start new thread!&quot;); });//Java8引入的lambda写法 thread.start();// 启动新线程，直接调用Thread实例的run()方法是无效的，线程开启不一定立即执行，由CPU调度决定// Thread.setPriority(int n) // 1~10, 默认值5 try { Thread.sleep(20);// sleep() 可能会抛出 InterruptedException，因为异常不能跨线程传播回 main() 中，因此必须在本地进行处理。线程中抛出的其它异常也同样需要在本地进行处理。 } catch (InterruptedException e) { } System.out.println(&quot;main end...&quot;); }} 线程的状态 在Java程序中，一个线程对象只能调用一次start()方法启动新线程，并在新线程中执行run()方法。一旦run()方法执行完毕，线程就结束了。因此，Java线程的状态有以下几种： New：新创建的线程，尚未执行； Runnable：运行中的线程，正在执行run()方法的Java代码； Blocked：运行中的线程，因为某些操作被阻塞而挂起； Waiting（不见不散）：运行中的线程，因为某些操作在等待中； Timed Waiting（过时不候）：运行中的线程，因为执行sleep()方法正在计时等待； Terminated：线程已终止，因为run()方法执行完毕。 线程终止的原因有： 线程正常终止：run()方法执行到return语句返回； 线程意外终止：run()方法因为未捕获的异常导致线程终止； 对某个线程的Thread实例调用stop()方法强制终止（强烈不推荐使用）。 通过对另一个线程对象调用join()方法可以等待其执行结束； 可以指定等待时间，超过等待时间线程仍然没有结束就不再等待； 对已经运行结束的线程调用join()方法会立刻返回。 中断线程 对目标线程调用interrupt()方法可以请求中断一个线程，目标线程通过检测isInterrupted()标志获取自身是否已中断。如果目标线程处于等待状态，该线程会捕获到InterruptedException； 目标线程检测到isInterrupted()为true或者捕获了InterruptedException都应该立刻结束自身线程； 通过标志位判断需要正确使用volatile关键字； volatile关键字解决了共享变量在线程间的可见性问题。 线程属性 线程优先级 setPriority 守护线程 守护线程是指为其他线程服务的线程。在JVM中，所有非守护线程都执行完毕后，无论有没有守护线程，虚拟机都会自动退出。因此，JVM退出时，不必关心守护线程是否已结束。如何创建守护线程呢？方法和普通线程一样，只是在调用start()方法前，调用setDaemon(true)把该线程标记为守护线程： 守护线程不能持有任何需要关闭的资源，例如打开文件等，因为虚拟机退出时，守护线程没有任何机会来关闭文件，这会导致数据丢失。 Thread t = new MyThread(); //调用start()方法前，调用setDaemon(true)把该线程标记为守护线程 t.setDaemon(true); //守护线程不能持有任何需要关闭的资源，例如打开文件等，因为虚拟机退出时，守护线程没有任何机会来关闭文件，这会导致数据丢失。 t.start(); //判断是否为守护线程 System.out.println(t.isDaemon()); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596### 线程组### 处理未捕获异常处理器## 多线程编程步骤1. 创建资源类，在资源类创建属性和操作方法2. 在资源类中添加操作方法【判断、干活、通知】3. 创建多个线程，调用资源类的操作方法## synchronizedsynchronized是Java中的关键字，是一种同步锁。它修饰的对象有以下几种：1. 修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象；2. 修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象；虽然可以使用synchronized来定义方法，但synchronized并不属于方法定义的一部分，因此，synchronized关键字不能被继承。如果在父类中的某个方法使用了synchronized关键字，而在子类中覆盖了这个方法，在子类中的这个方法默认情况下并不是同步的，而必须显式地在子类的这个方法中加上synchronized关键字才可以。当然，还可以在子类方法中调用父类中相应的方法，这样虽然子类中的方法不是同步的，但子类调用了父类的同步方法，因此，子类的方法也就相当于同步了。3. 修改一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象；4. 修改一个类，其作用的范围是synchronized后面括号括起来的部分，作用主的对象是这个类的所有对象。一个对象里面如果有多个synchronized方法，某一个时刻内，只要一个线程去调用其中的一个synchronized方法了，其它的线程都只能等待，换句话说，某一个时刻内，只能有唯一一个线程去访问这些synchronized方法锁的是当前对象this，被锁定后，其它的线程都不能进入到当前对象的其它的synchronized方法加个普通方法后发现和同步锁无关换成两个对象后，不是同一把锁了，情况立刻变化。synchronized实现同步的基础：Java中的每一个对象都可以作为锁。**具体表现为以下3种形式。****对于普通同步方法，锁是当前实例对象。****对于静态同步方法，锁是当前类的Class对象。****对于同步方法块，锁是Synchonized括号里配置的对象**当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。也就是说如果一个实例对象的非静态同步方法获取锁后，该实例对象的其他非静态同步方法必须等待获取锁的方法释放锁后才能获取锁，可是别的实例对象的非静态同步方法因为跟该实例对象的非静态同步方法用的是不同的锁，所以毋须等待该实例对象已获取锁的非静态同步方法释放锁就可以获取他们自己的锁。所有的静态同步方法用的也是同一把锁——类对象本身，这两把锁是两个不同的对象，所以静态同步方法与非静态同步方法之间是不会有竞态条件的。但是一旦一个静态同步方法获取锁后，其他的静态同步方法都必须等待该方法释放锁后才能获取锁，而不管是同一个实例对象的静态同步方法之间，还是不同的实例对象的静态同步方法之间，只要它们同一个类的实例对象！## 线程同步- 多线程同时读写共享变量时，会造成逻辑错误，因此需要通过synchronized同步；同步的本质就是给指定对象加锁，加锁后才能继续执行后续代码；注意加锁对象必须是同一个实例；对JVM定义的单个原子操作不需要同步。## 同步方法- 用synchronized修饰方法可以把整个方法变为同步代码块，synchronized方法加锁对象是this； 通过合理的设计和数据封装可以让一个类变为“线程安全”； 一个类没有特殊说明，默认不是thread-safe； 多线程能否安全访问某个非线程安全的实例，需要具体问题具体分析。## 死锁- Java的synchronized锁是隐式【自动上锁解锁】的**可重入锁（递归锁）**，Lock是显式的可重入锁；死锁产生的条件是多线程各自持有不同的锁，并互相试图获取对方已持有的锁，导致无限等待；避免死锁的方法是多线程获取锁的顺序要一致。```javapackage com.atguigu.sync;import java.util.concurrent.TimeUnit;/** * 演示死锁 */public class DeadLock { //创建两个对象 static Object a = new Object(); static Object b = new Object(); public static void main(String[] args) { new Thread(()-&gt;{ synchronized (a) { System.out.println(Thread.currentThread().getName()+&quot; 持有锁a，试图获取锁b&quot;); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (b) { System.out.println(Thread.currentThread().getName()+&quot; 获取锁b&quot;); } } },&quot;A&quot;).start(); new Thread(()-&gt;{ synchronized (b) { System.out.println(Thread.currentThread().getName()+&quot; 持有锁b，试图获取锁a&quot;); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (a) { System.out.println(Thread.currentThread().getName()+&quot; 获取锁a&quot;); } } },&quot;B&quot;).start(); }} 验证是否为死锁 配置JDK环境变量 项目中打开终端 输入jps -l 查看当前类的进程号 jstack -进程号可查看是否有死锁及相关信息 使用wait和notify wait和notify用于多线程协调运行： 在synchronized内部可以调用wait()使线程进入等待状态； 必须在已获得的锁对象上调用wait()方法； 在synchronized内部可以调用notify()或notifyAll()唤醒其他等待线程； 必须在已获得的锁对象上调用notify()或notifyAll()方法； 已唤醒的线程还需要重新获得锁后才能继续执行。 虚假唤醒 wait()在哪里睡就在那里醒使用时应使用while而不是if 123456if(num!=1){ this.wait(); //notifyAll（）唤醒之后会直接从this.wait（）语句开始执行，跳过了if判断}while(num!=1){ this.wait();} 使用ReentrantLock ReentrantLock可以替代synchronized进行同步； ReentrantLock获取锁更安全； 必须先获取到锁，再进入try {...}代码块，最后使用finally保证释放锁； 可以使用tryLock()尝试获取锁。 设置为ReentrantLock（true）使其成为公平锁 123456789101112131415161718192021222324252627Lock lock = new ReentrantLock(); Thread t1 = new Thread() { @Override public void run() { try { log(&quot;线程启动&quot;); log(&quot;试图占有对象：lock&quot;); //lock同步 lock.lock(); log(&quot;占有对象：lock&quot;); log(&quot;进行5秒的业务操作&quot;); Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } finally { log(&quot;释放对象：lock&quot;); //手动释放 lock.unlock(); } log(&quot;线程结束&quot;); } }; t1.setName(&quot;t1&quot;); t1.start(); 使用Condition Condition可以替代wait和notify； Condition对象必须从Lock对象获取。 12private Lock lock=new ReentrantLock();private Condition condition=lock.newCondition(); 读写锁 乐观锁悲观锁 表锁行锁 表锁整张表，而行锁锁单条数据且可能发生死锁。 读锁写锁 读锁为共享锁，写锁为独占锁，都会发生死锁。 现实中有这样一种场景：对共享资源有读和写的操作，且写操作没有读操作那么频繁。在没有写操作的时候，多个线程同时读一个资源没有任何问题，所以应该允许多个线程同时读取共享资源；但是如果一个线程想去写这些共享资源，就不应该允许其他线程对该资源进行读和写的操作了。 针对这种场景，JAVA的并发包提供了读写锁ReentrantReadWriteLock，它表示两个锁，一个是读操作相关的锁，称为共享锁；一个是写相关的锁，称为排他锁 线程进入读锁的前提条件： • 没有其他线程的写锁 • 没有写请求, 或者有写请求，但调用线程和持有锁的线程是同一个(可重入锁)。 线程进入写锁的前提条件： • 没有其他线程的读锁 • 没有其他线程的写锁 而读写锁有以下三个重要的特性： （1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。 （2）重进入：读锁和写锁都支持线程重进入。 （3）锁降级：遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁。 使用ReadWriteLock 使用ReadWriteLock可以提高读取效率： ReadWriteLock只允许一个线程写入； ReadWriteLock允许多个线程在没有写入时同时读取； ReadWriteLock适合读多写少的场景。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package com.atguigu.readwrite;import java.util.HashMap;import java.util.Map;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReadWriteLock;import java.util.concurrent.locks.ReentrantReadWriteLock;//资源类class MyCache { //创建map集合 private volatile Map&lt;String,Object&gt; map = new HashMap&lt;&gt;(); //创建读写锁对象 private ReadWriteLock rwLock = new ReentrantReadWriteLock(); //放数据 public void put(String key,Object value) { //添加写锁 rwLock.writeLock().lock(); try { System.out.println(Thread.currentThread().getName()+&quot; 正在写操作&quot;+key); //暂停一会 TimeUnit.MICROSECONDS.sleep(300); //放数据 map.put(key,value); System.out.println(Thread.currentThread().getName()+&quot; 写完了&quot;+key); } catch (InterruptedException e) { e.printStackTrace(); } finally { //释放写锁 rwLock.writeLock().unlock(); } } //取数据 public Object get(String key) { //添加读锁 rwLock.readLock().lock(); Object result = null; try { System.out.println(Thread.currentThread().getName()+&quot; 正在读取操作&quot;+key); //暂停一会 TimeUnit.MICROSECONDS.sleep(300); result = map.get(key); System.out.println(Thread.currentThread().getName()+&quot; 取完了&quot;+key); } catch (InterruptedException e) { e.printStackTrace(); } finally { //释放读锁 rwLock.readLock().unlock(); } return result; }}public class ReadWriteLockDemo { public static void main(String[] args) throws InterruptedException { MyCache myCache = new MyCache(); //创建线程放数据 for (int i = 1; i &lt;=5; i++) { final int num = i; new Thread(()-&gt;{ myCache.put(num+&quot;&quot;,num+&quot;&quot;); },String.valueOf(i)).start(); } TimeUnit.MICROSECONDS.sleep(300); //创建线程取数据 for (int i = 1; i &lt;=5; i++) { final int num = i; new Thread(()-&gt;{ myCache.get(num+&quot;&quot;); },String.valueOf(i)).start(); } }} 使用StampedLock StampedLock提供了乐观读锁，可取代ReadWriteLock以进一步提升并发性能；StampedLock是不可重入锁。 使用Concurrent集合 使用java.util.concurrent包提供的线程安全的并发集合可以大大简化多线程编程： 多线程同时读写并发集合是安全的； 尽量使用Java标准库提供的并发集合，避免自己编写同步代码。 使用Atomic 使用java.util.concurrent.atomic提供的原子操作可以简化多线程编程： 原子操作实现了无锁的线程安全； 适用于计数器，累加器等。 增加值并返回新值：int addAndGet(int delta) 加1后返回新值：int incrementAndGet() 获取当前值：int get() 用CAS方式设置：int compareAndSet(int expect, int update) 使用线程池 JDK提供了ExecutorService实现了线程池功能： 线程池内部维护一组线程，可以高效执行大量小任务； 执行器Executors提供了静态方法创建不同类型的ExecutorService；必须调用shutdown()关闭ExecutorService；ScheduledThreadPool可以定期调度多个任务。 使用Future 对线程池提交一个Callable任务，可以获得一个Future对象； 可以用Future在将来某个时刻获取结果。 使用CompletableFuture CompletableFuture可以指定异步处理流程： thenAccept()处理正常结果； exceptional()处理异常结果； thenApplyAsync()用于串行化另一个CompletableFuture； anyOf()和allOf()用于并行化多个CompletableFuture。 使用ForkJoin Fork/Join是一种基于“分治”的算法：通过分解任务，并行执行，最后合并结果得到最终结果。 ForkJoinPool线程池可以把一个大任务分拆成小任务并行执行，任务类必须继承自RecursiveTask或RecursiveAction。 使用Fork/Join模式可以进行并行计算以提高效率。 使用ThreadLocal ThreadLocal表示线程的“局部变量”，它确保每个线程的ThreadLocal变量都是各自独立的； ThreadLocal适合在一个线程的处理流程中保持上下文（避免了同一参数在所有方法中传递）； 使用ThreadLocal要用try ... finally结构，并在finally中清除。 集合线程安全 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public static void main(String[] args) { //创建ArrayList集合// List&lt;String&gt; list = new ArrayList&lt;&gt;(); // Vector解决// List&lt;String&gt; list = new Vector&lt;&gt;(); //Collections解决// List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;&gt;()); // CopyOnWriteArrayList写时复制技术解决// List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;();// for (int i = 0; i &lt;30; i++) {// new Thread(()-&gt;{// //向集合添加内容// list.add(UUID.randomUUID().toString().substring(0,8));// //从集合获取内容// System.out.println(list);// },String.valueOf(i)).start();// } //演示Hashset// Set&lt;String&gt; set = new HashSet&lt;&gt;();// Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;&gt;();// for (int i = 0; i &lt;30; i++) {// new Thread(()-&gt;{// //向集合添加内容// set.add(UUID.randomUUID().toString().substring(0,8));// //从集合获取内容// System.out.println(set);// },String.valueOf(i)).start();// } //演示HashMap// Map&lt;String,String&gt; map = new HashMap&lt;&gt;(); Map&lt;String,String&gt; map = new ConcurrentHashMap&lt;&gt;(); for (int i = 0; i &lt;30; i++) { String key = String.valueOf(i); new Thread(()-&gt;{ //向集合添加内容 map.put(key,UUID.randomUUID().toString().substring(0,8)); //从集合获取内容 System.out.println(map); },String.valueOf(i)).start(); } } JUC三大辅助类 JUC中提供了三种常用的辅助类，通过这些辅助类可以很好的解决线程数量过多时Lock锁的频繁操作。这三种辅助类为： • CountDownLatch: 减少计数 • CyclicBarrier: 循环栅栏 • Semaphore: 信号灯 CountDownLatch:减少计数 CountDownLatch类可以设置一个计数器，然后通过countDown方法来进行减1的操作，使用await方法等待计数器不大于0，然后继续执行await方法之后的语句。 • CountDownLatch主要有两个方法，当一个或多个线程调用await方法时，这些线程会阻塞 • 其它线程调用countDown方法会将计数器减1(调用countDown方法的线程不会阻塞) • 当计数器的值变为0时，因await方法阻塞的线程会被唤醒，继续执行 12345678910111213141516171819202122232425262728package com.atguigu.juc;import java.util.concurrent.CountDownLatch;//演示 CountDownLatchpublic class CountDownLatchDemo { //6个同学陆续离开教室之后，班长锁门 public static void main(String[] args) throws InterruptedException { //创建CountDownLatch对象，设置初始值 CountDownLatch countDownLatch = new CountDownLatch(6); //6个同学陆续离开教室之后 for (int i = 1; i &lt;=6; i++) { new Thread(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot; 号同学离开了教室&quot;); //计数 -1 countDownLatch.countDown(); },String.valueOf(i)).start(); } //等待 countDownLatch.await(); System.out.println(Thread.currentThread().getName()+&quot; 班长锁门走人了&quot;);}} CyclicBarrier: 循环栅栏 ​ CyclicBarrier看英文单词可以看出大概就是循环阻塞的意思，在使用中CyclicBarrier的构造方法第一个参数是目标障碍数，每次执行CyclicBarrier一次障碍数会加一，如果达到了目标障碍数，才会执行cyclicBarrier.await()之后的语句。可以将CyclicBarrier理解为加1操作 12345678910111213141516171819202122232425262728293031package com.atguigu.juc;import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;//集齐7颗龙珠就可以召唤神龙public class CyclicBarrierDemo {//创建固定值private static final int NUMBER = 7;public static void main(String[] args) { //创建CyclicBarrier CyclicBarrier cyclicBarrier = new CyclicBarrier(NUMBER,()-&gt;{ System.out.println(&quot;*****集齐7颗龙珠就可以召唤神龙&quot;); }); //集齐七颗龙珠过程 for (int i = 1; i &lt;=7; i++) { new Thread(()-&gt;{ try { System.out.println(Thread.currentThread().getName()+&quot; 星龙被收集到了&quot;); //等待 cyclicBarrier.await(); } catch (Exception e) { e.printStackTrace(); } },String.valueOf(i)).start(); }}} Semaphore: 信号灯 ​ Semaphore的构造方法中传入的第一个参数是最大信号量（可以看成最大线程池），每个信号量初始化为一个最多只能分发一个许可证。使用acquire方法获得许可证，release方法释放许可。场景: 抢车位, 6部汽车3个停车位 123456789101112131415161718192021222324252627282930313233343536package com.atguigu.juc;import java.util.Random;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;//6辆汽车，停3个车位public class SemaphoreDemo { public static void main(String[] args) { //创建Semaphore，设置许可数量 Semaphore semaphore = new Semaphore(3); //模拟6辆汽车 for (int i = 1; i &lt;=6; i++) { new Thread(()-&gt;{ try { //抢占 semaphore.acquire(); System.out.println(Thread.currentThread().getName()+&quot; 抢到了车位&quot;); //设置随机停车时间 TimeUnit.SECONDS.sleep(new Random().nextInt(5)); System.out.println(Thread.currentThread().getName()+&quot; ------离开了车位&quot;); } catch (InterruptedException e) { e.printStackTrace(); } finally { //释放 semaphore.release(); } },String.valueOf(i)).start(); } }} 阻塞队列 BlockingQueue Concurrent包中，BlockingQueue很好的解决了多线程中，如何高效安全“传输”数据的问题。通过这些高效并且线程安全的队列类，为我们快速搭建高质量的多线程程序带来极大的便利。本文详细介绍了BlockingQueue家庭中的所有成员，包括他们各自的功能以及常见使用场景。 阻塞队列，顾名思义，首先它是一个队列, 通过一个共享的队列，可以使得数据由队列的一端输入，从另外一端输出； 当队列是空的，从队列中获取元素的操作将会被阻塞 当队列是满的，从队列中添加元素的操作将会被阻塞 试图从空的队列中获取元素的线程将会被阻塞，直到其他线程往空的队列插入新的元素 试图向已满的队列中添加新元素的线程将会被阻塞，直到其他线程从队列中移除一个或多个元素或者完全清空，使队列变得空闲起来并后续新增 常用的队列主要有以下两种： • 先进先出（FIFO）：先插入的队列的元素也最先出队列，类似于排队的功能。从某种程度上来说这种队列也体现了一种公平性 • 后进先出（LIFO）：后插入队列的元素最先出队列，这种队列优先处理最近发生的事件(栈) 在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起 为什么需要BlockingQueue 好处是我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了 在concurrent包发布以前，在多线程环境下，我们必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。 多线程环境中，通过队列可以很容易实现数据共享，比如经典的“生产者”和“消费者”模型中，通过队列可以很便利地实现两者之间的数据共享。假设我们有若干生产者线程，另外又有若干个消费者线程。如果生产者线程需要把准备好的数据共享给消费者线程，利用队列的方式来传递数据，就可以很方便地解决他们之间的数据共享问题。但如果生产者和消费者在某个时间段内，万一发生数据处理速度不匹配的情况呢？理想情况下，如果生产者产出数据的速度大于消费者消费的速度，并且当生产出来的数据累积到一定程度的时候，那么生产者必须暂停等待一下（阻塞生产者线程），以便等待消费者线程把累积的数据处理完毕，反之亦然。 • 当队列中没有数据的情况下，消费者端的所有线程都会被自动阻塞（挂起），直到有数据放入队列 • 当队列中填满数据的情况下，生产者端的所有线程都会被自动阻塞（挂起），直到队列中有空的位置，线程被自动唤醒 BlockingQueue核心方法 1.放入数据 • offer(anObject):表示如果可能的话,将anObject加到BlockingQueue里,即如果BlockingQueue可以容纳,则返回true,否则返回false.（本方法不阻塞当前执行方法的线程） • offer(E o, long timeout, TimeUnit unit)：可以设定等待的时间，如果在指定的时间内，还不能往队列中加入BlockingQueue，则返回失败 • put(anObject):把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断直到BlockingQueue里面有空间再继续. 2.获取数据 • poll(time): 取走BlockingQueue里排在首位的对象,若不能立即取出,则可以等time参数规定的时间,取不到时返回null • poll(long timeout, TimeUnit unit)：从BlockingQueue取出一个队首的对象，如果在指定时间内，队列一旦有数据可取，则立即返回队列中的数据。否则知道时间超时还没有数据可取，返回失败。 • take(): 取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到BlockingQueue有新的数据被加入; • drainTo(): 一次性从BlockingQueue获取所有可用的数据对象（还可以指定获取数据的个数），通过该方法，可以提升获取数据效率；不需要多次分批加锁或释放锁。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.atguigu.queue;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.BlockingQueue;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;//阻塞队列public class BlockingQueueDemo { public static void main(String[] args) throws InterruptedException { //创建阻塞队列 BlockingQueue&lt;String&gt; blockingQueue = new ArrayBlockingQueue&lt;&gt;(3); //第一组// System.out.println(blockingQueue.add(&quot;a&quot;));//// System.out.println(blockingQueue.add(&quot;b&quot;));//// System.out.println(blockingQueue.add(&quot;c&quot;));//// //System.out.println(blockingQueue.element());//////// //System.out.println(blockingQueue.add(&quot;w&quot;));//// System.out.println(blockingQueue.remove());//// System.out.println(blockingQueue.remove());//// System.out.println(blockingQueue.remove());//// System.out.println(blockingQueue.remove()); //第二组// System.out.println(blockingQueue.offer(&quot;a&quot;));// System.out.println(blockingQueue.offer(&quot;b&quot;));// System.out.println(blockingQueue.offer(&quot;c&quot;));// System.out.println(blockingQueue.offer(&quot;www&quot;));//// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll());// System.out.println(blockingQueue.poll()); //第三组 put 没空间阻塞 take 没数据阻塞// blockingQueue.put(&quot;a&quot;);// blockingQueue.put(&quot;b&quot;);// blockingQueue.put(&quot;c&quot;);// //blockingQueue.put(&quot;w&quot;);//// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take());// System.out.println(blockingQueue.take()); //第四组 System.out.println(blockingQueue.offer(&quot;a&quot;)); System.out.println(blockingQueue.offer(&quot;b&quot;)); System.out.println(blockingQueue.offer(&quot;c&quot;)); System.out.println(blockingQueue.offer(&quot;w&quot;,3L, TimeUnit.SECONDS)); }} ArrayBlockingQueue(常用) 基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定长数组，以便缓存队列中的数据对象，这是一个常用的阻塞队列，除了一个定长数组外，ArrayBlockingQueue内部还保存着两个整形变量，分别标识着队列的头部和尾部在数组中的位置。 ArrayBlockingQueue在生产者放入数据和消费者获取数据，都是共用同一个锁对象，由此也意味着两者无法真正并行运行，这点尤其不同于LinkedBlockingQueue；按照实现原理来分析，ArrayBlockingQueue完全可以采用分离锁，从而实现生产者和消费者操作的完全并行运行。Doug Lea之所以没这样去做，也许是因为ArrayBlockingQueue的数据写入和获取操作已经足够轻巧，以至于引入独立的锁机制，除了给代码带来额外的复杂性外，其在性能上完全占不到任何便宜。 ArrayBlockingQueue和LinkedBlockingQueue间还有一个明显的不同之处在于，前者在插入或删除元素时不会产生或销毁任何额外的对象实例，而后者则会生成一个额外的Node对象。这在长时间内需要高效并发地处理大批量数据的系统中，其对于GC的影响还是存在一定的区别。而在创建ArrayBlockingQueue时，我们还可以控制对象的内部锁是否采用公平锁，默认采用非公平锁。 一句话总结: 由数组结构组成的有界阻塞队列。 LinkedBlockingQueue(常用) 基于链表的阻塞队列，同ArrayListBlockingQueue类似，其内部也维持着一个数据缓冲队列（该队列由一个链表构成），当生产者往队列中放入一个数据时，队列会从生产者手中获取数据，并缓存在队列内部，而生产者立即返回；只有当队列缓冲区达到最大值缓存容量时（LinkedBlockingQueue可以通过构造函数指定该值），才会阻塞生产者队列，直到消费者从队列中消费掉一份 数据，生产者线程会被唤醒，反之对于消费者这端的处理也基于同样的原理。而LinkedBlockingQueue之所以能够高效的处理并发数据，还因为其对于生产者端和消费者端分别采用了独立的锁来控制数据同步，这也意味着在高并发的情况下生产者和消费者可以并行地操作队列中的数据，以此来提高整个队列的并发性能。 ArrayBlockingQueue和LinkedBlockingQueue是两个最普通也是最常用的阻塞队列，一般情况下，在处理多线程间的生产者消费者问题，使用这两个类足以。 一句话总结: 由链表结构组成的有界（但大小默认值为integer.MAX_VALUE）阻塞队列。 DelayQueue DelayQueue中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素。DelayQueue是一个没有大小限制的队列，因此往队列中插入数据的操作（生产者）永远不会被阻塞，而只有获取数据的操作（消费者）才会被阻塞。 一句话总结: 使用优先级队列实现的延迟无界阻塞队列。 PriorityBlockingQueue 基于优先级的阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定），但需要注意的是PriorityBlockingQueue并不会阻塞数据生产者，而只会在没有可消费的数据时，阻塞数据的消费者。 因此使用的时候要特别注意，生产者生产数据的速度绝对不能快于消费者消费数据的速度，否则时间一长，会最终耗尽所有的可用堆内存空间。 在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是公平锁。 一句话总结: 支持优先级排序的无界阻塞队列。 SynchronousQueue 一种无缓冲的等待队列，类似于无中介的直接交易，有点像原始社会中的生产者和消费者，生产者拿着产品去集市销售给产品的最终消费者，而消费者必须亲自去集市找到所要商品的直接生产者，如果一方没有找到合适的目标，那么对不起，大家都在集市等待。相对于有缓冲的BlockingQueue来说，少了一个中间经销商的环节（缓冲区），如果有经销商，生产者直接把产品批发给经销商，而无需在意经销商最终会将这些产品卖给那些消费者，由于经销商可以库存一部分商品，因此相对于直接交易模式，总体来说采用中间经销商的模式会吞吐量高一些（可以批量买卖）；但另一方面，又因为经销商的引入，使得产品从生产者到消费者中间增加了额外的交易环节，单个产品的及时响应性能可能会降低。 声明一个SynchronousQueue有两种不同的方式，它们之间有着不太一样的行为。 公平模式和非公平模式的区别: • 公平模式：SynchronousQueue会采用公平锁，并配合一个FIFO队列来阻塞多余的生产者和消费者，从而体系整体的公平策略； • 非公平模式（SynchronousQueue默认）：SynchronousQueue采用非公平锁，同时配合一个LIFO队列来管理多余的生产者和消费者，而后一种模式，如果生产者和消费者的处理速度有差距，则很容易出现饥渴的情况，即可能有某些生产者或者是消费者的数据永远都得不到处理。 一句话总结: 不存储元素的阻塞队列，也即单个元素的队列。 LinkedTransferQueue LinkedTransferQueue是一个由链表结构组成的无界阻塞TransferQueue队列。相对于其他阻塞队列，LinkedTransferQueue多了tryTransfer和transfer方法。 LinkedTransferQueue采用一种预占模式。意思就是消费者线程取元素时，如果队列不为空，则直接取走数据，若队列为空，那就生成一个节点（节点元素为null）入队，然后消费者线程被等待在这个节点上，后面生产者线程入队时发现有一个元素为null的节点，生产者线程就不入队了，直接就将元素填充到 该节点，并唤醒该节点等待的线程，被唤醒的消费者线程取走元素，从调用的方法返回。 一句话总结: 由链表组成的无界阻塞队列。 LinkedBlockingDeque LinkedBlockingDeque是一个由链表结构组成的双向阻塞队列，即可以从队列的两端插入和移除元素。 对于一些指定的操作，在插入或者获取队列元素时如果队列状态不允许该操作可能会阻塞住该线程直到队列状态变更为允许操作，这里的阻塞一般有两种情况 • 插入元素时: 如果当前队列已满将会进入阻塞状态，一直等到队列有空的位置时再讲该元素插入，该操作可以通过设置超时参数，超时后返回 false 表示操作失败，也可以不设置超时参数一直阻塞，中断后抛出InterruptedException异常 • 读取元素时: 如果当前队列为空会阻塞住直到队列不为空然后返回元素，同样可以通过设置超时参数 一句话总结: 由链表组成的双向阻塞队列 小结 在多线程领域：所谓阻塞，在某些情况下会挂起线程（即阻塞），一旦条件满足，被挂起的线程又会自动被唤起 为什么需要BlockingQueue? 在concurrent包发布以前，在多线程环境下，我们每个程序员都必须去自己控制这些细节，尤其还要兼顾效率和线程安全，而这会给我们的程序带来不小的复杂度。使用后我们不需要关心什么时候需要阻塞线程，什么时候需要唤醒线程，因为这一切BlockingQueue都给你一手包办了 线程池 ​ 线程池（英语：thread pool）：一种线程使用模式。线程过多会带来调度开销，进而影响缓存局部性和整体性能。而线程池维护着多个线程，等待着监督管理者分配可并发执行的任务。这避免了在处理短时间任务时创建与销毁线程的代价。线程池不仅能够保证内核的充分利用，还能防止过分调度。 例子： 10年前单核CPU电脑，假的多线程，像马戏团小丑玩多个球，CPU需要来回切换。 现在是多核电脑，多个线程各自跑在独立的CPU上，不用切换效率高。 线程池的优势： 线程池做的工作只要是控制运行的线程数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量，超出数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 它的主要特点为： • 降低资源消耗: 通过重复利用已创建的线程降低线程创建和销毁造成的销耗。 • 提高响应速度: 当任务到达时，任务可以不需要等待线程创建就能立即执行。 • 提高线程的可管理性: 线程是稀缺资源，如果无限制的创建，不仅会销耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 • Java中的线程池是通过Executor框架实现的，该框架中用到了Executor，Executors，ExecutorService，ThreadPoolExecutor这几个类 创建常用三种线程池的方式 12345678910111213141516171819202122232425262728293031323334package com.atguigu.pool;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;//演示线程池三种常用分类public class ThreadPoolDemo1 { public static void main(String[] args) { //一池五线程 ExecutorService threadPool1 = Executors.newFixedThreadPool(5); //5个窗口 //一池一线程 ExecutorService threadPool2 = Executors.newSingleThreadExecutor(); //一个窗口 //一池可扩容线程 ExecutorService threadPool3 = Executors.newCachedThreadPool(); //10个顾客请求 try { for (int i = 1; i &lt;=10; i++) { //执行 threadPool3.execute(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot; 办理业务&quot;); }); } }catch (Exception e) { e.printStackTrace(); }finally { //关闭 threadPool3.shutdown(); } }} 常用参数(重点) • corePoolSize线程池的核心线程数 • maximumPoolSize能容纳的最大线程数 • keepAliveTime空闲线程存活时间 • unit 存活的时间单位 • workQueue 存放提交但未执行任务的队列 • threadFactory 创建线程的工厂类 • handler 等待队列满后的拒绝策略 线程池中，有三个重要的参数，决定影响了拒绝策略：corePoolSize - 核心线程数，也即最小的线程数。workQueue - 阻塞队列 。 maximumPoolSize - 最大线程数 当提交任务数大于 corePoolSize 的时候，会优先将任务放到 workQueue 阻塞队列中。当阻塞队列饱和后，会扩充线程池中线程数，直到达到 maximumPoolSize 最大线程数配置。此时，再多余的任务，则会触发线程池的拒绝策略了。 总结起来，也就是一句话，当提交的任务数大于（workQueue.size() + maximumPoolSize ），就会触发线程池的拒绝策略。 拒绝策略(重点) CallerRunsPolicy: 当触发拒绝策略，只要线程池没有关闭的话，则使用调用线程直接运行任务。一般并发比较小，性能要求不高，不允许失败。但是，由于调用者自己运行任务，如果任务提交速度过快，可能导致程序阻塞，性能效率上必然的损失较大 AbortPolicy: 丢弃任务，并抛出拒绝执行 RejectedExecutionException 异常信息。线程池默认的拒绝策略。必须处理好抛出的异常，否则会打断当前的执行流程，影响后续的任务执行。 DiscardPolicy: 直接丢弃，其他啥都没有 DiscardOldestPolicy: 当触发拒绝策略，只要线程池没有关闭的话，丢弃阻塞队列 workQueue 中最老的一个任务，并将新任务加入 线程池的种类与创建 newCachedThreadPool(常用) 作用：创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程. 特点: • 线程池中数量没有固定，可达到最大值（Interger. MAX_VALUE） • 线程池中的线程可进行缓存重复利用和回收（回收默认时间为1分钟） • 当线程池中，没有可用线程，会重新创建一个线程 创建方式： 1234567891011/** * 可缓存线程池 * @return */public static ExecutorService newCachedThreadPool(){ /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */ return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); } newFixedThreadPool(常用) 作用：创建一个可重用固定线程数的线程池，以共享的无界队列方式来运行这些线程。在任意点，在大多数线程会处于处理任务的活动状态。如果在所有线程处于活动状态时提交附加任务，则在有可用线程之前，附加任务将在队列中等待。如果在关闭前的执行期间由于失败而导致任何线程终止，那么一个新线程将代替它执行后续的任务（如果需要）。在某个线程被显式地关闭之前，池中的线程将一直存在。场景: 适用于可以预测线程数量的业务中，或者服务器负载较重，对线程数有严格限制的场景 特征： • 线程池中的线程处于一定的量，可以很好的控制线程的并发量 • 线程可以重复被使用，在显示关闭之前，都将一直存在 • 超出一定量的线程被提交时候需在队列中等待 创建方式： 123456789101112/** * 固定长度线程池 * @return */ public static ExecutorService newFixedThreadPool(){ /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */ return new ThreadPoolExecutor(10, 10, 0L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); } newSingleThreadExecutor(常用) 作用：创建一个使用单个 worker 线程的 Executor，以无界队列方式来运行该线程。（注意，如果因为在关闭前的执行期间出现失败而终止了此单个线程，那么如果需要，一个新线程将代替它执行后续的任务）。可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。与其他等效的newFixedThreadPool不同，可保证无需重新配置此方法所返回的执行程序即可使用其他的线程。场景: 适用于需要保证顺序执行各个任务，并且在任意时间点，不会同时有多个线程的场景。 特征： 线程池中最多执行1个线程，之后提交的线程活动将会排在队列中以此执行 创建方式： 1234567891011/** * 单一线程池 * @return */ /** * corePoolSize线程池的核心线程数 * maximumPoolSize能容纳的最大线程数 * keepAliveTime空闲线程存活时间 * unit 存活的时间单位 * workQueue 存放提交但未执行任务的队列 * threadFactory 创建线程的工厂类:可以省略 * handler 等待队列满后的拒绝策略:可以省略 */public static ExecutorService newSingleThreadExecutor(){ return new ThreadPoolExecutor(1, 1, 0L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy()); } newScheduleThreadPool(了解) 作用: 线程池支持定时以及周期性执行任务，创建一个corePoolSize为传入参数，最大线程数为整形的最大数的线程池。场景: 适用于需要多个后台线程执行周期任务的场景。 特征: （1）线程池中具有指定数量的线程，即便是空线程也将保留 （2）可定时或者延迟执行线程活动 创建方式: 12public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize, ThreadFactory threadFactory) { return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory); } newWorkStealingPool jdk1.8提供的线程池，底层使用的是ForkJoinPool实现，创建一个拥有多个任务队列的线程池，可以减少连接数，创建当前可用cpu核数的线程来并行执行任务。场景: 适用于大耗时，可并行执行的场景 创建方式: 12345678public static ExecutorService newWorkStealingPool(int parallelism) { /** * parallelism：并行级别，通常默认为JVM可用的处理器个数 * factory：用于创建ForkJoinPool中使用的线程。 * handler：用于处理工作线程未处理的异常，默认为null * asyncMode：用于控制WorkQueue的工作模式:队列---反队列*/ return new ForkJoinPool(parallelism, ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true); } 案例： 1 底层原理 在创建了线程池后，线程池中的线程数为零 当调用execute()方法添加一个请求任务时，线程池会做出如下判断： 2.1 如果正在运行的线程数量小于corePoolSize，那么马上创建线程运行这个任务； 2.2 如果正在运行的线程数量大于或等于corePoolSize，那么将这个任务放入队列； 2.3 如果这个时候队列满了且正在运行的线程数量还小于maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务； 2.4 如果队列满了且正在运行的线程数量大于或等于maximumPoolSize，那么线程池会启动饱和拒绝策略来执行。 当一个线程完成任务时，它会从队列中取下一个任务来执行 当一个线程无事可做超过一定的时间（keepAliveTime）时，线程会判断： 4.1 如果当前运行的线程数大于corePoolSize，那么这个线程就被停掉。 4.2 所以线程池的所有任务完成后，它最终会收缩到corePoolSize的大小。 注意事项(重要) 项目中创建多线程时，使用常见的三种线程池创建方式，单一、可变、定长都有一定问题，原因是FixedThreadPool和SingleThreadExecutor底层都是用LinkedBlockingQueue实现的，这个队列最大长度为Integer.MAX_VALUE，容易导致OOM。所以实际生产一般自己通过ThreadPoolExecutor的7个参数，自定义线程池 创建线程池推荐适用ThreadPoolExecutor及其7个参数手动创建 o corePoolSize线程池的核心线程数 o maximumPoolSize能容纳的最大线程数 o keepAliveTime空闲线程存活时间 o unit 存活的时间单位 o workQueue 存放提交但未执行任务的队列 o threadFactory 创建线程的工厂类 o handler 等待队列满后的拒绝策略 ​ 手动创建示例 12345678910111213141516171819202122232425262728293031323334package com.atguigu.pool;import java.util.concurrent.*;//自定义线程池创建public class ThreadPoolDemo2 { public static void main(String[] args) { ExecutorService threadPool = new ThreadPoolExecutor( 2, 5, 2L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.AbortPolicy() ); //10个顾客请求 try { for (int i = 1; i &lt;=10; i++) { //执行 threadPool.execute(()-&gt;{ System.out.println(Thread.currentThread().getName()+&quot; 办理业务&quot;); }); } }catch (Exception e) { e.printStackTrace(); }finally { //关闭 threadPool.shutdown(); } }} 为什么不允许适用不允许Executors.的方式手动创建线程池,如下图 Fork/Join Fork/Join框架简介 Fork/Join它可以将一个大的任务拆分成多个子任务进行并行处理，最后将子任务结果合并成最后的计算结果，并进行输出。Fork/Join框架要完成两件事情： Fork：把一个复杂任务进行分拆，大事化小 Join：把分拆任务的结果进行合并 任务分割：首先Fork/Join框架需要把大的任务分割成足够小的子任务，如果子任务比较大的话还要对子任务进行继续分割 执行任务并合并结果：分割的子任务分别放到双端队列里，然后几个启动线程分别从双端队列里获取任务执行。子任务执行完的结果都放在另外一个队列里，启动一个线程从队列里取数据，然后合并这些数据。 在Java的Fork/Join框架中，使用两个类完成上述操作 • ForkJoinTask:我们要使用Fork/Join框架，首先需要创建一个ForkJoin任务。该类提供了在任务中执行fork和join的机制。通常情况下我们不需要直接集成ForkJoinTask类，只需要继承它的子类，Fork/Join框架提供了两个子类： a.RecursiveAction：用于没有返回结果的任务 b.RecursiveTask:用于有返回结果的任务 • ForkJoinPool:ForkJoinTask需要通过ForkJoinPool来执行 • RecursiveTask: 继承后可以实现递归(自己调自己)调用的任务 Fork/Join框架的实现原理 ​ ForkJoinPool由ForkJoinTask数组和ForkJoinWorkerThread数组组成，ForkJoinTask数组负责将存放以及将程序提交给ForkJoinPool，而ForkJoinWorkerThread负责执行这些任务。 Fork方法 ​ Fork方法的实现原理： 当我们调用ForkJoinTask的fork方法时，程序会把任务放在ForkJoinWorkerThread的pushTask的workQueue中，异步地执行这个任务，然后立即返回结果 123456public final ForkJoinTask&lt;V&gt; fork() { Thread t; if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ((ForkJoinWorkerThread)t).workQueue.push(this); else ForkJoinPool.common.externalPush(this); return this; } pushTask方法把当前任务存放在ForkJoinTask数组队列里。然后再调用ForkJoinPool的signalWork()方法唤醒或创建一个工作线程来执行任务。代码如下： 123456789101112final void push(ForkJoinTask&lt;?&gt; task) { ForkJoinTask&lt;?&gt;[] a; ForkJoinPool p; int b = base, s = top, n; if ((a = array) != null) { // ignore if queue removed int m = a.length - 1; // fenced write for task visibility U.putOrderedObject(a, ((m &amp; s) &lt;&lt; ASHIFT) + ABASE, task); U.putOrderedInt(this, QTOP, s + 1); if ((n = s - b) &lt;= 1) { if ((p = pool) != null) p.signalWork(p.workQueues, this);//执行 } else if (n &gt;= m) growArray(); } } join方法 Join方法的主要作用是阻塞当前线程并等待获取结果。让我们一起看看ForkJoinTask的join方法的实现，代码如下： 123456public final V join() { int s; if ((s = doJoin() &amp; DONE_MASK) != NORMAL) reportException(s); return getRawResult(); } 它首先调用doJoin方法，通过doJoin()方法得到当前任务的状态来判断返回什么结果，任务状态有4种： 已完成（NORMAL）、被取消（CANCELLED）、信号（SIGNAL）和出现异常（EXCEPTIONAL） • 如果任务状态是已完成，则直接返回任务结果。 • 如果任务状态是被取消，则直接抛出CancellationException • 如果任务状态是抛出异常，则直接抛出对应的异常 让我们分析一下doJoin方法的实现 12345678910111213141516private int doJoin() { int s; Thread t; ForkJoinWorkerThread wt; ForkJoinPool.WorkQueue w; return (s = status) &lt; 0 ? s : ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) ? (w = (wt = (ForkJoinWorkerThread)t).workQueue). tryUnpush(this) &amp;&amp; (s = doExec()) &lt; 0 ? s : wt.pool.awaitJoin(w, this, 0L) : externalAwaitDone(); } final int doExec() { int s; boolean completed; if ((s = status) &gt;= 0) { try { completed = exec(); } catch (Throwable rex) { return setExceptionalCompletion(rex); } if (completed) s = setCompletion(NORMAL); } return s; } 在doJoin()方法流程如下: 首先通过查看任务的状态，看任务是否已经执行完成，如果执行完成，则直接返回任务状态； 如果没有执行完，则从任务数组里取出任务并执行。 如果任务顺利执行完成，则设置任务状态为NORMAL，如果出现异常，则记录异常，并将任务状态设置为EXCEPTIONAL。 Fork/Join框架的异常处理 ForkJoinTask在执行的时候可能会抛出异常，但是我们没办法在主线程里直接捕获异常，所以ForkJoinTask提供了isCompletedAbnormally()方法来检查任务是否已经抛出异常或已经被取消了，并且可以通过ForkJoinTask的getException方法获取异常。 getException方法返回Throwable对象，如果任务被取消了则返回CancellationException。如果任务没有完成或者没有抛出异常则返回null。 入门案例 场景: 生成一个计算任务，计算1+2+3.........+1000,每100个数切分一个子任务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.atguigu.forkjoin;import java.util.concurrent.*;class MyTask extends RecursiveTask&lt;Integer&gt; { //拆分差值不能超过10，计算10以内运算 private static final Integer VALUE = 10; private int begin ;//拆分开始值 private int end;//拆分结束值 private int result ; //返回结果 //创建有参数构造 public MyTask(int begin,int end) { this.begin = begin; this.end = end; } //拆分和合并过程 @Override protected Integer compute() { //判断相加两个数值是否大于10 if((end-begin)&lt;=VALUE) { //相加操作 for (int i = begin; i &lt;=end; i++) { result = result+i; } } else {//进一步拆分 //获取中间值 int middle = (begin+end)/2; //拆分左边 MyTask task01 = new MyTask(begin,middle); //拆分右边 MyTask task02 = new MyTask(middle+1,end); //调用方法拆分 task01.fork(); task02.fork(); //合并结果 result = task01.join()+task02.join(); } return result; }}public class ForkJoinDemo { public static void main(String[] args) throws ExecutionException, InterruptedException { //创建MyTask对象 MyTask myTask = new MyTask(0,100); //创建分支合并池对象 ForkJoinPool forkJoinPool = new ForkJoinPool(); ForkJoinTask&lt;Integer&gt; forkJoinTask = forkJoinPool.submit(myTask); //获取最终合并之后结果 Integer result = forkJoinTask.get(); System.out.println(result); //关闭池对象 forkJoinPool.shutdown(); }} CompletableFuture CompletableFuture简介 同步【不在等待】、异步【不在先做其他的】 CompletableFuture在Java里面被用于异步编程，异步通常意味着非阻塞，可以使得我们的任务单独运行在与主线程分离的其他线程中，并且通过回调可以在主线程中得到异步任务的执行状态，是否完成，和是否异常等信息。 CompletableFuture实现了Future, CompletionStage接口，实现了Future接口就可以兼容现在有线程池框架，而CompletionStage接口才是异步编程的接口抽象，里面定义多种异步方法，通过这两者集合，从而打造出了强大的CompletableFuture类。 Future与CompletableFuture Futrue在Java里面，通常用来表示一个异步任务的引用，比如我们将任务提交到线程池里面，然后我们会得到一个Futrue，在Future里面有isDone方法来 判断任务是否处理结束，还有get方法可以一直阻塞直到任务结束然后获取结果，但整体来说这种方式，还是同步的，因为需要客户端不断阻塞等待或者不断轮询才能知道任务是否完成。 Future的主要缺点如下： （1）不支持手动完成 我提交了一个任务，但是执行太慢了，我通过其他路径已经获取到了任务结果，现在没法把这个任务结果通知到正在执行的线程，所以必须主动取消或者一直等待它执行完成 （2）不支持进一步的非阻塞调用 通过Future的get方法会一直阻塞到任务完成，但是想在获取任务之后执行额外的任务，因为Future不支持回调函数，所以无法实现这个功能 （3）不支持链式调用 对于Future的执行结果，我们想继续传到下一个Future处理使用，从而形成一个链式的pipline调用，这在Future中是没法实现的。 （4）不支持多个Future合并 比如我们有10个Future并行执行，我们想在所有的Future运行完毕之后，执行某些函数，是没法通过Future实现的。 （5）不支持异常处理 Future的API没有任何的异常处理的api，所以在异步运行时，如果出了问题是不好定位的。 CompletableFuture入门 使用CompletableFuture 场景:主线程里面创建一个CompletableFuture，然后主线程调用get方法会阻塞，最后我们在一个子线程中使其终止。 123456789101112131415/** * 主线程里面创建一个CompletableFuture，然后主线程调用get方法会阻塞，最后我们在一个子线程中使其终止 * @param args */ public static void main(String[] args) throws Exception{ CompletableFuture&lt;String&gt; future = new CompletableFuture&lt;&gt;(); new Thread(() -&gt; { try{ System.out.println(Thread.currentThread().getName() + &quot;子线程开始干活&quot;); //子线程睡5秒 Thread.sleep(5000); //在子线程中完成主线程 future.complete(&quot;success&quot;); }catch (Exception e){ e.printStackTrace(); } }, &quot;A&quot;).start(); //主线程调用get方法阻塞 System.out.println(&quot;主线程调用get方法获取结果为: &quot; + future.get()); System.out.println(&quot;主线程完成,阻塞结束!!!!!!&quot;); } 没有返回值的异步任务 12345678910111213/** * 没有返回值的异步任务 * @param args */ public static void main(String[] args) throws Exception{ System.out.println(&quot;主线程开始&quot;); //运行一个没有返回值的异步任务 CompletableFuture&lt;Void&gt; future = CompletableFuture.runAsync(() -&gt; { try { System.out.println(&quot;子线程启动干活&quot;); Thread.sleep(5000); System.out.println(&quot;子线程完成&quot;); } catch (Exception e) { e.printStackTrace(); }}); //主线程阻塞 future.get(); System.out.println(&quot;主线程结束&quot;); } 有返回值的异步任务 1234567891011121314151617/** * 没有返回值的异步任务 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); //运行一个有返回值的异步任务 CompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(&quot;子线程开始任务&quot;); Thread.sleep(5000); } catch (Exception e) { e.printStackTrace(); } return &quot;子线程完成了!&quot;; }); //主线程阻塞 String s = future.get(); System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + s); } 线程依赖 当一个线程依赖另一个线程时，可以使用 thenApply 方法来把这两个线程串行化。 123456789101112131415161718192021private static Integer num = 10; /** * 先对一个数加10,然后取平方 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(&quot;加10任务开始&quot;); num += 10; } catch (Exception e) { e.printStackTrace(); } return num; }).thenApply(integer -&gt; { return num * num; }); Integer integer = future.get(); System.out.println(&quot;主线程结束, 子线程的结果为:&quot; + integer); } 消费处理结果 thenAccept 消费处理结果, 接收任务的处理结果，并消费处理，无返回结果。 12345678910111213141516171819public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture.supplyAsync(() -&gt; { try { System.out.println(&quot;加10任务开始&quot;); num += 10; } catch (Exception e) { e.printStackTrace(); } return num; }).thenApply(integer -&gt; { return num * num; }).thenAccept(new Consumer&lt;Integer&gt;() { @Override public void accept(Integer integer) { System.out.println(&quot;子线程全部处理完成,最后调用了accept,结果为:&quot; + integer); } }); } 异常处理 exceptionally异常处理,出现异常时触发 12345678910111213public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { int i = 1 / 0; System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }).exceptionally(ex -&gt; { System.out.println(ex.getMessage()); return -1; }); System.out.println(future.get()); } handle类似于thenAccept/thenRun方法,是最后一步的处理调用,但是同时可以处理异常 123456789101112131415161718public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }).handle((i, ex) -&gt; { System.out.println(&quot;进入handle方法&quot;); if (ex != null) { System.out.println(&quot;发生了异常,内容为:&quot; + ex.getMessage()); return -1; } else { System.out.println(&quot;正常完成,内容为: &quot; + i); return i; } }); System.out.println(future.get()); } 结果合并 thenCompose合并两个有依赖关系的CompletableFutures的执行结果 123456789101112131415public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); //第一步加10 CompletableFuture&lt;Integer&gt; future = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }); //合并 CompletableFuture&lt;Integer&gt; future1 = future.thenCompose(i -&gt; //再来一个CompletableFuture CompletableFuture.supplyAsync(() -&gt; { return i + 1; })); System.out.println(future.get()); System.out.println(future1.get());} thenCombine合并两个没有依赖关系的CompletableFutures任务 1234567891011121314151617181920212223public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }); CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;乘以10任务开始&quot;); num = num * 10; return num; }); //合并两个结果 CompletableFuture&lt;Object&gt; future = job1.thenCombine(job2, new BiFunction&lt;Integer, Integer, List&lt;Integer&gt;&gt;() { @Override public List&lt;Integer&gt; apply(Integer a, Integer b) { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); return list; } }); System.out.println(&quot;合并结果为:&quot; + future.get()); } 合并多个任务的结果allOf与anyOf allOf: 一系列独立的future任务，等其所有的任务执行完后做一些事情 123456789101112131415161718192021222324252627282930313233/** * 先对一个数加10,然后取平方 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); List&lt;CompletableFuture&gt; list = new ArrayList&lt;&gt;(); CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;加10任务开始&quot;); num += 10; return num; }); list.add(job1); CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;乘以10任务开始&quot;); num = num * 10; return num; }); list.add(job2); CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;减以10任务开始&quot;); num = num * 10; return num; }); list.add(job3); CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; { System.out.println(&quot;除以10任务开始&quot;); num = num * 10; return num; }); list.add(job4); //多任务合并 List&lt;Integer&gt; collect = list.stream().map(CompletableFuture&lt;Integer&gt;::join).collect(Collectors.toList()); System.out.println(collect); } anyOf: 只要在多个future里面有一个返回，整个任务就可以结束，而不需要等到每一个future结束 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 先对一个数加10,然后取平方 * @param args */ public static void main(String[] args) throws Exception { System.out.println(&quot;主线程开始&quot;); CompletableFuture&lt;Integer&gt;[] futures = new CompletableFuture[4]; CompletableFuture&lt;Integer&gt; job1 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(5000); System.out.println(&quot;加10任务开始&quot;); num += 10; return num; } catch (Exception e) { return 0; } }); futures[0] = job1; CompletableFuture&lt;Integer&gt; job2 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(2000); System.out.println(&quot;乘以10任务开始&quot;); num = num * 10; return num; } catch (Exception e) { return 1; } }); futures[1] = job2; CompletableFuture&lt;Integer&gt; job3 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(3000); System.out.println(&quot;减以10任务开始&quot;); num = num * 10; return num; } catch (Exception e) { return 2; } }); futures[2] = job3; CompletableFuture&lt;Integer&gt; job4 = CompletableFuture.supplyAsync(() -&gt; { try { Thread.sleep(4000); System.out.println(&quot;除以10任务开始&quot;); num = num * 10; return num; } catch (Exception e) { return 3; } }); futures[3] = job4; CompletableFuture&lt;Object&gt; future = CompletableFuture.anyOf(futures); System.out.println(future.get()); } 反射 IO 磁盘操作 File对象 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import java.io.File;import java.io.FilenameFilter;import java.io.IOException;import java.nio.file.Path;import java.nio.file.Paths;/**磁盘操作**/public class FileObject { public static void main(String[] args) throws IOException {// 即使传入的文件或目录不存在，代码也不会出错，因为构造一个File对象，并不会导致任何磁盘操作,调用方法时才会磁盘操作 //绝对路径是以根目录开头的完整路径Java字符串中以//代表/// File表示文件或目录 //\\u202A系统复制产生 File file = new File(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\lixianglun\\\\fileTest.txt&quot;); //相对路径（可以用.表示当前目录，..表示上级目录）绝对路径去掉当前目录 File afile = new File(&quot;..\\\\fileTest.txt&quot;); File catalog=new File(&quot;C:\\\\Users\\\\Administrator\\\\Desktop&quot;); System.out.println(file); //返回构造方法传入的路径 System.out.println(file.getPath()); //返回绝对路径 System.out.println(file.getAbsolutePath()); //返回的是规范路径。 System.out.println(afile.getCanonicalPath()); //根据当前平台打印&quot;\\&quot;或&quot;/&quot; System.out.println(File.separator); System.out.println(file.isFile()); //判断当前文件，目录是否存在 System.out.println(file.isDirectory()); System.out.println(file.canRead()); System.out.println(file.canWrite()); System.out.println(file.canExecute()); System.out.println(afile.length()); //add、delete// if (file.createNewFile()) {// // 文件创建成功:// // TOD O:TODO提交代码会提醒// if (file.delete()) {// // 删除文件成功:// }// }// 递归列出目录下的所有文件// public static void listAllFiles(File dir) {// if (dir == null || !dir.exists()) {// return;// }// if (dir.isFile()) {// System.out.println(dir.getName());// return;// }// for (File file : dir.listFiles()) {// listAllFiles(file);// }//}//有些时候，程序需要读写一些临时文件，File对象提供了createTempFile()来创建一个临时文件，以及deleteOnExit()在JVM退出时自动删除该文件。 System.out.println(catalog.list()); //目录下的文件和子目录名 System.out.println(catalog.listFiles()); //listFiles()提供了一系列重载方法，可以过滤不想要的文件和目录 // 仅列出.exe文件 File[] fs2 = catalog.listFiles(new FilenameFilter() { @Override public boolean accept(File dir, String name) { return name.endsWith(&quot;.exe&quot;); // 返回true表示接受该文件 } }); printFiles(fs2); } /**boolean mkdir()：创建当前File对象表示的目录； //boolean mkdirs()：创建当前File对象表示的目录，并在必要时将不存在的父目录也创建出来； //boolean delete()：删除当前File对象表示的目录，当前目录必须为空才能删除成功。*/ static void printFiles(File[] files) { System.out.println(&quot;==========&quot;); if (files != null) { for (File f : files) { System.out.println(f); } } System.out.println(&quot;==========&quot;); // 构造一个Path对象 Path p1 = Paths.get(&quot;.&quot;, &quot;project&quot;, &quot;study&quot;); System.out.println(p1); // 转换为绝对路径 Path p2 = p1.toAbsolutePath(); System.out.println(p2); // 转换为规范路径 Path p3 = p2.normalize(); System.out.println(p3); // 转换为File对象 File f = p3.toFile(); System.out.println(f); // 可以直接遍历Path for (Path p : Paths.get(&quot;src/main&quot;).toAbsolutePath()) { System.out.println(&quot; &quot; + p); } }} 字节操作 InputStream 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;/**InputStream并不是一个接口，而是一个抽象类，它是所有输入流的超类。这个抽象类定义的一个最重要的方法就是int read()，签名如下：public abstract int read() throws IOException;这个方法会读取输入流的下一个字节，并返回字节表示的int值（0~255）。如果已读到末尾，返回-1表示不能继续读取了。 * @author lxl*/public class InputStreamL { public static void main(String[] args) throws IOException { //inputStream字节输入流，抽象类，只提供方法声明，不提供具体实现 // 创建一个FileInputStream对象: InputStream inputStream = new FileInputStream(&quot;fileTest.txt&quot;); for (; ; ) {//死循环，比while（true）更好，因为编译后指令更少，而且没有判断跳转，更加简洁 // 反复调用read()方法，直到返回-1 int n = inputStream.read(); if (n == -1) { break; } // 打印byte的值 System.out.println(n); } inputStream.close(); // 关闭流释放对应的底层资源，便让操作系统把资源释放掉，否则，应用程序占用的资源会越来越多，不但白白占用内存，还会影响其他应用程序的运行。// 利用缓冲区一次读取多个字节 try (InputStream input = new FileInputStream(&quot;fileTest.txt&quot;)) { // 定义1000个字节大小的缓冲区: byte[] buffer = new byte[1000]; int n; // 读取到缓冲区,read()方法是阻塞的（Blocking），必须等待read()方法返回才能执行下一行代码 while ((n = input.read(buffer)) != -1) { System.out.println(&quot;read &quot; + n + &quot; bytes.&quot;); } } try { //准备文件lol.txt其中的内容是AB，对应的ASCII分别是65 66 File f =new File(&quot;fileTest.txt&quot;); //创建基于文件的输入流 FileInputStream fis =new FileInputStream(f); //创建字节数组，其长度就是文件的长度 byte[] all =new byte[(int) f.length()]; //以字节流的形式读取文件所有内容 fis.read(all); for (byte b : all) { //打印出来是65 66 System.out.println(b); } //每次使用完流，都应该进行关闭 fis.close(); } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } }}/*装饰者模式Java I/O 使用了装饰者模式来实现。以 InputStream 为例，InputStream 是抽象组件；FileInputStream 是 InputStream 的子类，属于具体组件，提供了字节流的输入操作；FilterInputStream 属于抽象装饰者，装饰者用于装饰组件，为组件提供额外的功能。例如 BufferedInputStream 为 FileInputStream 提供缓存的功能。实例化一个具有缓存功能的字节流对象时，只需要在 FileInputStream 对象上再套一层 BufferedInputStream 对象即可。FileInputStream fileInputStream = new FileInputStream(filePath);BufferedInputStream bufferedInputStream = new BufferedInputStream(fileInputStream);DataInputStream 装饰者提供了对更多数据类型进行输入的操作，比如 int、double 等基本类型。*/ OutputStream 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.io.*;/**为什么要有flush()？因为向磁盘、网络写入数据的时候，出于效率的考虑，操作系统并不是输出一个字节就立刻写入到文件或者发送到网络，而是把输出的字节先放到内存的一个缓冲区里（本质上就是一个byte[]数组），等到缓冲区写满了，再一次性写入文件或者网络。对于很多IO设备来说，一次写一个字节和一次写1000个字节，花费的时间几乎是完全一样的，所以OutputStream有个flush()方法，能强制把缓冲区内容输出。通常情况下，我们不需要调用这个flush()方法，因为缓冲区写满了OutputStream会自动调用它并且，在调用close()方法关闭OutputStream之前，也会自动调用flush()方法。特别情况，缓冲区过大，需马上发送的时候手动flush * @author lxl*/public class OutputStreamL { /**实现文件复制*/ public static void copyFile(String src, String dist) throws IOException { FileInputStream in = new FileInputStream(src); FileOutputStream out = new FileOutputStream(dist); byte[] buffer = new byte[20 * 1024]; int cnt; // read() 最多读取 buffer.length 个字节 // 返回的是实际读取的个数 // 返回 -1 的时候表示读到 eof，即文件尾 while ((cnt = in.read(buffer, 0, buffer.length)) != -1) { out.write(buffer, 0, cnt); } in.close(); } public static void main(String[] args) throws IOException { //文件存在则覆盖，不存在创建 try(OutputStream output = new FileOutputStream(&quot;F:\\\\PROJECT\\\\IDEA\\\\mylearnproject\\\\src\\\\main\\\\resources\\\\fileTest.txt&quot;)){ byte[] data = {34, 35}; // H output.write(72); // e output.write(101); // l output.write(108); // l output.write(108); // o output.write(data);// output.write(111); // o // 一次性写入若干字节 output.write(&quot;Hello瓦达无多&quot;.getBytes(&quot;UTF-8&quot;)); // } output.close();//try之后编译器在此自动为我们写入finally并调用close() }}} 字符操作 Reader InputStream是一个字节流，即以byte为单位读取，而Reader是一个字符流，即以char为单位读取 InputStream Reader 字节流，以byte为单位 字符流，以char为单位 读取字节（-1，0~255）：int read() 读取字符（-1，0~65535）：int read() 读到字节数组：int read(byte[] b) 读到字符数组：int read(char[] c) java.io.Reader是所有字符输入流的超类，它最主要的方法是： public int read() throws IOException; 这个方法读取字符流的下一个字符，并返回字符表示的int，范围是0~65535。如果已读到末尾，返回-1。 FileReader打开文件并获取Reader // 创建一个FileReader对象: Reader reader = new FileReader(&quot;src/readme.txt&quot;); // 字符编码是??? for (;😉 { int n = reader.read(); // 反复调用read()方法，直到返回-1 if (n == -1) { break; } System.out.println((char)n); // 打印char } reader.close(); // 关闭流 避免乱码问题，我们需要在创建FileReader时指定编码： 123456789101112131415161718192021 Reader reader = new FileReader(&quot;src/readme.txt&quot;, StandardCharsets.UTF_8); - try (resource)来保证Reader在无论有没有IO错误的时候都能够正确地关闭 - Reader还提供了一次性读取若干字符并填充到char[]数组的方法：public int read(char[] c) throws IOException它返回实际读入的字符个数，最大不超过char[]数组的长度。返回-1表示流结束。 - CharArrayReader把一个char[]数组变成一个Reader - try (Reader reader = new CharArrayReader(&quot;Hello&quot;.toCharArray())) {}- StringReader- InputStreamReader - 除了特殊的CharArrayReader和StringReader，普通的Reader实际上是基于InputStream构造的，因为Reader需要从InputStream中读入字节流（byte），然后，根据编码设置，再转换为char就可以实现字符流。如果我们查看FileReader的源码，它在内部实际上持有一个FileInputStream。 既然Reader本质上是一个基于InputStream的byte到char的转换器，那么，如果我们已经有一个InputStream，想把它转换为Reader，是完全可行的。InputStreamReader就是这样一个转换器，它可以把任何InputStream转换为Reader。 - // 持有InputStream: InputStream input = new FileInputStream(&quot;src/readme.txt&quot;);// 变换为Reader:Reader reader = new InputStreamReader(input, &quot;UTF-8&quot;); - try (Reader reader = new InputStreamReader(new FileInputStream(&quot;src/readme.txt&quot;), &quot;UTF-8&quot;)) {// TODO:} Writer Writer就是带编码转换器的OutputStream，它把char转换为byte并输出 OutputStream Writer 字节流，以byte为单位 字符流，以char为单位 写入字节（0~255）：void write(int b) 写入字符（0~65535）：void write(int c) 写入字节数组：void write(byte[] b) 写入字符数组：void write(char[] c) 无对应方法 写入String：void write(String s) Writer是所有字符输出流的超类，它提供的方法主要有： 写入一个字符（0~65535）：void write(int c)； 写入字符数组的所有字符：void write(char[] c)； 写入String表示的所有字符：void write(String s)。 - FileWriter - try (Writer writer = new FileWriter(&quot;readme.txt&quot;, StandardCharsets.UTF_8)) { writer.write('H'); // 写入单个字符 writer.write(&quot;Hello&quot;.toCharArray()); // 写入char[] writer.write(&quot;Hello&quot;); // 写入String } - CharArrayWriter - StringWriter - OutputStreamWriter String 的编码方式 String 可以看成一个字符序列，可以指定一个编码方式将它编码为字节序列，也可以指定一个编码方式将一个字节序列解码为 String。 String str1 = &quot;中文&quot;; d byte[] bytes = str1.getBytes(&quot;UTF-8&quot;); String str2 = new String(bytes, &quot;UTF-8&quot;); System.out.println(str2); 在调用无参数 getBytes() 方法时，默认的编码方式不是 UTF-16be。双字节编码的好处是可以使用一个 char 存储中文和英文，而将 String 转为 bytes[] 字节数组就不再需要这个好处，因此也就不再需要双字节编码。getBytes() 的默认编码方式与平台有关，一般为 UTF-8。 byte[] bytes = str1.getBytes(); 对象操作 - 序列化是指把一个Java对象变成二进制内容，本质上就是一个byte[]数组,序列化后可以把byte[]保存到文件中，或者把byte[]通过网络传输到远程，这样，就相当于把Java对象存储到文件或者通过网络传输出去了 - 一个Java对象要能序列化，必须实现一个特殊的java.io.Serializable接口 - 把一个Java对象变为byte[]数组，需要使用ObjectOutputStream。它负责把一个Java对象写入一个字节流 - public class Main { public static void main(String[] args) throws IOException { ByteArrayOutputStream buffer = new ByteArrayOutputStream(); try (ObjectOutputStream output = new ObjectOutputStream(buffer)) { // 写入int: output.writeInt(12345); // 写入String: output.writeUTF(&quot;Hello&quot;); // 写入Object: output.writeObject(Double.valueOf(123.456)); } System.out.println(Arrays.toString(buffer.toByteArray())); } } - 反序列化，即把一个二进制内容（也就是byte[]数组）变回Java对象。有了反序列化，保存到文件中的byte[]数组又可以“变回”Java对象，或者从网络上读取byte[]并把它“变回”Java对象。 - ObjectInputStream负责从一个字节流读取Java对象 - try (ObjectInputStream input = new ObjectInputStream(...)) { int n = input.readInt(); String s = input.readUTF(); Double d = (Double) input.readObject(); } - readObject()可能抛出的异常有： ClassNotFoundException：没有找到对应的Class； InvalidClassException：Class不匹配。 - 安全性 - Java的序列化机制仅适用于Java，如果需要与其它语言交换数据，必须使用通用的序列化方法，例如JSON。 网络操作 NIO 流与块 I/O 与 NIO 最重要的区别是数据打包和传输的方式，I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。 面向流的 I/O 一次处理一个字节数据：一个输入流产生一个字节数据，一个输出流消费一个字节数据。为流式数据创建过滤器非常容易，链接几个过滤器，以便每个过滤器只负责复杂处理机制的一部分。不利的一面是，面向流的 I/O 通常相当慢。 面向块的 I/O 一次处理一个数据块，按块处理数据比按流处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 I/O 包和 NIO 已经很好地集成了，java.io.* 已经以 NIO 为基础重新实现了，所以现在它可以利用 NIO 的一些特性。例如，java.io.* 包中的一些类包含以块的形式读写数据的方法，这使得即使在面向流的系统中，处理速度也会更快。 通道与缓冲区 通道 通道 Channel 是对原 I/O 包中的流的模拟，可以通过它读取和写入数据。 通道与流的不同之处在于，流只能在一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)，而通道是双向的，可以用于读、写或者同时用于读写。 通道包括以下类型： FileChannel：从文件中读写数据； DatagramChannel：通过 UDP 读写网络中数据； SocketChannel：通过 TCP 读写网络中数据； ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel,本身并不传数据。 FileChannel 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package pers.lxl.mylearnproject.javase.io.nio.channel;import java.io.IOException;import java.io.RandomAccessFile;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;public class FileChannelDemo { public static void main(String[] args) throws IOException { // 读// 需要通过使用一个InputStream、OutputStream或RandomAccessFile来获取一个FileChannel实例 RandomAccessFile aFile = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); FileChannel inChannel = aFile.getChannel(); ByteBuffer buf = ByteBuffer.allocate(48);// 获取当前通道位置 long pos = inChannel.position();// 设置当前通道位置 inChannel.position(pos + 6);// 返回通道当前大小 System.out.println(&quot;此时通道大小：&quot;+inChannel.size());// 截取(删除)文件7字节后部分 inChannel.truncate(7); int bytesRead = inChannel.read(buf); while (bytesRead != -1) { System.out.println(&quot;读取： &quot; + bytesRead); buf.flip(); while (buf.hasRemaining()) { System.out.print((char) buf.get()); } buf.clear(); bytesRead = inChannel.read(buf); } //强制将通道里的剩余数据写到磁盘中 inChannel.force(true); aFile.close(); System.out.println(&quot;操作结束&quot;); //覆盖起始位置开始的对应位置 写 RandomAccessFile aFile1 = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis(); ByteBuffer buf1 = ByteBuffer.allocate(48); FileChannel inChannel1 = aFile1.getChannel(); buf1.clear(); buf1.put(newData.getBytes()); buf1.flip(); while (buf1.hasRemaining()) { inChannel1.write(buf1); } inChannel1.close(); RandomAccessFile aaFile = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); FileChannel fromChannel = aaFile.getChannel(); RandomAccessFile bbFile = new RandomAccessFile(&quot;src/main/resources/transferFrom.txt&quot;, &quot;rw&quot;); FileChannel toChannel = bbFile.getChannel(); long position = 0; long count = fromChannel.size(); // fromChannel to toChannel toChannel.transferFrom(fromChannel, position, count); // fromChannel to toChannel fromChannel.transferTo(position, count, toChannel); aaFile.close(); bbFile.close(); System.out.println(&quot;通道传输over!&quot;); }} ServerSocketChannel （1）新的socket通道类可以运行非阻塞模式并且是可选择的，可以激活大程序（如网络服务器和中间件组件）巨大的可伸缩性和灵活性。本节中我们会看到，再也没有为每个socket连接使用一个线程的必要了，也避免了管理大量线程所需的上下文交换开销。借助新的NIO类，一个或几个线程就可以管理成百上千的活动socket连接了并且只有很少甚至可能没有性能损失。所有的socket通道类(DatagramChannel、SocketChannel和ServerSocketChannel)都继承了位于java.nio.channels.spi包中的AbstractSelectableChannel。这意味着我们可以用一个Selector对象来执行socket通道的就绪选择（readiness selection）。 （2）请注意DatagramChannel和SocketChannel实现定义读和写功能的接口而ServerSocketChannel不实现。ServerSocketChannel负责监听传入的连接和创建新的SocketChannel对象，它本身从不传输数据。 （3）在我们具体讨论每一种socket通道前，您应该了解socket和socket通道之间的关系。通道是一个连接I/O服务导管并提供与该服务交互的方法。就某个socket而言，它不会再次实现与之对应的socket通道类中的socket协议API，而java.net中已经存在的socket通道都可以被大多数协议操作重复使用。 全部socket通道类（DatagramChannel、SocketChannel和ServerSocketChannel）在被实例化时都会创建一个对等socket对象。这些是我们所熟悉的来自java.net的类（Socket、ServerSocket和DatagramSocket），它们已经被更新以识别通道。对等socket可以通过调用socket( )方法从一个通道上获取。此外，这三个java.net类现在都有getChannel( )方法。 （4）要把一个socket通道置于非阻塞模式，我们要依靠所有socket通道类的公有超级类：SelectableChannel。就绪选择（readiness selection）是一种可以用来查询通道的机制，该查询可以判断通道是否准备好执行一个目标操作，如读或写。非阻塞I/O和可选择性是紧密相连的，那也正是管理阻塞模式的API代码要在SelectableChannel超级类中定义的原因。 设置或重新设置一个通道的阻塞模式是很简单的，只要调用configureBlocking( )方法即可，传递参数值为true则设为阻塞模式，参数值为false值设为非阻塞模式。可以通过调用isBlocking( )方法来判断某个socket通道当前处于哪种模式。 ​ 非阻塞socket通常被认为是服务端使用的，因为它们使同时管理很多socket通道变得更容易。但是，在客户端使用一个或几个非阻塞模式的socket通道也是有益处的，例如，借助非阻塞socket通道，GUI程序可以专注于用户请求并且同时维护与一个或多个服务器的会话。在很多程序上，非阻塞模式都是有用的。 偶尔地，我们也会需要防止socket通道的阻塞模式被更改。API中有一个blockingLock( )方法，该方法会返回一个非透明的对象引用。返回的对象是通道实现修改阻塞模式时内部使用的。只有拥有此对象的锁的线程才能更改通道的阻塞模式。 ​ ServerSocketChannel是一个基于通道的socket监听器。它同我们所熟悉的java.net.ServerSocket执行相同的任务，不过它增加了通道语义，因此能够在非阻塞模式下运行。 ​ 由于ServerSocketChannel没有bind()方法，因此有必要取出对等的socket并使用它来绑定到一个端口以开始监听连接。我们也是使用对等ServerSocket的API来根据需要设置其他的socket选项。 ​ 同java.net.ServerSocket一样，ServerSocketChannel也有accept( )方法。一旦创建了一个ServerSocketChannel并用对等socket绑定了它，然后您就可以在其中一个上调用accept()。如果您选择在ServerSocket上调用accept( )方法，那么它会同任何其他的ServerSocket表现一样的行为：总是阻塞并返回一个java.net.Socket对象。如果您选择在ServerSocketChannel上调用accept( )方法则会返回SocketChannel类型的对象，返回的对象能够在非阻塞模式下运行。 换句话说： ​ ServerSocketChannel的accept()方法会返回SocketChannel类型对象，SocketChannel可以在非阻塞模式下运行。其它Socket的accept()方法会阻塞返回一个Socket对象。如果ServerSocketChannel以非阻塞模式被调用，当没有传入连接在等待时，ServerSocketChannel.accept( )会立即返回null。正是这种检查连接而不阻塞的能力实现了可伸缩性并降低了复杂性。可选择性也因此得到实现。我们可以使用一个选择器实例来注册ServerSocketChannel对象以实现新连接到达时自动通知的功能。 12345678910111213141516171819202122232425262728293031323334353637import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;//http://localhost:1234/public class ServerSocketChannelDemo { public static final String GREETING = &quot;Hello java nio.\\r\\n&quot;; public static void main(String[] argv) throws Exception { int port = 1234; // default if (argv.length &gt; 0) { port = Integer.parseInt(argv[0]); } ByteBuffer buffer = ByteBuffer.wrap(GREETING.getBytes());// 打开 ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(port)); ssc.configureBlocking(false); while (true) { System.out.println(&quot;Waiting for connections&quot;);// 监听新的链接，阻塞会在此阻塞住进程非阻塞会返回null SocketChannel sc = ssc.accept(); if (sc == null) { System.out.println(&quot;null&quot;); Thread.sleep(2000); } else { System.out.println(&quot;Incoming connection from: &quot; + sc.socket().getRemoteSocketAddress());// 指针指向0 buffer.rewind(); sc.write(buffer);// 关闭 sc.close(); } } }} SocketChannel ​ Java NIO中的SocketChannel是一个连接到TCP网络套接字的通道。 A selectable channel for stream-oriented connecting sockets. 以上是Java docs中对于SocketChannel的描述：SocketChannel是一种面向流连接sockets套接字的可选择通道。从这里可以看出： ​ • SocketChannel是用来连接Socket套接字 ​ •SocketChannel主要用途用来处理网络I/O的通道 ​ • SocketChannel是基于TCP连接传输 ​ • SocketChannel实现了可选择通道，可以被多路复用的 ​ 特征： （1）对于已经存在的socket不能创建SocketChannel （2）SocketChannel中提供的open接口创建的Channel并没有进行网络级联，需要使用connect接口连接到指定地址 （3）未进行连接的SocketChannle执行I/O操作时，会抛出NotYetConnectedException （4）SocketChannel支持两种I/O模式：阻塞式和非阻塞式 （5）SocketChannel支持异步关闭。如果SocketChannel在一个线程上read阻塞，另一个线程对该SocketChannel调用shutdownInput，则读阻塞的线程将返回-1表示没有读取任何数据；如果SocketChannel在一个线程上write阻塞，另一个线程对该SocketChannel调用shutdownWrite，则写阻塞的线程将抛出AsynchronousCloseException （6）SocketChannel支持设定参数 ​ SO_SNDBUF 套接字发送缓冲区大小 ​ SO_RCVBUF 套接字接收缓冲区大小 ​ SO_KEEPALIVE 保活连接 ​ O_REUSEADDR 复用地址 ​ SO_LINGER 有数据传输时延缓关闭Channel (只有在非阻塞模式下有用) ​ TCP_NODELAY 禁用Nagle算法 12345678910111213141516171819202122232425262728293031323334package pers.lxl.mylearnproject.javase.io.nio.channel;import java.io.IOException;import java.net.InetSocketAddress;import java.net.StandardSocketOptions;import java.nio.ByteBuffer;import java.nio.channels.SocketChannel;public class SocketChannelDemo { public static void main(String[] args) throws IOException { SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;www.baidu.com&quot;, 80)); SocketChannel socketChanne2 = SocketChannel.open(); socketChanne2.connect(new InetSocketAddress(&quot;www.baidu.com&quot;, 80));// 连接校验 socketChannel.isOpen(); // 测试SocketChannel是否为open状态 socketChannel.isConnected(); //测试SocketChannel是否已经被连接 socketChannel.isConnectionPending(); //测试SocketChannel是否正在进行连接 socketChannel.finishConnect(); //校验正在进行套接字连接的SocketChannel是否已经完成连接// 读写模式 前面提到SocketChannel支持阻塞和非阻塞两种模式： socketChannel.configureBlocking(false);// 通过setOptions方法可以设置socket套接字的相关参数 socketChannel.setOption(StandardSocketOptions.SO_KEEPALIVE, Boolean.TRUE).setOption(StandardSocketOptions.TCP_NODELAY, Boolean.TRUE);// 可以通过getOption获取相关参数的值。如默认的接收缓冲区大小是8192byte。 socketChannel.getOption(StandardSocketOptions.SO_KEEPALIVE); socketChannel.getOption(StandardSocketOptions.SO_RCVBUF);// 读写 ByteBuffer byteBuffer = ByteBuffer.allocate(16); socketChannel.read(byteBuffer); socketChannel.close(); System.out.println(&quot;read over&quot;); }} DatagramChannel ​ 正如SocketChannel对应Socket，ServerSocketChannel对应ServerSocket，每一个DatagramChannel对象也有一个关联的DatagramSocket对象。正如SocketChannel模拟连接导向的流协议（如TCP/IP），DatagramChannel则模拟包导向的无连接协议（如UDP/IP）。DatagramChannel是无连接的，每个数据报（datagram）都是一个自包含的实体，拥有它自己的目的地址及不依赖其他数据报的数据负载。与面向流的的socket不同，DatagramChannel可以发送单独的数据报给不同的目的地址。同样，DatagramChannel对象也可以接收来自任意地址的数据包。每个到达的数据报都含有关于它来自何处的信息（源地址）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import org.junit.jupiter.api.Test;import java.io.IOException;import java.net.InetSocketAddress;import java.net.SocketAddress;import java.nio.ByteBuffer;import java.nio.channels.DatagramChannel;import java.nio.charset.Charset;public class DatagramChannelDemo { /** * 发包的datagram * * @throws IOException * @throws InterruptedException */ @Test public void sendDatagram() throws IOException, InterruptedException { DatagramChannel sendChannel = DatagramChannel.open(); InetSocketAddress sendAddress = new InetSocketAddress(&quot;127.0.0.1&quot;, 9999); while (true) {// 发送 sendChannel.send(ByteBuffer.wrap(&quot;发包&quot;.getBytes(&quot;UTF-8&quot;)), sendAddress); System.out.println(&quot;发包端发包&quot;); Thread.sleep(1000); } } /** * 收包端 * * @throws IOException */ @Test public void receive() throws IOException { DatagramChannel receiveChannel = DatagramChannel.open(); //打开9999端口接收UDP数据包 InetSocketAddress receiveAddress = new InetSocketAddress(9999); receiveChannel.bind(receiveAddress); ByteBuffer receiveBuffer = ByteBuffer.allocate(512); while (true) { receiveBuffer.clear();// 接收 SocketAddress sendAddress = receiveChannel.receive(receiveBuffer); receiveBuffer.flip(); System.out.print(sendAddress.toString() + &quot; &quot;); System.out.println(Charset.forName(&quot;UTF-8&quot;).decode(receiveBuffer)); } } /** * 只接收和发送9999的数据包 * * @throws IOException */ @Test public void testConect1() throws IOException { DatagramChannel connChannel = DatagramChannel.open(); connChannel.bind(new InetSocketAddress(9999)); //UDP不存在真正意义上的连接，这里的连接是向特定服务地址用read和write接收发送数据包。 connChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 9999)); connChannel.write(ByteBuffer.wrap(&quot;发包&quot;.getBytes(&quot;UTF-8&quot;))); ByteBuffer readBuffer = ByteBuffer.allocate(512); //read()和write()只有在connect()后才能使用，不然会抛NotYetConnectedException异常。用read()接收时，如果没有接收到包，会抛PortUnreachableException异常。 while (true) { try { readBuffer.clear(); connChannel.read(readBuffer); readBuffer.flip(); System.out.println(Charset.forName(&quot;UTF-8&quot;).decode(readBuffer)); } catch (Exception e) { } } }} Scatter/Gather ​ Java NIO开始支持scatter/gather，scatter/gather用于描述从Channel中读取或者写入到Channel的操作。 ​ **分散（scatter）**从Channel中读取是指在读操作时将读取的数据写入多个buffer中。因此，Channel将从Channel中读取的数据“分散（scatter）”到多个Buffer中。 ​ **聚集（gather）**写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。 scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 Scattering Reads Scattering Reads是指数据从一个channel读取到多个buffer中。 1234ByteBuffer header = ByteBuffer.allocate(128); ByteBuffer body = ByteBuffer.allocate(1024); ByteBuffer[] bufferArray = { header, body }; channel.read(bufferArray); ​ 注意buffer首先被插入到数组，然后再将数组作为channel.read() 的输入参数。read()方法按照buffer在数组中的顺序将从channel中读取的数据写入到buffer，当一个buffer被写满后，channel紧接着向另一个buffer中写。 Scattering Reads在移动下一个buffer前，必须填满当前的buffer，这也意味着它不适用于动态消息(译者注：消息大小不固定)。换句话说，如果存在消息头和消息体，消息头必须完成填充（例如 128byte），Scattering Reads才能正常工作。 Gathering Writes ​ Gathering Writes是指数据从多个buffer写入到同一个channel。 1234ByteBuffer header = ByteBuffer.allocate(128); ByteBuffer body = ByteBuffer.allocate(1024); //write data into buffers ByteBuffer[] bufferArray = { header, body };channel.write(bufferArray); ​ buffers数组是write()方法的入参，write()方法会按照buffer在数组中的顺序，将数据写入到channel，注意只有position和limit之间的数据才会被写入。因此，如果一个buffer的容量为128byte，但是仅仅包含58byte的数据，那么这58byte的数据将被写入到channel中。因此与Scattering Reads相反，Gathering Writes能较好的处理动态消息。 缓冲区 发送给一个通道的所有数据都必须首先放到缓冲区中，同样地，从通道中读取的任何数据都要先读到缓冲区中。也就是说，不会直接对通道进行读写数据，而是要先经过缓冲区。 缓冲区实质上是一个数组，但它不仅仅是一个数组。缓冲区提供了对数据的结构化访问，而且还可以跟踪系统的读/写进程。 ​ 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成NIO Buffer对象，并提供了一组方法，用来方便的访问该块内存。缓冲区实际上是一个容器对象，更直接的说，其实就是一个数组，在NIO库中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的； 在写入数据时，它也是写入到缓冲区中的；任何时候访问 NIO 中的数据，都是将它放到缓冲区中。而在面向流I/O系统中，所有数据都是直接写入或者直接将数据读取到Stream对象中。在NIO中，所有的缓冲区类型都继承于抽象类Buffer，最常用的就是ByteBuffer，对于Java中的基本类型，基本都有一个具体Buffer类型与之相对应，它们之间的继承关系如下图所示： 缓冲区包括以下类型： ByteBuffer CharBuffer ShortBuffer IntBuffer LongBuffer FloatBuffer DoubleBuffer 缓冲区状态变量 capacity：最大容量； position：当前已经读写的字节数； limit：还可以读写的字节数。 Buffer的基本用法 1、使用Buffer读写数据，一般遵循以下四个步骤： （1）写入数据到Buffer （2）调用flip()方法 （3）从Buffer中读取数据 （4）调用clear()方法或者compact()方法 当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式。在读模式下，可以读取之前写入到buffer的所有数据。一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 Buffer与IntBuffer例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package pers.lxl.mylearnproject.javase.io.nio.buffer;import org.junit.jupiter.api.Test;import java.io.IOException;import java.io.RandomAccessFile;import java.nio.ByteBuffer;import java.nio.IntBuffer;import java.nio.channels.FileChannel;public class BufferDemo { //使用Buffer的例子 @Test public void testConect2() throws IOException { RandomAccessFile aFile = new RandomAccessFile(&quot;src/main/resources/fileTest.txt&quot;, &quot;rw&quot;); FileChannel inChannel = aFile.getChannel(); //create buffer with capacity of 48 bytes ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = inChannel.read(buf); //read into buffer. while (bytesRead != -1) { buf.flip(); //make buffer ready for read while (buf.hasRemaining()) { System.out.print((char) buf.get()); // read 1 byte at a time } buf.clear(); //make buffer ready for writing bytesRead = inChannel.read(buf); } } //使用IntBuffer的例子 @Test public void testConect3() throws IOException { // 分配新的int缓冲区，参数为缓冲区容量 // 新缓冲区的当前位置将为零，其界限(限制位置)将为其容量。 // 它将具有一个底层实现数组，其数组偏移量将为零。 IntBuffer buffer = IntBuffer.allocate(8); for (int i = 0; i &lt; buffer.capacity(); ++i) { int j = 2 * (i + 1); // 将给定整数写入此缓冲区的当前位置，当前位置递增 buffer.put(j); } // 重设此缓冲区，将限制设置为当前位置，然后将当前位置设置为0 buffer.flip(); // 查看在当前位置和限制位置之间是否有元素 while (buffer.hasRemaining()) { // 读取此缓冲区当前位置的整数，然后当前位置递增 int j = buffer.get(); System.out.print(j + &quot; &quot;); } }} Buffer的capacity、position和limit 为了理解Buffer的工作原理，需要熟悉它的三个属性： Capacity 、 Position 、 limit position和limit的含义取决于Buffer处在读模式还是写模式。不管Buffer处在什么模式，capacity的含义总是一样的。 这里有一个关于capacity，position和limit在读写模式中的说明 （1）capacity 作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 （2）position 1）写数据到Buffer中时，position表示写入数据的当前位置，position的初始值为0。当一个byte、long等数据写到Buffer后， position会向下移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1（因为position的初始值为0）. 2）读数据到Buffer中时，position表示读入数据的当前位置，如position=2时表示已开始读入了3个byte，或从第3个byte开始读取。通过ByteBuffer.flip()切换到读模式时position会被重置为0，当Buffer从position读入数据后，position会下移到下一个可读入的数据Buffer单元。 （3）limit 1）写数据时，limit表示可对Buffer最多写入多少个数据。写模式下，limit等于Buffer的capacity。 2）读数据时，limit表示Buffer里有多少可读数据（not null的数据），因此能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）。 Buffer的类型 Java NIO 有以下Buffer类型 ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些Buffer类型代表了不同的数据类型。换句话说，就是可以通过char，short，int，long，float 或 double类型来操作缓冲区中的字节。 Buffer分配和写数据 Buffer分配 要想获得一个Buffer对象首先要进行分配。 每一个Buffer类都有一个allocate方法。 下面是一个分配48字节capacity的ByteBuffer的例子。 ByteBuffer buf = ByteBuffer.allocate(48); 这是分配一个可存储1024个字符的CharBuffer： CharBuffer buf = CharBuffer.allocate(1024); 向Buffer中写数据 写数据到Buffer有两种方式： （1）从Channel写到Buffer。 （2）通过Buffer的put()方法写到Buffer里。 从Channel写到Buffer的例子 int bytesRead = inChannel.read(buf); //read into buffer. 通过put方法写Buffer的例子： buf.put(127); put方法有很多版本，允许你以不同的方式把数据写入到Buffer中。例如， 写到一个指定的位置，或者把一个字节数组写入到Buffer flip()方法 flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 （现在能读取多少个byte、char等）。 从Buffer中读取数据 从Buffer中读取数据有两种方式： （1）从Buffer读取数据到Channel。 （2）使用get()方法从Buffer中读取数据。 从Buffer读取数据到Channel的例子： //read from buffer into channel. int bytesWritten = inChannel.write(buf); 使用get()方法从Buffer中读取数据的例子 byte aByte = buf.get(); get方法有很多版本，允许你以不同的方式从Buffer中读取数据。例如，从指定position读取，或者从Buffer中读取数据到字节数组。 Buffer几个方法 rewind()方法方法 Buffer.rewind()将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等）。 clear()与compact()方法方法 ​ 一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。 如果调用的是**clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。 如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。 如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()**方法。 compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。 mark()与reset()方法方法 通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如： buffer.mark(); //call buffer.get() a couple of times, e.g. during parsing. buffer.reset(); //set position back to mark. 缓冲区操作 缓冲区分片 ​ 在NIO中，除了可以分配或者包装一个缓冲区对象外，还可以根据现有的缓冲区对象来创建一个子缓冲区，即在现有缓冲区上切出一片来作为一个新的缓冲区，但现有的缓冲区与创建的子缓冲区在底层数组层面上是数据共享的，也就是说，子缓冲区相当于是现有缓冲区的一个视图窗口。调用**slice()**方法可以创建一个子缓冲区。 1234567891011121314151617181920212223@Testpublic void testConect2() throws IOException { ByteBuffer buffer = ByteBuffer.allocate(10); // 缓冲区中的数据0-9 for (int i = 0; i &lt; buffer.capacity(); ++i) { buffer.put((byte) i); } // 创建子缓冲区 buffer.position(3); buffer.limit(7); ByteBuffer slice = buffer.slice(); // 改变子缓冲区的内容 for (int i = 0; i &lt; slice.capacity(); ++i) { byte b = slice.get(i); b *= 10; slice.put(i, b); } buffer.position(0); buffer.limit(buffer.capacity()); while (buffer.remaining() &gt; 0) { System.out.println(buffer.get()); }} 只读缓冲区 ​ 只读缓冲区非常简单，可读禁写。可以通过调用缓冲区的asReadOnlyBuffer()方法，将任何常规缓冲区转 换为只读缓冲区，这个方法返回一个与原缓冲区完全相同的缓冲区，并与原缓冲区共享数据，只不过它是只读的。原变只读变，如果原缓冲区的内容发生了变化，只读缓冲区的内容也随之发生变化： 12345678910111213141516171819202122@Test public void testConect4() throws IOException { ByteBuffer buffer = ByteBuffer.allocate(10); // 缓冲区中的数据0-9 for (int i = 0; i &lt; buffer.capacity(); ++i) { buffer.put((byte) i); } // 创建只读缓冲区 ByteBuffer readonly = buffer.asReadOnlyBuffer(); // 改变原缓冲区的内容 for (int i = 0; i &lt; buffer.capacity(); ++i) { byte b = buffer.get(i); b *= 10; buffer.put(i, b); } readonly.position(0); readonly.limit(buffer.capacity()); // 只读缓冲区的内容也随之改变 while (readonly.remaining() &gt; 0) { System.out.println(readonly.get()); } } ​ 如果尝试修改只读缓冲区的内容，则会报ReadOnlyBufferException异常。只读缓冲区对于保护数据很有用。在将缓冲区传递给某个 对象的方法时，无法知道这个方法是否会修改缓冲区中的数据。创建一个只读的缓冲区可以保证该缓冲区不会被修改。只可以把常规缓冲区转换为只读缓冲区，而不能将只读的缓冲区转换为可写的缓冲区。 直接缓冲区 ​ 直接缓冲区是为加快I/O速度，使用一种特殊方式为其分配内存的缓冲区，JDK文档中的描述为：给定一个直接字节缓冲区，Java虚拟机将尽最大努力直接对它执行本机I/O操作。也就是说，它会在每一次调用底层操作系统的本机I/O操作之前(或之后)，尝试避免将缓冲区的内容拷贝到一个中间缓冲区中 或者从一个中间缓冲区中拷贝数据。要分配直接缓冲区，需要调用allocateDirect()方法，而不是allocate()方法，使用方式与普通缓冲区并无区别。 拷贝文件示例： 1234567891011121314151617181920 @Test public void testConect5() throws IOException { String infile = &quot;d:\\\\atguigu\\\\01.txt&quot;; FileInputStream fin = new FileInputStream(infile); FileChannel fcin = fin.getChannel(); String outfile = String.format(&quot;d:\\\\atguigu\\\\02.txt&quot;); FileOutputStream fout = new FileOutputStream(outfile); FileChannel fcout = fout.getChannel();// 使用allocateDirect，而不是allocate ByteBuffer buffer = ByteBuffer.allocateDirect(1024); while (true) { buffer.clear(); int r = fcin.read(buffer); if (r == -1) { break; } buffer.flip(); fcout.write(buffer); } } 内存映射文件I/O ​ 内存映射文件I/O是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的I/O快的多。内存映射文件I/O是通过使文件中的数据出现为 内存数组的内容来完成的，这其初听起来似乎不过就是将整个文件读到内存中，但是事实上并不是这样。一般来说，只有文件中实际读取或者写入的部分才会映射到内存中。示例代码： 1234567891011static private final int start = 0; static private final int size = 1024; static public void main(String args[]) throws Exception { RandomAccessFile raf = new RandomAccessFile(&quot;d:\\\\atguigu\\\\01.txt&quot;, &quot;rw&quot;); FileChannel fc = raf.getChannel(); MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, start, size); mbb.put(0, (byte) 97); mbb.put(1023, (byte) 122); raf.close(); } 选择器 Selector简介 1、 Selector和 Channel关系 ​ Selector 一般称 为选择器 ，也可以翻译为 多路复用器 。它是Java NIO核心组件中的一个，**用于检查一个或多个NIO Channel（通道）的状态是否处于可读、可写。**如此可以实现单线程管理多个channels,也就是可以管理多个网络链接。即使用Selector的好处在于： 使用更少的线程来就可以来处理通道了， 相比使用多个线程，避免了线程上下文切换带来的开销。NIO 实现了 IO 多路复用中的 Reactor 模型，一个线程 Thread 使用一个选择器 Selector 通过轮询的方式去监听多个通道 Channel 上的事件，从而让一个线程就可以处理多个事件。通过配置监听的通道 Channel 为非阻塞，那么当 Channel 上的 IO 事件还未到达时，就不会进入阻塞状态一直等待，而是继续轮询其它 Channel，找到 IO 事件已经到达的 Channel 执行。因为创建和切换线程的开销很大，因此使用一个线程来处理多个事件而不是一个线程处理一个事件，对于 IO 密集型的应用具有很好地性能。 2、 可选择通道 (SelectableChannel) （1）不是所有的Channel都可以被Selector 复用的。比方说，FileChannel就不能被选择器复用。判断一个Channel 能被Selector 复用，有一个前提：判断他是否继承了一个抽象类SelectableChannel。如果继承了SelectableChannel，则可以被复用，否则不能。 （2）SelectableChannel类提供了实现通道的可选择性所需要的公共方法。它是所有支持就绪检查的通道类的父类。所有socket通道，都继承了SelectableChannel类都是可选择的，包括从管道(Pipe)对象的中获得的通道。而FileChannel类，没有继承SelectableChannel，因此是不是可选通道。 （3）一个通道可以被注册到多个选择器上，但对每个选择器而言只能被注册一次。通道和选择器之间的关系，使用注册的方式完成。SelectableChannel可以被注册到Selector对象上，在注册的时候，需要指定通道的哪些操作，是Selector感兴趣的。 3、Channel注册到注册到Selector （1）使用Channel.register（Selector sel，int ops）方法，将一个通道注册到一个选择器时。第一个参数，指定通道要注册的选择器。第二个参数指定选择器需要查询的通道操作。 （2）可以供选择器查询的通道操作四类： 可读 : SelectionKey.OP_READ 可写 : SelectionKey.OP_WRITE 连接 : SelectionKey.OP_CONNECT 接收 : SelectionKey.OP_ACCEPT Selector对通道的多操作类型用“位或”操作符实现： 比如：int key = SelectionKey.OP_READ | SelectionKey.OP_WRITE ; （3）选择器查询的不是通道的操作，而是通道的某个操作的一种就绪状态。什么是操作的就绪状态？一旦通道具备完成某个操作的条件，表示该通道的某个操作已经就绪，就可以被Selector查询到，程序可以对通道进行对应的操作。比方说，某个SocketChannel通道可以连接到一个服务器，则处于“连接就绪”(OP_CONNECT)。再比方说，一个ServerSocketChannel服务器通道准备好接收新进入的连接，则处于“接收就绪”（OP_ACCEPT）状态。还比方说，一个有数据可读的通道，可以说是“读就绪”(OP_READ)。一个等待写数据的通道可以说是“写就绪”(OP_WRITE)。 4、选择键(SelectionKey) （1）Channel注册到后，并且一旦通道处于某种就绪的状态，就可以被选择器查询到。这个工作，使用选择器Selector的select（）方法完成。select方法的作用，对感兴趣的通道操作，进行就绪状态的查询。 （2）Selector可以不断的查询Channel中发生的操作的就绪状态。并且挑选感兴趣的操作就绪状态。一旦通道有操作的就绪状态达成，并且是Selector感兴趣的操作，就会被Selector选中，放入选择键集合中。 （3）一个选择键，首先是包含了注册在Selector的通道操作的类型，比方说SelectionKey.OP_READ。也包含了特定的通道与特定的选择器之间的注册关系。 开发应用程序是，选择键是编程的关键。NIO的编程，就是根据对应的选择键，进行不同的业务逻辑处理。 （4）选择键的概念，和事件的概念比较相似。一个选择键类似监听器模式里边的一个事件。由于Selector不是事件触发的模式，而是主动去查询的模式，所以不叫事件Event，而是叫SelectionKey选择键。 NIO 常常被叫做非阻塞 IO，主要是因为 NIO 在网络通信中的非阻塞特性被广泛使用。 Selector的使用方法 1.Selector 的创建 通过调用Selector.open()方法创建一个Selector对象，如下： // 1、获取Selector选择器 Selector selector = Selector.open(); 2.注册 Channel 到 Selector 要实现Selector管理Channel，需要将channel注册到相应的Selector上 12345// 1、获取Selector选择器 Selector selector = Selector.open(); // 2、获取通道 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 3.设置为非阻塞 serverSocketChannel.configureBlocking(false); // 4、绑定连接 serverSocketChannel.bind(new InetSocketAddress(9999)); // 5、将通道注册到选择器上,并制定监听事件为：“接收”事件 serverSocketChannel.register(selector,SelectionKey.OP_ACCEPT); 上面通过调用通道的register()方法会将它注册到一个选择器上。 首先需要注意的是： （1）与Selector一起使用时，Channel必须处于非阻塞模式下，否则将抛出异常IllegalBlockingModeException。这意味着，FileChannel不能与Selector一起使用，因为FileChannel不能切换到非阻塞模式，而套接字相关的所有的通道都可以。 （2）一个通道，并没有一定要支持所有的四种操作。比如服务器通道ServerSocketChannel支持Accept 接受操作，而SocketChannel客户端通道则不支持。可以通过通道上的validOps()方法，来获取特定通道下所有支持的操作集合。 3.轮询查询就绪操作轮询查询就绪操作 （1）通过Selector的select（）方法，可以查询出已经就绪的通道操作，这些就绪的状态集合，包存在一个元素是SelectionKey对象的Set集合中。 （2）下面是Selector几个重载的查询select()方法： select():阻塞到至少有一个通道在你注册的事件上就绪了。 select(long timeout)：和select()一样，但最长阻塞事件为timeout毫秒。 selectNow():非阻塞，只要有通道就绪就立刻返回。 select()方法返回的int值，表示有多少通道已经就绪，更准确的说，是自前一次select方法以来到这一次select方法之间的时间段上，有多少通道变成就绪状态。 例如：首次调用select()方法，如果有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。 一旦调用select()方法，并且返回值不为0时，在Selector中有一个selectedKeys()方法，用来访问已选择键集合，迭代集合的每一个选择键元素，根据就绪操作的类型，完成对应的操作： 12345678910111213 Set selectedKeys = selector.selectedKeys();Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if(key.isAcceptable()) { // a connection was accepted by a ServerSocketChannel. } else if (key.isConnectable()) { // a connection was established with a remote server. } else if (key.isReadable()) { // a channel is ready for reading } else if (key.isWritable()) { // a channel is ready for writing } keyIterator.remove(); } 4.停止选择的方法停止选择的方法 ​ 选择器执行选择的过程，系统底层会依次询问每个通道是否已经就绪，这个过程可能会造成调用线程进入阻塞状态,那么我们有以下三种方式可以唤醒在select（）方法中阻塞的线程。 wakeup()方法 ：通过调用Selector对象的wakeup（）方法让处在阻塞状态的select()方法立刻返回 该方法使得选择器上的第一个还没有返回的选择操作立即返回。如果当前没有进行中的选择操作，那么下一次对select()方法的一次调用将立即返回。 close()方法 ：通过close（）方法关闭Selector， 该方法使得任何一个在选择操作中阻塞的线程都被唤醒（类似wakeup（）），同时使得注册到该Selector的所有Channel被注销，所有的键将被取消，但是Channel本身并不会关闭。 示例代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134import org.junit.jupiter.api.Test;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.nio.charset.StandardCharsets;import java.util.Date;import java.util.Iterator;import java.util.Scanner;import java.util.Set;public class SelectorDemo { //服务端 @Test public void ServerDemo() throws Exception {// 1.获取服务端通道 ServerSocketChannel ssc = ServerSocketChannel.open();// 2.绑定端口号 ssc.socket().bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000));// 3.切换非阻塞模式 ssc.configureBlocking(false);// 4.创建选择器 Selector selector = Selector.open();// 5.注册channel，并且指定感兴趣的事件是 Accept ssc.register(selector, SelectionKey.OP_ACCEPT);// 6.创建buffer ByteBuffer readBuff = ByteBuffer.allocate(1024); ByteBuffer writeBuff = ByteBuffer.allocate(128); writeBuff.put(&quot;received&quot;.getBytes()); writeBuff.flip(); while (true) {// 7.检测通道就绪状态 int nReady = selector.select();// 8.遍历选择器，获取就绪通道集合 Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = keys.iterator(); while (it.hasNext()) { SelectionKey key = it.next(); it.remove();// 判断状态 if (key.isAcceptable()) {// 创建新的连接，并且把连接注册到selector上，进行监听，声明这个channel只对读操作感兴趣。 SocketChannel socketChannel = ssc.accept();// 切换到非阻塞 socketChannel.configureBlocking(false);// 注册 socketChannel.register(selector, SelectionKey.OP_READ); } else if (key.isReadable()) {// 获取通道 SocketChannel socketChannel = (SocketChannel) key.channel(); readBuff.clear(); socketChannel.read(readBuff); readBuff.flip(); System.out.println(&quot;received : &quot; + new String(readBuff.array())); key.interestOps(SelectionKey.OP_WRITE); } else if (key.isWritable()) { writeBuff.rewind(); SocketChannel socketChannel = (SocketChannel) key.channel(); socketChannel.write(writeBuff); key.interestOps(SelectionKey.OP_READ); } } } } //客户端代码 @Test public void ClientDemo() throws Exception {// 1.获取通道，绑定主机与端口号 SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000));// 2.切换到非阻塞模式 socketChannel.configureBlocking(false);// 3.创建buffer ByteBuffer writeBuffer = ByteBuffer.allocate(32); ByteBuffer readBuffer = ByteBuffer.allocate(32);// 4.写入buffer数据 Scanner scanner = new Scanner(System.in); while (scanner.hasNext()) { String str = scanner.next();// writeBuffer.put(&quot;hello&quot;.getBytes()); writeBuffer.put((new Date().toString() + &quot;----&gt;&quot; + str).getBytes());// 5.模式切换 writeBuffer.flip(); while (true) { // 将position设回0 writeBuffer.rewind();// 6.写入通道 socketChannel.write(writeBuffer);// 7.关闭 readBuffer.clear(); socketChannel.read(readBuffer); } } }// 输入方式 客户端 public static void main(String[] args) throws Exception { // 1.获取通道，绑定主机与端口号 SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000));// 2.切换到非阻塞模式 socketChannel.configureBlocking(false);// 3.创建buffer 超过会报错Exception in thread &quot;main&quot; java.nio.BufferOverflowException ByteBuffer writeBuffer = ByteBuffer.allocate(1024); ByteBuffer readBuffer = ByteBuffer.allocate(1024);// 4.写入buffer数据 Scanner scanner = new Scanner(System.in); while (scanner.hasNext()) { String str = scanner.next();// writeBuffer.put(&quot;hello&quot;.getBytes()); writeBuffer.put((new Date().toString() + &quot;----&gt;&quot; + str).getBytes());// 5.模式切换 writeBuffer.flip(); while (true) { // 将position设回0 writeBuffer.rewind();// 6.写入通道 socketChannel.write(writeBuffer);// 7.关闭 readBuffer.clear(); socketChannel.read(readBuffer); } } }} NIO编程步骤总结 第一步：创建Selector选择器 第二步：创建ServerSocketChannel通道，并绑定监听端口 第三步：设置Channel通道是非阻塞模式 第四步：把Channel注册到Socketor选择器上，监听连接事件 第五步：调用Selector的select方法（循环调用），监测通道的就绪状况 第六步：调用selectKeys方法获取就绪channel集合 第七步：遍历就绪channel集合，判断就绪事件类型，实现具体的业务操作 第八步：根据业务，决定是否需要再次注册监听事件，重复执行第三步操作 创建选择器 Selector selector = Selector.open(); 将通道注册到选择器上 ServerSocketChannel ssChannel = ServerSocketChannel.open(); ssChannel.configureBlocking(false); ssChannel.register(selector, SelectionKey.OP_ACCEPT); 通道必须配置为非阻塞模式，否则使用选择器就没有任何意义了，因为如果通道在某个事件上被阻塞，那么服务器就不能响应其它事件，必须等待这个事件处理完毕才能去处理其它事件，显然这和选择器的作用背道而驰。 在将通道注册到选择器上时，还需要指定要注册的具体事件，主要有以下几类： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 它们在 SelectionKey 的定义如下： public static final int OP_READ = 1 &lt;&lt; 0; public static final int OP_WRITE = 1 &lt;&lt; 2; public static final int OP_CONNECT = 1 &lt;&lt; 3; public static final int OP_ACCEPT = 1 &lt;&lt; 4; 可以看出每个事件可以被当成一个位域，从而组成事件集整数。例如： int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 监听事件 int num = selector.select(); 使用 select() 来监听到达的事件，它会一直阻塞直到有至少一个事件到达。 获取到达的事件 Set keys = selector.selectedKeys(); Iterator keyIterator = keys.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { // ... } else if (key.isReadable()) { // ... } keyIterator.remove(); } 事件循环 因为一次 select() 调用不能处理完所有的事件，并且服务器端有可能需要一直监听事件，因此服务器端处理事件的代码一般会放在一个死循环内。 while (true) { int num = selector.select(); Set keys = selector.selectedKeys(); Iterator keyIterator = keys.iterator(); while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); if (key.isAcceptable()) { // ... } else if (key.isReadable()) { // ... } keyIterator.remove(); } } Pipe Java NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。 1、创建管道 通过Pipe.open()方法打开管道。 Pipe pipe = Pipe.open(); 2、写入管道 要向管道写数据，需要访问sink通道。： Pipe.SinkChannel sinkChannel = pipe.sink(); 通过调用SinkChannel的write()方法，将数据写入SinkChannel： String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis(); ByteBuffer buf = ByteBuffer.allocate(48); buf.clear(); buf.put(newData.getBytes()); buf.flip(); while(buf.hasRemaining()) 3、从管道读取数据 访问source通道 Pipe.SourceChannel sourceChannel = pipe.source(); 调用source通道的read()方法来读取数据： ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = sourceChannel.read(buf); read()方法返回的int值会告诉我们多少字节被读进了缓冲区。 4、代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041package pers.lxl.mylearnproject.javase.io.nio.pipe;import org.junit.jupiter.api.Test;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.Pipe;public class PipeDemo {// 1.获取管道// 2获取sink通道// 3.创建缓冲区// 4.写入数据// 5.获取source通道// 6.创建source通道// 7.创建缓冲区，读取数据// 8.关闭通道 /**/ @Test public void testPipe() throws IOException { // 1、获取通道 Pipe pipe = Pipe.open(); // 2、获取sink管道，用来传送数据 Pipe.SinkChannel sinkChannel = pipe.sink(); // 3、申请一定大小的缓冲区 ByteBuffer byteBuffer = ByteBuffer.allocate(1024); byteBuffer.put(&quot;atguigu&quot;.getBytes()); byteBuffer.flip(); // 4、sink发送数据 sinkChannel.write(byteBuffer); // 5、创建接收pipe数据的source管道 Pipe.SourceChannel sourceChannel = pipe.source(); // 6、接收数据，并保存到缓冲区中 ByteBuffer byteBuffer2 = ByteBuffer.allocate(1024); int length = sourceChannel.read(byteBuffer2); System.out.println(new String(byteBuffer2.array(), 0, length)); sourceChannel.close(); sinkChannel.close(); }} FileLock 1、 FileLock 简介 ​ 文件锁在OS中很常见，如果多个程序同时访问、修改同一个文件，很容易因为文件数据不同步而出现问题。给文件加一个锁，同一时间，只能有一个程序修改此文件，或者程序都只能读此文件，这就解决了同步问题。 ​ 文件锁是进程级别的，不是线程级别的。文件锁可以解决多个进程并发访问、修改同一个文件的问题，但不能解决多线程并发访问、修改同一文件的问题。使用文件锁时，同一进程内的多个线程，可以同时访问、修改此文件。 ​ 文件锁是当前程序所属的JVM实例持有的，一旦获取到文件锁（对文件加锁），要调用release()，或者关闭对应的FileChannel对象，或者当前JVM退出，才会释放这个锁。 ​ 一旦某个进程（比如说JVM实例）对某个文件加锁，则在释放这个锁之前，此进程不能再对此文件加锁，就是说JVM实例在同一文件上的文件锁是不重叠的（进程级别不能重复在同一文件上获取锁）。 2、文件锁分类：文件锁分类： ​ 排它锁：又叫独占锁。对文件加排它锁后，该进程可以对此文件进行读写，该进程独占此文件，其他进程不能读写此文件，直到该进程释放文件锁。 ​ 共享锁：某个进程对文件加共享锁，其他进程也可以访问此文件，但这些进程都只能读此文件，不能写。线程是安全的。只要还有一个进程持有共享锁，此文件就只能读，不能写。 3、使用示例 //创建FileChannel对象，文件锁只能通过FileChannel对象来使用 FileChannel fileChannel=new FileOutputStream(&quot;./1.txt&quot;).getChannel(); //对文件加锁 FileLock lock=fileChannel.lock(); //对此文件进行一些读写操作。 //....... //释放锁 lock.release(); 文件锁要通过FileChannel对象使用。 4、获取文件锁方法、获取文件锁方法 有4种获取文件锁的方法： lock() //对整个文件加锁，默认为排它锁。 lock(long position, long size, booean shared) //自定义加锁方式。前2个参数指定要加锁的部分（可以只对此文件的部分内容加锁），第三个参数值指定是否是共享锁。 tryLock() //对整个文件加锁，默认为排它锁。 tryLock(long position, long size, booean shared) //自定义加锁方式。 如果指定为共享锁，则其它进程可读此文件，所有进程均不能写此文件，如果某进程试图对此文件进行写操作，会抛出异常。 5、locklock与tryLocktryLock的区别 lock是阻塞式的，如果未获取到文件锁，会一直阻塞当前线程，直到获取文件锁 tryLock和lock的作用相同，只不过tryLock是非阻塞式的，tryLock是尝试获取文件锁，获取成功就返回锁对象，否则返回null，不会阻塞当前线程。 6、FileLock两个方法： boolean isShared() //此文件锁是否是共享锁 boolean isValid() //此文件锁是否还有效 在某些OS上，对某个文件加锁后，不能对此文件使用通道映射。 7、完整例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.io.BufferedReader;import java.io.FileReader;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;import java.nio.channels.FileLock;import java.nio.file.Path;import java.nio.file.Paths;import java.nio.file.StandardOpenOption;public class FileLockDemo { public static void main(String[] args) throws IOException { String input = &quot;atguigu&quot;; System.out.println(&quot;输入 :&quot; + input); ByteBuffer buf = ByteBuffer.wrap(input.getBytes()); String fp = &quot;D:\\\\atguigu\\\\01.txt&quot;; Path pt = Paths.get(fp); FileChannel channel = FileChannel.open(pt, StandardOpenOption.WRITE, StandardOpenOption.APPEND); channel.position(channel.size() - 1); // position of a cursor at the end of file // 获得锁方法一：lock()，阻塞方法，当文件锁不可用时，当前进程会被挂起// lock = channel.lock(); // 无参lock()为独占锁// lock = channel.lock(0L, Long.MAX_VALUE, true); //有参lock()为共享锁，有写操作会报异常 // 获得锁方法二：trylock()，非阻塞的方法，当文件锁不可用时，tryLock()会得到null值 FileLock lock = channel.tryLock(0, Long.MAX_VALUE, false); System.out.println(&quot;共享锁shared: &quot; + lock.isShared()); channel.write(buf); channel.close(); // Releases the Lock System.out.println(&quot;写操作完成.&quot;); //读取数据 readPrint(fp); } public static void readPrint(String path) throws IOException { FileReader filereader = new FileReader(path); BufferedReader bufferedreader = new BufferedReader(filereader); String tr = bufferedreader.readLine(); System.out.println(&quot;读取内容: &quot;); while (tr != null) { System.out.println(&quot; &quot; + tr); tr = bufferedreader.readLine(); } filereader.close(); bufferedreader.close(); }} Path 1、 Path 简介 Java Path接口是Java NIO更新的一部分，同Java NIO一起已经包括在Java6和Java7中。Java Path接口是在Java7中添加到Java NIO的。Path接口位于java.nio.file包中，所以Path接口的完全限定名称为java.nio.file.Path。 Java Path实例表示文件系统中的路径。一个路径可以指向一个文件或一个目录。路径可以是绝对路径，也可以是相对路径。绝对路径包含从文件系统的根目录到它指向的文件或目录的完整路径。相对路径包含相对于其他路径的文件或目录的路径。 在许多方面，java.nio.file.Path接口类似于java.io.File类，但是有一些差别。不过，在许多情况下，可以使用Path接口来替换File类的使用。 2、创建Path实例 使用java.nio.file.Path实例必须创建一个Path实例。可以使用Paths类(java.nio.file.Paths)中的静态方法Paths.get()来创建路径实例。 示例代码: import java.nio.file.Path; import java.nio.file.Paths; public class PathDemo { public static void main(String[] args) { Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); } } 上述代码，可以理解为，Paths.get()方法是Path实例的工厂方法。 3、创建绝对路径 （1）创建绝对路径，通过调用Paths.get()方法，给定绝对路径文件作为参数来完成。 示例代码： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); 上述代码中，绝对路径是d:\\atguigu\\001.txt。在Java字符串中， \\是一个转义字符，需要编写\\，告诉Java编译器在字符串中写入一个\\字符。 （2）如果在Linux、MacOS等操作字体上，上面的绝对路径可能如下: Path path = Paths.get(&quot;/home/jakobjenkov/myfile.txt&quot;); 绝对路径现在为/home/jakobjenkov/myfile.txt. （3）如果在Windows机器上使用了从/开始的路径，那么路径将被解释为相对于当前驱动器。 4、创建相对路径 Java NIO Path类也可以用于处理相对路径。您可以使用Paths.get(basePath, relativePath)方法创建一个相对路径。 示例代码: //代码1 Path projects = Paths.get(&quot;d:\\atguigu&quot;, &quot;projects&quot;); //代码2 Path file = Paths.get(&quot;d:\\atguigu&quot;, &quot;projects\\002.txt&quot;); 代码1创建了一个Java Path的实例，指向路径(目录):d:\\atguigu\\projects 代码2创建了一个Path的实例，指向路径(文件):d:\\atguigu\\projects\\002.txt 5、Path.normalize() Path接口的normalize()方法可以使路径标准化。标准化意味着它将移除所有在路径字符串的中间的.和..代码，并解析路径字符串所引用的路径。 Path.normalize()示例: String originalPath = &quot;d:\\atguigu\\projects\\..\\yygh-project&quot;; Path path1 = Paths.get(originalPath); System.out.println(&quot;path1 = &quot; + path1); Path path2 = path1.normalize(); System.out.println(&quot;path2 = &quot; + path2); 输出结果：标准化的路径不包含projects..部分 Files Java NIO Files类(java.nio.file.Files)提供了几种操作文件系统中的文件的方法。以下内容介绍Java NIO Files最常用的一些方法。java.nio.file.Files类与java.nio.file.Path实例一起工作，因此在学习Files类之前，需要先了解Path类。 1、 Files.createDirectory() Files.createDirectory()方法，用于根据Path实例创建一个新目录 示例： Path path = Paths.get(&quot;d:\\sgg&quot;); try { Path newDir = Files.createDirectory(path); } catch(FileAlreadyExistsException e){ // 目录已经存在 } catch (IOException e) { // 其他发生的异常 e.printStackTrace(); } 第一行创建表示要创建的目录的Path实例。在try-catch块中，用路径作为参数调用Files.createDirectory()方法。如果创建目录成功，将返回一个Path实例，该实例指向新创建的路径。 如果该目录已经存在，则是抛出一个java.nio.file.FileAlreadyExistsException。如果出现其他错误，可能会抛出IOException。例如，如果想要的新目录的父目录不存在，则可能会抛出IOException。 2、Files.copy() （1）Files.copy()方法从一个路径拷贝一个文件到另外一个目录 示例： Path sourcePath = Paths.get(&quot;d:\\atguigu\\01.txt&quot;); Path destinationPath = Paths.get(&quot;d:\\atguigu\\002.txt&quot;); try { Files.copy(sourcePath, destinationPath); } catch(FileAlreadyExistsException e) { // 目录已经存在 } catch (IOException e) { // 其他发生的异常 e.printStackTrace(); } 首先，该示例创建两个Path实例。然后，这个例子调用Files.copy()，将两个Path实例作为参数传递。这可以让源路径引用的文件被复制到目标路径引用的文件中。 如果目标文件已经存在，则抛出一个java.nio.file.FileAlreadyExistsException异常。如果有其他错误，则会抛出一个IOException。例如，如果将该文件复制到不存在的目录，则会抛出IOException。 （2）覆盖已存在的文件 Files.copy()方法的第三个参数。如果目标文件已经存在，这个参数指示copy()方法覆盖现有的文件。 Files.copy(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING); 3、Files.move() Files.move()用于将文件从一个路径移动到另一个路径。移动文件与重命名相同，但是移动文件既可以移动到不同的目录，也可以在相同的操作中更改它的名称。 示例： Path sourcePath = Paths.get(&quot;d:\\atguigu\\01.txt&quot;); Path destinationPath = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); try { Files.move(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING); } catch (IOException e) { //移动文件失败 e.printStackTrace(); } Files.move()的第三个参数。这个参数告诉Files.move()方法来覆盖目标路径上的任何现有文件。 4、Files.delete() Files.delete()方法可以删除一个文件或者目录。 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); try { Files.delete(path); } catch (IOException e) { // 删除文件失败 e.printStackTrace(); } 创建指向要删除的文件的Path。然后调用Files.delete()方法。如果Files.delete()不能删除文件(例如，文件或目录不存在)，会抛出一个IOException。 5.Files.walkFileTree() （1）Files.walkFileTree()方法包含递归遍历目录树功能，将Path实例和FileVisitor作为参数。Path实例指向要遍历的目录，FileVisitor在遍历期间被调用。 （2）FileVisitor是一个接口，必须自己实现FileVisitor接口，并将实现的实例传递给walkFileTree()方法。在目录遍历过程中，您的FileVisitor实现的每个方法都将被调用。如果不需要实现所有这些方法，那么可以扩展SimpleFileVisitor类，它包含FileVisitor接口中所有方法的默认实现。 （3）FileVisitor接口的方法中，每个都返回一个FileVisitResult枚举实例。FileVisitResult枚举包含以下四个选项: CONTINUE 继续 TERMINATE 终止 SKIP_SIBLING 跳过同级 SKIP_SUBTREE 跳过子级 （4）查找一个名为001.txt的文件示例： Path rootPath = Paths.get(&quot;d:\\atguigu&quot;); String fileToFind = File.separator + &quot;001.txt&quot;; try { Files.walkFileTree(rootPath, new SimpleFileVisitor() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { String fileString = file.toAbsolutePath().toString(); //System.out.println(&quot;pathString = &quot; + fileString); if(fileString.endsWith(fileToFind)){ System.out.println(&quot;file found at path: &quot; + file.toAbsolutePath()); return FileVisitResult.TERMINATE; } return FileVisitResult.CONTINUE; } }); } catch(IOException e){ e.printStackTrace(); } （5）java.nio.file.Files类包含许多其他的函数，有关这些方法的更多信息，请查看java.nio.file.Files类的JavaDoc。 AsynchronousFileChannel 在Java 7中，Java NIO中添加了AsynchronousFileChannel，也就是是异步地将数据写入文件。 1、创建 AsynchronousFileChannel 通过静态方法open()创建 示例： Path path = Paths.get(&quot;d:\\atguigu\\01.txt&quot;); try { AsynchronousFileChannel fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); } catch (IOException e) { e.printStackTrace(); } open()方法的第一个参数指向与AsynchronousFileChannel相关联文件的Path实例。 第二个参数是一个或多个打开选项，它告诉AsynchronousFileChannel在文件上执行什么操作。在本例中，我们使用了StandardOpenOption.READ选项，表示该文件将被打开阅读。 2、通过Future读取数据 可以通过两种方式从AsynchronousFileChannel读取数据。第一种方式是调用返回Future的read()方法 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; Future operation = fileChannel.read(buffer, position); while(!operation.isDone()); buffer.flip(); byte[] data = new byte[buffer.limit()]; buffer.get(data); System.out.println(new String(data)); buffer.clear(); 上述代码： （1）创建了一个AsynchronousFileChannel， （2）创建一个ByteBuffer，它被传递给read()方法作为参数，以及一个0的位置。 （3）在调用read()之后，循环，直到返回的isDone()方法返回true。 （4）读取操作完成后，数据读取到ByteBuffer中，然后打印到System.out中。 3、通过CompletionHandler读取数据 第二种方法是调用read()方法，该方法将一个CompletionHandler作为参数 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.READ); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; fileChannel.read(buffer, position, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() { @Override public void completed(Integer result, ByteBuffer attachment) { System.out.println(&quot;result = &quot; + result); attachment.flip(); byte[] data = new byte[attachment.limit()]; attachment.get(data); System.out.println(new String(data)); attachment.clear(); } @Override public void failed(Throwable exc, ByteBuffer attachment) { } }); （1）读取操作完成，将调用CompletionHandler的completed()方法。 （2）对于completed()方法的参数传递一个整数，它告诉我们读取了多少字节，以及传递给read()方法的“附件”。“附件”是read()方法的第三个参数。在本代码中，它是ByteBuffer，数据也被读取。 （3）如果读取操作失败，则将调用CompletionHandler的failed()方法。 4、通过Future写数据 和读取一样，可以通过两种方式将数据写入一个AsynchronousFileChannel 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.WRITE); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; buffer.put(&quot;atguigu data&quot;.getBytes()); buffer.flip(); Future operation = fileChannel.write(buffer, position); buffer.clear(); while(!operation.isDone()); System.out.println(&quot;Write over&quot;); 首先，AsynchronousFileChannel以写模式打开。然后创建一个ByteBuffer，并将一些数据写入其中。然后，ByteBuffer中的数据被写入到文件中。最后，示例检查返回的Future，以查看写操作完成时的情况。 注意，文件必须已经存在。如果该文件不存在，那么write()方法将抛出一个java.nio.file.NoSuchFileException。 5、通过CompletionHandler写数据 示例： Path path = Paths.get(&quot;d:\\atguigu\\001.txt&quot;); if(!Files.exists(path)){ try { Files.createFile(path); } catch (IOException e) { e.printStackTrace(); } } AsynchronousFileChannel fileChannel = null; try { fileChannel = AsynchronousFileChannel.open(path, StandardOpenOption.WRITE); } catch (IOException e) { e.printStackTrace(); } ByteBuffer buffer = ByteBuffer.allocate(1024); long position = 0; buffer.put(&quot;atguigu data&quot;.getBytes()); buffer.flip(); fileChannel.write(buffer, position, buffer, new CompletionHandler&lt;Integer, ByteBuffer&gt;() { @Override public void completed(Integer result, ByteBuffer attachment) { System.out.println(&quot;bytes written: &quot; + result); } @Override public void failed(Throwable exc, ByteBuffer attachment) { System.out.println(&quot;Write failed&quot;); exc.printStackTrace(); } }); 当写操作完成时，将会调用CompletionHandler的completed()方法。如果写失败，则会调用failed()方法。 字符集（Charset） java中使用Charset来表示字符集编码对象 Charset常用静态方法 public static Charset forName(String charsetName)//通过编码类型获得Charset对象 public static SortedMap&lt;String,Charset&gt; availableCharsets()//获得系统支持的所有编码方式 public static Charset defaultCharset()//获得虚拟机默认的编码方式 public static boolean isSupported(String charsetName)//判断是否支持该编码类型 Charset常用普通方法 public final String name()//获得Charset对象的编码类型(String) public abstract CharsetEncoder newEncoder()//获得编码器对象 public abstract CharsetDecoder newDecoder()//获得解码器对象 代码示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import org.junit.jupiter.api.Test;import java.nio.ByteBuffer;import java.nio.CharBuffer;import java.nio.charset.CharacterCodingException;import java.nio.charset.Charset;import java.nio.charset.CharsetDecoder;import java.nio.charset.CharsetEncoder;import java.util.Map;import java.util.Set;public class CharsetDemo { @Test public void charSetEncoderAndDecoder() throws CharacterCodingException { Charset charset = Charset.forName(&quot;UTF-8&quot;); //1.获取编码器 CharsetEncoder charsetEncoder = charset.newEncoder(); // 2.获取解码器 CharsetDecoder charsetDecoder = charset.newDecoder(); // 3.获取需要解码编码的数据 CharBuffer charBuffer = CharBuffer.allocate(1024); charBuffer.put(&quot;字符集编码解码&quot;); charBuffer.flip(); //4.编码 ByteBuffer byteBuffer = charsetEncoder.encode(charBuffer); System.out.println(&quot;编码后：&quot;); for (int i = 0; i &lt; byteBuffer.limit(); i++) { System.out.println(byteBuffer.get()); } //5.解码 byteBuffer.flip(); CharBuffer charBuffer1 = charsetDecoder.decode(byteBuffer); System.out.println(&quot;解码后：&quot;); System.out.println(charBuffer1.toString()); System.out.println(&quot;指定其他格式解码:&quot;); Charset charset1 = Charset.forName(&quot;GBK&quot;); byteBuffer.flip(); CharBuffer charBuffer2 = charset1.decode(byteBuffer); System.out.println(charBuffer2.toString()); //6.获取Charset所支持的字符编码 Map&lt;String, Charset&gt; map = Charset.availableCharsets(); Set&lt;Map.Entry&lt;String, Charset&gt;&gt; set = map.entrySet(); for (Map.Entry&lt;String, Charset&gt; entry : set) { System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue().toString()); } }} NIO多线程聊天室例子 服务端 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.*;import java.nio.charset.Charset;import java.util.Iterator;import java.util.Set;//服务器端public class ChatServer { //服务器端启动的方法 public void startServer() throws IOException { //1 创建Selector选择器 Selector selector = Selector.open(); //2 创建ServerSocketChannel通道 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //3 为channel通道绑定监听端口 serverSocketChannel.bind(new InetSocketAddress(8000)); //设置非阻塞模式 serverSocketChannel.configureBlocking(false); //4 把channel通道注册到selector选择器上 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); System.out.println(&quot;服务器已经启动成功了&quot;); //5 循环，等待有新链接接入 while (true) for (; ; ) { //获取channel数量 int readChannels = selector.select(); if (readChannels == 0) { continue; } //获取可用的channel Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); //遍历集合 Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) { SelectionKey selectionKey = iterator.next(); //移除set集合当前selectionKey iterator.remove(); //6 根据就绪状态，调用对应方法实现具体业务操作 //6.1 如果accept状态 if (selectionKey.isAcceptable()) { acceptOperator(serverSocketChannel, selector); } //6.2 如果可读状态 if (selectionKey.isReadable()) { readOperator(selector, selectionKey); } } } } //处理可读状态操作 private void readOperator(Selector selector, SelectionKey selectionKey) throws IOException { //1 从SelectionKey获取到已经就绪的通道 SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); //2 创建buffer ByteBuffer byteBuffer = ByteBuffer.allocate(1024); //3 循环读取客户端消息 int readLength = socketChannel.read(byteBuffer); String message = &quot;&quot;; if (readLength &gt; 0) { //切换读模式 byteBuffer.flip(); //读取内容 message += Charset.forName(&quot;UTF-8&quot;).decode(byteBuffer); } //4 将channel再次注册到选择器上，监听可读状态 socketChannel.register(selector, SelectionKey.OP_READ); //5 把客户端发送消息，广播到其他客户端 if (message.length() &gt; 0) { //广播给其他客户端 System.out.println(message); castOtherClient(message, selector, socketChannel); } } //广播到其他客户端 private void castOtherClient(String message, Selector selector, SocketChannel socketChannel) throws IOException { //1 获取所有已经接入channel Set&lt;SelectionKey&gt; selectionKeySet = selector.keys(); //2 循环想所有channel广播消息 for (SelectionKey selectionKey : selectionKeySet) { //获取每个channel Channel tarChannel = selectionKey.channel(); //不需要给自己发送 if (tarChannel instanceof SocketChannel &amp;&amp; tarChannel != socketChannel) { ((SocketChannel) tarChannel).write(Charset.forName(&quot;UTF-8&quot;).encode(message)); } } } //处理接入状态操作 private void acceptOperator(ServerSocketChannel serverSocketChannel, Selector selector) throws IOException { //1 接入状态，创建socketChannel SocketChannel socketChannel = serverSocketChannel.accept(); //2 把socketChannel设置非阻塞模式 socketChannel.configureBlocking(false); //3 把channel注册到selector选择器上，监听可读状态 socketChannel.register(selector, SelectionKey.OP_READ); //4 客户端回复信息 socketChannel.write(Charset.forName(&quot;UTF-8&quot;).encode(&quot;欢迎进入聊天室，请注意隐私安全&quot;)); } //启动主方法 public static void main(String[] args) { try { new ChatServer().startServer(); } catch (IOException e) { e.printStackTrace(); } }} 客户端代码 123456789101112131415161718192021222324252627282930import java.io.IOException;import java.net.InetSocketAddress;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.util.Scanner;//客户端public class ChatClient { //启动客户端方法 public void startClient(String name) throws IOException { //连接服务端 SocketChannel socketChannel = SocketChannel.open(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000)); //接收服务端响应数据 Selector selector = Selector.open(); socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); //创建线程 new Thread(new ClientThread(selector)).start(); //向服务器端发送消息 Scanner scanner = new Scanner(System.in); while (scanner.hasNextLine()) { String msg = scanner.nextLine(); if (msg.length() &gt; 0) { socketChannel.write(Charset.forName(&quot;UTF-8&quot;).encode(name + &quot; : &quot; + msg)); } } }} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.nio.charset.Charset;import java.util.Iterator;import java.util.Set;public class ClientThread implements Runnable { private Selector selector; public ClientThread(Selector selector) { this.selector = selector; } @Override public void run() { try { for (; ; ) { //获取channel数量 int readChannels = selector.select(); if (readChannels == 0) { continue; } //获取可用的channel Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys(); //遍历集合 Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator(); while (iterator.hasNext()) { SelectionKey selectionKey = iterator.next(); //移除set集合当前selectionKey iterator.remove(); //如果可读状态 if (selectionKey.isReadable()) { readOperator(selector, selectionKey); } } } } catch (Exception e) { } } //处理可读状态操作 private void readOperator(Selector selector, SelectionKey selectionKey) throws IOException { //1 从SelectionKey获取到已经就绪的通道 SocketChannel socketChannel = (SocketChannel) selectionKey.channel(); //2 创建buffer ByteBuffer byteBuffer = ByteBuffer.allocate(1024); //3 循环读取客户端消息 int readLength = socketChannel.read(byteBuffer); String message = &quot;&quot;; if (readLength &gt; 0) { //切换读模式 byteBuffer.flip(); //读取内容 message += Charset.forName(&quot;UTF-8&quot;).decode(byteBuffer); } //4 将channel再次注册到选择器上，监听可读状态 socketChannel.register(selector, SelectionKey.OP_READ); //5 把客户端发送消息，广播到其他客户端 if (message.length() &gt; 0) { //广播给其他客户端 System.out.println(message); } }} 12345678import java.io.IOException;public class AClient { public static void main(String[] args) throws IOException { new ChatClient().startClient(&quot;lxl&quot;); }} 12345678import java.io.IOException;public class BClient { public static void main(String[] args) throws IOException { new ChatClient().startClient(&quot;xqm&quot;); }} others Filter模式 通过一个“基础”组件再叠加各种“附加”功能组件的模式，称之为Filter模式（或者装饰器模式：Decorator）。它可以让我们通过少量的类来实现各种功能的组合 编写FilterInputStream 操作Zip ZipInputStream JarInputStream是从ZipInputStream派生 读取zip包 创建一个ZipInputStream，通常是传入一个FileInputStream作为数据源，然后，循环调用getNextEntry()，直到返回null，表示zip流结束。 一个ZipEntry表示一个压缩文件或目录，如果是压缩文件，我们就用read()方法不断读取，直到返回-1： try (ZipInputStream zip = new ZipInputStream(new FileInputStream(...))) { ZipEntry entry = null; while ((entry = zip.getNextEntry()) != null) { String name = entry.getName(); if (!entry.isDirectory()) { int n; while ((n = zip.read()) != -1) { ... } } } } 写入zip包 可以直接写入内容到zip包。我们要先创建一个ZipOutputStream，通常是包装一个FileOutputStream，然后，每写入一个文件前，先调用putNextEntry()，然后用write()写入byte[]数据，写入完毕后调用closeEntry()结束这个文件的打包。 try (ZipOutputStream zip = new ZipOutputStream(new FileOutputStream(...))) { File[] files = ... for (File file : files) { zip.putNextEntry(new ZipEntry(file.getName())); zip.write(getFileDataAsBytes(file)); zip.closeEntry(); } } 读取classpath资源 把资源存储在classpath中可以避免文件路径依赖； Class对象的getResourceAsStream()可以从classpath中读取指定资源； 根据classpath读取资源时，需要检查返回的InputStream是否为null。 使用Files byte[] data = Files.readAllBytes(Paths.get(&quot;/path/to/file.txt&quot;)); // 默认使用UTF-8编码读取: String content1 = Files.readString(Paths.get(&quot;/path/to/file.txt&quot;)); // 可指定编码: String content2 = Files.readString(Paths.get(&quot;/path/to/file.txt&quot;), StandardCharsets.ISO_8859_1); // 按行读取并返回每行内容: List lines = Files.readAllLines(Paths.get(&quot;/path/to/file.txt&quot;)); / 写入二进制文件: byte[] data = ... Files.write(Paths.get(&quot;/path/to/file.txt&quot;), data); // 写入文本并指定编码: Files.writeString(Paths.get(&quot;/path/to/file.txt&quot;), &quot;文本内容...&quot;, StandardCharsets.ISO_8859_1); // 按行写入文本: List lines = ... Files.write(Paths.get(&quot;/path/to/file.txt&quot;), lines); 此外，Files工具类还有copy()、delete()、exists()、move()等快捷方法操作文件和目录。 最后需要特别注意的是，Files提供的读写方法，受内存限制，只能读写小文件，例如配置文件等，不可一次读入几个G的大文件。读写大型文件仍然要使用文件流，每次只读写一部分文件内容。 内存映射文件 内存映射文件 I/O 是一种读和写文件数据的方法，它可以比常规的基于流或者基于通道的 I/O 快得多。 向内存映射文件写入可能是危险的，只是改变数组的单个元素这样的简单操作，就可能会直接修改磁盘上的文件。修改数据与将数据保存到磁盘是没有分开的。 下面代码行将文件的前 1024 个字节映射到内存中，map() 方法返回一个 MappedByteBuffer，它是 ByteBuffer 的子类。因此，可以像使用其他任何 ByteBuffer 一样使用新映射的缓冲区，操作系统会在需要时负责执行映射。 MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, 0, 1024); OFFICE PIC CSV HTML 压缩文件 IO NIO 面向流 面向缓冲 阻塞IO 非阻塞IO 无 选择器 NET OSI七层模型 TCP/IP五层模型 IP 域名 端口 通信协议 TCP 三次握手 四次挥手 提高CPU，防止服务器降配 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.atguigu.java2;import java.io.IOException;class CPUTest { public static void main(String[] args) { CPUTestThread cpuTestThread = new CPUTestThread(); //修改for次数 for (int i = 0; i &lt; 20; i++) { Thread cpuTest = new Thread(cpuTestThread); cpuTest.start(); }//Windows Task Manager shows try { Runtime.getRuntime().exec(&quot;taskmgr&quot;); } catch (IOException e1) { e1.printStackTrace(); } }}class CPUTestThread implements Runnable { @Override public void run() { int busyTime = 10; int idleTime = busyTime; long startTime = 0; while (true) { startTime = System.currentTimeMillis(); System.out.println(System.currentTimeMillis()+&quot;,&quot;+startTime+&quot;,&quot;+(System.currentTimeMillis() - startTime));// busy loop while ((System.currentTimeMillis() - startTime) &lt;= busyTime) ;// idle loop try { Thread.sleep(idleTime); } catch (InterruptedException e) { System.out.println(e); } } }} 单元测试 JUnit //非常简单地组织测试代码，并随时运行它们，JUnit就会给出成功的测试和失败的测试， // 还可以生成测试报告，不仅包含测试的成功率，还可以统计测试的代码覆盖率，即被测试的代码本身有多少经过了测试。 // 对于高质量的代码来说，测试覆盖率应该在80%以上。 //此外，几乎所有的IDE工具都集成了JUnit，这样我们就可以直接在IDE中编写并运行JUnit测试。 //一是单元测试代码本身必须非常简单，能一下看明白，决不能再为测试代码编写测试； //二是每个单元测试应当互相独立，不依赖运行的顺序； //三是测试时不但要覆盖常用测试用例，还要特别注意测试边界条件，例如输入为0，null，空字符串&quot;&quot;等情况。 //测试可回滚 断言 import static org.junit.Assert.*; JUnit4 与 JUnit5 对比 1.SpringBoot 引入单元测试 The starter also brings the vintage engine so that you can run both JUnit 4 and JUnit 5 tests. If you have migrated your tests to JUnit 5, you should exclude JUnit 4 support, as shown in the following example: 在Spring Boot 2.2.X以后使用import org.junit.jupiter.api.Test Junit5 在Spring Boot 2.2.x之前使用import org.junit.Test Junit4 1234567891011121314151617181920212223&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;- JUnit — The de-facto standard for unit testing Java applications.- Spring Test &amp; Spring Boot Test — Utilities and integration test support for Spring Boot applications.- AssertJ — A fluent assertion library.- Hamcrest — A library of matcher objects (also known as constraints or predicates).- Mockito — A Java mocking framework.- JSONassert — An assertion library for JSON.- JsonPath — XPath for JSON. Spring Boot 2.2 之后的 pom.xml IDEA Ctrl+Shift+T (Window) 创建测试类 2.注解区别 @RunWith(SpringRunner.class) + @SpringBootTest =====》 @SpringBootTest 123456//JUnit4@RunWith(SpringRunner.class)@SpringBootTest//JUnit5//@RunWith被@ExtendWith替换。同时有下@SpringBootTest源码得知@SpringBootTest包含@ExtendWith@SpringBootTest 1234567@Target({ElementType.TYPE})@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@BootstrapWith(SpringBootTestContextBootstrapper.class)@ExtendWith({SpringExtension.class})public @interface SpringBootTest { 总结 Before BeforeEach After AfterEach BeforeClass BeforeAll AfterClass AfterAll Category Tag RunWith ExtendWith Rule ExtendWith ClassRule RegisterExtension 套件测试 常用注解 @BeforeClass：针对所有测试，只执行一次，且必须为static void @Before：初始化方法，执行当前测试类的每个测试方法前执行。 @Test：测试方法，在这里可以测试期望异常和超时时间 @After：释放资源，执行当前测试类的每个测试方法后执行 @AfterClass：针对所有测试，只执行一次，且必须为static void @Ignore：忽略的测试方法（只在测试类的时候生效，单独执行该测试方法无效） @RunWith:可以更改测试运行器 ，缺省值 org.junit.runner.Runner 一个单元测试类执行顺序为： @BeforeClass –&gt; @Before –&gt; @Test –&gt; @After –&gt; @AfterClass 每一个测试方法的调用顺序为： @Before –&gt; @Test –&gt; @After 常用测试类别 其他基础 [JVM与上层技术](JVM 与上层技术) hashcode与 equals与 == 相关概念 1.equals(Object obj)方法用来判断两个对象是否“相同”，如果“相同”则返回true，否则返回false。 hashCode()方法返回一个int数，在Object类中的默认实现是“将该对象的内部地址转换成一个整数返回”。 2.若两个对象equals(Object obj)返回true，则hashCode（）有必要也返回相同的int数。 3.若两个对象equals(Object obj)返回false，则hashCode（）不一定返回不同的int数。 4.若两个对象hashCode（）返回相同int数，则equals（Object obj）不一定返回true。 5.若两个对象hashCode（）返回不同int数，则equals（Object obj）一定返回false。 6.同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。 7.若重写了equals(Object obj)方法，则有必要重写hashCode()方法。 什么时候需要重写hashcode与equals？ 对象比较 equals与==区别？ 基本类型中，==判断值是否相等，引用类型中比较的是地址是否相等 equals只能用于引用类型，equals具体判断要在判断对象中看equals实现,默认是判断地址，重写之后看具体实现，一般是比较对象内容。 面向对象 封装继承多态 封装 ​ 将一系列操作封装成一个功能，隐藏实现细节，施加权限控制，让用户不再操心功能实现，减少代码冗余。 继承（extends） ​ 消除重复创建相同功能类，继承以复制基类功能属性并覆盖已进行扩展修改。 ​ 继承初始化链： 12345678/**存在继承的情况下，初始化顺序为： 父类（静态变量、静态语句块） 子类（静态变量、静态语句块） 父类（实例变量、普通语句块） 父类（构造函数） 子类（实例变量、普通语句块） 子类（构造函数）*/ 静态--&gt;实例--》普通语句块--》构造 多态 接口和实现类，相同接口有不同实现类以实现不同样式的相同功能。 final、finally、finalize ​ final通常含义为不可改变，无法改变由设计和效率两个方面。修饰类不可被继承（类中方法被隐式生命为final），修饰方法不可被覆盖（效果同被private修饰的方法，也被隐式final了），修饰变量不可被修改（基本变量修饰后即为常量），修饰对象只是限制了其引用地址但不会限制其对象值。finally是在try..catch语句中try执行后且未被将程序终止最后必须执行的语句块、finalize用于对象回收，属于Object的方法，调用并不一定会马上回收对象。 向上/下转型 向上即父用子，向下即子用父。 123456Father f1 = new Son(); // 这就叫 upcasting （向上转型)// 现在 f1 引用指向一个Son对象//并不是所有的对象都可以向下转型，只有当这个对象原本就是子类对象通过向上转型得到的时候才能够成功转型。Son s1 = (Son)f1; // 这就叫 downcasting (向下转型)// 现在f1 还是指向 Son对象 内部类、匿名内部类 没有名字的类或接口实现所以只能使用一次，无法被其他类实例化，用来简化代码编写。 匿名类可继承、重写父类的方法 使用匿名类时，必然是在某个类中直接用匿名类创建对象，因此匿名类一定是内部类 匿名类可以访问外嵌类中的成员变量和方法，匿名类的类体中不可以声明static成员变量和static方法 由于匿名类是一个子类，但没有类名，所以在用匿名类创建对象时，要直接使用父类的构造方法 static ​ 方便在没有创建对象的情况下来进行调用（方法/变量），静态方法中不能依赖非静态方法、属性，反之非静态成员方法中可访问静态成员变量、方法。 静态变量在内存中只有一个，非静态变量随对象创建而创建，各个变量之间互不影响。static不能用来修饰局部变量。静态代码块只有在类加载的时候创建一次，适合于只需要进行一次初始化的操作。静只静，非可静 CAP、BASE理论 AQS JDK新版本特征（lambda ） Lambda 表达式，简洁优雅就是生产力！ (baidu.com) 其他 局部变量需要显示初始化 保留字： 1）48个关键字：abstract、assert、boolean、break、byte、case、catch、char、class、continue、default、do、double、else、enum、extends、final、finally、float、for、if、implements、import、int、interface、instanceof、long、native、new、package、private、protected、public、return、short、static、strictfp、super、switch、synchronized、this、throw、throws、transient、try、void、volatile、while。 2）2个保留字（现在没用以后可能用到作为关键字）：goto、const。 3）3个特殊直接量：true、false、null。 基础类库（从上到下简化了解） 【java.io java.lang java.util】 【java.lang.reflect java.net javax.net.* java.nio.* java.util.concurrent.】 【java.lang.annotation javax.annotation.* java.lang.refjava.math java.rmi.*javax.rmi. java.security. javax.security. java.sqljavax.sql. javax.transaction. *java.text javax.xml.org.w3c.dom.org.xml.sax. javax.crypto. * javax.imageio. javax.jws. * java.util.jar java.util.logging java.util.prefs java.util.regex java.util.zip】 访问权限 private default protected public 同一个类中 √ √ √ √ 同一个包中 √ √ √ 子类中 √ √ 全局范围内 √ 基本数据类型 参考资源： 1.https://blog.csdn.net/xushiyu1996818/article/details/91983557","link":"/2021/02/24/Draft/2021/Java%E5%9F%BA%E7%A1%80%E6%B7%B1%E5%85%A5/"}],"tags":[{"name":"BUG","slug":"BUG","link":"/tags/BUG/"},{"name":"CRUD","slug":"CRUD","link":"/tags/CRUD/"},{"name":"工具教程","slug":"工具教程","link":"/tags/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/"},{"name":"Interesting Programs","slug":"Interesting-Programs","link":"/tags/Interesting-Programs/"},{"name":"Frame","slug":"Frame","link":"/tags/Frame/"},{"name":"NO GAME NO LIFE","slug":"NO-GAME-NO-LIFE","link":"/tags/NO-GAME-NO-LIFE/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"Software Engineer","slug":"Software-Engineer","link":"/tags/Software-Engineer/"},{"name":"待办","slug":"待办","link":"/tags/%E5%BE%85%E5%8A%9E/"},{"name":"音乐","slug":"音乐","link":"/tags/%E9%9F%B3%E4%B9%90/"},{"name":"模板","slug":"模板","link":"/tags/%E6%A8%A1%E6%9D%BF/"},{"name":"书影音","slug":"书影音","link":"/tags/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"name":"健身外型","slug":"健身外型","link":"/tags/%E5%81%A5%E8%BA%AB%E5%A4%96%E5%9E%8B/"},{"name":"学习方法","slug":"学习方法","link":"/tags/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"电脑知识","slug":"电脑知识","link":"/tags/%E7%94%B5%E8%84%91%E7%9F%A5%E8%AF%86/"},{"name":"每日算法","slug":"每日算法","link":"/tags/%E6%AF%8F%E6%97%A5%E7%AE%97%E6%B3%95/"},{"name":"English","slug":"English","link":"/tags/English/"},{"name":"每日面题","slug":"每日面题","link":"/tags/%E6%AF%8F%E6%97%A5%E9%9D%A2%E9%A2%98/"},{"name":"自媒体计划","slug":"自媒体计划","link":"/tags/%E8%87%AA%E5%AA%92%E4%BD%93%E8%AE%A1%E5%88%92/"},{"name":"临时记录","slug":"临时记录","link":"/tags/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/"},{"name":"信息系统项目管理师论文模板","slug":"信息系统项目管理师论文模板","link":"/tags/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/"},{"name":"Dubbo","slug":"Dubbo","link":"/tags/Dubbo/"},{"name":"文件上传下载","slug":"文件上传下载","link":"/tags/%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B8%8B%E8%BD%BD/"},{"name":"SpringCloud","slug":"SpringCloud","link":"/tags/SpringCloud/"},{"name":"信息系统项目管理师","slug":"信息系统项目管理师","link":"/tags/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88/"},{"name":"微信小程序","slug":"微信小程序","link":"/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"快查","slug":"快查","link":"/tags/%E5%BF%AB%E6%9F%A5/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"RabbitMQ","slug":"RabbitMQ","link":"/tags/RabbitMQ/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Python学习","slug":"Python学习","link":"/tags/Python%E5%AD%A6%E4%B9%A0/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"底层学习","slug":"底层学习","link":"/tags/%E5%BA%95%E5%B1%82%E5%AD%A6%E4%B9%A0/"},{"name":"编程语言","slug":"编程语言","link":"/tags/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"}],"categories":[{"name":"BUG记录","slug":"BUG记录","link":"/categories/BUG%E8%AE%B0%E5%BD%95/"},{"name":"程序基础","slug":"程序基础","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/"},{"name":"工具教程","slug":"工具教程","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/"},{"name":"其他","slug":"其他","link":"/categories/%E5%85%B6%E4%BB%96/"},{"name":"BUG","slug":"BUG记录/BUG","link":"/categories/BUG%E8%AE%B0%E5%BD%95/BUG/"},{"name":"Frame","slug":"Frame","link":"/categories/Frame/"},{"name":"CRUD","slug":"程序基础/CRUD","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/CRUD/"},{"name":"游戏人生","slug":"游戏人生","link":"/categories/%E6%B8%B8%E6%88%8F%E4%BA%BA%E7%94%9F/"},{"name":"主题工具","slug":"工具教程/主题工具","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E4%B8%BB%E9%A2%98%E5%B7%A5%E5%85%B7/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Office","slug":"工具教程/Office","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/Office/"},{"name":"程序人生","slug":"程序人生","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/"},{"name":"Interesting Programs","slug":"其他/Interesting-Programs","link":"/categories/%E5%85%B6%E4%BB%96/Interesting-Programs/"},{"name":"待办","slug":"待办","link":"/categories/%E5%BE%85%E5%8A%9E/"},{"name":"兴趣爱好","slug":"兴趣爱好","link":"/categories/%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD/"},{"name":"模板","slug":"模板","link":"/categories/%E6%A8%A1%E6%9D%BF/"},{"name":"书影音","slug":"书影音","link":"/categories/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"name":"博客搭建","slug":"工具教程/博客搭建","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"},{"name":"MyBatis-Plus","slug":"Frame/MyBatis-Plus","link":"/categories/Frame/MyBatis-Plus/"},{"name":"健康","slug":"健康","link":"/categories/%E5%81%A5%E5%BA%B7/"},{"name":"学习方法","slug":"学习方法","link":"/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"MyBatis","slug":"Frame/MyBatis","link":"/categories/Frame/MyBatis/"},{"name":"电脑知识","slug":"电脑知识","link":"/categories/%E7%94%B5%E8%84%91%E7%9F%A5%E8%AF%86/"},{"name":"每日任务","slug":"每日任务","link":"/categories/%E6%AF%8F%E6%97%A5%E4%BB%BB%E5%8A%A1/"},{"name":"English","slug":"English","link":"/categories/English/"},{"name":"NO GAME NO LIFE","slug":"游戏人生/NO-GAME-NO-LIFE","link":"/categories/%E6%B8%B8%E6%88%8F%E4%BA%BA%E7%94%9F/NO-GAME-NO-LIFE/"},{"name":"计划","slug":"计划","link":"/categories/%E8%AE%A1%E5%88%92/"},{"name":"Nginx","slug":"中间件/Nginx","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Nginx/"},{"name":"Software Engineer","slug":"程序人生/Software-Engineer","link":"/categories/%E7%A8%8B%E5%BA%8F%E4%BA%BA%E7%94%9F/Software-Engineer/"},{"name":"SpringMVC","slug":"Frame/SpringMVC","link":"/categories/Frame/SpringMVC/"},{"name":"Everyday-TODO","slug":"待办/Everyday-TODO","link":"/categories/%E5%BE%85%E5%8A%9E/Everyday-TODO/"},{"name":"音乐","slug":"兴趣爱好/音乐","link":"/categories/%E5%85%B4%E8%B6%A3%E7%88%B1%E5%A5%BD/%E9%9F%B3%E4%B9%90/"},{"name":"模板","slug":"模板/模板","link":"/categories/%E6%A8%A1%E6%9D%BF/%E6%A8%A1%E6%9D%BF/"},{"name":"书影音","slug":"书影音/书影音","link":"/categories/%E4%B9%A6%E5%BD%B1%E9%9F%B3/%E4%B9%A6%E5%BD%B1%E9%9F%B3/"},{"name":"健身外型","slug":"健康/健身外型","link":"/categories/%E5%81%A5%E5%BA%B7/%E5%81%A5%E8%BA%AB%E5%A4%96%E5%9E%8B/"},{"name":"如何学习一个新知识","slug":"学习方法/如何学习一个新知识","link":"/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9F%A5%E8%AF%86/"},{"name":"开发修电脑","slug":"电脑知识/开发修电脑","link":"/categories/%E7%94%B5%E8%84%91%E7%9F%A5%E8%AF%86/%E5%BC%80%E5%8F%91%E4%BF%AE%E7%94%B5%E8%84%91/"},{"name":"算法基础","slug":"每日任务/算法基础","link":"/categories/%E6%AF%8F%E6%97%A5%E4%BB%BB%E5%8A%A1/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"程序员英语","slug":"English/程序员英语","link":"/categories/English/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%8B%B1%E8%AF%AD/"},{"name":"每日面题","slug":"每日任务/每日面题","link":"/categories/%E6%AF%8F%E6%97%A5%E4%BB%BB%E5%8A%A1/%E6%AF%8F%E6%97%A5%E9%9D%A2%E9%A2%98/"},{"name":"自媒体计划","slug":"计划/自媒体计划","link":"/categories/%E8%AE%A1%E5%88%92/%E8%87%AA%E5%AA%92%E4%BD%93%E8%AE%A1%E5%88%92/"},{"name":"资源篇","slug":"工具教程/资源篇","link":"/categories/%E5%B7%A5%E5%85%B7%E6%95%99%E7%A8%8B/%E8%B5%84%E6%BA%90%E7%AF%87/"},{"name":"临时记录","slug":"临时记录","link":"/categories/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/"},{"name":"考证","slug":"考证","link":"/categories/%E8%80%83%E8%AF%81/"},{"name":"Dubbo","slug":"中间件/Dubbo","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/Dubbo/"},{"name":"JAVA应用","slug":"JAVA应用","link":"/categories/JAVA%E5%BA%94%E7%94%A8/"},{"name":"思维闪光","slug":"临时记录/思维闪光","link":"/categories/%E4%B8%B4%E6%97%B6%E8%AE%B0%E5%BD%95/%E6%80%9D%E7%BB%B4%E9%97%AA%E5%85%89/"},{"name":"SpringCloud","slug":"Frame/SpringCloud","link":"/categories/Frame/SpringCloud/"},{"name":"信息系统项目管理师论文模板","slug":"考证/信息系统项目管理师论文模板","link":"/categories/%E8%80%83%E8%AF%81/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88%E8%AE%BA%E6%96%87%E6%A8%A1%E6%9D%BF/"},{"name":"业务技术","slug":"JAVA应用/业务技术","link":"/categories/JAVA%E5%BA%94%E7%94%A8/%E4%B8%9A%E5%8A%A1%E6%8A%80%E6%9C%AF/"},{"name":"SpringBoot","slug":"Frame/SpringBoot","link":"/categories/Frame/SpringBoot/"},{"name":"信息系统项目管理师","slug":"考证/信息系统项目管理师","link":"/categories/%E8%80%83%E8%AF%81/%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E5%B8%88/"},{"name":"编程","slug":"编程","link":"/categories/%E7%BC%96%E7%A8%8B/"},{"name":"编程工具","slug":"编程工具","link":"/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"},{"name":"编程基础","slug":"编程基础","link":"/categories/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/"},{"name":"微信小程序","slug":"编程/微信小程序","link":"/categories/%E7%BC%96%E7%A8%8B/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"快查","slug":"编程工具/快查","link":"/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/%E5%BF%AB%E6%9F%A5/"},{"name":"计算机网络","slug":"编程基础/计算机网络","link":"/categories/%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"RabbitMQ","slug":"中间件/RabbitMQ","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/RabbitMQ/"},{"name":"实施维护","slug":"实施维护","link":"/categories/%E5%AE%9E%E6%96%BD%E7%BB%B4%E6%8A%A4/"},{"name":"编程语言","slug":"编程语言","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"},{"name":"Spring","slug":"Frame/Spring","link":"/categories/Frame/Spring/"},{"name":"服务器","slug":"实施维护/服务器","link":"/categories/%E5%AE%9E%E6%96%BD%E7%BB%B4%E6%8A%A4/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"Python学习","slug":"编程语言/Python学习","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Python%E5%AD%A6%E4%B9%A0/"},{"name":"前端","slug":"程序基础/前端","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/%E5%89%8D%E7%AB%AF/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"数据库/MongoDB","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/"},{"name":"MYSQL优化","slug":"数据库/MYSQL优化","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MYSQL%E4%BC%98%E5%8C%96/"},{"name":"Redis","slug":"程序基础/Redis","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/Redis/"},{"name":"设计模式","slug":"程序基础/设计模式","link":"/categories/%E7%A8%8B%E5%BA%8F%E5%9F%BA%E7%A1%80/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"},{"name":"GIS","slug":"前端/GIS","link":"/categories/%E5%89%8D%E7%AB%AF/GIS/"},{"name":"底层学习","slug":"底层学习","link":"/categories/%E5%BA%95%E5%B1%82%E5%AD%A6%E4%B9%A0/"},{"name":"JVM 与上层技术","slug":"底层学习/JVM-与上层技术","link":"/categories/%E5%BA%95%E5%B1%82%E5%AD%A6%E4%B9%A0/JVM-%E4%B8%8E%E4%B8%8A%E5%B1%82%E6%8A%80%E6%9C%AF/"},{"name":"Java","slug":"编程语言/Java","link":"/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/Java/"}]}